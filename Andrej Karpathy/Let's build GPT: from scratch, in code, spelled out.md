We build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video.

Links:
- Google colab for the video: https://colab.research.google.com/dri...
- GitHub repo for the video: https://github.com/karpathy/ng-video-...
- Playlist of the whole Zero to Hero series so far:    â€¢ The spelled-out intro to neural networks a...  
- nanoGPT repo: https://github.com/karpathy/nanoGPT
- my website: https://karpathy.ai
- my twitter:   / karpathy  
- our Discord channel:   / discord  

Supplementary links:
- Attention is All You Need paper: https://arxiv.org/abs/1706.03762
- OpenAI GPT-3 paper: https://arxiv.org/abs/2005.14165 
- OpenAI ChatGPT blog post: https://openai.com/blog/chatgpt/
- The GPU I'm training the model on is from Lambda GPU Cloud, I think the best and easiest way to spin up an on-demand GPU instance in the cloud that you can ssh to: https://lambdalabs.com . If you prefer to work in notebooks, I think the easiest path today is Google Colab.

Suggested exercises:
- EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).
- EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)
- EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?
- EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?

```
Chapters:
00:00:00 intro: ChatGPT, Transformers, nanoGPT, Shakespeare
baseline language modeling, code setup
00:07:52 reading and exploring the data
00:09:28 tokenization, train/val split
00:14:27 data loader: batches of chunks of data
00:22:11 simplest baseline: bigram language model, loss, generation
00:34:53 training the bigram model
00:38:00 port our code to a script
Building the "self-attention"
00:42:13 version 1: averaging past context with for loops, the weakest form of aggregation
00:47:11 the trick in self-attention: matrix multiply as weighted aggregation
00:51:54 version 2: using matrix multiply
00:54:42 version 3: adding softmax
00:58:26 minor code cleanup
01:00:18 positional encoding
01:02:00 THE CRUX OF THE VIDEO: version 4: self-attention
01:11:38 note 1: attention as communication
01:12:46 note 2: attention has no notion of space, operates over sets
01:13:40 note 3: there is no communication across batch dimension
01:14:14 note 4: encoder blocks vs. decoder blocks
01:15:39 note 5: attention vs. self-attention vs. cross-attention
01:16:56 note 6: "scaled" self-attention. why divide by sqrt(head_size)
Building the Transformer
01:19:11 inserting a single self-attention block to our network
01:21:59 multi-headed self-attention
01:24:25 feedforward layers of transformer block
01:26:48 residual connections
01:32:51 layernorm (and its relationship to our previous batchnorm)
01:37:49 scaling up the model! creating a few variables. adding dropout
Notes on Transformer
01:42:39 encoder vs. decoder vs. both (?) Transformers
01:46:22 super quick walkthrough of nanoGPT, batched multi-headed self-attention
01:48:53 back to ChatGPT, GPT-3, pretraining vs. finetuning, RLHF
01:54:32 conclusions

Corrections: 
00:57:00 Oops "tokens from the future cannot communicate", not "past". Sorry! :)
01:20:05 Oops I should be using the head_size for the normalization, not C
```

å½“ç„¶å¯ä»¥ï¼Œæˆ‘å¸®ä½ æŠŠè¿™æ®µå†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼Œæ–¹ä¾¿ç†è§£ï¼š

---

æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ª**ç”Ÿæˆå¼é¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆGPTï¼‰**ï¼Œå‚è€ƒã€ŠAttention is All You Needã€‹è®ºæ–‡å’Œ OpenAI çš„ GPT-2 / GPT-3ã€‚æˆ‘ä»¬è¿˜ä¼šè®¨è®ºå®ƒå’Œå¦‚ä»Šå¸­å·å…¨çƒçš„ ChatGPT ä¹‹é—´çš„è”ç³»ã€‚æˆ‘ä»¬è¿˜ä¼šæ¼”ç¤ºå¦‚ä½•ç”¨ GitHub Copilotï¼ˆå®ƒæœ¬èº«å°±æ˜¯ GPTï¼‰æ¥è¾…åŠ©æˆ‘ä»¬ç¼–å†™ GPT ä»£ç ï¼ˆå¾ˆæœ‰â€œå…ƒâ€æ„Ÿ ğŸ˜„ï¼‰ã€‚
æˆ‘å»ºè®®å¤§å®¶å…ˆå»çœ‹ä¹‹å‰çš„ "makemore" ç³»åˆ—è§†é¢‘ï¼Œç†Ÿæ‚‰**è‡ªå›å½’è¯­è¨€å»ºæ¨¡**æ¡†æ¶ã€å¼ é‡ï¼ˆtensorï¼‰çš„åŸºæœ¬æ“ä½œï¼Œä»¥åŠ PyTorch çš„ `nn` æ¨¡å—ï¼Œè¿™äº›åœ¨æœ¬è§†é¢‘ä¸­æ˜¯é»˜è®¤å·²æŒæ¡çš„ã€‚

ğŸ”— ç›¸å…³é“¾æ¥ï¼š

* æœ¬è§†é¢‘ç”¨åˆ°çš„ Google Colab: [https://colab.research.google.com/dri](https://colab.research.google.com/dri)...
* æœ¬è§†é¢‘çš„ GitHub ä»£ç åº“: [https://github.com/karpathy/ng-video-](https://github.com/karpathy/ng-video-)...
* â€œä»é›¶åˆ°é«˜æ‰‹â€ç³»åˆ—æ’­æ”¾åˆ—è¡¨:    â€¢ The spelled-out intro to neural networks a...
* nanoGPT ä»“åº“: [https://github.com/karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)
* æˆ‘çš„ä¸ªäººç½‘ç«™: [https://karpathy.ai](https://karpathy.ai)
* æˆ‘çš„ Twitter:   / karpathy
* æˆ‘ä»¬çš„ Discord é¢‘é“:   / discord

ğŸ“„ å‚è€ƒèµ„æ–™ï¼š

* ã€ŠAttention is All You Needã€‹è®ºæ–‡: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
* OpenAI GPT-3 è®ºæ–‡: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
* OpenAI ChatGPT åšå®¢: [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)
* æœ¬è§†é¢‘è®­ç»ƒæ¨¡å‹ç”¨çš„ GPU æ¥è‡ª Lambda GPU Cloudï¼Œæ˜¯ç›®å‰æˆ‘è§‰å¾—æœ€å¿«æ·å¼€å¯äº‘ç«¯ GPU å®ä¾‹çš„æ–¹å¼ï¼Œå¯ä»¥é€šè¿‡ ssh è¿œç¨‹è¿æ¥: [https://lambdalabs.com](https://lambdalabs.com)
* å¦‚æœå–œæ¬¢ç”¨ notebookï¼ŒGoogle Colab æ˜¯ç›®å‰æœ€ç®€å•çš„å…¥é—¨æ–¹å¼ã€‚

ğŸ’» æ¨èç»ƒä¹ é¢˜ï¼š
- **EX1**ï¼šæŒæ¡ N ç»´å¼ é‡ï¼ŒæŒ‘æˆ˜é¢˜ï¼šæŠŠ `Head` å’Œ `MultiHeadAttention` åˆå¹¶æˆä¸€ä¸ªç±»ï¼Œè®©å¤šä¸ªå¤´å¹¶è¡Œå¤„ç†ï¼ŒæŠŠâ€œheadâ€ä½œä¸ºé¢å¤– batch ç»´åº¦å¤„ç†ï¼ˆç­”æ¡ˆè§ nanoGPTï¼‰
- **EX2**ï¼šç”¨ä½ å–œæ¬¢çš„æ•°æ®é›†è®­ç»ƒ GPTï¼å¯ä»¥è®­ç»ƒ GPT æ¥åšåŠ æ³•ï¼Œä¾‹å¦‚ a+b=cï¼Œå»ºè®®è®©æ¨¡å‹é¢„æµ‹ c çš„æ•°å­—ï¼ŒæŒ‰é€†åºé¢„æµ‹ï¼ˆå› ä¸ºåŠ æ³•é€šå¸¸æ˜¯ä»ä½ä½å¼€å§‹çš„ï¼‰ï¼Œæ•°æ® loader éœ€è¦è°ƒæ•´ï¼Œä¸ç”¨ç”Ÿæˆ train.bin å’Œ val.binï¼Œè¾“å…¥ a+b è¿™éƒ¨åˆ†çš„ loss å¯ä»¥ç”¨ `y=-1` å±è”½ï¼ˆå‚è€ƒ CrossEntropyLoss çš„ ignore\_indexï¼‰ã€‚èƒ½å­¦ä¼šåŠ æ³•å—ï¼Ÿå¦‚æœèƒ½ï¼Œè¿›ä¸€æ­¥æŒ‘æˆ˜ï¼šåšä¸€ä¸ª GPT è®¡ç®—å™¨ï¼Œæ”¯æŒ + - \* /ã€‚è¿™æ˜¯é«˜é˜¶æŒ‘æˆ˜ï¼Œå¯èƒ½éœ€è¦ Chain of Thought æŠ€æœ¯ã€‚
- **EX3**ï¼šæ‰¾ä¸€ä¸ªè¶…å¤§æ•°æ®é›†ï¼Œè®© train å’Œ val loss ä¹‹é—´çœ‹ä¸å‡ºå·®è·ï¼Œå…ˆç”¨è¿™ä¸ªå¤§æ•°æ®é›†é¢„è®­ç»ƒ Transformerï¼Œç„¶åç”¨è¿™ä¸ªæ¨¡å‹åˆå§‹åŒ–ï¼Œfinetune åœ¨ tiny shakespeare æ•°æ®é›†ä¸Šï¼Œçœ‹çœ‹èƒ½ä¸èƒ½é€šè¿‡é¢„è®­ç»ƒè·å¾—æ›´ä½çš„ val lossã€‚
- **EX4**ï¼šè¯» transformer çš„è®ºæ–‡ï¼Œè‡ªå·±å®ç°ä¸€ä¸ªé¢å¤–çš„æ”¹è¿›ï¼Œçœ‹èƒ½å¦æå‡ GPT æ€§èƒ½ã€‚

```
ğŸ“… è§†é¢‘ç« èŠ‚æ—¶é—´è½´ï¼š
00:00:00 ä»‹ç»ï¼šChatGPTã€Transformerã€nanoGPTã€Shakespeareï¼ŒåŸºç¡€è¯­è¨€å»ºæ¨¡ï¼Œä»£ç å‡†å¤‡
00:07:52 è¯»å–å’Œæ¢ç´¢æ•°æ®
00:09:28 åˆ†è¯ï¼Œè®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†
00:14:27 æ•°æ® loaderï¼šæ•°æ®å— batch ç”Ÿæˆ
00:22:11 æœ€ç®€å•çš„ bigram è¯­è¨€æ¨¡å‹ï¼Œlossï¼Œæ–‡æœ¬ç”Ÿæˆ
00:34:53 è®­ç»ƒ bigram æ¨¡å‹
00:38:00 æŠŠä»£ç æ”¹æˆè„šæœ¬
ğŸ”¨ å®ç° self-attention
00:42:13 ç‰ˆæœ¬ 1ï¼šç”¨ for å¾ªç¯åšå¹³å‡èšåˆï¼Œæœ€å¼±å½¢å¼
00:47:11 self-attention æ ¸å¿ƒ trickï¼šçŸ©é˜µä¹˜æ³•åšåŠ æƒèšåˆ
00:51:54 ç‰ˆæœ¬ 2ï¼šç”¨çŸ©é˜µä¹˜æ³•
00:54:42 ç‰ˆæœ¬ 3ï¼šåŠ ä¸Š softmax
00:58:26 ä»£ç æ•´ç†
01:00:18 ä½ç½®ç¼–ç  (positional encoding)
01:02:00 æ ¸å¿ƒéƒ¨åˆ†ï¼šç‰ˆæœ¬ 4ï¼Œå®Œæ•´ self-attention
01:11:38 æ³¨è§£ 1ï¼šattention æ˜¯ä¸€ç§ä¿¡æ¯äº¤æµæœºåˆ¶
01:12:46 æ³¨è§£ 2ï¼šattention æ²¡æœ‰ç©ºé—´æ¦‚å¿µï¼Œå¤„ç†çš„æ˜¯é›†åˆ
01:13:40 æ³¨è§£ 3ï¼šbatch ä¹‹é—´ä¸é€šä¿¡
01:14:14 æ³¨è§£ 4ï¼šencoder block vs decoder block
01:15:39 æ³¨è§£ 5ï¼šattention / self-attention / cross-attention åŒºåˆ«
01:16:56 æ³¨è§£ 6ï¼šâ€œscaledâ€ self-attentionï¼Œä¸ºä»€ä¹ˆè¦é™¤ä»¥ sqrt(head\_size)
ğŸ§± æ­å»º Transformer
01:19:11 ç½‘ç»œä¸­æ’å…¥ self-attention block
01:21:59 å¤šå¤´ self-attention
01:24:25 Transformer block ä¸­çš„å‰é¦ˆå±‚ feedforward
01:26:48 æ®‹å·®è¿æ¥
01:32:51 LayerNorm åŠå…¶å’Œä¹‹å‰ BatchNorm çš„å…³ç³»
01:37:49 æ¨¡å‹æ‰©å±•ï¼å¼•å…¥å˜é‡ï¼ŒåŠ ä¸Š dropout
ğŸ“š Transformer ç»“æ„æ€»ç»“
01:42:39 encoder / decoder / encoder-decoder Transformer
01:46:22 å¿«é€Ÿè¿‡ä¸€é nanoGPTï¼Œå¤šå¤´ self-attention æ‰¹å¤„ç†
01:48:53 å›åˆ° ChatGPTã€GPT-3ã€é¢„è®­ç»ƒ vs å¾®è°ƒã€RLHF
01:54:32 æ€»ç»“

âš ï¸ å‹˜è¯¯ï¼š
00:57:00 é”™è¯´æˆâ€œtokens from the future cannot communicateâ€ï¼Œåº”æ˜¯â€œtokens from the past cannot communicateâ€
01:20:05 åº”è¯¥ç”¨ head_size å½’ä¸€åŒ–ï¼Œä¸æ˜¯ C
```

---

å¦‚æœä½ è¿˜éœ€è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ **æ€»ç»“ä¸€ä¸‹è¿™ä¸ªè§†é¢‘çš„å­¦ä¹ é‡ç‚¹**ï¼Œæˆ–è€…**åˆ—å‡ºä¸€ä¸ªé€‚åˆåˆå­¦è€…çš„å­¦ä¹ è·¯çº¿**ï¼Œè¦ä¸è¦æˆ‘é¡ºä¾¿æ•´ç†ä¸€ä¸‹ï¼Ÿ ğŸš€


# nanogpt-lecture (https://github.com/karpathy/ng-video-lecture)

Code created in the Neural Networks: Zero To Hero video lecture series, specifically on the first lecture on nanoGPT. Publishing here as a Github repo so people can easily hack it, walk through the git log history of it, etc.

NOTE: sadly I did not go too much into model initialization in the video lecture, but it is quite important for good performance. The current code will train and work fine, but its convergence is slower because it starts off in a not great spot in the weight space. Please see nanoGPT model.py for # init all weights comment, and especially how it calls the _init_weights function. Even more sadly, the code in this repo is a bit different in how it names and stores the various modules, so it's not possible to directly copy paste this code here. My current plan is to publish a supplementary video lecture and cover these parts, then I will also push the exact code changes to this repo. For now I'm keeping it as is so it is almost exactly what we actually covered in the video.

License
MIT

å½“ç„¶å¯ä»¥ï¼Œæˆ‘æ¥å¸®ä½ ç¿»è¯‘è§£é‡Šä¸€ä¸‹ï¼š

---

è¿™æ˜¯ **ã€Šç¥ç»ç½‘ç»œï¼šä»é›¶åˆ°é«˜æ‰‹ã€‹è§†é¢‘æ•™ç¨‹ç³»åˆ—**ä¸­ï¼Œå…³äº nanoGPT çš„**ç¬¬ä¸€èŠ‚è¯¾**é‡Œåˆ›å»ºçš„ä»£ç ã€‚
ä¹‹æ‰€ä»¥å‘å¸ƒæˆè¿™ä¸ª GitHub ä»“åº“ï¼Œä¸»è¦æ˜¯æ–¹ä¾¿å¤§å®¶å¯ä»¥ä¿®æ”¹ä»£ç ã€æŸ¥çœ‹ git å†å²è®°å½•ã€å­¦ä¹ ä»£ç æ¼”å˜è¿‡ç¨‹ç­‰ç­‰ã€‚

âš ï¸ **æ³¨æ„**ï¼š
åœ¨è§†é¢‘è¯¾é‡Œï¼Œæˆ‘æ²¡æœ‰è¯¦ç»†è®²è§£æ¨¡å‹åˆå§‹åŒ–éƒ¨åˆ†ï¼Œä½†å…¶å®è¿™æ˜¯**éå¸¸é‡è¦**çš„ï¼Œåˆå§‹åŒ–ä¼šç›´æ¥å½±å“æ¨¡å‹æ€§èƒ½ã€‚
å½“å‰çš„ä»£ç æ˜¯å¯ä»¥æ­£å¸¸è®­ç»ƒã€æ­£å¸¸å·¥ä½œçš„ï¼Œä½†å› ä¸ºæƒé‡åˆå§‹ä½ç½®é€‰å¾—ä¸æ˜¯å¾ˆå¥½ï¼Œ**æ”¶æ•›é€Ÿåº¦ä¼šåæ…¢**ã€‚

å¦‚æœä½ å»çœ‹ nanoGPT ä»“åº“é‡Œçš„ `model.py`ï¼Œé‡Œé¢æœ‰ `# init all weights` ç›¸å…³çš„æ³¨é‡Šï¼Œé‡ç‚¹æ˜¯çœ‹çœ‹ `_init_weights` å‡½æ•°æ˜¯æ€ä¹ˆè°ƒç”¨çš„ã€‚

ä¸è¿‡æœ‰ç‚¹å¯æƒœï¼Œè¿™é‡Œè§†é¢‘è¯¾ç”¨çš„ä»£ç å’Œ nanoGPT æ­£å¼ä»“åº“çš„ä»£ç åœ¨å‘½åå’Œæ¨¡å—å­˜å‚¨æ–¹å¼ä¸Šæœ‰äº›å·®å¼‚ï¼Œ**ä¸èƒ½ç›´æ¥å¤åˆ¶ç²˜è´´** nanoGPT ä»“åº“é‡Œçš„ä»£ç åˆ°è¿™ä¸ª repo æ¥ç”¨ã€‚

æˆ‘è®¡åˆ’ä¹‹åå½•ä¸€ä¸ª**è¡¥å……è§†é¢‘**ï¼Œä¸“é—¨è®²è§£è¿™éƒ¨åˆ†å†…å®¹ï¼Œåˆ°æ—¶å€™ä¹Ÿä¼šæŠŠä¿®æ”¹åçš„ä»£ç  push åˆ°è¿™ä¸ª repoã€‚
ç›®å‰æˆ‘å…ˆä¿æŒè¿™ä¸ª repo å’Œè§†é¢‘é‡Œçš„ä»£ç ç‰ˆæœ¬**å®Œå…¨ä¸€è‡´**ï¼Œä¾¿äºå¤§å®¶å¯¹ç…§è§†é¢‘å­¦ä¹ ã€‚

ğŸ“œ **License è®¸å¯åè®®**ï¼šMITï¼ˆè‡ªç”±å¼€æºï¼Œå¯ä»¥è‡ªç”±ä¿®æ”¹å’Œä½¿ç”¨ï¼‰

---

å¦‚æœä½ è¿˜æƒ³äº†è§£ï¼Œ**nanoGPT å’Œè§†é¢‘æ•™ç¨‹ç‰ˆçš„ GPT æœ‰å“ªäº›ä¸»è¦åŒºåˆ«**ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ æ€»ç»“ä¸€ä¸‹ï½è¦ä¸è¦ï¼Ÿ ğŸŒŸ

# intro: ChatGPT, Transformers, nanoGPT, Shakespeare

hi everyone so by now you have probably heard of chat GPT it has taken the world and AI Community by storm and it is a
system that allows you to interact with an AI and give it text based tasks so
for example we can ask chat GPT to write us a small Hau about how important it is that people understand Ai and then they
can use it to improve the world and make it more prosperous so when we run this AI knowledge brings prosperity for all
to see Embrace its power okay not bad and so you could see that chpt went from left to right and
generated all these words SE sort of sequentially now I asked it already the
exact same prompt a little bit earlier and it generated a slightly different outcome ai's power to grow ignorance
holds us back learn Prosperity weights so uh pretty good in both cases and
slightly different so you can see that chat GPT is a probabilistic system and for any one prompt it can give us
multiple answers sort of uh replying to it now this is just one example of a
problem people have come up with many many examples and there are entire websites that index interactions with
chpt and so many of them are quite humorous explain HTML to me like I'm a dog uh write release notes for chess 2
write a note about Elon Musk buying a Twitter and so on so as an example uh
please write a breaking news article about a leaf falling from a tree uh and a shocking turn of events a
leaf has fallen from a tree in the local park Witnesses report that the leaf which was previously attached to a branch of a tree attached itself and
fell to the ground very dramatic so you can see that this is a pretty remarkable system and it is what we call a language
model uh because it um it models the sequence of words or characters or
tokens more generally and it knows how sort of words follow each other in English language and so from its
perspective what it is doing is it is completing the sequence so I give it the start of a sequence and it completes the
sequence with the outcome and so it's a language model in that sense now I would
like to focus on the under the hood of um under the hood components of what makes CH GPT work so what is the neural
network under the hood that models the sequence of these words and that comes
from this paper called attention is all you need in 2017 a landmark paper a
landmark paper in AI that produced and proposed the Transformer architecture so GPT is uh short for
generally generatively pre-trained Transformer so Transformer is the neuron nut that actually does all the heavy
lifting under the hood it comes from this paper in 2017 now if you read this paper this uh reads like a pretty random
machine translation paper and that's because I think the authors didn't fully anticipate the impact that the Transformer would have on the field and
this architecture that they produced in the context of machine translation in their case actually ended up taking over
uh the rest of AI in the next 5 years after and so this architecture with
minor changes was copy pasted into a huge amount of applications in AI in more recent years and that includes at
the core of chat GPT now we are not going to what I'd like to do now is I'd
like to build out something like chat GPT but uh we're not going to be able to of course reproduce chat GPT this is a
very serious production grade system it is trained on uh a good chunk of
internet and then there's a lot of uh pre-training and fine-tuning stages to it and so it's very complicated what I'd
like to focus on is just to train a Transformer based language model and in
our case it's going to be a character level language model I still think that is uh very educational with respect to
how these systems work so I don't want to train on the chunk of Internet we need a smaller data set in this case I
propose that we work with uh my favorite toy data set it's called tiny Shakespeare and um what it is is
basically it's a concatenation of all of the works of sh Shakespeare in my understanding and so this is all of
Shakespeare in a single file uh this file is about 1 megab and it's just all
of Shakespeare and what we are going to do now is we're going to basically model how these characters uh follow each
other so for example given a chunk of these characters like this uh given some
context of characters in the past the Transformer neural network will look at the characters that I've highlighted and
is going to predict that g is likely to come next in the sequence and it's going to do that because we're going to train
that Transformer on Shakespeare and it's just going to try to produce uh character sequences that look like this
and in that process is going to model all the patterns inside this data so once we've trained the system i' just
like to give you a preview we can generate infinite Shakespeare and of course it's a fake thing that looks kind
of like Shakespeare um apologies for there's some Jank that
I'm not able to resolve in in here but um you can see how this is going
character by character and it's kind of like predicting Shakespeare like language so verily my Lord the sites
have left the again the king coming with my curses with precious pale and then
tranos say something else Etc and this is just coming out of the Transformer in a very similar manner as it would come
out in chat GPT in our case character by character in chat GPT uh it's coming out
on the token by token level and tokens are these sort of like little subword pieces so they're not Word level they're
kind of like word chunk level um and now I've already written
this entire code uh to train these Transformers um and it is in a GitHub
repository that you can find and it's called nanog GPT so nanog GPT is a repository that
you can find in my GitHub and it's a repository for training Transformers um on any given text and what I think is
interesting about it because there's many ways to train Transformers but this is a very simple implementation so it's just two files of 300 lines of code each
one file defines the GPT model the Transformer and one file trains it on some given Text data set and here I'm
showing that if you train it on a open web Text data set which is a fairly large data set of web pages then I
reproduce the the performance of gpt2 so gpt2 is an early version of open
AI GPT uh from 2017 if I recall correctly and I've only so far
reproduced the the smallest 124 million parameter model uh but basically this is just proving that the codebase is
correctly arranged and I'm able to load the uh neural network weights that openi
has released later so you can take a look at the finished code here in N GPT
but what I would like to do in this lecture is I would like to basically uh write this repository from scratch so
we're going to begin with an empty file and we're we're going to define a Transformer piece by piece we're going
to train it on the tiny Shakespeare data set and we'll see how we can then uh generate infinite Shakespeare and of
course this can copy paste to any arbitrary Text data set uh that you like uh but my goal really here is to just
make you understand and appreciate uh how under the hood chat GPT works and um
really all that's required is a Proficiency in Python and uh some basic understanding of um calculus and
statistics and it would help if you also see my previous videos on the same YouTube channel in particular my make more
series where I um Define smaller and simpler neural network language models
uh so multi perceptrons and so on it really introduces the language modeling framework and then uh here in this video
we're going to focus on the Transformer neural network itself okay so I created a new Google collab uh jup notebook here

ä¸‹é¢æŠŠè¿™æ®µå¼€åœºç™½æ‹†è§£æˆå¥½æ‡‚çš„è¦ç‚¹ï¼Œå¹¶è¡¥ä¸Šå¿…è¦èƒŒæ™¯ï¼š

# æ ¸å¿ƒåœ¨è¯´ä»€ä¹ˆ

* ä¾‹å­ä¸»è§’ï¼š**ChatGPT** æ˜¯ä¸€ç§èƒ½æŒ‰æŒ‡ä»¤ç”Ÿæˆæ–‡æœ¬çš„ **è¯­è¨€æ¨¡å‹**ï¼›å®ƒå¯¹åŒä¸€æç¤ºå¯èƒ½ç»™å‡ºä¸åŒç­”æ¡ˆï¼Œå› ä¸º**å¸¦æœ‰éšæœºæ€§ï¼ˆæ¦‚ç‡å¼ç”Ÿæˆï¼‰**ã€‚
* åº•å±‚åŸç†ï¼šChatGPT çš„â€œå‘åŠ¨æœºâ€æ˜¯ **Transformer** æ¶æ„ï¼ˆå‡ºè‡ª 2017 å¹´è®ºæ–‡ *Attention Is All You Need*ï¼‰ï¼Œè€Œ **GPT** æŒ‡çš„æ˜¯â€œ**G**enerative **P**re-trained **T**ransformerâ€ã€‚
* è¿™èŠ‚è¯¾ç›®æ ‡ï¼šæ‰‹æŠŠæ‰‹**è®­ç»ƒä¸€ä¸ªåŸºäº Transformer çš„è¯­è¨€æ¨¡å‹**â€”â€”ä¸æ˜¯å¤åˆ»å·¥ä¸šçº§çš„ ChatGPTï¼Œè€Œæ˜¯ä¸€ä¸ª**æ•™å­¦ç”¨çš„å°æ¨¡å‹**ã€‚
* æ•°æ®é›†ï¼šç”¨ **tiny Shakespeare**ï¼ˆçº¦ 1MBï¼ŒæŠŠèå£«æ¯”äºšæ–‡æœ¬æ‹¼åœ¨ä¸€èµ·ï¼‰ï¼Œåš **å­—ç¬¦çº§ï¼ˆcharacter-levelï¼‰** å»ºæ¨¡ï¼šç»™å®šå‰é¢çš„è‹¥å¹²å­—ç¬¦ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚
* ç”Ÿæˆæ¼”ç¤ºï¼šè®­ç»ƒå¥½åï¼Œæ¨¡å‹å¯ä»¥â€œä¸€ç›´ç»­å†™â€èå£«æ¯”äºšé£æ ¼çš„æ–‡æœ¬ï¼ˆå½“ç„¶åªæ˜¯â€œåƒâ€ï¼Œä¸æ˜¯åŸæ–‡ï¼‰ã€‚
* ä»£ç ä»“åº“ï¼šä½œè€…æœ‰ä¸ªæç®€å®ç° **nanoGPT**ï¼Œç»“æ„å¾ˆç®€å•ï¼š

  * ä¸€ä¸ªæ–‡ä»¶å®šä¹‰ **GPT/Transformer æ¨¡å‹**ï¼›
  * å¦ä¸€ä¸ªæ–‡ä»¶è´Ÿè´£**è®­ç»ƒ**ã€‚
    è¿™ä¸ªç®€åŒ–ç‰ˆåœ¨è¾ƒå¤§çš„å¼€æ”¾ç½‘é¡µæ•°æ®ï¼ˆOpenWebTextï¼‰ä¸Šï¼Œèƒ½**å¤ç° GPT-2ï¼ˆ124M å‚æ•°ï¼‰æœ€å°æ¨¡å‹çº§åˆ«**çš„è¡¨ç°ï¼ˆç”¨ä»¥è¯æ˜å®ç°æ€è·¯æ­£ç¡®ï¼‰ã€‚

  > æ—æ³¨ï¼šTransformer è®ºæ–‡æ˜¯ **2017** å¹´ï¼›**GPT-2** å‘å¸ƒåœ¨ **2019** å¹´ã€‚

# å…³é”®æ¦‚å¿µè¡¥å……

* **è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰**ï¼šå­¦ä¹ â€œåºåˆ—é‡Œä¸‹ä¸€ä¸ªè¯/å­—ç¬¦â€çš„åˆ†å¸ƒï¼›æ¨ç†æ—¶æŒ‰è¿™ä¸ªåˆ†å¸ƒ**é€æ­¥é‡‡æ ·**è¾“å‡ºï¼ˆå› æ­¤ä¼šæœ‰å¤šæ ·æ€§ï¼‰ã€‚
* **Token vs å­—ç¬¦**ï¼šChatGPT æŒ‰ **tokenï¼ˆå­è¯ï¼‰** ç”Ÿæˆï¼›æœ¬è¯¾ç”¨æ›´ç®€å•çš„ **å­—ç¬¦çº§**ï¼Œä½†åŸç†ä¸€è‡´ã€‚
* **Transformer ç»“æ„**ï¼ˆé«˜å±‚ï¼‰ï¼šåµŒå…¥ â†’ ä½ç½®ç¼–ç  â†’ å¤šå¤´è‡ªæ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œï¼ˆå †å å¤šå±‚ï¼‰â†’ çº¿æ€§æŠ•å½± â†’ Softmax é¢„æµ‹ä¸‹ä¸€ä¸ªç¬¦å·ã€‚

# è¿™èŠ‚è¯¾å…·ä½“ä¼šåšä»€ä¹ˆ

1. ä»ç©ºç™½ notebook å¼€å§‹ï¼Œ**é€å—å®ç°ä¸€ä¸ªæœ€å°å¯ç”¨çš„ Transformerï¼ˆGPTï¼‰**ã€‚
2. ç”¨ tiny Shakespeare **è®­ç»ƒä¸ºâ€œä¸‹ä¸€ä¸ªå­—ç¬¦é¢„æµ‹â€** æ¨¡å‹ã€‚
3. è®­ç»ƒå®Œåï¼Œ**ä»èµ·å§‹æç¤ºç”Ÿæˆ**â€œä»¿èå£«æ¯”äºšâ€æ–‡æœ¬ã€‚
4. å¼ºè°ƒï¼šç›¸åŒå¥—è·¯å¯æ¢æˆä»»æ„æ–‡æœ¬æ•°æ®é›†ï¼›nanoGPT ä»“åº“ç»™å‡ºçš„æ˜¯ä¸€ä¸ª**æ›´å®Œæ•´ä¸”ä¾ç„¶ç®€æ´**çš„å‚è€ƒå®ç°ã€‚

# ä¸ ChatGPT çš„å·®åˆ«

* **è§„æ¨¡**ï¼šChatGPT ç”¨çš„æ˜¯æµ·é‡æ•°æ® + è¶…å¤§æ¨¡å‹ + å¤šé˜¶æ®µï¼ˆé¢„è®­ç»ƒã€æŒ‡ä»¤å¯¹é½ã€RLHF ç­‰ï¼‰ï¼›è¯¾å ‚ç¤ºä¾‹æ˜¯**å°æ•°æ® + å°æ¨¡å‹ + å•é˜¶æ®µ**ã€‚
* **ç²’åº¦**ï¼šè¯¾å ‚ç¤ºä¾‹ç”¨**å­—ç¬¦çº§**ï¼Œå·¥ä¸šæ¨¡å‹é€šå¸¸ç”¨**å­è¯çº§ token**ï¼Œæ•ˆç‡å’Œè¡¨è¾¾èƒ½åŠ›æ›´å¼ºã€‚
* **å·¥ç¨‹åŒ–**ï¼šå·¥ä¸šç³»ç»Ÿè¿˜æ¶‰åŠåˆ†å¸ƒå¼è®­ç»ƒã€æ¨ç†ä¼˜åŒ–ã€å®‰å…¨ä¸å¯¹é½ç­–ç•¥ç­‰ï¼Œè¿™é‡Œéƒ½ä¸æ¶‰åŠã€‚

# å­¦ä¹ å‡†å¤‡

* éœ€è¦ä¼š **Python**ï¼Œæ‡‚ç‚¹**å¾®ç§¯åˆ†/æ¦‚ç‡ç»Ÿè®¡**æ›´å¥½ã€‚æœ€å¥½å…ˆçœ‹è¿‡ä½œè€…çš„ **makemore** ç³»åˆ—ï¼ˆé‡Œå¤´ä»æ›´å°çš„ç¥ç»ç½‘ç»œèµ·æ­¥ï¼Œå…ˆè®²æ¸…â€œè¯­è¨€å»ºæ¨¡â€çš„åŸºæœ¬æ¡†æ¶ï¼‰ã€‚

# ä¸€çœ¼çœ‹æ‡‚çš„è®­ç»ƒæµç¨‹ï¼ˆæ–‡å­—ç‰ˆï¼‰

> æ–‡æœ¬ â†’ åˆ‡æˆè®­ç»ƒæ ·æœ¬ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰ â†’ é€å…¥ Transformer â†’ é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ â†’
> è®¡ç®—äº¤å‰ç†µæŸå¤± â†’ åå‘ä¼ æ’­æ›´æ–°å‚æ•° â†’ å¾ªç¯è‹¥å¹²è½® â†’ ç”Ÿæˆæ—¶é€æ­¥é‡‡æ ·è¾“å‡ºã€‚

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥æŠŠâ€œæœ€å°å¯è·‘çš„å­—ç¬¦çº§ Transformer è®­ç»ƒè„šæœ¬â€ç²¾ç®€æˆ \~50 è¡Œçš„æ•™å­¦ç‰ˆç»™ä½ ï¼Œæ–¹ä¾¿ç›´æ¥ä¸Šæ‰‹ã€‚
 çš„å­¦ä¹ è·¯çº¿**ï¼Œè¦ä¸è¦æˆ‘å†è¡¥å……ï¼Ÿ ğŸŒŸ


# reading and exploring the data

and this will allow me to later easily share this code that we're going to develop together uh with you so you can follow along so this will be in a video
description uh later now here I've just done some preliminaries I downloaded the
data set the tiny Shakespeare data set at this URL and you can see that it's about a 1 Megabyte file then here I open
the input.txt file and just read in all the text of the string and we see that we are working with 1 million characters
roughly and the first 1,000 characters if we just print them out are basically what you would expect this is the first
1,000 characters of the tiny Shakespeare data set roughly up to here so so far so
good next we're going to take this text and the text is a sequence of characters in Python so when I call the set
Constructor on it I'm just going to get the set of all the characters that occur
in this text and then I call list on that to create a list of those characters instead of just a set so that
I have an ordering an arbitrary ordering and then I sort that so basically we get
just all the characters that occur in the entire data set and they're sorted now the number of them is going to be
our vocabulary size these are the possible elements of our sequences and we see that when I print here the
characters there's 65 of them in total there's a space character and then all kinds of special characters and then U
capitals and lowercase letters so that's our vocabulary and that's the sort of like possible uh characters that the
model can see or emit okay so next we will would like to develop some strategy

ä¸‹é¢è¿™æ®µè¯æ˜¯åœ¨è®²ã€Œå¦‚ä½•è¯»å–å¹¶ç†è§£ tiny Shakespeare æ•°æ®é›†ï¼Œå¹¶ä¸º**å­—ç¬¦çº§è¯­è¨€æ¨¡å‹**åšå‡†å¤‡ã€â€”â€”è¦ç‚¹å¦‚ä¸‹ï¼š

* å…ˆæŠŠ **tiny Shakespeare** æ–‡æœ¬ï¼ˆçº¦ **1MB**ï¼‰ä¸‹è½½åˆ°æœ¬åœ°ï¼Œç”¨ Python æ‰“å¼€ `input.txt` è¯»æˆä¸€ä¸ªé•¿å­—ç¬¦ä¸²ã€‚é•¿åº¦å¤§çº¦ **ä¸€ç™¾ä¸‡å­—ç¬¦**ï¼›æ‰“å°å‰ **1000 ä¸ªå­—ç¬¦**å°±æ˜¯ä½ çœ‹åˆ°çš„èå£«æ¯”äºšæ–‡æœ¬å¼€å¤´ã€‚
* æ¥ç€ç”¨ `set(text)` æ‰¾å‡º**æ‰€æœ‰å‡ºç°è¿‡çš„å­—ç¬¦**ï¼ˆå»é‡ï¼‰ï¼Œå†è½¬æˆ `list` å¹¶ `sort()` æ’åºï¼Œå¾—åˆ°ä¸€ä¸ª**æœ‰åºçš„å­—ç¬¦è¡¨**ï¼ˆå³**è¯è¡¨/vocabulary**ï¼‰ã€‚
* è¿™ä¸ªå­—ç¬¦è¡¨çš„å¤§å°å°±æ˜¯ **vocab size**ã€‚è¿™é‡Œæ˜¯ **65 ä¸ª**ï¼šåŒ…æ‹¬ç©ºæ ¼ã€æ ‡ç‚¹ã€å¤§å°å†™å­—æ¯ç­‰ã€‚ä¹‹åæ¨¡å‹èƒ½**è¾“å…¥/è¾“å‡º**çš„å°±åªèƒ½æ˜¯è¿™ 65 ç§å­—ç¬¦ã€‚

ä¸ºä»€ä¹ˆè¦è¿™æ ·åšï¼Ÿ

* æˆ‘ä»¬è¦è®­ç»ƒ**å­—ç¬¦çº§**è¯­è¨€æ¨¡å‹ï¼Œæ¨¡å‹çš„è¾“å…¥/è¾“å‡ºå¿…é¡»æ˜¯**ç¦»æ•£çš„ç¼–å·**ã€‚å…ˆåˆ—å‡ºâ€œæ‰€æœ‰å¯èƒ½å­—ç¬¦â€ï¼Œå†å»ºç«‹ **char â‡„ id** çš„åŒå‘æ˜ å°„ï¼Œæ‰èƒ½æŠŠæ–‡æœ¬å˜æˆæ•´æ•°åºåˆ—ä¾›æ¨¡å‹è®­ç»ƒä¸ç”Ÿæˆã€‚

ä½ å¯ä»¥ç”¨ä¸‹é¢è¿™æ®µæç®€ä»£ç å¤ç°ä¸Šé¢æ­¥éª¤ï¼ˆå«ç¼–ç /è§£ç å‡½æ•°ï¼‰ï¼Œç›´æ¥æ›¿æ¢ä½ çš„è·¯å¾„å³å¯ï¼š

```python
# 1) è¯»å–æ•°æ®
with open("input.txt", "r", encoding="utf-8") as f:
    text = f.read()
print("Total chars:", len(text))
print("Preview:", repr(text[:1000]))

# 2) æ„å»ºå­—ç¬¦è¡¨ï¼ˆvocabularyï¼‰
chars = sorted(list(set(text)))
vocab_size = len(chars)
print("Vocab size:", vocab_size)
print("Vocab:", chars)

# 3) å»ºç«‹æ˜ å°„ä¸ç¼–ç /è§£ç 
stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for i, ch in enumerate(chars)}

def encode(s: str):
    return [stoi[c] for c in s]       # å­—ç¬¦ä¸² -> æ•´æ•°åºåˆ—

def decode(ids):
    return ''.join(itos[i] for i in ids)  # æ•´æ•°åºåˆ— -> å­—ç¬¦ä¸²

# å°æµ‹è¯•
ids = encode("To be, or not to be.")
print(ids)
print(decode(ids))
```

å°æç¤º/å‘ä½ï¼š

* **ç©ºæ ¼ä¸æ¢è¡Œ**éƒ½æ˜¯åˆæ³•å­—ç¬¦ï¼Œåˆ«ä¸¢ï¼›å¦åˆ™æ¨¡å‹ä¼šå­¦ä¸åˆ°åˆ†è¯/åˆ†è¡Œçš„ç»“æ„æ„Ÿã€‚
* ç”¨ `sorted(set(...))` å¾—åˆ°çš„æ˜¯**æŒ‰å­—ç¬¦ç¼–ç æ’åº**çš„å›ºå®šé¡ºåºï¼ˆä¸æ˜¯é¢‘ç‡é¡ºåºï¼‰ï¼Œè®­ç»ƒæ²¡é—®é¢˜ï¼Œä½†è¯·**ä¿å­˜æ˜ å°„**ï¼ˆå¦‚å†™å…¥ JSONï¼‰ï¼Œä»¥ä¾¿ä¹‹åè§£ç ä¸€è‡´ã€‚
* ä¹‹åä¸€æ­¥é€šå¸¸ä¼šæŠŠæ•´æ®µæ–‡æœ¬ `encode` æˆ `torch.LongTensor`ï¼Œå†æŒ‰æ—¶é—´æ­¥åˆ‡å—ã€åš train/val åˆ‡åˆ†ï¼Œä¾› Transformer è®­ç»ƒã€‚éœ€è¦çš„è¯æˆ‘å¯ä»¥ç»™å‡ºæœ€å°å¯è·‘çš„æ•°æ®ç®¡é“ç¤ºä¾‹ã€‚


# tokenization, train/val split

to tokenize the input text now when people say tokenize they mean convert
the raw text as a string to some sequence of integers According to some uh notebook According to some vocabulary
of possible elements so as an example here we are going to be building a character level language model so we're
simply going to be translating individual characters into integers so let me show you uh a chunk of code that
sort of does that for us so we're building both the encoder and the decoder and let me just talk through what's
happening here when we encode an arbitrary text like hi there we're going to receive a
list of integers that represents that string so for example 46 47 Etc and then
we also have the reverse mapping so we can take this list and decode it to get
back the exact same string so it's really just like a translation to integers and back for arbitrary string
and for us it is done on a character level now the way this was achieved is we just
iterate over all the characters here and create a lookup table from the character to the integer and vice versa and then
to encode some string we simply translate all the characters individually and to decode it back we
use the reverse mapping and concatenate all of it now this is only one of many possible encodings or many possible sort
of tokenizers and it's a very simple one but there's many other schemas that people have come up with in practice so
for example Google uses a sentence piece uh so sentence piece will also encode text into um integers but in a
different schema and using a different vocabulary and sentence piece is a
subword uh sort of tokenizer and what that means is that um you're not encoding entire words but you're not
also encoding individual characters it's it's a subword unit level and that's
usually what's adopted in practice for example also openai has this Library called tick token that uses a bite pair
encode tokenizer um and that's what GPT uses and you can also just encode words into
like hell world into a list of integers so as an example I'm using the Tik token
Library here I'm getting the encoding for gpt2 or that was used for gpt2
instead of just having 65 possible characters or tokens they have 50,000
tokens and so when they encode the exact same string High there we only get a
list of three integers but those integers are not between 0 and 64 they are between Z and 5,
5,256 so basically you can trade off the code book size and the sequence lengths
so you can have very long sequences of integers with very small vocabularies or we can have short um sequences of
integers with very large vocabularies and so typically people use in practice
these subword encodings but I'd like to keep our token ier very simple so we're using character level tokenizer and that
means that we have very small code books we have very simple encode and decode functions uh but we do get very long
sequences as a result but that's the level at which we're going to stick with this lecture because it's the simplest
thing okay so now that we have an encoder and a decoder effectively a tokenizer we can tokenize the entire
training set of Shakespeare so here's a chunk of code that does that and I'm going to start to use the pytorch
library and specifically the torch. tensor from the pytorch library so we're going to take all of the text in tiny
Shakespeare encode it and then wrap it into a torch. tensor to get the data tensor so here's what the data tensor
looks like when I look at just the first 1,000 characters or the 1,000 elements of it so we see that we have a massive
sequence of integers and this sequence of integers here is basically an identical translation of the first
10,000 characters here so I believe for example that zero is a new line character and maybe one
one is a space not 100% sure but from now on the entire data set of text is re-represented as just it's just
stretched out as a single very large uh sequence of integers let me do one more thing before
we move on here I'd like to separate out our data set into a train and a validation split so in particular we're
going to take the first 90% of the data set and consider that to be the training data for the Transformer and we're going
to withhold the last 10% at the end of it to be the validation data and this will help us understand to what extent
our model is overfitting so we're going to basically hide and keep the validation data on the side because we
don't want just a perfect memorization of this exact Shakespeare we want a neural network that sort of creates
Shakespeare like uh text and so it should be fairly likely for it to produce the actual like stowed away uh
true Shakespeare text um and so we're going to use this to uh get a sense of
the overfitting okay so now we would like to start plugging these text sequences or integer sequences into the

ä¸‹é¢æŠŠè¿™æ®µâ€œ**tokenization ä¸è®­ç»ƒ/éªŒè¯åˆ’åˆ†**â€è®²æ¸…æ¥šï¼Œå¹¶ç»™ä¸€æ®µæœ€å°å¯è·‘çš„ç¤ºä¾‹ä»£ç ã€‚

# åœ¨åšä»€ä¹ˆ

1. **Tokenizationï¼ˆæ ‡è®°åŒ–ï¼‰**
   æŠŠåŸå§‹å­—ç¬¦ä¸²â†’**æ•´æ•°åºåˆ—**ã€‚åœ¨æœ¬è¯¾é‡Œç”¨**å­—ç¬¦çº§**ï¼šæ¯ä¸ªå­—ç¬¦ â†” ä¸€ä¸ªæ•´æ•° idï¼ˆé€šè¿‡ `stoi/itos` æ˜ å°„ï¼‰ã€‚

   * å¥½å¤„ï¼šå®ç°æœ€ç®€å•ã€æ²¡æœ‰ OOVï¼ˆæœªçŸ¥è¯ï¼‰é—®é¢˜ã€‚
   * ä»£ä»·ï¼š**åºåˆ—æ›´é•¿**ï¼ˆç›¸æ¯”å­è¯/è¯çº§ï¼‰ï¼Œè®­ç»ƒç•¥æ…¢ã€‚
   * ç°å®é‡Œå¸¸ç”¨ **å­è¯**ï¼ˆSentencePieceã€BPE/tiktokenï¼‰ï¼š**è¯è¡¨å¤§**ï¼ˆå¦‚ 50kï¼‰ï¼Œ**åºåˆ—çŸ­**ï¼›è€Œå­—ç¬¦çº§ç›¸åï¼š**è¯è¡¨å°**ï¼ˆæœ¬æ•°æ®é›† 65 ä¸ªå­—ç¬¦ï¼‰ï¼Œ**åºåˆ—é•¿**ã€‚

2. **æŠŠæ•´ä»½ Shakespeare æ–‡æœ¬ç¼–ç æˆä¸€ä¸ªè¶…é•¿çš„æ•´æ•°å‘é‡**
   è¿™ç›¸å½“äºå¯¹æ•´æœ¬ä¹¦åšâ€œé€å­—ç¬¦ç¼–å·â€ï¼š`data = [46, 47, 0, 1, ...]`ï¼ˆæ•°å­—å…·ä½“å¯¹åº”å“ªä¸ªå­—ç¬¦å–å†³äºä½ çš„ `chars` æ’åºï¼‰ã€‚

3. **è®­ç»ƒ/éªŒè¯åˆ’åˆ†ï¼ˆtrain/val splitï¼‰**
   å–**å‰ 90%** ä½œä¸ºè®­ç»ƒï¼Œ**å 10%** ä½œä¸ºéªŒè¯ã€‚éªŒè¯é›†**ä¸å‚ä¸è®­ç»ƒ**ï¼Œåªç”¨æ¥è¯„ä¼°æ˜¯å¦**è¿‡æ‹Ÿåˆ**ã€‚

   * è¯­è¨€å»ºæ¨¡é€šå¸¸ä¿ç•™**é¡ºåº**ï¼ˆç”¨**è¿ç»­åˆ‡å‰²**è€Œéæ‰“ä¹±ï¼‰ï¼Œä»¥ä¾¿éªŒè¯ä¸è®­ç»ƒåˆ†å¸ƒæ¥è¿‘ã€åˆä¸æ³„éœ²å†…å®¹ã€‚

---

# æç®€ç¤ºä¾‹ä»£ç ï¼ˆå­—ç¬¦çº§ tokenizer + PyTorch å¼ é‡ + 90/10 åˆ’åˆ†ï¼‰

```python
# 0) è¯»å–æ–‡æœ¬
with open("input.txt", "r", encoding="utf-8") as f:
    text = f.read()
print("Total chars:", len(text))
print("Preview:", repr(text[:200]))

# 1) æ„å»ºå­—ç¬¦çº§è¯è¡¨ä¸æ˜ å°„
chars = sorted(list(set(text)))     # æœ‰åºå­—ç¬¦è¡¨ï¼ˆå¦‚åŒ…å«ç©ºæ ¼ã€æ¢è¡Œã€æ ‡ç‚¹ã€å¤§å°å†™å­—æ¯ç­‰ï¼‰
vocab_size = len(chars)
print("Vocab size:", vocab_size)
print("Vocab sample:", chars[:20])

stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for i, ch in enumerate(chars)}

def encode(s: str):
    return [stoi[c] for c in s]          # str -> List[int]

def decode(ids):
    return "".join(itos[i] for i in ids) # List[int] -> str

# 2) æ•´ä½“ç¼–ç ä¸ºä¸€ä¸ªé•¿å‘é‡ï¼Œå¹¶è½¬æˆ PyTorch å¼ é‡
import torch
data = torch.tensor(encode(text), dtype=torch.long)  # è¯­è¨€æ¨¡å‹éœ€è¦ long (int64)
print("Data shape:", data.shape)                     # ä¾‹å¦‚: torch.Size([~1_000_000])

# 3) 90/10 è®­ç»ƒ/éªŒè¯åˆ’åˆ†ï¼ˆè¿ç»­åˆ‡ç‰‡ï¼‰
n = int(0.9 * len(data))
train_data = data[:n]
val_data   = data[n:]

print("Train size:", train_data.shape, "Val size:", val_data.shape)

# 4) å°æµ‹è¯•
example = "Hi there"
ids = encode(example)
print("Encoded:", ids)
print("Decoded:", decode(ids))
```

---

# å®æˆ˜è¦ç‚¹/å°å‘

* **ä¿ç•™ç©ºæ ¼ä¸æ¢è¡Œ**ï¼šéƒ½æ˜¯åˆæ³•å­—ç¬¦ï¼Œåˆ«ä¸¢ï¼›ä¸¢äº†æ¨¡å‹ä¼šå­¦ä¸åˆ°è¡Œ/æ®µç»“æ„ã€‚
* **dtype** ç”¨ `torch.long`ï¼ˆäº¤å‰ç†µç­‰æŸå¤±å‡½æ•°è¦æ±‚ç±»åˆ«ç´¢å¼•æ˜¯ longï¼‰ã€‚
* **æ˜ å°„è¦å›ºå®š**ï¼š`sorted(set(text))` çš„é¡ºåºè¦ä¿ç•™ï¼ˆå»ºè®®æŠŠ `chars` å­˜åˆ° JSONï¼‰ï¼Œä¿è¯è®­ç»ƒä¸æ¨ç†ä¸€è‡´ã€‚
* **è¿ç»­åˆ‡åˆ†**æ›´è´´è¿‘è¯­è¨€å»ºæ¨¡çš„æ—¶åºåˆ†å¸ƒï¼›å¦‚æœä½ è·¨æ–‡æ¡£è®­ç»ƒï¼Œä¹Ÿå¯åšåˆ†æ–‡æ¡£åˆ‡åˆ†ä»¥é¿å…æ³„éœ²ã€‚
* ä¹‹åå–‚æ¨¡å‹æ—¶ï¼Œè¿˜éœ€è¦ä» `train_data`/`val_data` **åˆ‡çª—å£ï¼ˆblock\_sizeï¼‰** å– batchï¼ˆå¦‚é•¿åº¦ 256 çš„ä¸Šä¸‹æ–‡ï¼‰ï¼›è¿™ä¸€æ­¥å°±æ˜¯ä¸‹ä¸€ä¸ªç¯èŠ‚çš„æ•°æ®åŠ è½½å™¨å®ç°ã€‚

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥æ¥ç€ç»™å‡ºä¸€ä¸ª**æœ€å°çš„æ•°æ®åŠ è½½å™¨ï¼ˆget\_batchï¼‰**ï¼Œç›´æ¥äº§å‡º `(x, y)` è®­ç»ƒå¯¹ï¼Œå’Œä¸€ä¸ªæœ€å°çš„ **nn.Transformer/GPT** éª¨æ¶ã€‚


# data loader: batches of chunks of data

Transformer so that it can train and learn those patterns now the important thing to realize is we're never going to
actually feed entire text into a Transformer all at once that would be computationally very expensive and
prohibitive so when we actually train a Transformer on a lot of these data sets we only work with chunks of the data set
and when we train the Transformer we basically sample random little chunks out of the training set and train on
just chunks at a time and these chunks have basically some kind of a length and
some maximum length now the maximum length typically at least in the code I usually write is called block size you
can you can uh find it under different names like context length or something like that let's start with the block
size of just eight and let me look at the first train data characters the first block size plus one characters
I'll explain why plus one in a second so this is the first nine characters in the sequence in the
training set now what I'd like to point out is that when you sample a chunk of data like this so say the these nine
characters out of the training set this actually has multiple examples packed into it and uh that's because all of
these characters follow each other and so what this thing is going to say when
we plug it into a Transformer is we're going to actually simultaneously train it to make prediction at every one of
these positions now in the in a chunk of nine characters there's actually eight indiv
ual examples packed in there so there's the example that when 18 when in the
context of 18 47 likely comes next in a context of 18 and 47 56 comes next in a
context of 18 47 56 57 can come next and so on so that's the eight individual
examples let me actually spell it out with code so here's a chunk of code to
illustrate X are the inputs to the Transformer it will just be the first block size characters y will be the uh
next block size characters so it's offset by one and that's because y are the targets for each position in the
input and then here I'm iterating over all the block size of eight and the context is always all the characters in
x uh up to T and including T and the target is always the teth character but
in the targets array y so let me just run this and basically it spells out what I
said in words uh these are the eight examples hidden in a chunk of nine characters that we uh sampled from the
training set I want to mention one more thing we train on all the eight examples
here with context between one all the way up to context of block size and we train on that not just for computational
reasons because we happen to have the sequence already or something like that it's not just done for efficiency it's
also done um to make the Transformer Network be used to seeing contexts all
the way from as little as one all the way to block size and we'd like the transform to be used to seeing
everything in between and that's going to be useful later during inference because while we're sampling we can
start the sampling generation with as little as one character of context and the Transformer knows how to predict the
next character with all the way up to just context of one and so then it can predict everything up to block size and
after block size we have to start truncating because the Transformer will will never um receive more than block
size inputs when it's predicting the next character Okay so we've looked at the time dimension of the tensors that are
going to be feeding into the Transformer there's one more Dimension to care about and that is the batch Dimension and so
as we're sampling these chunks of text we're going to be actually every time we're going to feed them into a
Transformer we're going to have many batches of multiple chunks of text that are all like stacked up in a single
tensor and that's just done for efficiency just so that we can keep the gpus busy uh because they are very good
at parallel processing of um of data and so we just want to process multiple
chunks all at the same time but those chunks are processed completely independently they don't talk to each other and so on so let me basically just
generalize this and introduce a batch Dimension here's a chunk of code let me just run it and then I'm
going to explain what it does so here because we're going to start sampling random locations in the
data set to pull chunks from I am setting the seed so that um in the
random number generator so that the numbers I see here are going to be the same numbers you see later if you try to reproduce this now the batch size here
is how many independent sequences we are processing every forward backward pass of the
Transformer the block size as I explained is the maximum context length to make those predictions so let's say B
size four block size eight and then here's how we get batch for any arbitrary split if the split is a
training split then we're going to look at train data otherwise at valid data that gives us the data array and then
when I Generate random positions to grab a chunk out of I actually grab I
actually generate batch size number of Random offsets so because this is four
we are ex is going to be a uh four numbers that are randomly generated between zero and Len of data minus block
size so it's just random offsets into the training set and then X's as I explained are the
first first block size characters starting at I the Y's are the offset by
one of that so just add plus one and then we're going to get those chunks for
every one of integers I INX and use a torch. stack to take all those uh uh
one-dimensional tensors as we saw here and we're going to um stack them up at
rows and so they all become a row in a 4x8 tensor
so here's where I'm printing then when I sample a batch XB and YB the inputs to
the Transformer now are the input X is the 4x8 tensor four uh rows of eight
columns and each one of these is a chunk of the training set and then the targets here are in the
associated array Y and they will come in to the Transformer all the way at the end uh to um create the loss function
uh so they will give us the correct answer for every single position inside X and then these are the four
independent rows so spelled out as we did before uh this 4x8 array contains a
total of 32 examples and they're completely independent as far as the Transformer is
concerned uh so when the input is 24 the target is 43 or rather 43 here in the Y
array when the input is 2443 the target is 58 uh when the input is 24 43 58 the
target is 5 Etc or like when it is a 52 581 the target is
58 right so you can sort of see this spelled out these are the 32 independent examples packed in to a single batch of
the input X and then the desired targets are in y and so now this integer tensor
of um X is going to feed into the Transformer and that Transformer is
going to simultaneously process all these examples and then look up the correct um integers to predict in every
one of these positions in the tensor y okay so now that we have our batch of input that we'd like to feed into a

å½“ç„¶å¯ä»¥ï¼Œä¸‹é¢æ˜¯è¿™ä¸€æ®µçš„ä¸­æ–‡è§£é‡Šï¼š

---

### ã€Data Loaderï¼šåˆ†å— + æ‰¹é‡åŠ è½½æ•°æ®ã€‘

æ¥ä¸‹æ¥è¦å‡†å¤‡æŠŠæ•°æ®é€è¿› Transformer è®­ç»ƒäº†ï¼Œè¿™é‡Œæœ‰å‡ ä¸ªé‡è¦çš„æ¦‚å¿µï¼š

---

### 1ï¸âƒ£ ä¸èƒ½æŠŠæ•´ä¸ªæ–‡æœ¬ä¸€æ¬¡æ€§é€è¿›å»

* Transformer ä¸€æ¬¡å¤„ç†å¤ªé•¿åºåˆ—ä¼šéå¸¸æ¶ˆè€—å†…å­˜ï¼ˆè®¡ç®—ä»£ä»·å¤ªå¤§ï¼‰ã€‚
* æ‰€ä»¥ï¼Œè®­ç»ƒæ—¶æˆ‘ä»¬æ˜¯**éšæœºæŠ½å–ä¸€äº›å°çš„â€œå—ï¼ˆchunkï¼‰â€**ï¼Œæ¯æ¬¡è®­ç»ƒä¸€ä¸ªå°å—ã€‚
* è¿™æ ·åšå¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œæ¨¡å‹ä¹Ÿèƒ½â€œè§å¤šè¯†å¹¿â€ï¼Œå­¦åˆ°æ•´ä½“æ¨¡å¼ã€‚

---

### 2ï¸âƒ£ Block sizeï¼ˆå—é•¿åº¦ï¼‰

* è¿™äº›â€œå—â€éƒ½æœ‰ä¸€ä¸ªå›ºå®šçš„é•¿åº¦ï¼Œå« **block size**ï¼ˆä¹Ÿå« context lengthï¼‰ã€‚
* ä¾‹å¦‚è¿™é‡Œè®¾æˆ 8ï¼Œæ„æ€æ˜¯æˆ‘ä»¬æ‹¿ 8 ä¸ªå­—ç¬¦åšâ€œä¸Šä¸‹æ–‡â€å»é¢„æµ‹åé¢çš„å­—ç¬¦ã€‚
* å®é™…ä»£ç ä¸­ä¼šå– block size + 1 ä¸ªå­—ç¬¦ â€”â€” æ¯”å¦‚æ‹¿ 9 ä¸ªå­—ç¬¦ï¼Œå…¶å®åŒ…å«äº† 8 ä¸ªè®­ç»ƒæ ·æœ¬ã€‚

---

### 3ï¸âƒ£ ä¸€ä¸ªå—é‡Œé¢å…¶å®æœ‰å¤šä¸ªè®­ç»ƒæ ·æœ¬

ä¸¾ä¾‹ï¼Œæ‹¿ 9 ä¸ªå­—ç¬¦ï¼š

```
"To be or "
```

æ‹†æˆè®­ç»ƒæ ·æœ¬ï¼Œå…¶å®æ˜¯**å¤šä¸ªä½ç½®çš„é¢„æµ‹ä»»åŠ¡**ï¼š

* ç”¨ç¬¬ 1 ä¸ªå­—ç¬¦ â†’ é¢„æµ‹ç¬¬ 2 ä¸ªå­—ç¬¦
* ç”¨å‰ 2 ä¸ªå­—ç¬¦ â†’ é¢„æµ‹ç¬¬ 3 ä¸ªå­—ç¬¦
* ç”¨å‰ 3 ä¸ªå­—ç¬¦ â†’ é¢„æµ‹ç¬¬ 4 ä¸ªå­—ç¬¦
* ...
* ç”¨å‰ 8 ä¸ªå­—ç¬¦ â†’ é¢„æµ‹ç¬¬ 9 ä¸ªå­—ç¬¦

æ‰€ä»¥ï¼š**ä¸€ä¸ª chunk é‡ŒåŒ…å«å¤šä¸ªè®­ç»ƒæ ·æœ¬**ã€‚

---

### 4ï¸âƒ£ ä¸ºä»€ä¹ˆè¿™ä¹ˆè®¾è®¡ï¼Ÿ

* ä¸æ˜¯åªä¸ºäº†æé«˜æ•ˆç‡
* è¿˜å¸Œæœ›æ¨¡å‹**é€‚åº”ä¸åŒé•¿åº¦çš„ä¸Šä¸‹æ–‡**
* å› ä¸ºå®é™…ç”Ÿæˆæ—¶ï¼Œå¯èƒ½åˆšå¼€å§‹åªæœ‰ 1 ä¸ªå­—ç¬¦ï¼Œé€æ¸æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦
* è®­ç»ƒæ—¶æ¨¡å‹å°±å­¦ä¼šäº†å¦‚ä½•åœ¨ä¸åŒ context é•¿åº¦ä¸‹é¢„æµ‹ä¸‹ä¸€ä¸ª token

---

### 5ï¸âƒ£ æ‰¹å¤„ç†ï¼ˆBatchï¼‰

é™¤äº†ä¸€ä¸ª chunk é‡Œæœ‰å¤šä¸ªè®­ç»ƒæ ·æœ¬ï¼Œè¿˜è¦åš **Batch è®­ç»ƒ**ï¼š

* ä¸€æ¬¡è®­ç»ƒä¸ä»…å¤„ç† 1 ä¸ª chunkï¼Œè€Œæ˜¯åŒæ—¶å¤„ç†å¾ˆå¤š chunkï¼Œç»„æˆä¸€ä¸ª batch
* è¿™æ ·å¯ä»¥å……åˆ†åˆ©ç”¨ GPU çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦

ä»£ç é‡Œä¾‹å­æ˜¯ï¼š

* batch\_size = 4ï¼ˆä¸€æ¬¡å¤„ç† 4 ä¸ª chunkï¼‰
* block\_size = 8ï¼ˆæ¯ä¸ª chunk é•¿åº¦ 8ï¼‰

æ‰€ä»¥ï¼š

* æ¯æ¬¡é€è¿› Transformer çš„æ•°æ®æ˜¯ä¸€ä¸ª 4 x 8 çš„ tensorï¼ˆ4 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª 8 ä¸ª tokenï¼‰
* å¯¹åº”çš„ target ä¹Ÿæ˜¯ä¸€ä¸ª 4 x 8 çš„ tensorï¼Œè¡¨ç¤ºæ¯ä¸ª token ä½ç½®çš„â€œæ­£ç¡®ç­”æ¡ˆâ€

---

### 6ï¸âƒ£ éšæœºé‡‡æ · chunk

* æ¯æ¬¡è®­ç»ƒï¼Œéƒ½ä¼š**éšæœºä»è®­ç»ƒé›†é‡ŒæŠ½å‡ºè‹¥å¹²ä½ç½®ï¼ˆoffsetï¼‰**ï¼Œæ‹¿å‡ºæ¥ç»„æˆ batch
* è¿™æ ·æ¨¡å‹ä¸ä¼šè®°ä½æŸæ®µç‰¹å®šä½ç½®çš„å†…å®¹ï¼Œèƒ½æ›´å¥½æ³›åŒ–

---

### 7ï¸âƒ£ æ€»ç»“æ•´ä¸ªæµç¨‹

1ï¸âƒ£ æŠŠæ•´ä¸ªæ–‡æœ¬ç¼–ç æˆæ•´æ•°åºåˆ—ï¼ˆå¤§æ•°ç»„ï¼‰
2ï¸âƒ£ è®­ç»ƒæ—¶ï¼š

* éšæœºæŠ½å– batch\_size ä¸ª offset
* å¯¹æ¯ä¸ª offsetï¼Œå– block\_size + 1 ä¸ª token â†’ å½¢æˆ chunk
* ç”Ÿæˆå¯¹åº”çš„ Xï¼ˆè¾“å…¥ï¼‰ å’Œ Yï¼ˆç›®æ ‡ï¼‰
  3ï¸âƒ£ X å’Œ Y å°±å¯ä»¥é€è¿› Transformer åšè®­ç»ƒäº†

---

### 8ï¸âƒ£ Transformer å¦‚ä½•å¤„ç†

* Xï¼šå°±æ˜¯ 4 x 8 ä¸ª tokenï¼ˆ4 ä¸ª chunkï¼Œæ¯ chunk 8 ä¸ª tokenï¼‰
* Transformer ç”¨æ¥é¢„æµ‹ï¼šX é‡Œæ¯ä¸€ä½ç½®â€œä¸‹ä¸€ä¸ª tokenâ€æ˜¯ä»€ä¹ˆ â†’ å¯¹åº” Y ä¸­çš„å€¼
* æ¯ä¸€ä¸ª (X\[i,j], Y\[i,j]) å°±æ˜¯ä¸€ä¸ªè®­ç»ƒæ ·æœ¬

---

è¿™æ ·å°±å®Œæˆäº† **Data Loader çš„æ ¸å¿ƒè®¾è®¡**ï¼Œæ¨¡å‹è®­ç»ƒå°±èƒ½ç¨³å®šè¿›è¡Œã€‚

---

å¦‚æœä½ éœ€è¦çš„è¯ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ ç”»ä¸ª**æ•°æ®æµç¤ºæ„å›¾**ï¼Œæˆ–è€…æ•´ç†ä¸€ä¸ª **batch + chunk + context** çš„æ€»ç»“è¡¨ï¼Œå¸®åŠ©ä½ æ›´å¥½è®°ä½ï¼è¦ä¸è¦æˆ‘æ•´ç†ä¸€ä¸‹ï¼Ÿ ğŸŒŸ

```python
def get_batch(split):
    # generate a small batch of data of inputs x and targets y
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    return x, y
```

è§£é‡Šä¸€ä¸‹


å½“ç„¶å¯ä»¥ï¼Œä¸‹é¢æˆ‘å¸®ä½ è¯¦ç»†è§£é‡Šä¸€ä¸‹è¿™ä¸€æ®µä»£ç  `get_batch(split)`ï¼š

---

### ğŸ“Œ å‡½æ•°åŠŸèƒ½ï¼š

**ç”Ÿæˆä¸€ä¸ª batch çš„è¾“å…¥æ•°æ® (x) å’Œå¯¹åº”çš„ç›®æ ‡æ•°æ® (y)**ï¼Œä¾›æ¨¡å‹è®­ç»ƒç”¨ã€‚

---

### ğŸ“Œ è¾“å…¥å‚æ•°ï¼š

`split` â€”â€” å­—ç¬¦ä¸²ï¼Œå†³å®šæ˜¯ç”¨è®­ç»ƒé›†ï¼ˆ'train'ï¼‰è¿˜æ˜¯éªŒè¯é›†ï¼ˆ'val'ï¼‰

---

### ğŸ“Œ å…·ä½“æ­¥éª¤ï¼š

#### â‘  é€‰æ•°æ®é›†

```python
data = train_data if split == 'train' else val_data
```

å¦‚æœå‚æ•°æ˜¯ 'train'ï¼Œå°±ç”¨ `train_data`ï¼›
å¦‚æœæ˜¯ 'val'ï¼Œå°±ç”¨ `val_data`ã€‚

---

#### â‘¡ ç”Ÿæˆ batch\_size ä¸ªéšæœºèµ·å§‹ä½ç½®

```python
ix = torch.randint(len(data) - block_size, (batch_size,))
```

* `torch.randint` ç”Ÿæˆ batch\_size ä¸ªéšæœºæ•´æ•°ï¼ŒèŒƒå›´æ˜¯ \[0, len(data) - block\_size)ã€‚
* è¿™äº›æ•´æ•°æ˜¯**chunk çš„èµ·å§‹ä½ç½®**ã€‚
* ä¸ºä»€ä¹ˆå‡æ‰ block\_sizeï¼Ÿå› ä¸ºåé¢è¦å–å‡ºé•¿åº¦ä¸º block\_size + 1 çš„ç‰‡æ®µï¼Œä¸èƒ½è¶Šç•Œã€‚

---

#### â‘¢ æ„å»ºè¾“å…¥åºåˆ— x

```python
x = torch.stack([data[i:i+block_size] for i in ix])
```

* å¯¹æ¯ä¸ªéšæœºä½ç½® iï¼Œå–å‡ºä» `i` åˆ° `i+block_size` çš„ç‰‡æ®µã€‚
* ç”¨ list comprehension å¤„ç† batch\_size ä¸ªä½ç½®ã€‚
* ç”¨ `torch.stack` æŠŠå®ƒä»¬å †å æˆä¸€ä¸ª batchï¼Œshape æ˜¯ `(batch_size, block_size)`ã€‚
* è¿™ä¸ª `x` å°±æ˜¯ Transformer çš„è¾“å…¥ï¼ˆæ¯è¡Œæ˜¯ä¸€ä¸ª chunkï¼‰ã€‚

---

#### â‘£ æ„å»ºç›®æ ‡åºåˆ— y

```python
y = torch.stack([data[i+1:i+block_size+1] for i in ix])
```

* å’Œ `x` ç±»ä¼¼ï¼Œä¸è¿‡åç§» 1 ä¸ªå­—ç¬¦ï¼Œè¡¨ç¤º\*\*â€œæ­£ç¡®ç­”æ¡ˆâ€\*\*ï¼š

  * å¯¹äº x é‡Œçš„æ¯ä¸€ä¸ª tokenï¼Œy é‡Œå¯¹åº”ä½ç½®å­˜çš„æ˜¯**ä¸‹ä¸€ä¸ª token**ã€‚
* è¿™æ ·æ¨¡å‹è®­ç»ƒæ—¶ï¼Œå°±æ˜¯å­¦ä¼š**é¢„æµ‹ä¸‹ä¸€ä¸ª token**ã€‚

---

#### â‘¤ è¿”å› x, y

```python
return x, y
```

* è¿”å›å€¼ x, yï¼Œshape éƒ½æ˜¯ `(batch_size, block_size)`
* å¯ä»¥ç›´æ¥å–‚è¿› Transformer è®­ç»ƒï¼Œè®¡ç®— lossã€‚

---

### ğŸ“Œ ä¸¾ä¸ªç®€å•ä¾‹å­ï¼š

å‡è®¾ï¼š

```
batch_size = 2  
block_size = 4  
data = [1,2,3,4,5,6,7,8,9,10]  
```

å‡è®¾é‡‡æ ·å‡ºä¸¤ä¸ªä½ç½®ï¼š

```
ix = [2, 5]  
```

é‚£å¯¹åº”çš„ batch å°±æ˜¯ï¼š

**xï¼š**

```
[ data[2:6] ] -> [3,4,5,6]  
[ data[5:9] ] -> [6,7,8,9]  
```

**yï¼š**

```
[ data[3:7] ] -> [4,5,6,7]  
[ data[6:10] ] -> [7,8,9,10]  
```

---

### ğŸ“Œ æ€»ç»“ä¸€å¥è¯ï¼š

è¿™ä¸ª `get_batch()` å‡½æ•°å°±æ˜¯ï¼š
**éšæœºé€‰ batch\_size ä¸ªä½ç½®ï¼Œå–å‡º block\_size é•¿åº¦çš„ x å’Œå¯¹åº”çš„ yï¼Œä½œä¸ºä¸€ä¸ª batch è¾“å…¥æ¨¡å‹è®­ç»ƒã€‚**

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘ä¹Ÿå¯ä»¥è¡¥å……ä¸€ä¸‹ï¼š

âœ… ä¸ºä»€ä¹ˆè¦ç”¨ offset 1 ä½œä¸º yï¼Ÿ

âœ… batch\_size å’Œ block\_size è®¾ç½®æˆå¤šå°‘åˆé€‚ï¼Ÿ

âœ… è®­ç»ƒè¿‡ç¨‹ä¸­ get\_batch æ€ä¹ˆé…åˆ optimizer ç”¨ï¼Ÿ

è¦ä¸è¦æˆ‘å†è¯¦ç»†è®²è®²ï¼Ÿ ğŸŒŸ


# simplest baseline: bigram language model, loss, generation

Transformer let's start basically feeding this into neural networks now we're going to start off with the
simplest possible neural network which in the case of language modeling in my opinion is the Byram language model and
we've covered the Byram language model in my make more series in a lot of depth and so here I'm going to sort of go
faster and let's just Implement pytorch module directly that implements the byr language
model so I'm importing the pytorch um NN module uh for
reproducibility and then here I'm constructing a Byram language model which is a subass of NN
module and then I'm calling it and I'm passing it the inputs and the targets
and I'm just printing now when the inputs on targets come here you see that I'm just taking the index uh the inputs
X here which I rename to idx and I'm just passing them into this token embedding table so it's going on here is
that here in the Constructor we are creating a token embedding table and it
is of size vocap size by vocap size and we're using an. embedding which
is a very thin wrapper around basically a tensor of shape voap size by vocab size and what's happening here is that
when we pass idx here every single integer in our input is going to refer to this embedding table and it's going
to pluck out a row of that embedding table corresponding to its index so 24
here will go into the embedding table and we'll pluck out the 24th row and then 43 will go here and pluck out the
43d row Etc and then pytorch is going to arrange all of this into a batch by Time
by channel uh tensor in this case batch is four time is eight and C which is the
channels is vocab size or 65 and so we're just going to pluck out all those rows arrange them in a b by T by C and
now we're going to interpret this as the logits which are basically the scores for the next character in the sequence
and so what's happening here is we are predicting what comes next based on just the individual identity of a single
token and you can do that because um I mean currently the tokens are not talking to each other and they're not
seeing any context except for they're just seeing themselves so I'm a f I'm a token number five and then I can
actually make pretty decent predictions about what comes next just by knowing that I'm token five because some characters uh know um C follow other
characters in in typical scenarios so we saw a lot of this in a lot more depth in the make more series and here if I just
run this then we currently get the predictions the scores the lits for
every one of the 4x8 positions now that we've made predictions about what comes next we'd like to evaluate the loss
function and so in make more series we saw that a good way to measure a loss or like a quality of the predictions is to
use the negative log likelihood loss which is also implemented in pytorch under the name cross entropy so what we'
like to do here is loss is the cross entropy on the predictions and the
targets and so this measures the quality of the logits with respect to the Targets in other words we have the
identity of the next character so how well are we predicting the next character based on the lits and
intuitively the correct um the correct dimension of low jits uh depending on
whatever the target is should have a very high number and all the other dimensions should be very low number
right now the issue is that this won't actually this is what we want we want to basically output the logits and the
loss this is what we want but unfortunately uh this won't actually run
we get an error message but intuitively we want to uh measure this now when we
go to the pytorch um cross entropy documentation here um we're trying to
call the cross entropy in its functional form uh so that means we don't have to create like a module for it but here
when we go to the documentation you have to look into the details of how pitor expects these inputs and basically the
issue here is ptor expects if you have multi-dimensional input which we do because we have a b BYT by C tensor then
it actually really wants the channels to be the second uh Dimension here so if
you um so basically it wants a b by C BYT instead of a b by T by C and so it's
just the details of how P torch treats um these kinds of inputs and so we don't
actually want to deal with that so what we're going to do instead is we need to basically reshape our logits so here's
what I like to do I like to take basically give names to the dimensions so lit. shape is B BYT by C and unpack
those numbers and then let's uh say that logits equals lit. View and we want it
to be a b * c b * T by C so just a two- dimensional
array right so we're going to take all the we're going to take all of these um
positions here and we're going to uh stretch them out in a onedimensional sequence and uh preserve the channel
Dimension as the second dimension so we're just kind of like stretching out the array so it's two- dimensional and in that case it's going
to better conform to what pytorch uh sort of expects in its Dimensions now we
have to do the same to targets because currently targets are um of shape B by T
and we want it to be just B * T so onedimensional now alternatively you
could always still just do minus one because pytor will guess what this should be if you want to lay it out uh
but let me just be explicit and say p * t once we've reshaped this it will match the cross entropy case and then we
should be able to evaluate our loss okay so that R now and we can do
loss and So currently we see that the loss is 4.87 now because our uh we have 65
possible vocabulary elements we can actually guess at what the loss should be and in
particular we covered negative log likelihood in a lot of detail we are expecting log or lawn of um 1 over 65
and negative of that so we're expecting the loss to be about 4.1 17 but we're
getting 4.87 and so that's telling us that the initial predictions are not uh super diffuse they've got a little bit
of entropy and so we're guessing wrong uh so uh yes but actually we're I a we
are able to evaluate the loss okay so now that we can evaluate the quality of the model on some data we'd like to also
be able to generate from the model so let's do the generation now I'm going to go again a little bit faster here
because I covered all this already in previous videos so here's a generate function for the
model so we take some uh we take the the same kind of input idx here and
basically this is the current uh context of some characters in a batch in some
batch so it's also B BYT and the job of generate is to basically take this B BYT
and extend it to be B BYT + 1 plus 2 plus 3 and so it's just basically it continues the generation in all the
batch dimensions in the time Dimension So that's its job and it will do that for Max new tokens so you can see here
on the bottom there's going to be some stuff here but on the bottom whatever is predicted is concatenated on top of the
previous idx along the First Dimension which is the time Dimension to create a b BYT + one so that becomes a new idx so
the job of generate is to take a b BYT and make it a b BYT plus 1 plus 2 plus three as many as we want Max new tokens
so this is the generation from the model now inside the generation what what are we doing we're taking the current
indices we're getting the predictions so we get uh those are in the low jits and
then the loss here is going to be ignored because um we're not we're not using that and we have no targets that
are sort of ground truth targets that we're going to be comparing with then once we get the logits we are only
focusing on the last step so instead of a b by T by C we're going to pluck out
the negative-1 the last element in the time Dimension because those are the predictions for what comes next so that
gives us the logits which we then convert to probabilities via softmax and then we use tor. multinomial to sample
from those probabilities and we ask pytorch to give us one sample and so idx
next will become a b by one because in each uh one of the batch Dimensions
we're going to have a single prediction for what comes next so this num samples equals one will make this be a
one and then we're going to take those integers that come from the sampling process according to the probability
distribution given here and those integers got just concatenated on top of the current sort of like running stream
of integers and this gives us a b BYT + one and then we can return that now one
thing here is you see how I'm calling self of idx which will end up going to
the forward function I'm not providing any Targets So currently this would give an error because targets is uh is uh
sort of like not given so targets has to be optional so targets is none by default and then if targets is none then
there's no loss to create so it's just loss is none but else all of this
happens and we can create a loss so this will make it so um if we have the
targets we provide them and get a loss if we have no targets it will'll just get the loits so this here will generate from
the model um and let's take that for a ride
now oops so I have another code chunk here which will generate for the model
from the model and okay this is kind of crazy so maybe let me let me break this down so these are the idx
right I'm creating a batch will be just one time will be just one so I'm
creating a little one by one tensor and it's holding a zero and the D type the
data type is uh integer so zero is going to be how we kick off the generation and
remember that zero is uh is the element standing for a new line character so
it's kind of like a reasonable thing to to feed in as the very first character in a sequence to be the new
line um so it's going to be idx which we're going to feed in here then we're going to ask for 100 tokens
and then. generate will continue that now because uh generate works on the
level of batches we we then have to index into the zero throw to basically unplug the um the single batch Dimension
that exists and then that gives us a um
time steps just a onedimensional array of all the indices which we will convert to simple python list from pytorch
tensor so that that can feed into our decode function and uh convert those
integers into text so let me bring this back and we're generating 100 tokens let's
run and uh here's the generation that we achieved so obviously it's garbage and
the reason it's garbage is because this is a totally random model so next up we're going to want to train this model
now one more thing I wanted to point out here is this function is written to be General but it's kind of like ridiculous
right now because we're feeding in all this we're building out this context and we're concatenating
it all and we're always feeding it all into the model but that's kind of
ridiculous because this is just a simple Byram model so to make for example this prediction about K we only needed this W
but actually what we fed into the model is we fed the entire sequence and then we only looked at the very last piece
and predicted K so the only reason I'm writing it in this way is because right now this is a byr model but I'd like to
keep keep this function fixed and I'd like it to work um later when our
characters actually um basically look further in the history and so right now
the history is not used so this looks silly uh but eventually the history will be used and so that's why we want to uh
do it this way so just a quick comment on that so now we see that this is um random so let's train the model so it
becomes a bit less random okay let's Now train the model so first what I'm going to do is I'm going to create a pyour

å½“ç„¶å¯ä»¥ï¼è¿™ä¸€æ®µè®²çš„æ˜¯**æœ€ç®€å•çš„è¯­è¨€æ¨¡å‹ â€”â€” bigram è¯­è¨€æ¨¡å‹**ï¼Œä»¥åŠå¦‚ä½•è®¡ç®— loss å’Œåšç”Ÿæˆã€‚æˆ‘å¸®ä½ è¯¦ç»†æ‹†è§£ä¸€ä¸‹ï¼š

---

### ğŸ“Œ ä»€ä¹ˆæ˜¯ bigram è¯­è¨€æ¨¡å‹ï¼Ÿ

bigram æ¨¡å‹çš„æ„æ€æ˜¯ï¼š
**â€œæ ¹æ®å½“å‰è¿™ä¸ªå­—ç¬¦ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚â€**
å®ƒæ²¡æœ‰ç”¨æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œ**åªçœ‹å½“å‰ä½ç½®çš„å­—ç¬¦**ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªã€‚

---

### ğŸ“Œ ç½‘ç»œç»“æ„

ä½œè€…ä»æœ€ç®€å•çš„æ¨¡å‹å¼€å§‹ï¼Œç›´æ¥ç”¨ PyTorch å®šä¹‰ä¸€ä¸ª `BigramLanguageModel` ç±»ï¼ˆç»§æ‰¿ `nn.Module`ï¼‰ï¼š

```python
class BigramLanguageModel(nn.Module):
```

#### 1ï¸âƒ£ token embedding table

åœ¨ `__init__` é‡Œåˆ›å»ºäº†ä¸€ä¸ª **token embedding è¡¨**ï¼Œå½¢çŠ¶æ˜¯ `(vocab_size, vocab_size)`ï¼š

```python
self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)
```

* æœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ª **è¯è¡¨å¤§å° Ã— è¯è¡¨å¤§å°** çš„çŸ©é˜µã€‚
* è¾“å…¥æ˜¯ä¸€ä¸ª token çš„ç´¢å¼•ï¼Œæ¯”å¦‚ 24ï¼Œ
  â†’ å®ƒä¼šæŠŠ embedding table é‡Œç¬¬ 24 è¡Œæ‹¿å‡ºæ¥ä½œä¸º embeddingã€‚
* è¿™æ · batch é‡Œçš„æ¯ä¸ª tokenï¼Œéƒ½ä¼šè¢«æ˜ å°„æˆä¸€ä¸ª vocab\_size ç»´çš„å‘é‡ã€‚

---

### ğŸ“Œ forward è¿‡ç¨‹

* `x` è¾“å…¥å½¢çŠ¶æ˜¯ `(batch_size, block_size)`ï¼Œå°±æ˜¯ä¸€æ‰¹ token åºåˆ—ã€‚
* ç»è¿‡ embeddingï¼Œå˜æˆ `(batch_size, block_size, vocab_size)`ï¼Œç›¸å½“äºä¸ºæ¯ä¸ªä½ç½®éƒ½é¢„æµ‹å‡ºâ€œä¸‹ä¸€ä¸ª token å„ç±»åˆ«çš„æ‰“åˆ†ï¼ˆlogitsï¼‰â€ã€‚

```python
logits = self.token_embedding_table(idx)  # shape: (B, T, vocab_size)
```

* ä½† bigram æ¨¡å‹æœ¬èº«å…¶å® **åªç”¨å½“å‰ token æ¥é¢„æµ‹ä¸‹ä¸€ä¸ª token**ï¼Œå¹¶ä¸çœ‹ä¸Šä¸‹æ–‡ã€‚

---

### ğŸ“Œ è®¡ç®— loss

è®­ç»ƒæ—¶éœ€è¦å®šä¹‰ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œè¡¡é‡é¢„æµ‹å’Œç›®æ ‡çš„å·®è·ã€‚

å¸¸ç”¨çš„å°±æ˜¯ **äº¤å‰ç†µï¼ˆcross entropyï¼‰**ï¼ŒPyTorch é‡Œæœ‰ç°æˆçš„ `F.cross_entropy`ã€‚

#### ä½†æ˜¯è¦æ³¨æ„å½¢çŠ¶é—®é¢˜ï¼š

* PyTorch æœŸæœ›è¾“å…¥ logits å½¢çŠ¶æ˜¯ `(batch_size, vocab_size, block_size)`ï¼Œ
  è€Œ bigram æ¨¡å‹å½“å‰è¾“å‡ºçš„æ˜¯ `(batch_size, block_size, vocab_size)`ï¼Œæ‰€ä»¥è¦ reshapeã€‚

```python
logits = logits.view(batch_size * block_size, vocab_size)
targets = targets.view(batch_size * block_size)
```

* reshape åï¼Œå¯ä»¥ç›´æ¥ç”¨ `F.cross_entropy(logits, targets)` è®¡ç®— lossã€‚

---

### ğŸ“Œ ç»“æœåˆ†æ

* ä¸€å¼€å§‹éšæœºåˆå§‹åŒ–çš„æ¨¡å‹ï¼Œloss å¤§æ¦‚æ˜¯ 4.87
* ç†è®ºä¸Šï¼Œå¦‚æœæ¨¡å‹æ˜¯å®Œå…¨éšæœºçš„ï¼Œloss åº”è¯¥æ¥è¿‘ **`-ln(1/65) â‰ˆ 4.17`**ï¼Œ
  å› ä¸ºè¯è¡¨æœ‰ 65 ä¸ªå­—ç¬¦ï¼Œå®Œå…¨éšæœºé¢„æµ‹çš„äº¤å‰ç†µå°±æ˜¯è¿™ä¸ªå€¼ã€‚
* æ‰€ä»¥ 4.87 æ¯” 4.17 å¤§ä¸€äº›ï¼Œè¯´æ˜æ¨¡å‹è¿˜æ²¡å¼€å§‹å­¦ä¹ ã€‚

---

### ğŸ“Œ æ–‡æœ¬ç”Ÿæˆï¼ˆgenerationï¼‰

ä½œè€…å®ç°äº†ä¸€ä¸ª `generate()` å‡½æ•°ï¼Œèƒ½æ ¹æ®å½“å‰è¾“å…¥åºåˆ—ï¼Œé€æ­¥å¾€åç”Ÿæˆæ›´å¤š tokenã€‚

#### ç”Ÿæˆæµç¨‹ï¼š

1ï¸âƒ£ ç»™ä¸€ä¸ªåˆå§‹ token `idx`ï¼ˆæ¯”å¦‚æ¢è¡Œç¬¦ï¼Œ0ï¼‰
2ï¸âƒ£ æ¯ä¸€æ­¥ï¼š

* ç”¨æ¨¡å‹ç®— logits
* åªå– logits çš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼ˆæœ€åä¸€ä¸ª tokenï¼‰
* ç”¨ softmax è½¬æˆæ¦‚ç‡åˆ†å¸ƒ
* éšæœº sample å‡ºä¸€ä¸ª token
* æŠŠè¿™ä¸ªæ–° token æ¥åˆ°åºåˆ—åé¢
  3ï¸âƒ£ é‡å¤è¿™ä¸ªè¿‡ç¨‹ N æ­¥ï¼Œå°±å¯ä»¥ç”Ÿæˆé•¿åº¦ä¸º N çš„åºåˆ—ã€‚

---

### ğŸ“Œ ä¸ºä»€ä¹ˆå†™æˆè¿™æ ·ï¼Ÿ

* ç°åœ¨è¿™ä¸ª bigram æ¨¡å‹å…¶å® â€œåªçœ‹å½“å‰ tokenâ€ï¼Œå†å² token å¹¶æ²¡ç”¨ã€‚
* ä½†æ˜¯ä½œè€…æ•…æ„æŠŠ generate() å†™æˆèƒ½å¤„ç†ä»»æ„é•¿åº¦ä¸Šä¸‹æ–‡ï¼ˆæŠŠå†å²æ‹¼ä¸Šå»ï¼‰ï¼Œ
  å› ä¸º **åé¢æˆ‘ä»¬ä¼šå‡çº§æˆ Transformer**ï¼Œé‚£æ—¶å†å² token å°±ä¼šç”¨åˆ°äº†ã€‚
* æå‰æŠŠæ¡†æ¶å†™å¥½ï¼Œåé¢ä¸ç”¨æ”¹ä»£ç ã€‚

---

### ğŸ“Œ å°ç»“ï¼š

* ç°åœ¨è¿™ä¸ª bigram è¯­è¨€æ¨¡å‹éå¸¸ç®€å•ï¼Œåªèƒ½å­¦â€œä¸¤ä¸ªå­—ç¬¦ä¹‹é—´çš„å…³ç³»â€ï¼Œæ²¡æ³•å­¦é•¿è·ç¦»ä¸Šä¸‹æ–‡ã€‚
* ç”Ÿæˆå‡ºæ¥çš„æ–‡æœ¬æ˜¯â€œåƒåœ¾â€ï¼Œå› ä¸ºæ¨¡å‹è¿˜æ²¡è®­ç»ƒå¥½ã€‚
* ä¸‹ä¸€æ­¥å°±æ˜¯è¦è®­ç»ƒè¿™ä¸ªæ¨¡å‹ï¼Œè®©å®ƒæ…¢æ…¢å­¦ä¼š tiny Shakespeare é‡Œçš„å­—ç¬¦å…³ç³»ã€‚

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘è¿˜å¯ä»¥è¡¥å……ï¼š
âœ… ä¸ºä»€ä¹ˆ bigram æ˜¯è¯­è¨€å»ºæ¨¡é‡Œæœ€ç®€å•çš„ baselineï¼Ÿ
âœ… ä¸ºä»€ä¹ˆ logits è¦ reshapeï¼Ÿ
âœ… bigram ç”Ÿæˆçš„æ–‡æœ¬ä¼šæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ

è¦ä¸è¦æˆ‘å†è¯¦ç»†è®²è®²ï¼Ÿ ğŸŒŸ

print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))

è§£é‡Šä¸€ä¸‹

å¥½ï¼Œè¿™å¥ä»£ç æˆ‘å¸®ä½ è¯¦ç»†è§£é‡Šä¸€ä¸‹ï¼š

```python
print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))
```

---

### ğŸ“Œ è¿™ä¸€å¥æ•´ä½“åœ¨åšä»€ä¹ˆï¼Ÿ

ğŸ‘‰ **ç”¨ bigram æ¨¡å‹ m ç”Ÿæˆ 100 ä¸ª tokenï¼Œè½¬æˆå­—ç¬¦ä¸²ï¼Œæ‰“å°å‡ºæ¥ã€‚**

---

### ğŸ“Œ é€æ­¥æ‹†è§£

---

#### 1ï¸âƒ£ `torch.zeros((1, 1), dtype=torch.long)`

```python
idx = torch.zeros((1, 1), dtype=torch.long)
```

* æ„é€ ä¸€ä¸ª `(1, 1)` çš„ tensorï¼Œé‡Œé¢å¡« 0ï¼Œæ•´æ•°ç±»å‹ï¼ˆlongï¼‰ã€‚
* å«ä¹‰æ˜¯ï¼š**batch size = 1ï¼Œåºåˆ—é•¿åº¦ = 1ï¼Œåˆå§‹ token = 0**ã€‚

é€šå¸¸ 0 æ˜¯æ¢è¡Œç¬¦ `\n`ï¼Œä½œä¸ºâ€œèµ·å§‹ tokenâ€å¾ˆåˆç†ã€‚

---

#### 2ï¸âƒ£ è°ƒç”¨ generate()

```python
m.generate(idx, max_new_tokens=100)
```

* è°ƒç”¨æ¨¡å‹çš„ `generate()` å‡½æ•°ï¼Œ
* åˆå§‹åºåˆ—æ˜¯ `[0]`ï¼Œ
* **è¿ç»­ç”Ÿæˆ 100 ä¸ªæ–°çš„ token**ï¼Œæ‹¼æ¥åœ¨åŸåºåˆ—åé¢ã€‚

è¿”å›ç»“æœ shape æ˜¯ï¼š

```python
(idx.shape) â†’ (1, 1 + 100) â†’ (1, 101)
```

ä¹Ÿå°±æ˜¯ï¼š

```
[[ 0, 34, 5, 9, 18, 12, ..., 21 ]]  â† ä¸€æ¡ç”Ÿæˆå‡ºæ¥çš„ token åºåˆ—
```

---

#### 3ï¸âƒ£ `[0]`

```python
... [0]
```

* å–å‡º batch é‡Œçš„ç¬¬ä¸€ä¸ªåºåˆ—ï¼ˆå› ä¸º batch\_size=1ï¼‰
* å¾—åˆ° 1 ç»´ arrayï¼Œshape æ˜¯ `(101,)`ï¼Œ101 ä¸ª token id

---

#### 4ï¸âƒ£ `.tolist()`

```python
.tolist()
```

* æŠŠ PyTorch tensor è½¬æˆ python listï¼Œæ–¹ä¾¿ decodeã€‚

ä¾‹å¦‚ï¼š

```
[0, 34, 5, 9, 18, 12, ..., 21]
```

---

#### 5ï¸âƒ£ `decode()`

```python
decode([...])
```

* æŠŠ token id åˆ—è¡¨ â†’ è½¬æ¢æˆå­—ç¬¦ä¸²ã€‚
* decode è¿‡ç¨‹æ˜¯æ ¹æ®ä¹‹å‰çš„ **token\_to\_char æ˜ å°„è¡¨**ï¼ŒæŠŠ token id è¿˜åŸæˆå­—ç¬¦ã€‚

---

#### 6ï¸âƒ£ print()

```python
print(...)
```

* æ‰“å°å‡ºç”Ÿæˆçš„å­—ç¬¦ä¸²ã€‚

---

### ğŸ“Œ ä¸¾ä¸ªä¾‹å­

å‡è®¾ç”Ÿæˆç»“æœæ˜¯ï¼š

```
[0, 34, 5, 9, 18, 12, 7, 22, ...]
```

decode åå¾—åˆ°å­—ç¬¦ä¸²ï¼š

```
"\nABcdefGH..."
```

æœ€ç»ˆæ‰“å°å‡ºæ¥å°±æ˜¯ï¼š

```
ABcdefGH...
```

---

### ğŸ“Œ æ€»ç»“ä¸€å¥è¯

è¿™ä¸€æ•´å¥ä»£ç ï¼š

```python
print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))
```

çš„ä½œç”¨æ˜¯ï¼š

ğŸ‘‰ **â€œä» token 0 å¼€å§‹ï¼Œè°ƒç”¨ bigram è¯­è¨€æ¨¡å‹ç”Ÿæˆ 100 ä¸ª tokenï¼Œè½¬æˆå­—ç¬¦ï¼Œæ‰“å°å‡ºæ¥ã€‚â€**

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘å¯ä»¥ç»§ç»­è®²è®²ï¼š

âœ… ä¸ºä»€ä¹ˆéœ€è¦ tolist()ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆ batch size = 1ï¼Ÿ
âœ… è¿™ä¸ªç”Ÿæˆç»“æœé€šå¸¸çœ‹èµ·æ¥åƒä»€ä¹ˆï¼Ÿ bigram æ¨¡å‹ vs Transformer å·®åˆ«å¤§å—ï¼Ÿ

è¦ä¸è¦æˆ‘å†è¯¦ç»†è®²ä¸€è®²ï¼ŸğŸŒŸ

ä¸ºä»€ä¹ˆè¯´ logits æ˜¯ by batch, by time, by channel

è¿™æ˜¯å› ä¸ºåœ¨è¯­è¨€æ¨¡å‹çš„å‰å‘è¾“å‡ºé‡Œï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶ä¿ç•™ä¸‰ä¸ªç»´åº¦çš„ä¿¡æ¯ï¼Œæ¯ä¸ªç»´åº¦çš„å«ä¹‰åˆ†åˆ«å¯¹åº” **batch**ã€**time** å’Œ **channel**ï¼š

---

### 1. **batch ç»´åº¦ (B)**

* è¡¨ç¤ºä¸€æ¬¡å‰å‘ä¸­å¹¶è¡Œå¤„ç†äº†å¤šå°‘ä¸ªç‹¬ç«‹çš„åºåˆ—æ ·æœ¬ã€‚
* ä¾‹å¦‚ `batch_size=4`ï¼Œé‚£ä¹ˆæ¨¡å‹åŒæ—¶å¤„ç† 4 æ®µä¸åŒçš„æ–‡æœ¬ï¼Œæ¯æ®µæ–‡æœ¬äº’ä¸å½±å“ã€‚
* è¿™ä¸€ç»´å°±æ˜¯ PyTorch å¼ é‡çš„ç¬¬ 0 ç»´ï¼Œå½¢çŠ¶é‡Œç¬¬ä¸€ä¸ªæ•°å­— Bã€‚

---

### 2. **time ç»´åº¦ (T)**

* è¡¨ç¤ºåºåˆ—çš„**æ—¶é—´æ­¥**ï¼ˆtoken åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼‰ï¼Œä¹Ÿå°±æ˜¯ä¸Šä¸‹æ–‡é•¿åº¦ã€‚
* ä¾‹å¦‚ `block_size=8`ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰ 8 ä¸ªä½ç½®è¦é¢„æµ‹ä¸‹ä¸€ tokenã€‚
* åœ¨å¼ é‡ä¸­ï¼Œè¿™æ˜¯ç¬¬ 1 ç»´ã€‚å¯¹äºä¸€ä¸ªæ ·æœ¬æ¥è¯´ï¼Œä»ä½ç½® 0 åˆ°ä½ç½® T-1ï¼Œæ¨¡å‹è¦åŒæ—¶è¾“å‡ºæ‰€æœ‰ä½ç½®çš„é¢„æµ‹åˆ†å¸ƒã€‚

---

### 3. **channel ç»´åº¦ (C)**

* è¡¨ç¤ºåœ¨æ¯ä¸ªæ—¶é—´ä½ç½®ä¸Šï¼Œæ¨¡å‹è¦é¢„æµ‹**ä¸‹ä¸€ä¸ª token çš„æ‰€æœ‰å¯èƒ½ç±»åˆ«çš„åˆ†æ•°ï¼ˆlogitsï¼‰**ã€‚
* ä¹Ÿå°±æ˜¯è¯è¡¨å¤§å° `vocab_size`ï¼Œä¾‹å¦‚ 65ï¼ˆå­—ç¬¦çº§ï¼‰æˆ–è€… 50257ï¼ˆGPT-2 çš„ subword token æ•°ï¼‰ã€‚
* åœ¨å¼ é‡ä¸­ï¼Œè¿™æ˜¯ç¬¬ 2 ç»´ï¼Œæ¯ä¸ªä½ç½®ä¸Šçš„é•¿åº¦ C çš„å‘é‡ï¼Œå°±æ˜¯ Softmax å‰çš„åŸå§‹åˆ†æ•°ã€‚

---

### 4. **ç»“åˆèµ·æ¥çš„å½¢çŠ¶**

å¦‚æœä½ çš„è¾“å…¥ `xb` æ˜¯å½¢çŠ¶ `[B, T]`ï¼ˆæ•´æ•° idï¼‰ï¼Œç»è¿‡ embedding å’Œç½‘ç»œè®¡ç®—ï¼Œè¾“å‡ºçš„ `logits` å°±æ˜¯ï¼š

```
logits.shape == (B, T, C)
```

ä¸¾ä¾‹ï¼š

* `B=4`ï¼ˆbatch é‡Œ 4 ä¸ªåºåˆ—ï¼‰
* `T=8`ï¼ˆæ¯ä¸ªåºåˆ—é•¿åº¦ 8ï¼‰
* `C=65`ï¼ˆè¯è¡¨å¤§å° 65ï¼‰
  é‚£ä¹ˆ `logits` çš„å½¢çŠ¶æ˜¯ **\[4, 8, 65]**ï¼š
* `logits[0, 3, :]` è¡¨ç¤º **batch ä¸­ç¬¬ 0 ä¸ªæ ·æœ¬**ã€**æ—¶é—´ä½ç½® 3** ä¸Šï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶å¯¹ 65 ä¸ªå¯èƒ½å­—ç¬¦çš„å…¨éƒ¨åˆ†æ•°ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç”»ä¸€ä¸ªå°ç¤ºæ„å›¾ï¼ŒæŠŠ **Bã€Tã€C** ä¸‰ä¸ªç»´åº¦åœ¨å¼ é‡ä¸­çš„ä½ç½®å’Œå«ä¹‰å¯è§†åŒ–ï¼Œè®©ä½ æ›´ç›´è§‚åœ°ç†è§£ã€‚ä½ éœ€è¦æˆ‘ç”»å—ï¼Ÿ

ä¸ºä»€ä¹ˆå« Time ç»´åº¦

åœ¨è¯­è¨€æ¨¡å‹çš„è¾“å‡ºå½¢çŠ¶é‡Œï¼ŒæŠŠç¬¬äºŒä¸ªç»´åº¦å« **Time**ï¼ˆæ—¶é—´ï¼‰æ˜¯å› ä¸ºï¼š

* è¿™ä¸ªç»´åº¦å¯¹åº”çš„æ˜¯**åºåˆ—ä¸­ token çš„ä½ç½®**ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹å¤„ç†â€œæ—¶é—´åºåˆ—â€æ—¶çš„æ—¶é—´æ­¥ï¼ˆtime stepï¼‰ã€‚
* åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œæ–‡æœ¬å¯ä»¥çœ‹ä½œæ˜¯ç¦»æ•£çš„æ—¶é—´åºåˆ—ï¼š

  * ç¬¬ 0 ä¸ªä½ç½®æ˜¯åºåˆ—çš„èµ·ç‚¹
  * ç¬¬ 1 ä¸ªä½ç½®æ˜¯ä¸Šä¸€ä¸ª token ä¹‹åçš„â€œä¸‹ä¸€æ—¶åˆ»â€
  * ä¾æ¬¡ç±»æ¨ç›´åˆ°ç¬¬ T-1 ä¸ªä½ç½®
* Transformerã€RNNã€CNN ç­‰åºåˆ—æ¨¡å‹åœ¨è¿™ä¸ªç»´åº¦ä¸Šæ²¿ç€ä½ç½®é€æ­¥å¤„ç†ä¿¡æ¯ï¼Œç›¸å½“äºåœ¨â€œæ—¶é—´è½´â€ä¸Šç§»åŠ¨ã€‚
* å³ä½¿è¾“å…¥çš„æ˜¯é™æ€æ–‡æœ¬ï¼Œæ•°å­¦ä¸Šä¾ç„¶ä¼šæŠŠå®ƒçœ‹ä½œä¸€ä¸ªæŒ‰é¡ºåºæ’åˆ—çš„åºåˆ—ï¼ŒæŠŠåºå·ç†è§£æˆ**æ—¶é—´ç´¢å¼•**ï¼Œè¿™æ ·å¯ä»¥æ²¿ç”¨æ—¶åºå»ºæ¨¡é‡Œçš„æ¦‚å¿µã€‚

æ‰€ä»¥ `[B, T, C]` é‡Œï¼š

* **B**ï¼šbatchï¼ˆä¸€æ¬¡å¹¶è¡Œå¤„ç†çš„åºåˆ—æ¡æ•°ï¼‰
* **T**ï¼štimeï¼ˆæ¯æ¡åºåˆ—çš„é•¿åº¦ï¼Œå³æ—¶é—´æ­¥æ•°ï¼‰
* **C**ï¼šchannelï¼ˆæ¯ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾ç»´åº¦ï¼Œæ¯”å¦‚ logits å¯¹åº”è¯è¡¨å¤§å°ï¼‰

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç”»ä¸€ä¸ªå°ç¤ºæ„å›¾ï¼ŒæŠŠè¿™ä¸ª **time** ç»´åº¦åœ¨æ–‡æœ¬åºåˆ—é¢„æµ‹é‡Œçš„å«ä¹‰å¯è§†åŒ–ã€‚è¿™æ ·ä½ èƒ½ç›´è§‚çœ‹åˆ°ä¸ºä»€ä¹ˆå«å®ƒâ€œæ—¶é—´â€ç»´åº¦ã€‚ä½ è¦æˆ‘ç”»å—ï¼Ÿ


ä¸ºä»€ä¹ˆå« channel ç»´åº¦

åœ¨ `[B, T, C]` è¿™ä¸ªå½¢çŠ¶é‡Œï¼Œ**channel** ç»´åº¦æ˜¯è¿™ä¹ˆå«çš„ï¼Œä¸»è¦æ˜¯å€Ÿç”¨äº†**ä¿¡å·å¤„ç† / è®¡ç®—æœºè§†è§‰**é‡Œçš„å‘½åä¹ æƒ¯ï¼š

---

### 1. å†å²æ¥æºï¼šCNN / ä¿¡å·å¤„ç†

* åœ¨å›¾åƒä¸­ï¼Œæœ€åä¸€ç»´é€šå¸¸æ˜¯é¢œè‰²é€šé“ï¼ˆRGB å°±æ˜¯ 3 ä¸ª channelï¼‰ã€‚
* åœ¨æ—¶åºä¿¡å·é‡Œï¼Œæœ€åä¸€ç»´å¯èƒ½æ˜¯æ¯ä¸ªæ—¶é—´ç‚¹çš„å¤šä¸ªä¼ æ„Ÿå™¨è¯»æ•°ï¼Œä¹Ÿè¢«å«åšé€šé“ã€‚
* é€šé“çš„æœ¬è´¨å«ä¹‰æ˜¯ï¼š**åœ¨åŒä¸€ä½ç½®ï¼ˆtime stepï¼‰ï¼Œæœ‰å¤šå°‘æ¡â€œå¹¶è¡Œçš„ç‰¹å¾çº¿â€åŒæ—¶æè¿°å®ƒ**ã€‚

---

### 2. åœ¨ NLP / è¯­è¨€æ¨¡å‹é‡Œ

* åœ¨è¯­è¨€æ¨¡å‹çš„ logits ä¸­ï¼Œæœ€åä¸€ç»´å¤§å°æ˜¯ **vocab\_size**ã€‚
* å¯¹äºæŸä¸ªæ‰¹æ¬¡ b å’Œæ—¶é—´æ­¥ tï¼Œ`logits[b, t, :]` æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º vocab\_size çš„å‘é‡ï¼Œè¡¨ç¤º**è¯¥ä½ç½®é¢„æµ‹ä¸‹ä¸€ä¸ª token çš„æ‰€æœ‰ç±»åˆ«åˆ†æ•°**ã€‚
* è¿™é‡Œâ€œæ‰€æœ‰ç±»åˆ«â€å°±åƒæ˜¯â€œå¤šä¸ªé€šé“â€ï¼Œæ¯ä¸ªé€šé“å¯¹åº”è¯è¡¨é‡Œçš„ä¸€ä¸ªå¯èƒ½ tokenã€‚
* æ‰€ä»¥æŠŠæœ€åä¸€ç»´å« **channel ç»´åº¦**ï¼Œæ„æ€æ˜¯**ç‰¹å¾çš„ç§ç±»æ•°**ï¼ˆåœ¨åˆ†ç±»é—®é¢˜ä¸­å°±æ˜¯ç±»åˆ«æ•°ï¼‰ã€‚

---

### 3. ä¸ºä»€ä¹ˆä¸ç›´æ¥å« vocab ç»´åº¦

* åœ¨å®ç°å’Œæ–‡æ¡£ä¸­ï¼ŒPyTorchã€TensorFlow ç­‰æ¡†æ¶ä¼šç»Ÿä¸€ç”¨ **(batch, time, channel)** æ¥æè¿°ä¸‰ç»´æ•°æ®ï¼Œå› ä¸ºï¼š

  * è¿™ä¸ªå‘½ååœ¨ä¸åŒä»»åŠ¡ï¼ˆéŸ³é¢‘ã€è§†é¢‘ã€æ–‡æœ¬ï¼‰ä¸­éƒ½é€šç”¨ã€‚
  * **channel** ä¸ä¸€å®šæ˜¯ç±»åˆ«ï¼Œä¹Ÿå¯ä»¥æ˜¯éšè—å±‚ç‰¹å¾çš„ç»´åº¦ï¼ˆembedding dimï¼‰ã€‚
  * æ–¹ä¾¿å¤ç”¨ç›¸åŒçš„ API/æ¨¡å—ï¼Œè€Œä¸ç”¨å› ä¸ºä»»åŠ¡ä¸åŒæ”¹åå­—ã€‚

---

### 4. æ€»ç»“

* **channel ç»´åº¦** = åŒä¸€æ‰¹æ¬¡ã€åŒä¸€æ—¶é—´ä½ç½®ä¸Šçš„**ç‰¹å¾å‘é‡ç»´åº¦**ã€‚
* åœ¨è¯­è¨€æ¨¡å‹çš„è¾“å‡º logits ä¸­ï¼Œchannel æ•° = è¯è¡¨å¤§å°ï¼Œæ¯ä¸ªâ€œé€šé“â€å¯¹åº”ä¸€ä¸ª token ç±»åˆ«çš„é¢„æµ‹åˆ†æ•°ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç”»ä¸€å¼  3D å¼ é‡çš„ç»“æ„å›¾ï¼ŒæŠŠ batchã€timeã€channel ä¸‰ä¸ªç»´åº¦çš„å«ä¹‰ç›´è§‚åœ°æ ‡å‡ºæ¥ï¼Œè¿™æ ·ä½ ä¸€çœ¼å°±èƒ½çœ‹å‡º channel åœ¨ NLP é‡Œå¯¹åº”çš„æ˜¯ä»€ä¹ˆã€‚ä½ è¦æˆ‘ç”»å—ï¼Ÿ

logits = logits[:, -1, :] # becomes (B, C) 

è¯¦ç»†è§£é‡Šä¸€ä¸‹

è¿™è¡Œä»£ç æ˜¯åœ¨**ä»ä¸‰ç»´å¼ é‡ä¸­å–å‡ºâ€œæœ€åä¸€ä¸ªæ—¶é—´æ­¥â€çš„æ‰€æœ‰é¢„æµ‹åˆ†æ•°**ï¼Œå¹¶æŠŠå½¢çŠ¶ä» `(B, T, C)` å˜æˆ `(B, C)`ï¼š

```python
logits = logits[:, -1, :]  # ç”± (B, T, C) â†’ (B, C)
```

è¯¦è§£è¦ç‚¹ï¼š

* è¾“å…¥ `logits` çš„å½¢çŠ¶æ˜¯ `(B, T, C)`

  * Bï¼šbatch å¤§å°ï¼ˆä¸€æ¬¡å¹¶è¡Œçš„åºåˆ—æ¡æ•°ï¼‰
  * Tï¼štime æ­¥æ•°ï¼ˆåºåˆ—é•¿åº¦ï¼‰
  * Cï¼šchannelï¼ˆæ¯ä¸ªä½ç½®çš„ç±»åˆ«/è¯è¡¨å¤§å°ï¼Œå¯¹åº”æ¯ä¸ª token çš„logitsï¼‰

* ç´¢å¼•è¯­ä¹‰

  * `:` å–è¯¥ç»´åº¦**æ‰€æœ‰**å…ƒç´ 
  * `-1` è¡¨ç¤ºè¯¥ç»´åº¦**æœ€åä¸€ä¸ªç´¢å¼•**
  * æ‰€ä»¥ `[:, -1, :]` è¡¨ç¤ºï¼šå¯¹æ¯ä¸ªæ ·æœ¬ï¼ˆB ç»´å…¨å–ï¼‰ï¼Œåœ¨æ—¶é—´ç»´åªè¦**æœ€åä¸€ä¸ªä½ç½®**ï¼Œå¹¶å–è¯¥ä½ç½®ä¸Šçš„**æ‰€æœ‰é€šé“**ï¼ˆC ç»´å…¨å–ï¼‰

* ç»´åº¦â€œé™ä¸€ç»´â€

  * åœ¨ PyTorch é‡Œï¼Œå¯¹æŸä¸€ç»´ä½¿ç”¨**å•ä¸ªæ•´æ•°ç´¢å¼•**ï¼ˆè¿™é‡Œæ˜¯ `-1`ï¼‰ä¼š**ç§»é™¤**è¿™ä¸€ç»´åº¦
  * å› æ­¤ `(B, T, C)` ç»è¿‡ `[:, -1, :]` åï¼Œæ—¶é—´ç»´è¢«å»æ‰ï¼Œå¾—åˆ° `(B, C)`
  * ç­‰ä»·å†™æ³•ï¼š`logits = logits.select(dim=1, index=-1)`

* ä¸ºä»€ä¹ˆè¦å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥

  * è‡ªå›å½’ç”Ÿæˆæ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦â€œå½“å‰ä¸Šä¸‹æ–‡æœ«å°¾ä¹‹åçš„ä¸‹ä¸€ä¸ª tokenâ€çš„åˆ†å¸ƒ
  * æ¨¡å‹ä¼šç»™å‡ºæ¯ä¸ªæ—¶é—´ä½ç½®çš„é¢„æµ‹ï¼Œä½†ç”¨äºé‡‡æ ·ä¸‹ä¸€ä¸ª token çš„ï¼Œæ­£æ˜¯**åºåˆ—æœ€åä¸€ä¸ªä½ç½®çš„ logits**

* ä¸ä¿ç•™ç»´åº¦çš„åŒºåˆ«

  * å¦‚æœæƒ³**ä¿ç•™**æ—¶é—´ç»´ï¼ˆä¾¿äºæŸäº›æ‹¼æ¥æ“ä½œï¼‰ï¼Œå¯å†™ï¼š`logits = logits[:, -1:, :]`ï¼Œå½¢çŠ¶ä¼šæ˜¯ `(B, 1, C)`
  * ä½†é€šå¸¸ä¸ºäº†åé¢åš `softmax` å’Œ `multinomial` æ›´æ–¹ä¾¿ï¼Œè¿™é‡Œç›´æ¥ç”¨ `(B, C)`

* å°ä¾‹å­

  * å‡è®¾ `B=2, T=4, C=3`ï¼Œ`logits.shape == (2,4,3)`
  * `logits[:, -1, :].shape == (2,3)`ï¼Œè¡¨ç¤ºæ¯ä¸ªæ ·æœ¬åœ¨ç¬¬ 4 ä¸ªæ—¶é—´æ­¥å¤„ï¼Œå¯¹ 3 ä¸ªç±»åˆ«çš„æ‰“åˆ†

* è®¡ç®—/æ¢¯åº¦æ³¨æ„

  * è¿™ä¸€æ­¥å¹¶æœªåšä»»ä½•èšåˆï¼Œåªæ˜¯**é€‰å–åˆ‡ç‰‡**ï¼›æ¢¯åº¦åªä¼šä»**è¢«é€‰ä¸­çš„æœ€åä¸€æ­¥**å›ä¼ 
  * å°½ç®¡åªç”¨åˆ°äº†æœ€åä¸€æ­¥çš„ logitsï¼Œå‰å‘æ—¶ä»è®¡ç®—äº†æ‰€æœ‰æ—¶é—´æ­¥ï¼ˆä¸ºä»£ç é€šç”¨æ€§ã€ä¾¿äºæ›¿æ¢æˆçœ‹é•¿ä¸Šä¸‹æ–‡çš„æ¨¡å‹ï¼‰ã€‚åœ¨æ›´é«˜æ•ˆçš„å®ç°é‡Œå¯ä»¥åšç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—æ—©å…ˆæ­¥éª¤çš„æ¿€æ´»å€¼ï¼ˆå¦‚ Transformer çš„ KV cacheï¼‰ã€‚


# training the bigram model

optimization object so here we are using the optimizer ATM W um now in a make
more series we've only ever use tastic gradi in descent the simplest possible Optimizer which you can get using the
SGD instead but I want to use Adam which is a much more advanced and popular Optimizer and it works extremely well
for uh typical good setting for the learning rate is roughly 3 E4 uh but for
very very small networks like is the case here you can get away with much much higher learning rates R3 or even
higher probably but let me create the optimizer object which will basically take the gradients and uh update the
parameters using the gradients and then here our batch size
up above was only four so let me actually use something bigger let's say 32 and then for some number of steps um
we are sampling a new batch of data we're evaluating the loss uh we're zeroing out all the gradients from the
previous step getting the gradients for all the parameters and then using those gradients to up update our parameters so
typical training loop as we saw in the make more series so let me now uh run
this for say 100 iterations and let's see what kind of losses we're going to
get so we started around 4.7 and now we're getting to down to like 4.6 4.5 Etc so the optimization is
definitely happening but um let's uh sort of try to increase number of
iterations and only print at the end because we probably want train for
longer okay so we're down to 3.6
roughly roughly down to
three this is the most janky
optimization okay it's working let's just do 10,000 and then from here we want to
copy this and hopefully that we're going to get something reason and of course it's not going to be Shakespeare from a
byr model but at least we see that the loss is improving and uh hopefully we're
expecting something a bit more reasonable okay so we're down at about 2.5 is let's see what we get okay
dramatic improvements certainly on what we had here so let me just increase the number of tokens okay so we see that
we're starting to get something at least like reasonable is
um certainly not shakes spear but uh the model is making progress so that is the
simplest possible model so now what I'd like to do
is obviously this is a very simple model because the tokens are not talking to each other so given the previous context
of whatever was generated we're only looking at the very last character to make the predictions about what comes next so now these uh now these tokens
have to start talking to each other and figuring out what is in the context so that they can make better predictions
for what comes next and this is how we're going to kick off the uh Transformer okay so next I took the code

å½“ç„¶å¯ä»¥ï¼æˆ‘æ¥å¸®ä½ è¯¦ç»†è®²è®²è¿™ä¸€æ®µã€è®­ç»ƒ bigram è¯­è¨€æ¨¡å‹ã€‘çš„å†…å®¹ï¼š

---

### ğŸ“Œ è¿™ä¸€æ®µåœ¨è®²ä»€ä¹ˆï¼Ÿ

ğŸ‘‰ **å¦‚ä½•ç”¨ä¼˜åŒ–å™¨ï¼ˆOptimizerï¼‰è®­ç»ƒ bigram è¯­è¨€æ¨¡å‹**ï¼Œè®©å®ƒçš„ loss ä¸‹é™ï¼Œæ¨¡å‹â€œå­¦ä¼šâ€æ›´å¥½åœ°é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚

---

### ğŸ“Œ ä¼˜åŒ–å™¨é€‰æ‹©

* åœ¨ä¹‹å‰çš„ `makemore` ç³»åˆ—é‡Œï¼Œä½œè€…åªç”¨è¿‡æœ€ç®€å•çš„ä¼˜åŒ–å™¨ **SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰**ï¼š

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
```

* ä½†è¿™æ¬¡ä½œè€…ç”¨çš„æ˜¯æ›´é«˜çº§çš„ **Adam** ä¼˜åŒ–å™¨ï¼š

```python
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
```

**Adam ä¼˜åŒ–å™¨**ï¼š

* æ˜¯ç›®å‰æ·±åº¦å­¦ä¹ é‡Œéå¸¸å¸¸ç”¨çš„ä¼˜åŒ–å™¨ï¼Œ
* æ”¶æ•›æ›´å¿«ï¼Œæ•ˆæœæ›´ç¨³å®šï¼Œ
* ä¸å®¹æ˜“å› ä¸ºå­¦ä¹ ç‡é€‰ä¸å¥½è€Œéœ‡è¡ã€‚

---

### ğŸ“Œ å­¦ä¹ ç‡ï¼ˆlearning rateï¼‰

* é€šå¸¸æ¥è¯´ï¼Œlearning rate å¯ä»¥è®¾æˆï¼š

```
3e-4 = 0.0003  
```

* ä½†æ˜¯ bigram æ¨¡å‹éå¸¸ç®€å•ï¼Œç½‘ç»œè§„æ¨¡å¾ˆå°ï¼Œæ‰€ä»¥å¯ä»¥ç”¨æ›´é«˜çš„å­¦ä¹ ç‡ï¼Œæ¯”å¦‚ `3` ç”šè‡³æ›´é«˜ï¼Œè®­ç»ƒæ›´å¿«æ”¶æ•›ã€‚

---

### ğŸ“Œ batch size

* åŸæ¥ batch\_size=4ï¼Œå¤ªå°äº†ï¼Œä½œè€…æ”¹æˆäº†ï¼š

```
batch_size = 32
```

* ç”¨å¤§ä¸€äº›çš„ batch\_sizeï¼Œè®­ç»ƒæ›´ç¨³å®šï¼Œæ¢¯åº¦ä¼°è®¡æ›´å‡†ç¡®ã€‚

---

### ğŸ“Œ è®­ç»ƒå¾ªç¯ï¼ˆTraining loopï¼‰

æ ¸å¿ƒæµç¨‹ï¼š

```text
å¾ªç¯ N æ¬¡ï¼š
    1. é‡‡æ ·ä¸€ä¸ªæ–°çš„ batch
    2. è®¡ç®— loss
    3. æ¸…é›¶ä¸Šä¸€æ­¥çš„æ¢¯åº¦
    4. åå‘ä¼ æ’­ï¼ˆbackpropagationï¼‰ï¼Œç®—å‡ºå½“å‰æ¢¯åº¦
    5. ç”¨ Adam ä¼˜åŒ–å™¨æ›´æ–°æ¨¡å‹å‚æ•°
```

---

### ğŸ“Œ è®­ç»ƒæ•ˆæœ

* åˆšå¼€å§‹ loss å¤§çº¦æ˜¯ 4.7
* è®­ç»ƒå‡ ç™¾æ­¥åï¼Œä¸‹é™åˆ° 4.5ã€4.0 â€¦
* è®­ç»ƒ 10000 æ­¥ä»¥åï¼Œloss å¤§çº¦é™åˆ° 2.5

---

### ğŸ“Œ ç”Ÿæˆæ•ˆæœ

* å¼€å§‹æ—¶ bigram æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æ˜¯åƒåœ¾ï¼ˆå…¨éšæœºï¼‰
* è®­ç»ƒä¸€æ®µæ—¶é—´åï¼Œè™½ç„¶è¿˜ä¸æ˜¯èå£«æ¯”äºšï¼Œä½†èƒ½ç”Ÿæˆâ€œçœ‹èµ·æ¥ç¨å¾®åˆç†â€çš„å­—ç¬¦åºåˆ—ï¼Œloss ä¹Ÿåœ¨å˜å°
* è¯´æ˜æ¨¡å‹å¼€å§‹å­¦ä¹ åˆ°äº† tiny Shakespeare æ•°æ®é›†é‡Œçš„ **å­—ç¬¦ co-occurrence è§„å¾‹**ï¼ˆå“ªäº›å­—ç¬¦ç»å¸¸è·Ÿå“ªäº›å­—ç¬¦æ­é…ï¼‰

---

### ğŸ“Œ ä¸ºä»€ä¹ˆ bigram æ¨¡å‹æ•ˆæœæœ‰é™ï¼Ÿ

* bigram è¯­è¨€æ¨¡å‹ **â€œåªç”¨å½“å‰ tokenâ€** é¢„æµ‹ä¸‹ä¸€ä¸ª token
* ä¸çœ‹å‰æ–‡ä¸Šä¸‹æ–‡ï¼Œé¢„æµ‹èƒ½åŠ›æœ‰é™
* æ‰€ä»¥ç”Ÿæˆçš„æ–‡æœ¬ **æ²¡æœ‰é•¿è·ç¦»çš„é€»è¾‘ã€è¯­æ³•ã€å¥æ³•ç»“æ„**
* ä½œè€…è¯´ï¼šâ€œtokens æ²¡æœ‰ç›¸äº’â€˜å¯¹è¯â€™ï¼Œæ¨¡å‹åªçœ‹æœ€åä¸€ä¸ªå­—ç¬¦ï¼Œæ— æ³•æ•æ‰ä¸Šä¸‹æ–‡â€

---

### ğŸ“Œ ä¸‹ä¸€æ­¥è®¡åˆ’

ä½œè€…å‡†å¤‡**å¼•å…¥ Transformer**ï¼š

* è®© tokens èƒ½äº’ç›¸â€œäº¤æµâ€ï¼Œ
* Transformer èƒ½åˆ©ç”¨æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œå­¦åˆ°æ›´å¤æ‚çš„ç»“æ„ â†’ é¢„æµ‹æ•ˆæœä¼šæ›´å¥½

---

### ğŸ“Œ æ€»ç»“ä¸€å¥è¯

è¿™ä¸€æ®µæ˜¯æ•™ä½ ï¼š

ğŸ‘‰ **å¦‚ä½•ç”¨ Adam ä¼˜åŒ–å™¨è®­ç»ƒ bigram è¯­è¨€æ¨¡å‹ï¼Œloss é€æ­¥ä¸‹é™ï¼Œç”Ÿæˆæ•ˆæœè¶Šæ¥è¶Šåˆç†**ï¼Œä½†å—é™äº bigram æœ¬èº«åªèƒ½çœ‹åˆ°å½“å‰ tokenï¼Œæ¨¡å‹èƒ½åŠ›æœ‰é™ â†’ ä¸‹ä¸€æ­¥æ”¹ç”¨ Transformerã€‚

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘å¯ä»¥ç»§ç»­è®²ï¼š

âœ… ä¸ºä»€ä¹ˆ bigram loss å¤§çº¦å¯ä»¥é™åˆ° 2.5ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆ batch size å¤§ä¼šè®­ç»ƒæ›´ç¨³å®šï¼Ÿ
âœ… ä¸ºä»€ä¹ˆ bigram ä¸è¶³ä»¥ç”Ÿæˆâ€œåƒèå£«æ¯”äºšâ€çš„æ–‡æœ¬ï¼Ÿ

è¦ä¸è¦æˆ‘å†è¡¥å……ï¼ŸğŸŒŸ

```
batch_size = 32
for steps in range(10000): # increase number of steps for good results...

    # sample a batch of data
    xb, yb = get_batch('train')

    # evaluate the loss
    logits, loss = m(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()
```
è§£é‡Šä¸€ä¸‹

è¿™æ®µä»£ç æ˜¯åœ¨ç”¨ PyTorch è®­ç»ƒä½ çš„è¯­è¨€æ¨¡å‹ï¼Œæˆ‘æŒ‰æ­¥éª¤è§£é‡Šï¼š

---

**1. batch\_size = 32**

* æ¯æ¬¡è®­ç»ƒæ—¶å– 32 æ¡æ ·æœ¬ä½œä¸ºä¸€ä¸ªæ‰¹æ¬¡ï¼ˆbatchï¼‰æ¥è®¡ç®—æ¢¯åº¦ã€‚
* å¥½å¤„æ˜¯æ¢¯åº¦ä¼°è®¡æ›´ç¨³å®šï¼Œæ¯”ä¸€æ¬¡åªç”¨ä¸€ä¸ªæ ·æœ¬ï¼ˆSGDï¼‰å™ªå£°å°ï¼Œåˆæ¯”ä¸€æ¬¡ç”¨æ•´ä¸ªæ•°æ®é›†ï¼ˆå…¨é‡æ¢¯åº¦ä¸‹é™ï¼‰æ•ˆç‡é«˜ã€‚

---

**2. for steps in range(10000):**

* è®­ç»ƒå¾ªç¯ï¼Œæ‰§è¡Œ 10,000 æ¬¡è¿­ä»£ï¼ˆstepï¼‰ã€‚
* æ¯ä¸€æ¬¡å¾ªç¯å°±æ˜¯ä¸€æ¬¡æ¢¯åº¦æ›´æ–°ï¼ˆparameter updateï¼‰ã€‚

---

**3. xb, yb = get\_batch('train')**

* ä»è®­ç»ƒé›†é‡Œéšæœºå–ä¸€ä¸ªæ‰¹æ¬¡ï¼ˆbatchï¼‰æ•°æ®ã€‚
* `xb`ï¼ˆinputsï¼‰ï¼šæ¨¡å‹çš„è¾“å…¥ï¼Œå½¢çŠ¶æ˜¯ `[batch_size, time_steps]`ã€‚
* `yb`ï¼ˆtargetsï¼‰ï¼šç›®æ ‡è¾“å‡ºï¼ˆground truthï¼‰ï¼Œä¹Ÿæ˜¯ `[batch_size, time_steps]`ã€‚
* è¿™ä¸ªå‡½æ•°é€šå¸¸ä¼šä»ä¸€ä¸ªå¾ˆé•¿çš„æ–‡æœ¬æ•´æ•°åºåˆ—ä¸­ï¼Œéšæœºæˆªå–ä¸€äº›è¿ç»­ç‰‡æ®µä½œä¸ºè®­ç»ƒæ ·æœ¬ã€‚

---

**4. logits, loss = m(xb, yb)**

* `m(...)` è°ƒç”¨æ¨¡å‹çš„ `forward` æ–¹æ³•ï¼Œè¿”å›é¢„æµ‹ç»“æœï¼ˆlogitsï¼‰å’Œå½“å‰æ‰¹æ¬¡çš„æŸå¤±ï¼ˆlossï¼‰ã€‚
* `logits`ï¼šé¢„æµ‹çš„æ¯ä¸ªæ—¶é—´æ­¥ã€æ¯ä¸ªè¯çš„å¾—åˆ†ï¼ˆ\[B, T, C]ï¼‰ã€‚
* `loss`ï¼šäº¤å‰ç†µæŸå¤±ï¼Œè¡¡é‡é¢„æµ‹å’Œç›®æ ‡çš„å·®è·ã€‚

---

**5. optimizer.zero\_grad(set\_to\_none=True)**

* æŠŠä¸Šä¸€æ¬¡è®¡ç®—çš„æ¢¯åº¦æ¸…é›¶ï¼Œé˜²æ­¢æ¢¯åº¦ç´¯ç§¯ã€‚
* `set_to_none=True` ä¼šæŠŠæ¢¯åº¦è®¾ä¸º `None` è€Œä¸æ˜¯ `0`ï¼Œå¯ä»¥å‡å°‘å†…å­˜å¼€é”€å’ŒåŠ å¿«è®¡ç®—ã€‚

---

**6. loss.backward()**

* åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰ã€‚
* PyTorch ä¼šæ ¹æ®æŸå¤±å€¼ï¼Œè‡ªåŠ¨è®¡ç®—æ¯ä¸ªæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ï¼ˆæ¢¯åº¦å‚¨å­˜åœ¨ `param.grad` ä¸­ï¼‰ã€‚

---

**7. optimizer.step()**

* æ ¹æ®å½“å‰æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
* å…·ä½“æ›´æ–°æ–¹å¼å–å†³äºä¼˜åŒ–å™¨ç±»å‹ï¼Œæ¯”å¦‚ SGD å°±æ˜¯ `param -= lr * grad`ï¼ŒAdam ä¼šç”¨æ›´å¤æ‚çš„è‡ªé€‚åº”è§„åˆ™æ›´æ–°ã€‚

---

**æ€»ç»“æµç¨‹**
æ¯ä¸€æ­¥è®­ç»ƒåšçš„äº‹æƒ…å°±æ˜¯ï¼š

1. **å–ä¸€æ‰¹æ•°æ®**ï¼ˆget\_batchï¼‰
2. **æ­£å‘ä¼ æ’­**ï¼ˆforward â†’ logits, lossï¼‰
3. **æ¢¯åº¦æ¸…é›¶**ï¼ˆzero\_gradï¼‰
4. **åå‘ä¼ æ’­**ï¼ˆbackwardï¼‰
5. **å‚æ•°æ›´æ–°**ï¼ˆstepï¼‰

ä¸æ–­é‡å¤è¿™ 5 æ­¥ï¼Œæ¨¡å‹çš„å‚æ•°ä¼šé€æ¸ä¼˜åŒ–ï¼Œloss ä¹Ÿä¼šé€æ¸ä¸‹é™ã€‚

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼  **PyTorch è®­ç»ƒå¾ªç¯çš„æµç¨‹å›¾**ï¼ŒæŠŠ `get_batch â†’ forward â†’ backward â†’ step` çš„æ•°æ®æµå’Œæ¢¯åº¦æµç›´è§‚è¡¨ç¤ºå‡ºæ¥ï¼Œè¿™æ ·æ›´å®¹æ˜“è®°ä½ã€‚
ä½ è¦æˆ‘ç”»å—ï¼Ÿ


# port our code to a script

that we developed in this juper notebook and I converted it to be a script and I'm doing this because I just want to
simplify our intermediate work into just the final product that we have at this point so in the top here I put all the
hyp parameters that we to find I introduced a few and I'm going to speak to that in a little bit otherwise a lot
of this should be recognizable uh reproducibility read data get the encoder and the decoder create the train
into splits uh use the uh kind of like data loader um that gets a batch of the
inputs and Targets this is new and I'll talk about it in a second now this is
the Byram language model that we developed and it can forward and give us a logits and loss and it can
generate and then here we are creating the optimizer and this is the training
Loop so everything here should look pretty familiar now some of the small things that I added number one I added
the ability to run on a GPU if you have it so if you have a GPU then you can this will use Cuda instead of just CPU
and everything will be a lot more faster now when device becomes Cuda then we need to make sure that when we load the
data we move it to device when we create the model we want to move uh the model parameters to
device so as an example here we have the N an embedding table and it's got a
weight inside it which stores the uh sort of lookup table so so that would be moved to the GPU so that all the
calculations here happen on the GPU and they can be a lot faster and then finally here when I'm creating the
context that feeds in to generate I have to make sure that I create it on the device number two what I introduced is
uh the fact that here in the training Loop here I was just printing the um l.
item inside the training Loop but this is a very noisy measurement of the current loss because every batch will be
more or less lucky and so what I want to do usually um is uh I have an estimate
loss function and the estimate loss basically then um goes up here and it
averages up the loss over multiple batches so in particular we're going to iterate eval iter times and we're going
to basically get our loss and then we're going to get the average loss for both splits and so this will be a lot less
noisy so here when we call the estimate loss we're we're going to report the uh pretty accurate train and validation
loss now when we come back up you'll notice a few things here I'm setting the model to evaluation phase and down here
I'm resetting it back to training phase now right now for our model as is this doesn't actually do anything because the
only thing inside this model is this uh nn. embedding and um this this um
Network would behave both would behave the same in both evaluation mode and training mode we have no drop off layers
we have no batm layers Etc but it is a good practice to Think Through what mode your neural network is in because some
layers will have different Behavior Uh at inference time or training time and
there's also this context manager torch up nograd and this is just telling pytorch that everything that happens
inside this function we will not call do backward on and so pytorch can be a lot
more efficient with its memory use because it doesn't have to store all the intermediate variables uh because we're
never going to call backward and so it can it can be a lot more memory efficient in that way so also a good
practice to tpy torch when we don't intend to do back propagation so right now this script is
about 120 lines of code of and that's kind of our starter code I'm calling it
b.p and I'm going to release it later now running this script gives us output in the terminal
and it looks something like this it basically as I ran this code uh it was
giving me the train loss and Val loss and we see that we convert to somewhere around 2.5 with the pyr model and then here's
the sample that we produced at the end and so we have everything packaged up in the script and we're in a good
position now to iterate on this okay so we are almost ready to start writing our very first self attention block for

å½“ç„¶å¯ä»¥ï¼Œæˆ‘æ¥å¸®ä½ è¯¦ç»†è§£é‡Šè¿™ä¸€æ®µã€æŠŠ bigram è®­ç»ƒä»£ç è½¬æˆè„šæœ¬ã€‘çš„å†…å®¹ï¼š

---

### ğŸ“Œ ä¸»è¦å†…å®¹

ğŸ‘‰ **æŠŠä¹‹å‰åœ¨ Jupyter Notebook é‡Œå†™çš„ä»£ç ï¼Œæ•´ç†æˆä¸€ä¸ª Python è„šæœ¬æ–‡ä»¶**ï¼ˆæ¯”å¦‚ `bigram.py`ï¼‰ï¼Œæ–¹ä¾¿ä»¥åå¤ç”¨ã€æ‰©å±•ã€ä¼˜åŒ–ã€‚

---

### ğŸ“Œ ä¸ºä»€ä¹ˆè¦è½¬æˆè„šæœ¬ï¼Ÿ

* Notebook é‡Œä»£ç æ˜¯ä¸€æ­¥ä¸€æ­¥å†™çš„ï¼Œé€‚åˆæ¼”ç¤ºå’Œè°ƒè¯•
* è„šæœ¬ï¼ˆ.py æ–‡ä»¶ï¼‰æ›´é€‚åˆåš**å®Œæ•´çš„è®­ç»ƒæµç¨‹**ï¼Œå¯ä»¥æ–¹ä¾¿åœ¨å‘½ä»¤è¡Œè·‘ã€å¤ç°ç»“æœ
* ä½œä¸ºâ€œé˜¶æ®µæ€§ç‰ˆæœ¬â€ï¼ŒæŠŠç›®å‰å†™å¥½çš„ bigram æ¨¡å‹æ”¶æ‹¾å¹²å‡€ï¼Œå‡†å¤‡ç»§ç»­æ‰©å±•ï¼ˆæ¯”å¦‚åé¢åŠ  self-attentionï¼‰

---

### ğŸ“Œ è„šæœ¬å†…å®¹ç»“æ„

---

#### 1ï¸âƒ£ è¶…å‚æ•°

```python
# æŠŠè¶…å‚æ•°é›†ä¸­å†™åœ¨è„šæœ¬æœ€ä¸Šæ–¹
# ä¾‹å¦‚ batch_size, block_size, learning_rate, max_iters, etc.
```

---

#### 2ï¸âƒ£ æ•°æ®å¤„ç†

* è¯»æ•°æ®
* ç”Ÿæˆ encode/decode å‡½æ•°
* åˆ’åˆ† train / val æ•°æ®
* data\_loaderï¼šå®ç° get\_batch()ï¼Œæ¯æ¬¡ä»æ•°æ®é‡Œå– batch

---

#### 3ï¸âƒ£ bigram æ¨¡å‹

```python
# ç›´æ¥ç”¨æˆ‘ä»¬ä¹‹å‰å†™çš„ BigramLanguageModel
# åŒ…æ‹¬ forward() å’Œ generate() ä¸¤ä¸ªæ–¹æ³•
```

---

#### 4ï¸âƒ£ ä¼˜åŒ–å™¨

```python
# é€‰ç”¨ Adam ä¼˜åŒ–å™¨
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
```

---

#### 5ï¸âƒ£ è®­ç»ƒå¾ªç¯ï¼ˆtraining loopï¼‰

```text
for step in range(max_iters):
    1. get_batch
    2. forward pass
    3. compute loss
    4. backward pass
    5. optimizer.step()
```

---

#### 6ï¸âƒ£ æ–°å¢å†…å®¹ï¼šæ”¯æŒ GPU è®­ç»ƒ

```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
```

* è‡ªåŠ¨æ£€æµ‹æœ‰æ²¡æœ‰ GPUï¼Œæœ‰çš„è¯ç”¨ GPUï¼Œå¯ä»¥å¿«å¾ˆå¤š
* éœ€è¦æŠŠï¼š

  * æ•°æ® tensor `.to(device)`
  * æ¨¡å‹ `.to(device)`
  * ç”Ÿæˆçš„æ—¶å€™çš„ context tensor ä¹Ÿè¦ `.to(device)`

---

#### 7ï¸âƒ£ æ–°å¢å†…å®¹ï¼šestimate\_loss å‡½æ•°

* åŸæ¥è®­ç»ƒ loop é‡Œç›´æ¥æ‰“å° lossï¼Œ**å¤ª noisyï¼ˆæ³¢åŠ¨å¤§ï¼‰**
* ç°åœ¨å®šä¹‰ä¸€ä¸ª `estimate_loss()` å‡½æ•°ï¼Œåšå¤šæ¬¡ batch æ±‚å¹³å‡ï¼Œç»“æœæ›´å¹³æ»‘ã€æ›´å‡†ç¡®
* æ¯éš”å‡ æ­¥æ‰“å°å¹³å‡çš„ train\_loss å’Œ val\_loss

---

#### 8ï¸âƒ£ è®­ç»ƒ / æ¨ç†æ¨¡å¼åˆ‡æ¢

```python
model.eval()  # è¯„ä¼°æ¨¡å¼
model.train()  # è®­ç»ƒæ¨¡å¼
```

* ç›®å‰ bigram æ¨¡å‹é‡Œç”¨ä¸åˆ°ï¼ˆæ²¡æœ‰ dropout / batchnormï¼‰
* ä½†æ˜¯**å…»æˆå¥½ä¹ æƒ¯**ï¼Œå› ä¸ºåé¢ Transformer é‡Œéœ€è¦åˆ‡æ¢æ¨¡å¼

---

#### 9ï¸âƒ£ with torch.no\_grad()

```python
with torch.no_grad():
```

* ç”Ÿæˆæ–‡æœ¬æ—¶ä¸ä¼šç”¨åˆ°åå‘ä¼ æ’­
* ç”¨ `no_grad` å¯ä»¥èŠ‚çœå†…å­˜ï¼ŒåŠ å¿«æ¨ç†é€Ÿåº¦
* ä¹Ÿæ˜¯è‰¯å¥½çš„ PyTorch ä¹ æƒ¯

---

### ğŸ“Œ è¿è¡Œè„šæœ¬çš„æ•ˆæœ

* è„šæœ¬å¤§çº¦ 120 è¡Œå·¦å³
* è¿è¡Œåï¼Œæ§åˆ¶å°ä¼šçœ‹åˆ°ï¼š

```text
step 1000: train loss 3.2, val loss 3.1
step 2000: train loss 2.7, val loss 2.6
step 3000: train loss 2.5, val loss 2.5
...
```

* æœ€ç»ˆ loss å¤§çº¦é™åˆ° 2.5ï¼ˆbigram æ¨¡å‹æé™ï¼‰
* æœ«å°¾ä¼šç”Ÿæˆä¸€æ®µæ–‡æœ¬ï¼Œæ˜æ˜¾æ¯”åˆå§‹æœªè®­ç»ƒæ—¶çš„â€œä¹±ç â€æ›´åˆç†

---

### ğŸ“Œ æœ€ç»ˆæ€»ç»“

è¿™ä¸€æ®µè®²çš„æ˜¯ï¼š

ğŸ‘‰ **æŠŠ bigram è®­ç»ƒè¿‡ç¨‹å°è£…æˆå¹²å‡€çš„ Python è„šæœ¬ bigram.py**
ğŸ‘‰ æ”¯æŒ GPU
ğŸ‘‰ æ”¯æŒå¹³æ»‘è¯„ä¼° loss
ğŸ‘‰ è®­ç»ƒåˆ° 2.5 loss å·¦å³ï¼Œå‡†å¤‡è¿›å…¥ä¸‹ä¸€æ­¥ â€”â€” **å®ç° self-attention blockï¼Œå‡çº§ä¸º Transformerï¼**

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘å¯ä»¥ç»§ç»­è®²è®²ï¼š

âœ… ä¸ºä»€ä¹ˆåŠ äº† estimate\_lossï¼Œæ•ˆæœæ›´å¥½ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆæ¨¡å‹ eval() / train() å¾ˆé‡è¦ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆ bigram loss æé™å¤§æ¦‚åœ¨ 2.5ï¼Ÿ

è¦ä¸è¦æˆ‘å†è¡¥å……ï¼ŸğŸŒŸ

```
@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out
```

è§£é‡Šä¸€ä¸‹

æˆ‘ç»™ä½ åˆ†æ­¥éª¤è§£é‡Šä¸€ä¸‹è¿™æ®µ `estimate_loss()` å‡½æ•°åœ¨åšä»€ä¹ˆï¼š

---

### 1. `@torch.no_grad()`

* è¿™æ˜¯ **PyTorch çš„è£…é¥°å™¨**ï¼Œä½œç”¨æ˜¯å‘Šè¯‰ PyTorch **åœ¨è¿™ä¸ªå‡½æ•°é‡Œä¸è¦è®¡ç®—æ¢¯åº¦**ã€‚
* å¥½å¤„ï¼š

  1. èŠ‚çœæ˜¾å­˜ï¼ˆä¸å­˜æ¢¯åº¦è®¡ç®—ä¸­é—´ç»“æœï¼‰
  2. æé«˜æ¨ç†é€Ÿåº¦
* å› ä¸ºè¿™é‡Œæ˜¯åœ¨åšè¯„ä¼°ï¼ˆestimate lossï¼‰ï¼Œä¸éœ€è¦æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œæ‰€ä»¥å…³é—­æ¢¯åº¦è®¡ç®—æ˜¯åˆç†çš„ã€‚

---

### 2. `out = {}`

* å‡†å¤‡ä¸€ä¸ªç©ºå­—å…¸ï¼Œæœ€åä¼šå­˜å‚¨è®­ç»ƒé›†ï¼ˆtrainï¼‰å’ŒéªŒè¯é›†ï¼ˆvalï¼‰çš„å¹³å‡æŸå¤±å€¼ã€‚

---

### 3. `model.eval()`

* å°†æ¨¡å‹åˆ‡æ¢åˆ° **è¯„ä¼°æ¨¡å¼ï¼ˆevaluation modeï¼‰**ã€‚
* è¿™æ ·ä¼šå…³é—­ Dropoutã€BatchNorm ç­‰è®­ç»ƒæ—¶çš„éšæœºè¡Œä¸ºï¼Œè®©è¯„ä¼°ç»“æœç¨³å®šã€‚
* è¿™ä¸ªæ¨¡å¼ä¼šä¸€ç›´ä¿æŒï¼Œç›´åˆ°ä½ æ‰‹åŠ¨è°ƒç”¨ `model.train()`ã€‚

---

### 4. `for split in ['train', 'val']:`

* å¾ªç¯ä¸¤æ¬¡ï¼Œåˆ†åˆ«è®¡ç®— **è®­ç»ƒé›†** å’Œ **éªŒè¯é›†** çš„å¹³å‡ lossã€‚
* `split` ä»£è¡¨å½“å‰å¤„ç†çš„æ•°æ®é›†ç±»å‹ã€‚

---

### 5. `losses = torch.zeros(eval_iters)`

* åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸º `eval_iters` çš„å¼ é‡ï¼Œç”¨æ¥ä¿å­˜å¤šæ¬¡è¯„ä¼°å¾—åˆ°çš„ loss å€¼ã€‚
* å¤šæ¬¡è¯„ä¼°æ˜¯ä¸ºäº†å‡å°‘éšæœºæ€§å¸¦æ¥çš„æ³¢åŠ¨ï¼Œå¾—åˆ°æ›´ç¨³å®šçš„å¹³å‡å€¼ã€‚

---

### 6. `for k in range(eval_iters):`

* å¾ªç¯ `eval_iters` æ¬¡ï¼ˆæ¯”å¦‚ 100 æ¬¡ï¼‰ï¼Œæ¯æ¬¡å–ä¸€ä¸ª batch æ¥è®¡ç®— lossã€‚

---

### 7. `X, Y = get_batch(split)`

* ä»å½“å‰ `split`ï¼ˆtrain æˆ– valï¼‰æ•°æ®é›†ä¸­éšæœºå–ä¸€ä¸ªæ‰¹æ¬¡ï¼ˆbatchï¼‰ã€‚
* `X`ï¼šè¾“å…¥åºåˆ—ï¼ˆtoken IDï¼‰ï¼Œå½¢çŠ¶ `[B, T]`
* `Y`ï¼šç›®æ ‡åºåˆ—ï¼ˆé¢„æµ‹ç›®æ ‡ï¼‰ï¼Œå½¢çŠ¶ `[B, T]`

---

### 8. `logits, loss = model(X, Y)`

* è°ƒç”¨æ¨¡å‹çš„ `forward` æ–¹æ³•ï¼Œå¾—åˆ°é¢„æµ‹ç»“æœï¼ˆlogitsï¼‰å’Œè¯¥æ‰¹æ¬¡çš„æŸå¤±ï¼ˆlossï¼‰ã€‚

---

### 9. `losses[k] = loss.item()`

* `loss.item()`ï¼šä»å¼ é‡ä¸­å–å‡º Python æ ‡é‡ï¼ˆfloatï¼‰ã€‚
* æŠŠè¿™ä¸ª loss å­˜åˆ° `losses[k]` ä¸­ã€‚

---

### 10. `out[split] = losses.mean()`

* è®¡ç®—æ‰€æœ‰ `eval_iters` æ¬¡ loss çš„å¹³å‡å€¼ã€‚
* å­˜å…¥ `out` å­—å…¸ï¼Œæ¯”å¦‚ï¼š

  ```python
  out['train'] = 1.89
  out['val'] = 2.15
  ```

---

### 11. `model.train()`

* åˆ‡å›è®­ç»ƒæ¨¡å¼ï¼ˆtraining modeï¼‰ï¼Œè¿™æ ·åç»­è®­ç»ƒå¯ä»¥æ­£å¸¸ä½¿ç”¨ Dropoutã€BatchNorm ç­‰åŠŸèƒ½ã€‚

---

### 12. `return out`

* è¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„å¹³å‡æŸå¤±å€¼ã€‚

---

### **æ€»ç»“ä½œç”¨**

`estimate_loss()` çš„åŠŸèƒ½å°±æ˜¯ **åœ¨ä¸è®¡ç®—æ¢¯åº¦çš„æƒ…å†µä¸‹ï¼Œç”¨å¤šä¸ª batch ä¼°è®¡å½“å‰æ¨¡å‹åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„å¹³å‡æŸå¤±**ï¼Œå¹¶ä¸”ä¿è¯è¯„ä¼°è¿‡ç¨‹ä¸ä¼šå½±å“åç»­è®­ç»ƒã€‚
å®ƒå¸¸è¢«ç”¨æ¥ **å‘¨æœŸæ€§ç›‘æ§æ¨¡å‹æ•ˆæœ**ï¼Œæ¯”å¦‚æ¯éš” 500 æ­¥æ‰“å°ä¸€æ¬¡ `{train_loss, val_loss}`ã€‚

---

å¦‚æœä½ éœ€è¦çš„è¯ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼  **`estimate_loss()` çš„æµç¨‹å›¾**ï¼Œç›´è§‚å±•ç¤º `train/val â†’ å¤šæ¬¡ batch è¯„ä¼° â†’ å¹³å‡ â†’ åˆ‡å›è®­ç»ƒæ¨¡å¼` çš„è¿‡ç¨‹ï¼Œè¿™æ ·ä½ ä¸€çœ¼å°±èƒ½è®°ä½å®ƒçš„é€»è¾‘ã€‚
ä½ è¦æˆ‘ç”»å—ï¼Ÿ



# version 1: averaging past context with for loops, the weakest form of aggregation

processing these uh tokens now before we actually get there I want to get you
used to a mathematical trick that is used in the self attention inside a Transformer and is really just like at
the heart of an an efficient implementation of self attention and so I want to work with this toy example to
just get you used to this operation and then it's going to make it much more clear once we actually get to um to it
uh in the script again so let's create a b BYT by C where BT and C are just 48 and two in the toy
example and these are basically channels and we have uh batches and we have the
time component and we have information at each point in the sequence so
see now what we would like to do is we would like these um tokens so we have up
to eight tokens here in a batch and these eight tokens are currently not talking to each other and we would like
them to talk to each other we'd like to couple them and in particular we don't
we we want to couple them in a very specific way so the token for example at the fifth location it should not
communicate with tokens in the sixth seventh and eighth location because uh those are future tokens in
the sequence the token on the fifth location should only talk to the one in the fourth third second and first so
it's only so information only flows from previous context to the current time step and we cannot get any information
from the future because we are about to try to predict the future so what is the easiest way for
tokens to communicate okay the easiest way I would say is okay if we're up to
if we're a fifth token and I'd like to communicate with my past the simplest way we can do that is to just do a
weight is to just do an average of all the um of all the preceding elements so
for example if I'm the fif token I would like to take the channels uh that make up that are information at my step but
then also the channels from the fourth step third step second step and the first step I'd like to average those up
and then that would become sort of like a feature Vector that summarizes me in the context of my history now of course
just doing a sum or like an average is an extremely weak form of interaction like this communication is uh extremely
lossy we've lost a ton of information about the spatial Arrangements of all those tokens uh but that's okay for now
we'll see how we can bring that information back later for now what we would like to do is for every single
batch element independently for every teeth token in that sequence we'd like
to now calculate the average of all the vectors in all the previous tokens and
also at this token so let's write that out um I have a small snippet here and
instead of just fumbling around let me just copy paste it and talk to it so in other words we're going to
create X and B is short for bag of words because bag of words is um is kind of
like um a term that people use when you are just averaging up things so this is just a bag of words basically there's a
word stored on every one of these eight locations and we're doing a bag of words we're just averaging
so in the beginning we're going to say that it's just initialized at Zero and then I'm doing a for Loop here so we're not being efficient yet that's coming
but for now we're just iterating over all the batch Dimensions independently iterating over time and then the
previous uh tokens are at this uh batch Dimension and then everything up to and
including the teeth token okay so when we slice out X in this way X prev
Becomes of shape um how many T elements there were in the past and then of
course C so all the two-dimensional information from these little tokens so
that's the previous uh sort of chunk of um tokens from my current sequence and
then I'm just doing the average or the mean over the zero Dimension so I'm averaging out the time here and I'm just
going to get a little c one dimensional Vector which I'm going to store in X bag of words so I can run this and and uh
this is not going to be very informative because let's see so this is X of Zer so
this is the zeroth batch element and then expo at zero now you see how the at
the first location here you see that the two are equal and that's because it's we're just doing an average of this one
token but here this one is now an average of these two and now this one is
an average of these three and so on so uh and this last one is the average
of all of these elements so vertical average just averaging up all the tokens now gives this outcome
here so this is all well and good uh but this is very inefficient now the trick

å½“ç„¶å¯ä»¥ï¼Œæˆ‘æ¥å¸®ä½ è¯¦ç»†æ‹†è§£è¿™ä¸€æ®µå†…å®¹ï¼š

---

### ğŸ“Œ è¿™ä¸€æ®µåœ¨è®²ä»€ä¹ˆï¼Ÿ

ğŸ‘‰ **åœ¨æ­£å¼å†™ self-attention ä¹‹å‰ï¼Œå…ˆé€šè¿‡ä¸€ä¸ªç®€å•ä¾‹å­ç†è§£ self-attention èƒŒåçš„â€œçŸ©é˜µæŠ€å·§â€**
ğŸ‘‰ ç›®æ ‡æ˜¯è®©ä½ å…ˆä¹ æƒ¯â€œåºåˆ—ä¸­çš„ token å¦‚ä½•ç›¸äº’é€šä¿¡â€è¿™ä¸ªæ€æƒ³ã€‚

---

### ğŸ“Œ ä¸ºä»€ä¹ˆè¦åšè¿™ä¸ªä¾‹å­ï¼Ÿ

* å½“å‰æˆ‘ä»¬è®­ç»ƒçš„ bigram æ¨¡å‹é‡Œ **token ä¹‹é—´æ˜¯ç‹¬ç«‹çš„**ï¼Œäº’ç›¸ä¹‹é—´â€œçœ‹ä¸åˆ°å¯¹æ–¹â€ã€‚
* æƒ³è¦æ›´å¼ºå¤§çš„æ¨¡å‹ï¼Œtoken ä¹‹é—´å¿…é¡»èƒ½â€œäº¤æµâ€ â€”â€” å½“å‰ token è¦èƒ½â€œçœ‹åˆ°â€å‰é¢çš„ tokenï¼Œæ‰æ›´å®¹æ˜“é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚
* Transformer ä¸­çš„ self-attention å°±æ˜¯å®ç°è¿™ç§â€œäº¤æµâ€çš„æœºåˆ¶ã€‚
* ä½† self-attention é‡Œé¢çš„å®ç°æŠ€å·§ï¼Œå¾ˆå¤šæ˜¯ç”¨çŸ©é˜µè¿ç®—å†™çš„ï¼Œå¯èƒ½ä¸€å¼€å§‹ä¸ç›´è§‚ã€‚
* æ‰€ä»¥ä½œè€…ç”¨è¿™ä¸ª**for å¾ªç¯ç‰ˆæœ¬**çš„ä¾‹å­ï¼Œå…ˆå¸¦ä½ ç†è§£â€œtoken å¦‚ä½•èšåˆå‰é¢ä¿¡æ¯â€è¿™ä¸ªæ€æƒ³ï¼Œä¹‹åå†ä¼˜åŒ–æˆçŸ©é˜µä¹˜æ³•ã€‚

---

### ğŸ“Œ å°ä¾‹å­è®¾ç½®

```
B = 4  # batch size
T = 8  # sequence length
C = 2  # channels / embedding ç»´åº¦
```

åˆ›å»ºä¸€ä¸ª (B, T, C) çš„å¼ é‡ `x`ï¼Œå¯ä»¥ç†è§£ä¸ºï¼š

```
batch é‡Œæœ‰ B æ¡åºåˆ—  
æ¯æ¡åºåˆ—é•¿åº¦æ˜¯ Tï¼ˆæœ‰ T ä¸ª tokenï¼‰  
æ¯ä¸ª token ç”¨ C ç»´å‘é‡è¡¨ç¤º  
```

---

### ğŸ“Œ æˆ‘ä»¬è¦åšä»€ä¹ˆï¼Ÿ

å¯¹äº **åºåˆ—ä¸­çš„ç¬¬ t ä¸ª token**ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒèƒ½â€œçœ‹è§â€å‰é¢çš„ tokenï¼Œ
å…·ä½“åšæ³•ï¼š

* å¯¹ç¬¬ t ä¸ª tokenï¼Œå–å‡ºå®ƒå‰é¢æ‰€æœ‰ tokenï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰
* **æŠŠè¿™äº› token çš„å‘é‡åšâ€œå¹³å‡â€**ï¼Œä½œä¸ºæ–°çš„ç‰¹å¾å‘é‡

æ¢å¥è¯è¯´ï¼š

```
ç¬¬5ä¸ª token â†’ å¹³å‡ [ç¬¬1,2,3,4,5ä¸ª token çš„ embedding å‘é‡]  
```

---

### ğŸ“Œ ä¸ºä»€ä¹ˆè¦åšå¹³å‡ï¼Ÿ

* å¹³å‡æ˜¯ä¸€ç§**æœ€ç®€å•çš„â€œä¿¡æ¯èšåˆâ€æ–¹æ³•**
* å½“ç„¶ï¼Œå¹³å‡æ˜¯å¾ˆå¼±çš„ï¼ˆä¿¡æ¯æŸå¤±å¾ˆå¤§ï¼‰ï¼Œä½†è¿™ä¸ªä¾‹å­åªæ˜¯ä¸ºäº†å¸®åŠ©ç†è§£
* ä¹‹åæˆ‘ä»¬ä¼šç”¨ self-attention å­¦å‡ºæ›´èªæ˜çš„èšåˆæ–¹å¼ï¼ˆåŠ æƒæ±‚å’Œï¼Œæƒé‡ç”±æ¨¡å‹å­¦å‡ºæ¥ï¼‰

---

### ğŸ“Œ ä¸ºä»€ä¹ˆä¸èƒ½çœ‹æœªæ¥ tokenï¼Ÿ

* å› ä¸ºè¯­è¨€æ¨¡å‹æ˜¯â€œè‡ªå›å½’â€çš„ï¼š
  â†’ åªèƒ½ç”¨â€œå·²æœ‰çš„ä¸Šä¸‹æ–‡â€é¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼Œä¸èƒ½å·çœ‹â€œæœªæ¥â€
* æ‰€ä»¥åªèšåˆ **å½“å‰ä½ç½®ä¹‹å‰çš„ token**ï¼Œä¸èƒ½ç”¨æœªæ¥ token

---

### ğŸ“Œ for å¾ªç¯ç‰ˆå®ç°æ€è·¯

å¯¹ batch é‡Œçš„æ¯ä¸€æ¡åºåˆ—ï¼Œåšï¼š

```
for b in range(B):
    for t in range(T):
        x_prev = x[b, :t+1, :]  # å–å‡ºå‰ t ä¸ª tokenï¼ˆå«å½“å‰ï¼‰
        avg = x_prev.mean(dim=0)  # å¯¹æ—¶é—´ç»´åšå¹³å‡
        x_bagofwords[b, t] = avg  # å­˜åˆ°æ–° tensor
```

---

### ğŸ“Œ ä¸¾ä¾‹

å¦‚æœæŸä¸€æ¡åºåˆ—æ˜¯ï¼š

```
token 1: [0.2, 0.4]  
token 2: [0.6, 0.3]  
token 3: [0.1, 0.5]  
token 4: [0.9, 0.8]  
```

é‚£ä¹ˆå¯¹äº token 4ï¼Œæ–°çš„ç‰¹å¾æ˜¯ï¼š

```
avg( token 1, 2, 3, 4 )  
= ( [0.2,0.4] + [0.6,0.3] + [0.1,0.5] + [0.9,0.8] ) / 4
```

---

### ğŸ“Œ è§‚å¯Ÿç»“æœ

ä½œè€…æ‰“å°å‡ºæ¥ï¼š

```
x_bagofwords[b, t]
```

å¯ä»¥çœ‹åˆ°ï¼š

```
t=0 â†’ åªå¹³å‡è‡ªå·±  
t=1 â†’ å¹³å‡ å‰2ä¸ª token  
t=2 â†’ å¹³å‡ å‰3ä¸ª token  
t=3 â†’ å¹³å‡ å‰4ä¸ª token  
...
```

---

### ğŸ“Œ ç›®å‰çš„é—®é¢˜

* ç”¨ for å¾ªç¯å®ç°æ•ˆç‡éå¸¸ä½ï¼ˆæ…¢ï¼‰
* Transformer é‡Œä¼šç”¨ **çŸ©é˜µä¹˜æ³•** + **mask** æ¥å®ç°è¿™ä¸ªè¿‡ç¨‹ï¼Œé«˜æ•ˆä¸”èƒ½ç”¨ GPU æ‰§è¡Œ

---

### ğŸ“Œ å°ç»“ä¸€å¥è¯

ğŸ‘‰ è¿™ä¸ª toy ä¾‹å­æ˜¯å¸®ä½ ç†è§£ï¼š

**â€œå¦‚ä½•æŠŠå½“å‰ token å’Œå‰é¢çš„ token ä¿¡æ¯èšåˆåœ¨ä¸€èµ·â€**

âœ… ç”¨å¹³å‡æ˜¯æœ€å¼±ç‰ˆæœ¬
âœ… Self-attention åé¢ä¼šå®ç°æ›´èªæ˜çš„åŠ æƒæ±‚å’Œ
âœ… åªèƒ½çœ‹è¿‡å»ï¼Œä¸èƒ½çœ‹æœªæ¥

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘å¯ä»¥ç»§ç»­è®²è®²ï¼š

âœ… è¿™ä¸ªâ€œå¹³å‡â€æ“ä½œï¼Œå’Œ self-attention çš„åŠ æƒå’Œæœ‰å•¥åŒºåˆ«ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆ self-attention é‡Œç”¨â€œçŸ©é˜µä¹˜æ³•â€èƒ½é«˜æ•ˆå®ç°ï¼Ÿ
âœ… ä¸ºå•¥è¦ mask future tokensï¼Ÿ

è¦ä¸è¦æˆ‘å†å¸®ä½ ç»†è®²ä¸€ä¸‹ï¼ŸğŸŒŸ

```python
# consider the following toy example:

torch.manual_seed(1337)
B,T,C = 4,8,2 # batch, time, channels
x = torch.randn(B,T,C)
x.shape
```

ä¸ºä»€ä¹ˆB,T,Cå«åšbatch, time, channels

å¥½é—®é¢˜ï¼æˆ‘æ¥å¸®ä½ è§£é‡Šä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆè¿™é‡Œçš„ **B, T, C** å«åšï¼š

```text
B = batch  
T = time  
C = channels
```

---

### ğŸ“Œ 1ï¸âƒ£ B = batch

```
B = 4
```

ğŸ‘‰ **batch size**ï¼Œè¡¨ç¤ºâ€œåŒæ—¶å¤„ç†å¤šå°‘æ¡æ•°æ®â€ã€‚

* åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œä¸ºäº†æé«˜æ•ˆç‡ï¼Œé€šå¸¸æ˜¯â€œ**æ‰¹é‡å¤„ç†**â€æ•°æ®ï¼Œè€Œä¸æ˜¯ä¸€æ¡ä¸€æ¡å¤„ç†ã€‚
* è¿™ä¸ªæ‰¹é‡çš„å¤§å°å°±å« **batch size**ï¼Œé€šå¸¸ç”¨ `B` è¡¨ç¤ºã€‚

ä¾‹å­ï¼š
å¦‚æœ B=4ï¼Œè¯´æ˜è¿™æ¬¡ forward å¤„ç† 4 æ¡æ•°æ®ï¼ˆ4 æ¡åºåˆ—ï¼‰ã€‚

---

### ğŸ“Œ 2ï¸âƒ£ T = time

```
T = 8
```

ğŸ‘‰ **time steps**ï¼Œåœ¨è¿™é‡Œç›¸å½“äºâ€œåºåˆ—é•¿åº¦â€æˆ–è€…â€œtoken ä¸ªæ•°â€ã€‚

* å› ä¸ºæˆ‘ä»¬åœ¨åš **è¯­è¨€æ¨¡å‹**ï¼Œè¾“å…¥æ•°æ®æ˜¯ä¸€æ®µæ–‡æœ¬ï¼ˆæˆ–è€… token åºåˆ—ï¼‰ã€‚
* è¯­è¨€æ¨¡å‹æ˜¯ **è‡ªå›å½’çš„**ï¼ŒæŒ‰ token åºåˆ—ä»å·¦åˆ°å³é¢„æµ‹ã€‚
* æ‰€ä»¥å¯ä»¥æŠŠ token åºåˆ—çš„é•¿åº¦çœ‹æˆ **æ—¶é—´è½´**ï¼Œt=0 è¡¨ç¤ºç¬¬ä¸€ä¸ª tokenï¼Œt=1 ç¬¬äºŒä¸ª tokenï¼Œâ€¦â€¦ t=T-1 è¡¨ç¤ºæœ€åä¸€ä¸ª tokenã€‚
* æ‰€ä»¥ T é€šå¸¸å« **time steps**ï¼Œä¹Ÿå¯ä»¥ç†è§£æˆ **sequence length**ï¼ˆåºåˆ—é•¿åº¦ï¼‰ã€‚

ä¾‹å­ï¼š
T=8 â†’ æ¯æ¡åºåˆ—é‡Œæœ‰ 8 ä¸ª tokenã€‚

---

### ğŸ“Œ 3ï¸âƒ£ C = channels

```
C = 2
```

ğŸ‘‰ **channels**ï¼Œä¹Ÿå« **embedding ç»´åº¦**ï¼Œæ¯ä¸ª token è¢«è¡¨ç¤ºæˆå¤šå°‘ç»´çš„å‘é‡ã€‚

* è¯­è¨€æ¨¡å‹é‡Œï¼Œæ¯ä¸ª token éƒ½ä¼šç”¨ä¸€ä¸ªå‘é‡æ¥è¡¨ç¤ºè¿™ä¸ª token çš„â€œæ„ä¹‰â€ã€‚
* è¿™ä¸ªå‘é‡æœ‰å¤šå°‘ç»´å‘¢ï¼Ÿå°±æ˜¯ C ç»´ã€‚
* è¿™å…¶å®ç±»ä¼¼å›¾åƒå·ç§¯é‡Œâ€œé€šé“ï¼ˆchannelsï¼‰â€çš„æ¦‚å¿µï¼Œæ‰€ä»¥ä¸€èˆ¬ä¹Ÿå« **channels**ã€‚
* åœ¨ NLP é‡Œï¼Œä¹Ÿå« **embedding dim**ã€‚

ä¾‹å­ï¼š
C=2 â†’ æ¯ä¸ª token è¢«è¡¨ç¤ºæˆä¸€ä¸ª 2 ç»´å‘é‡ã€‚

å½“ç„¶ï¼Œå®é™…æ¨¡å‹é‡Œ C ä¸€èˆ¬ä¼šå¤§å¾ˆå¤šï¼Œæ¯”å¦‚ 64ã€128ã€512â€¦â€¦

---

### ğŸ“Œ ç»¼åˆè§£é‡Š

æ‰€ä»¥ï¼š

```python
x = torch.randn(B, T, C)
```

å®é™…ä¸Šå°±æ˜¯åˆ›å»ºäº†ä¸€ä¸ªï¼š

```
( batch size = 4, sequence length = 8, embedding dim = 2 )
```

çš„ 3 ç»´å¼ é‡ï¼š

```
(4, 8, 2)
```

å¯ä»¥ç†è§£æˆï¼š

```
batch é‡Œæœ‰ 4 æ¡åºåˆ—  
æ¯æ¡åºåˆ—æœ‰ 8 ä¸ª token  
æ¯ä¸ª token æ˜¯ 2 ç»´å‘é‡
```

---

### ğŸ“Œ ä¸ºä»€ä¹ˆç”¨è¿™ç§ç»´åº¦ï¼Ÿ

å› ä¸º Transformer è¯­è¨€æ¨¡å‹è®­ç»ƒæ—¶ï¼Œè¾“å…¥ tensor å°±æ˜¯è¿™ç§ç»“æ„ï¼š

```
(B, T, C)
```

* batch ç»´åº¦ B â†’ ä¸€æ¬¡è®­ç»ƒå¤šå°‘æ¡åºåˆ—
* æ—¶é—´/åºåˆ—ç»´åº¦ T â†’ æ¯æ¡åºåˆ—æœ‰å¤šå°‘ token
* é€šé“/embedding ç»´åº¦ C â†’ æ¯ä¸ª token æ˜¯å¤šå°‘ç»´å‘é‡

---

### ğŸ“Œ æ€»ç»“ä¸€å¥è¯

```python
B, T, C = 4, 8, 2
x = torch.randn(B, T, C)
```

å°±æ˜¯ **æ¨¡æ‹Ÿäº† Transformer ä¸­è¾“å…¥çš„ä¸€æ‰¹â€œtoken å‘é‡åºåˆ—â€**ï¼Œshape æ˜¯ (batch, time, channels)ã€‚

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘å¯ä»¥ç»§ç»­è®²è®²ï¼š

âœ… Transformer é‡Œä¸ºä»€ä¹ˆç”¨ (B, T, C) è¿™ç§ layoutï¼Ÿ
âœ… ä¸ºä»€ä¹ˆ channels ç»´åº¦é€šå¸¸é€‰ 128/256/512ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆå« time steps è€Œä¸æ˜¯ token stepsï¼Ÿ

è¦ä¸è¦æˆ‘å†ç»†è®²ä¸€ä¸‹ï¼ŸğŸŒŸ


```python
# We want x[b,t] = mean_{i<=t} x[b,i]
xbow = torch.zeros((B,T,C))
for b in range(B):
    for t in range(T):
        xprev = x[b,:t+1] # (t,C)
        xbow[b,t] = torch.mean(xprev, 0)
```

è§£é‡Šä¸€ä¸‹

å½“ç„¶å¯ä»¥ï¼Œè¿™æ®µä»£ç å…¶å®æ˜¯å¯¹åˆšæ‰é‚£ä¸ªâ€œå¹³å‡ä¸Šä¸‹æ–‡â€çš„ä¾‹å­åšäº†æ˜ç¡®å®ç°ï¼Œå’±ä»¬æ¥è¯¦ç»†æ‹†è§£ï¼š

---

### ğŸ“Œ ç›®æ ‡

```python
# We want:
x[b,t] = mean_{i <= t} x[b,i]
```

æ„æ€æ˜¯ï¼š

**å¯¹äºç¬¬ b æ¡åºåˆ—çš„ç¬¬ t ä¸ª tokenï¼Œ
æˆ‘ä»¬å¸Œæœ›å®ƒçš„æ–°ç‰¹å¾ = å®ƒå‰é¢æ‰€æœ‰ tokenï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰çš„å¹³å‡ embeddingã€‚**

---

### ğŸ“Œ ä»£ç 

```python
xbow = torch.zeros((B, T, C))
```

* åˆ›å»ºä¸€ä¸ª **ç©ºçš„è¾“å‡ºå¼ é‡** `xbow`ï¼Œå½¢çŠ¶ `(B, T, C)`ï¼Œ
* ç”¨æ¥å­˜â€œå¹³å‡åçš„ç‰¹å¾â€ã€‚

---

### ğŸ“Œ åŒé‡ for å¾ªç¯

```python
for b in range(B):      # å¯¹ batch é‡Œæ¯ä¸€æ¡åºåˆ—
    for t in range(T):  # å¯¹åºåˆ—é‡Œæ¯ä¸€ä¸ª token ä½ç½®
```

å¤–å±‚å¾ªç¯ â†’ éå† batch é‡Œçš„æ¯ä¸€æ¡åºåˆ—
å†…å±‚å¾ªç¯ â†’ éå†è¯¥åºåˆ—çš„æ¯ä¸€ä¸ª tokenï¼ˆæŒ‰æ—¶é—´æ­¥ Tï¼‰

---

### ğŸ“Œ å–å‡ºå‰ i<=t ä¸ª token

```python
xprev = x[b, :t+1]  # å½¢çŠ¶ (t+1, C)
```

* å¯¹å½“å‰åºåˆ— `b`ï¼Œå–å‡ºâ€œä» 0 åˆ° t çš„æ‰€æœ‰ tokenâ€ â†’ å…± t+1 ä¸ª token
* æ¯ä¸ª token æ˜¯ C ç»´å‘é‡ â†’ ç»“æœ shape æ˜¯ `(t+1, C)`

---

### ğŸ“Œ è®¡ç®—å¹³å‡

```python
xbow[b, t] = torch.mean(xprev, 0)  # ç»´åº¦ 0 ä¸Šæ±‚å¹³å‡
```

* å¯¹ `xprev` åœ¨ \*\*æ—¶é—´ç»´ï¼ˆç¬¬ 0 ç»´ï¼‰\*\*æ±‚å¹³å‡
* å¾—åˆ°ä¸€ä¸ª C ç»´å‘é‡
* å­˜åˆ° `xbow[b, t]` ä½ç½®

---

### ğŸ“Œ ä¸¾ä¾‹

å‡è®¾æŸæ¡åºåˆ—æ˜¯ï¼š

| token ä½ç½® t | embedding (C=2) å‘é‡ |
| ---------- | ------------------ |
| 0          | \[0.2, 0.4]        |
| 1          | \[0.6, 0.3]        |
| 2          | \[0.1, 0.5]        |
| 3          | \[0.9, 0.8]        |

åˆ™ï¼š

* t=0 æ—¶ â†’ åªå– token 0 â†’ å¹³å‡ = token 0 è‡ªå·±
* t=1 æ—¶ â†’ å¹³å‡ token 0 å’Œ token 1
* t=2 æ—¶ â†’ å¹³å‡ token 0,1,2
* t=3 æ—¶ â†’ å¹³å‡ token 0,1,2,3

---

### ğŸ“Œ ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåšï¼Ÿ

* è¿™æ˜¯**æœ€ç®€å•çš„ token ä¹‹é—´â€œäº¤æµâ€æ–¹å¼**
* é€šè¿‡å¯¹å‰é¢ token çš„å¹³å‡ï¼Œ
  æ¯ä¸ª token å¾—åˆ°ä¸€ä¸ªâ€œåŒ…å«å†å²ä¿¡æ¯çš„è¡¨ç¤ºâ€
* è™½ç„¶å¾ˆå¼±ï¼ˆä¿¡æ¯ä¸¢å¤±å¾ˆå¤šï¼‰ï¼Œä½†å¸®åŠ©ç†è§£ Transformer é‡Œ self-attention çš„æ€æƒ³ï¼š

**â€œå½“å‰ token æ€ä¹ˆç”¨å‰é¢çš„ token ä¿¡æ¯æ¥å¢å¼ºè‡ªå·±â€**

---

### ğŸ“Œ æ•´ä½“æµç¨‹æ€»ç»“

```python
for b in range(B):
    for t in range(T):
        xbow[b, t] = å‰ t ä¸ª token çš„ embedding å¹³å‡
```

å¾—åˆ°ï¼š

```python
xbow.shape = (B, T, C)
```

è¿™ä¸ª `xbow` å°±æ˜¯â€œå†å²å¹³å‡å¢å¼ºç‰ˆâ€çš„ç‰¹å¾å¼ é‡ã€‚

---

### ğŸ“Œ æ€»ç»“ä¸€å¥è¯

ğŸ‘‰ è¿™æ®µä»£ç æ˜¯ï¼š
**â€œæ‰‹åŠ¨ç”¨ for å¾ªç¯ï¼Œå®ç° token ä¹‹é—´ä¿¡æ¯æµåŠ¨çš„ toy ç‰ˆ â€”â€” å½“å‰ token å‘å‰çœ‹ï¼Œå–å†å² token çš„å¹³å‡ï¼Œä½œä¸ºå½“å‰ç‰¹å¾ã€‚â€**

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘å¯ä»¥é¡ºä¾¿è®²è®²ï¼š

âœ… è¿™ä¸ªâ€œå¹³å‡â€ å’Œ â€œself-attention åŠ æƒå’Œâ€ æœ‰å•¥å…³ç³»ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆä¸èƒ½çœ‹æœªæ¥ tokenï¼Ÿ
âœ… è¿™æ®µä»£ç çš„è®¡ç®—å¤æ‚åº¦æ˜¯å¤šå°‘ï¼Ÿ

è¦ä¸è¦æˆ‘å†å¸®ä½ è®²è®²ï¼ŸğŸŒŸ


å¯¹ xprev åœ¨ **æ—¶é—´ç»´ï¼ˆç¬¬ 0 ç»´ï¼‰**æ±‚å¹³å‡ æ˜¯ä»€ä¹ˆæ„æ€


è¿™ä¸ªé—®é¢˜é—®å¾—ç‰¹åˆ«å¥½ï¼Œæˆ‘æ¥è¯¦ç»†è§£é‡Šä¸€ä¸‹ï¼š

---

### ğŸ“Œ ä»€ä¹ˆå«â€œæ—¶é—´ç»´ï¼ˆç¬¬ 0 ç»´ï¼‰â€ï¼Ÿ

å’±ä»¬å…ˆçœ‹è¿™æ®µä»£ç é‡Œçš„ `xprev`ï¼š

```python
xprev = x[b, :t+1]  # å½¢çŠ¶ (t+1, C)
```

* `x` æœ¬æ¥ shape æ˜¯ `(B, T, C)`
  â†’ ä¹Ÿå°±æ˜¯ **(batch, time, channels)**
* è¿™é‡Œ `x[b, :t+1]` å–çš„æ˜¯ï¼š

  * ç¬¬ b ä¸ªåºåˆ—ï¼Œ
  * **å‰ t+1 ä¸ª token**ï¼Œ
  * æ¯ä¸ª token æ˜¯ C ç»´å‘é‡

æ‰€ä»¥ `xprev` çš„ shape æ˜¯ï¼š

```
(t+1, C)
```

ä¹Ÿå°±æ˜¯ï¼š

| ç»´åº¦ | å«ä¹‰                            |
| -- | ----------------------------- |
| 0  | æ—¶é—´ç»´ï¼ˆtoken é¡ºåºï¼‰ â†’ æœ‰ t+1 ä¸ª token |
| 1  | channels â†’ æ¯ä¸ª token æ˜¯ C ç»´å‘é‡   |

---

### ğŸ“Œ â€œå¯¹æ—¶é—´ç»´ï¼ˆç¬¬ 0 ç»´ï¼‰æ±‚å¹³å‡â€ æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

```python
torch.mean(xprev, 0)
```

è¿™ä¸ª `dim=0` è¯´æ˜ï¼š

ğŸ‘‰ åœ¨ **ç¬¬ 0 ç»´ï¼ˆæ—¶é—´ç»´ï¼‰** ä¸Šæ±‚å¹³å‡ï¼Œ
ğŸ‘‰ ä¹Ÿå°±æ˜¯ â€œå¯¹æ‰€æœ‰å‰ t+1 ä¸ª token çš„åŒä¸€é€šé“æ±‚å¹³å‡â€ã€‚

---

### ğŸ“Œ ä¸¾ä¾‹

å‡è®¾ï¼š

```
t+1 = 3  # å–äº†å‰ 3 ä¸ª token  
C = 2    # æ¯ä¸ª token æ˜¯ 2 ç»´å‘é‡
```

é‚£ä¹ˆ `xprev` å¯èƒ½æ˜¯ï¼š

```
[[0.2, 0.4],  # ç¬¬ 1 ä¸ª token
 [0.6, 0.3],  # ç¬¬ 2 ä¸ª token
 [0.1, 0.5]]  # ç¬¬ 3 ä¸ª token
```

shapeï¼š

```
(3, 2)
```

---

æ‰§è¡Œï¼š

```python
torch.mean(xprev, 0)
```

æ•ˆæœå°±æ˜¯ï¼š

```
[ (0.2 + 0.6 + 0.1)/3, (0.4 + 0.3 + 0.5)/3 ]
```

ä¹Ÿå°±æ˜¯ï¼š

```
å¹³å‡ç¬¬ 1 ç»´ï¼ˆchannel 0ï¼‰ï¼štoken1[0], token2[0], token3[0] çš„å¹³å‡  
å¹³å‡ç¬¬ 2 ç»´ï¼ˆchannel 1ï¼‰ï¼štoken1[1], token2[1], token3[1] çš„å¹³å‡  
```

---

### ğŸ“Œ å½¢è±¡ç†è§£

å‡è®¾ï¼š

```
æ—¶é—´ç»´ (token é¡ºåº)ï¼š â†‘  
channelsï¼ˆç»´åº¦ï¼‰ï¼šâ†’  
```

å¯¹ **â€œæ—¶é—´è½´ä¸Šå¤šä¸ª tokenâ€**ï¼Œåœ¨ **åŒä¸€ channel æ–¹å‘** æ±‚å¹³å‡ï¼Œ
ç»“æœå°±æ˜¯ä¸€ä¸ª C ç»´å‘é‡ï¼š

```
[[token1 embedding]  
 [token2 embedding]  
 [token3 embedding]]  --â†’  å¹³å‡ â†’ ä¸€ä¸ª embedding å‘é‡
```

---

### ğŸ“Œ æ€»ç»“ä¸€å¥è¯

ğŸ‘‰ â€œå¯¹æ—¶é—´ç»´æ±‚å¹³å‡â€ å°±æ˜¯ï¼š

**â€œæŠŠå‰é¢ t+1 ä¸ª token çš„ embedding å‘é‡ï¼ŒæŒ‰é€šé“æ–¹å‘ç®—å¹³å‡ï¼Œå¾—åˆ°ä¸€ä¸ª C ç»´å‘é‡ã€‚â€**

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥è¡¥å……è®²ï¼š

âœ… ä¸ºä»€ä¹ˆæ˜¯ dim=0ï¼Ÿè€Œä¸æ˜¯ dim=1ï¼Ÿ
âœ… è¿™ä¸ªå¹³å‡å’Œ Transformer é‡ŒåŠ æƒæ±‚å’Œçš„å…³ç³»ï¼Ÿ
âœ… ç”¨ torch.cumsum() å¯ä»¥æ›¿ä»£ for å¾ªç¯å—ï¼Ÿ

è¦ä¸è¦æˆ‘å†è¯¦ç»†è®²è®²ï¼ŸğŸŒŸ


# the trick in self-attention: matrix multiply as weighted aggregation

is that we can be very very efficient about doing this using matrix multiplication so that's the
mathematical trick and let me show you what I mean let's work with the toy example here let me run it and I'll
explain I have a simple Matrix here that is a 3X3 of all ones a matrix B of just
random numbers and it's a 3x2 and a matrix C which will be 3x3 multip 3x2
which will give out a 3x2 so here we're just using um matrix multiplication so a
multiply B gives us C okay so how are these numbers in C um
achieved right so this number in the top left is the first row of a dot product
with the First Column of B and since all the the row of a right now is all just
ones then the do product here with with this column of B is just going to do a sum of these of this column so 2 + 6 + 6
is 14 the element here in the output of C is also the first column here the first
row of a multiplied now with the second column of B so 7 + 4 + 5 is 16 now you
see that there's repeating elements here so this 14 again is because this row is again all ones and it's multiplying the
First Column of B so we get 14 and this one is and so on so this last number
here is the last row do product last column now the trick here is uh the
following this is just a boring number of um it's just a boring array of all
ones but torch has this function called Trail which is short for a
triangular uh something like that and you can wrap it in torch up once and it will just return the lower triangular
portion of this okay so now it will basically zero out
uh these guys here so we just get the lower triangular part well what happens if we do
that so now we'll have a like this and B like this and now what are we getting here in C well what is this number well
this is the first row times the First Column and because this is zeros
uh these elements here are now ignored so we just get a two and then this number here is the first row times the
second column and because these are zeros they get ignored and it's just seven this seven multiplies this one but
look what happened here because this is one and then zeros we what ended up happening is we're just plucking out the
row of this row of B and that's what we got now here we have one 1 Z so here 110
do product with these two columns will now give us 2 + 6 which is 8 and 7 + 4 which is 11 and because this is 111 we
ended up with the addition of all of them and so basically depending on how many ones and zeros we have here we are
basically doing a sum currently of a variable number of these rows and that
gets deposited into C So currently we're doing sums because these are ones but we can also do
average right and you can start to see how we could do average uh of the rows of B uh sort of in an incremental
fashion because we don't have to we can basically normalize these rows so that they sum to one and then we're going to
get an average so if we took a and then we did aals aide torch. sum in the um of a in the um
oneth Dimension and then let's keep them as true so so therefore the broadcasting
will work out so if I rerun this you see now that these rows now sum to one so
this row is one this row is 0. 5.5 Z and here we get 1/3 and now when we do a
multiply B what are we getting here we are just getting the first row first row
here now we are getting the average of the first two rows okay so 2 and six average is four
and four and seven average is 5.5 and on the bottom here we are now getting the average of these three rows
so the average of all of elements of B are now deposited here and so you can
see that by manipulating these uh elements of this multiplying Matrix and
then multiplying it with any given Matrix we can do these averages in this
incremental fashion because we just get um and we can manipulate that based on
the elements of a okay so that's very convenient so let's let's swing back up here and see how we can vectorize this

å½“ç„¶å¯ä»¥ï¼Œæˆ‘æ¥è¯¦ç»†è§£é‡Šè¿™ä¸€æ®µçš„å†…å®¹ï¼Œè¿™æ®µéå¸¸å…³é”®ï¼Œå…³ç³»åˆ° self-attention çš„æ ¸å¿ƒæŠ€å·§ï¼š

---

### ğŸ“Œ è¿™ä¸€æ®µè®²ä»€ä¹ˆï¼Ÿ

ğŸ‘‰ **self-attention å…¶å®å°±æ˜¯â€œåŠ æƒæ±‚å’Œâ€ï¼Œè€ŒåŠ æƒæ±‚å’Œå¯ä»¥ç”¨çŸ©é˜µä¹˜æ³•éå¸¸é«˜æ•ˆåœ°å®ç°**ã€‚
ğŸ‘‰ è¿™ä¸ªä¾‹å­æ˜¯ä¸ºäº†å¸®ä½ ç†è§£ï¼Œ**çŸ©é˜µä¹˜æ³•æ€ä¹ˆå°±èƒ½åšâ€œç´¯åŠ ã€å¹³å‡â€è¿™ç§æ“ä½œï¼Ÿ**

---

### ğŸ“Œ å°ä¾‹å­è®¾ç½®

```python
A = 3x3 çŸ©é˜µï¼Œå…¨æ˜¯ 1  
B = 3x2 çŸ©é˜µï¼Œéšæœºå€¼  
C = A @ B â†’ ç»“æœæ˜¯ 3x2 çŸ©é˜µ
```

---

### ğŸ“Œ æ™®é€šçŸ©é˜µä¹˜æ³•

```
C = A @ B
```

æ¯ä¸ª C\[i,j] æ˜¯ï¼š

```
A[i,:]ï¼ˆç¬¬ i è¡Œï¼‰ å’Œ B[:,j]ï¼ˆç¬¬ j åˆ—ï¼‰ çš„ç‚¹ç§¯
```

---

### ğŸ“Œ ä¸ºä»€ä¹ˆ C é‡Œæ˜¯â€œç´¯åŠ â€çš„æ•ˆæœï¼Ÿ

å› ä¸º A çš„è¡Œæ˜¯å…¨ 1ï¼š

```
[1,1,1]
```

ç‚¹ç§¯å°±å˜æˆ â€œB æŸä¸€åˆ—çš„æ‰€æœ‰å€¼æ±‚å’Œâ€ï¼š

```
[2,6,6] â†’ å’Œ = 14
[7,4,5] â†’ å’Œ = 16
```

æ‰€ä»¥ï¼š

```
C[0,0] = 14
C[0,1] = 16
```

---

### ğŸ“Œ æ”¹ç”¨ä¸‹ä¸‰è§’çŸ©é˜µï¼ˆtrilï¼‰

```python
A = torch.tril(torch.ones(3,3))
```

æ„æ€æ˜¯ï¼š

```
A = ä¸‹ä¸‰è§’çŸ©é˜µ  
[[1,0,0],
 [1,1,0],
 [1,1,1]]
```

---

### ğŸ“Œ ä¸ºä»€ä¹ˆè¦ç”¨ trilï¼Ÿ

ğŸ‘‰ å› ä¸ºï¼š

* è¯­è¨€æ¨¡å‹ä¸èƒ½çœ‹æœªæ¥ token
* åªèƒ½èšåˆè‡ªå·± + ä¹‹å‰çš„ token ä¿¡æ¯

**ä¸‹ä¸‰è§’çŸ©é˜µæ­£å¥½å°±å®ç°äº†è¿™ç§ maskï¼Œæœªæ¥ token æƒé‡=0**ï¼

---

### ğŸ“Œ çŸ©é˜µä¹˜æ³•åæ•ˆæœ

```
C = A @ B
```

* ç¬¬ä¸€è¡Œï¼šåªå– token1ï¼Œç›´æ¥å¤åˆ¶åˆ° C\[0,:]
* ç¬¬äºŒè¡Œï¼šå‰ 2 ä¸ª token æ±‚å’Œ â†’ å­˜åˆ° C\[1,:]
* ç¬¬ä¸‰è¡Œï¼šå‰ 3 ä¸ª token æ±‚å’Œ â†’ å­˜åˆ° C\[2,:]

---

### ğŸ“Œ æ”¹æˆâ€œå¹³å‡â€

å¯ä»¥ç”¨ï¼š

```python
A /= A.sum(dim=1, keepdim=True)
```

* æŠŠ A æ¯ä¸€è¡Œ normalizeï¼Œè¡Œå’Œ=1
* çŸ©é˜µä¹˜æ³•å°±å˜æˆ â€œ**åŠ æƒå¹³å‡**â€

---

### ğŸ“Œ å°ç»“è¿™ä¸€ trick

ğŸ‘‰ **çŸ©é˜µä¹˜æ³• A @ B å¯ä»¥é«˜æ•ˆå®ç°ï¼š**

* å‰ t ä¸ª token ç´¯åŠ 
* æˆ–è€… å‰ t ä¸ª token å¹³å‡
* A çš„ tril + normalize å°±èƒ½æ§åˆ¶è¿™ç§è¡Œä¸º

---

### ğŸ“Œ self-attention çš„å…³ç³»

Transformer é‡Œçš„ self-attention æœ¬è´¨ä¸Šæ˜¯ï¼š

```
C = softmax(scores) @ V
```

* softmax(scores) â†’ å°±æ˜¯ä¸€ä¸ª A çŸ©é˜µï¼Œè¡¨ç¤º token é—´çš„æ³¨æ„åŠ›ï¼ˆåŠ æƒç³»æ•°ï¼‰
* V â†’ å°±æ˜¯ token çš„ embedding çŸ©é˜µ
* çŸ©é˜µä¹˜æ³• â†’ å°±æ˜¯â€œæŒ‰æ³¨æ„åŠ›åŠ æƒæ±‚å’Œâ€

---

### ğŸ“Œ è¿™æ®µçš„æ ¸å¿ƒæ€æƒ³

ğŸ‘‰ â€œé€šè¿‡è®¾è®¡ä¸€ä¸ª maskï¼ˆä¸‹ä¸‰è§’çŸ©é˜µ trilï¼‰ï¼Œå† normalizeï¼Œé…åˆçŸ©é˜µä¹˜æ³•ï¼Œå°±èƒ½é«˜æ•ˆå®ç° token ä¹‹é—´çš„åŠ æƒæ±‚å’Œï¼ˆä¿¡æ¯äº¤æµï¼‰ã€‚â€

---

### ğŸ“Œ æ€»ç»“ä¸€å¥è¯

ğŸ‘‰ **self-attention é‡Œçš„ä¿¡æ¯æµåŠ¨ï¼Œå…¶å®å°±æ˜¯çŸ©é˜µä¹˜æ³•å®ç°çš„â€œåŠ æƒå¹³å‡â€ï¼Œ
è€Œè¿™ä¸ª toy ä¾‹å­å¸®ä½ å…ˆç†è§£â€œä¸ºå•¥çŸ©é˜µä¹˜æ³•å°±èƒ½åšåˆ°â€ï¼**

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥ç»§ç»­è¡¥å……è®²è®²ï¼š

âœ… self-attention ä¸­ softmax æ˜¯æ€ä¹ˆèµ·ä½œç”¨çš„ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆ mask å¿…é¡»ç”¨ trilï¼Ÿ
âœ… è¿™ä¸ªçŸ©é˜µä¹˜æ³•å’Œ for å¾ªç¯ç›¸æ¯”æœ‰å¤šå¿«ï¼Ÿ

è¦ä¸è¦æˆ‘å†å¸®ä½ å±•å¼€è®²è®²ï¼ŸğŸŒŸ


# version 2: using matrix multiply

and make it much more efficient using what we've learned so in particular we are going to produce an
array a but here I'm going to call it we short for weights but this is our
a and this is how much of every row we want to average up and it's going to be
an average because you can see that these rows sum to one so this is our a and then our B in
this example of course is X so what's going to happen here now is
that we are going to have an expo 2 and this Expo 2 is going to be way
multiplying RX so let's think this true way is T BYT
and this is Matrix multiplying in pytorch a b by T by C and it's giving us uh different what
shape so pytorch will come here and it will see that these shapes are not the same so it will create a batch Dimension
here and this is a batched matrix multiply and so it will apply this matrix multiplication in all the batch
elements um in parallel and individually and then for each batch element there
will be a t BYT multiplying T by C exactly as we had
below so this will now create B by T by C and Expo 2 will now become identical
to Expo so we can see that torch. all close of
xbo and xbo 2 should be true now so this kind of like convinces us
that uh these are in fact um the same so xbo and xbo 2 if I just print
them uh okay we're not going to be able to okay we're not going to be able to just stare it down but
um well let me try Expo basically just at the zeroth element and Expo two at the zeroth element so just the first
batch and we should see that this and that should be identical which they are right so what happened here the
trick is we were able to use batched Matrix multiply to do this uh
aggregation really and it's a weighted aggregation and the weights are specified in this um T BYT array and
we're basically doing weighted sums and uh these weighted sums are are U according to uh the weights inside here
they take on sort of this triangular form and so that means that a token at the teth dimension will only get uh sort
of um information from the um tokens perceiving it so that's exactly what we
want and finally I would like to rewrite it in one more way and we're going to see why that's useful so this is the

å½“ç„¶å¯ä»¥ï¼æˆ‘æ¥å¸®ä½ è¯¦ç»†è§£é‡Šä¸€ä¸‹ **â€œversion 2: using matrix multiplyâ€** è¿™ä¸€æ®µçš„å†…å®¹ï¼š

---

### ğŸ“Œ è¿™ä¸€æ®µçš„æ ¸å¿ƒæ€æƒ³

ğŸ‘‰ åˆšæ‰ â€œfor å¾ªç¯ + å‡å€¼â€ çš„å®ç°æ•ˆç‡å¤ªä½ï¼Œ
ğŸ‘‰ å…¶å®å¯ä»¥ç”¨ **çŸ©é˜µä¹˜æ³•ï¼ˆbatched matmulï¼‰** å®ç°ï¼Œé€Ÿåº¦ä¼šå¿«å¾ˆå¤šï¼ŒGPU ä¹Ÿèƒ½åŠ é€Ÿã€‚

---

### ğŸ“Œ å˜é‡å¯¹åº”å…³ç³»

* **weightsï¼ˆwï¼‰** å°±æ˜¯ä¹‹å‰è¯´çš„çŸ©é˜µ Aï¼Œä¸‹ä¸‰è§’çŸ©é˜µ tril å½’ä¸€åŒ–æˆâ€œåŠ æƒç³»æ•°â€ã€‚

  * shape = (T, T)
  * æ¯ä¸€è¡Œ sum = 1 â†’ å°±æ˜¯åšåŠ æƒå¹³å‡
* **B** å°±æ˜¯ xï¼Œæœ¬æ¥ shape æ˜¯ (B, T, C)
* **expo2 = w @ x**
  â†’ è¿™ä¸ªå°±æ˜¯æˆ‘ä»¬æƒ³å¾—åˆ°çš„ â€œå†å²ä¿¡æ¯å¹³å‡ç‰¹å¾â€

---

### ğŸ“Œ å…·ä½“åšæ³•

#### 1ï¸âƒ£ å®šä¹‰ w (weights)

```python
w = torch.tril(torch.ones(T, T))  
w /= w.sum(dim=1, keepdim=True)
```

* å…ˆç”¨ tril ç”Ÿæˆä¸‹ä¸‰è§’çŸ©é˜µ
* å†å¯¹æ¯ä¸€è¡Œå½’ä¸€åŒ–ï¼Œä½¿è¡Œå’Œ=1
  â†’ è¿™æ ·åšå‡ºæ¥çš„ w å…¶å®å°±æ˜¯ â€œå¹³å‡å†å²ä¿¡æ¯â€çš„ç³»æ•°çŸ©é˜µ

---

#### 2ï¸âƒ£ æ‰§è¡Œ batched çŸ©é˜µä¹˜æ³•

```python
expo2 = w @ x  # å®é™…ç”¨çš„æ˜¯ batched matmul
```

ä½†æ˜¯è¿™é‡Œç›´æ¥è¿™ä¹ˆå†™ï¼Œpytorch ä¼šè‡ªåŠ¨è¿›è¡Œå¹¿æ’­ï¼š

* w æ˜¯ (T, T)
* x æ˜¯ (B, T, C)

Pytorch å‘ç° batch ç»´ä¸åŒ¹é…ï¼Œä¼š **è‡ªåŠ¨åœ¨ w å‰é¢åŠ  batch ç»´åº¦**ï¼Œåš batched matmulï¼š

* æ¯ä¸ª batch ç‹¬ç«‹è®¡ç®—
* ç›¸å½“äºå¯¹æ¯æ¡åºåˆ—åš `(T, T) @ (T, C)` â†’ å¾—åˆ° (T, C)

æ‰€ä»¥ï¼š

```
expo2.shape = (B, T, C)
```

---

### ğŸ“Œ ä¸ºä»€ä¹ˆç»“æœå’Œ for å¾ªç¯ç‰ˆä¸€æ ·ï¼Ÿ

```python
torch.allclose(xbow, expo2)
```

è¿™ä¸ªè¯­å¥åœ¨éªŒè¯ï¼š
â€œfor å¾ªç¯å®ç°çš„ `xbow` å’Œ çŸ©é˜µä¹˜æ³•å®ç°çš„ `expo2`ï¼Œç»“æœæ˜¯ä¸æ˜¯ä¸€æ ·çš„ï¼Ÿâ€

* ç»“æœæ˜¯ Trueï¼Œè¯´æ˜ä¸¤ç§å®ç° **æ•°å­¦ä¸Šç­‰ä»·**ã€‚

---

### ğŸ“Œ ä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼Ÿ

* ç”¨ batched matmulï¼š

  * æ›´å¿«ï¼ˆç‰¹åˆ«åœ¨ GPU ä¸Šï¼‰
  * å†™æ³•æ›´ç®€æ´
  * æ²¡æœ‰ for å¾ªç¯ï¼Œæ•ˆç‡é«˜
* æœ¬è´¨æ€æƒ³ï¼š

  * ç”¨ä¸€ä¸ª (T,T) çŸ©é˜µä½œä¸ºâ€œæ³¨æ„åŠ›æƒé‡â€
  * ä¹˜åˆ° (T,C) çš„ embedding çŸ©é˜µ
  * å¾—åˆ°æ–°çš„ â€œä¿¡æ¯èšåˆç‰¹å¾â€

---

### ğŸ“Œ å’Œ self-attention çš„å…³ç³»ï¼Ÿ

ğŸ‘‰ è¿™å…¶å®å°±æ˜¯ self-attention çš„ â€œæ ¸å¿ƒæ•°å­¦ç»“æ„â€ï¼š

```
attention_output = softmax(attention_scores) @ V
```

* softmax(attention\_scores) â†’ å°±åƒ wï¼Œä¸‹ä¸‰è§’ + å½’ä¸€åŒ–
* V â†’ å°±æ˜¯ token embeddingï¼Œshape (B, T, C)
* æœ€ç»ˆå°±æ˜¯ï¼š**çŸ©é˜µä¹˜æ³•é«˜æ•ˆåšâ€œä¿¡æ¯èšåˆâ€**

---

### ğŸ“Œ æ€»ç»“ä¸€å¥è¯

ğŸ‘‰ è¿™æ®µæ•™ä½ ï¼š

**â€œæŠŠ token é—´çš„åŠ æƒæ±‚å’Œï¼Œç”¨çŸ©é˜µä¹˜æ³• (batched matmul) é«˜æ•ˆå®ç°ï¼Œå’Œ for å¾ªç¯ç‰ˆå®Œå…¨ç­‰ä»·ï¼Œä½†é€Ÿåº¦å¿«å¾ˆå¤šã€‚â€**

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥ç»§ç»­è¡¥å……ï¼š

âœ… è¿™ä¸ª â€œçŸ©é˜µä¹˜æ³•â€ å’Œ Transformer é‡Œçš„ QK softmax æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆå¿…é¡»ç”¨ä¸‹ä¸‰è§’ï¼Ÿ
âœ… batched matmul æ˜¯æ€ä¹ˆå®ç°å¹¶è¡Œçš„ï¼Ÿ

è¦ä¸è¦æˆ‘å†å¸®ä½ è¯¦ç»†è®²è®²ï¼ŸğŸŒŸ

```
wei = torch.tril(torch.ones(T, T))
wei = wei / wei.sum(1, keepdim=True)
print(wei)
xbow2 = wei @ x
```

weiçš„shapeæ˜¯ (T, T)  xçš„shapeæ˜¯ (B, T, C)
ä¸ºä»€ä¹ˆ weiå¯ä»¥å’Œxç›¸ä¹˜

è¿™æ˜¯å› ä¸º PyTorch åœ¨åšçŸ©é˜µä¹˜æ³•æ—¶ï¼Œ`@`ï¼ˆæˆ–è€… `torch.matmul`ï¼‰æ”¯æŒ **æ‰¹é‡çŸ©é˜µä¹˜æ³•**ï¼ˆbatched matrix multiplicationï¼‰ï¼Œä¼šè‡ªåŠ¨å¤„ç† **broadcast** è§„åˆ™ã€‚

---

## 1. ä½ çš„å¼ é‡å½¢çŠ¶

* **wei**: `(T, T)`
  ä¸€ä¸ªäºŒç»´æ–¹é˜µï¼ˆæ—¶é—´æ­¥ Ã— æ—¶é—´æ­¥ï¼‰ï¼Œæ¯”å¦‚ `(8, 8)`ã€‚
* **x**: `(B, T, C)`

  * `B`ï¼šbatch sizeï¼Œæ¯”å¦‚ 4
  * `T`ï¼šåºåˆ—é•¿åº¦ï¼Œæ¯”å¦‚ 8
  * `C`ï¼šembedding ç»´åº¦ï¼Œæ¯”å¦‚ 65

---

## 2. ä¸ºä»€ä¹ˆ `(T, T)` å¯ä»¥å’Œ `(B, T, C)` ç›¸ä¹˜ï¼Ÿ

åœ¨ `wei @ x` ä¸­ï¼š

* `@` è°ƒç”¨çš„æ˜¯ `torch.matmul(wei, x)`
* **è§„åˆ™**ï¼šå¦‚æœç¬¬ä¸€ä¸ªå¼ é‡æ˜¯ 2Dï¼ˆ`(T, T)`ï¼‰ï¼Œç¬¬äºŒä¸ªå¼ é‡æ˜¯ 3Dï¼ˆ`(B, T, C)`ï¼‰ï¼ŒPyTorch ä¼šè‡ªåŠ¨æŠŠç¬¬ä¸€ä¸ªå¼ é‡ **broadcast** åˆ° `(B, T, T)`ï¼Œç„¶ååš **æ‰¹é‡çŸ©é˜µä¹˜æ³•**ã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼Œç­‰ä»·äºï¼š

```python
wei_expanded = wei.unsqueeze(0).expand(B, T, T)  # (B, T, T)
xbow2 = torch.matmul(wei_expanded, x)           # (B, T, C)
```

---

## 3. çŸ©é˜µä¹˜æ³•ç»´åº¦åŒ¹é…è§„åˆ™

çŸ©é˜µä¹˜æ³• `(A @ B)` è¦æ±‚ï¼š

* `A` çš„ **æœ€åä¸€ç»´** é•¿åº¦ == `B` çš„ **å€’æ•°ç¬¬äºŒç»´** é•¿åº¦
* ç»“æœçš„å½¢çŠ¶æ˜¯ï¼š`A` çš„å‰é¢ç»´åº¦ + `B` çš„æœ€åä¸€ç»´

åœ¨è¿™é‡Œï¼š

* `wei`: `(T, T)` â†’ æœ€åä¸€ç»´æ˜¯ `T`
* `x`: `(B, T, C)` â†’ å€’æ•°ç¬¬äºŒç»´æ˜¯ `T`
* åŒ¹é…æˆåŠŸåï¼Œç»“æœæ˜¯ `(B, T, C)`

---

## 4. ä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼Ÿ

è¿™æ ·æ¯ä¸ª batch çš„åºåˆ—éƒ½ä¼šç”¨ç›¸åŒçš„ `wei` åšåŠ æƒå¹³å‡ï¼ˆæ¯”å¦‚ä¸Šä¸‹æ–‡çª—å£çš„æ³¨æ„åŠ›æƒé‡ï¼‰ï¼Œè€Œä¸” PyTorch ä¼šä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰ batch çš„è®¡ç®—ï¼Œéå¸¸é«˜æ•ˆã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼  **wei å’Œ x çš„æ‰¹é‡çŸ©é˜µä¹˜æ³•ç¤ºæ„å›¾**ï¼Œä½ ä¸€çœ‹å°±æ˜ç™½å¹¿æ’­å’Œä¹˜æ³•æ˜¯æ€ä¹ˆè¿›è¡Œçš„ã€‚ä½ è¦æˆ‘ç”»å—ï¼Ÿ


# version 3: adding softmax

third version and it's also identical to the first and second but let me talk through it it uses
softmax so Trill here is this Matrix
lower triangular ones way begins as all
zero okay so if I just print way in the beginning it's all zero then I
used masked fill so what this is doing is we. masked fill it's all zeros and
I'm saying for all the elements where Trill is equal equal Z make them be
negative Infinity so all the elements where Trill is zero will become negative Infinity now so this is what we get and
then the final line here is softmax so if I take a softmax along
every single so dim is negative one so along every single row if I do softmax
what is that going to do well softmax is um is also like a
normalization operation right and so spoiler alert you get the exact same
Matrix let me bring back to softmax and recall that in softmax we're going to exponentiate every single one
of these and then we're going to divide by the sum and so if we exponentiate
every single element here we're going to get a one and here we're going to get uh basically zero 0 z0 Z everywhere else
and then when we normalize we just get one here we're going to get one one and then zeros and then softmax will again
divide and this will give us 5.5 and so on and so this is also the uh the same
way to produce uh this mask now the reason that this is a bit more interesting and the reason we're going
to end up using it in self attention is that these weights here
begin uh with zero and you can think of this as like an interaction strength or like an affinity so basically it's
telling us how much of each uh token from the past do we want to Aggregate
and average up and then this line is saying tokens from
the past cannot communicate by setting them to negative Infinity we're saying that we will not aggregate anything from
those tokens and so basically this then goes through softmax and through the weighted
and this is the aggregation through matrix multiplication and so what this is now is you can think of these as um these
zeros are currently just set by us to be zero but a quick preview is that these
affinities between the tokens are not going to be just constant at zero they're going to be data dependent these
tokens are going to start looking at each other and some tokens will find other tokens more or less interesting
and depending on what their values are they're going to find each other interesting to different amounts and I'm
going to call those affinities I think and then here we are saying the future cannot communicate with the past we're
we're going to clamp them and then when we normalize and sum we're going to aggregate uh sort of their values
depending on how interesting they find each other and so that's the preview for self attention and basically long story
short from this entire section is that you can do weighted aggregations of your past
Elements by having by using matrix multiplication of a lower triangular
fashion and then the elements here in the lower triangular part are telling you how much of each element uh fuses
into this position so we're going to use this trick now to develop the self attention block block so first let's get

å½“ç„¶å¯ä»¥ï¼Œè¿™æ®µæ˜¯éå¸¸å…³é”®çš„å†…å®¹ï¼Œå’±ä»¬ä»”ç»†è®²ä¸€ä¸‹ version 3ï¼š**adding softmax**ï¼Œè¿™æ˜¯ç†è§£ self-attention çš„é‡ç‚¹æ­¥éª¤ã€‚

---

### ğŸ“Œ è¿™ä¸€æ®µè®²ä»€ä¹ˆï¼Ÿ

ğŸ‘‰ åˆšæ‰ version 2 ç”¨çš„æ˜¯ï¼š

```
tril + å¹³å‡ = çŸ©é˜µä¹˜æ³•
```

ğŸ‘‰ ç°åœ¨ version 3 å¼•å…¥äº† **softmax**ï¼Œ
å°±æ›´æ¥è¿‘ Transformer é‡Œçš„ **self-attention** çœŸæ­£åšæ³•äº†ã€‚

---

### ğŸ“Œ æ“ä½œæ­¥éª¤æ‹†è§£

---

#### 1ï¸âƒ£ å®šä¹‰ mask (Tril)

```python
tril = torch.tril(torch.ones(T, T))
```

ç”Ÿæˆä¸‹ä¸‰è§’çŸ©é˜µï¼š

```
[[1, 0, 0],
 [1, 1, 0],
 [1, 1, 1]]
```

â†’ ä½œç”¨ï¼šé˜²æ­¢ â€œçœ‹æœªæ¥ tokenâ€ï¼Œä¿è¯ â€œåªèƒ½èšåˆè‡ªå·±å’Œå†å² tokenâ€ã€‚

---

#### 2ï¸âƒ£ å®šä¹‰ weights (way)

```python
way = torch.zeros(T, T)
```

å…ˆåˆå§‹åŒ–ä¸ºå…¨ 0ï¼Œè¡¨ç¤º **å½“å‰ token å¯¹å…¶ä»– token çš„â€œæ³¨æ„åŠ›å¼ºåº¦â€**ï¼ˆaffinityï¼‰ã€‚

---

#### 3ï¸âƒ£ åš masked\_fill

```python
way.masked_fill(tril == 0, float('-inf'))
```

ğŸ‘‰ æŠŠ **æœªæ¥ tokenï¼ˆtril=0ï¼‰å¯¹åº”ä½ç½®è®¾æˆ -âˆ**ï¼š

```
[[0, -inf, -inf],
 [0,    0, -inf],
 [0,    0,    0]]
```

---

#### 4ï¸âƒ£ softmax å½’ä¸€åŒ–

```python
way = F.softmax(way, dim=-1)
```

ğŸ‘‰ é‡ç‚¹ï¼š

* softmax( -âˆ ) â†’ 0
* softmax( 0 ) â†’ 1 / N

ç»“æœå°±æ˜¯ï¼š

```
[[1, 0, 0],
 [0.5, 0.5, 0],
 [1/3, 1/3, 1/3]]
```

---

### ğŸ“Œ ä¸ºä»€ä¹ˆè¦ softmaxï¼Ÿ

* softmax ä¼šè‡ªåŠ¨ **å½’ä¸€åŒ–æƒé‡**ï¼Œå˜æˆ **åŠ æƒå¹³å‡**ã€‚
* softmax æ˜¯å¯å¾®åˆ†çš„ï¼Œåé¢å¯ä»¥ç”¨ç½‘ç»œ **å­¦ä¹ æ³¨æ„åŠ›åˆ†æ•°**ï¼

---

### ğŸ“Œ æœ€é‡è¦çš„æ¦‚å¿µ

```
è¿™äº› weightsï¼Œä¸å†æ˜¯äººä¸ºè®¾å®šï¼ˆtril + å¹³å‡ï¼‰ï¼Œ  
è€Œæ˜¯å¯ä»¥ç”¨æ¨¡å‹è‡ªåŠ¨å­¦å‡º token é—´çš„â€œå…´è¶£ç¨‹åº¦â€ã€‚
```

æ¯”å¦‚ï¼š

```
æŸä¸ª token å¯èƒ½å¯¹å‰é¢æŸäº› token æ›´â€œå…³æ³¨â€ï¼Œå¯¹å…¶ä»– token ä¸å¤ªå…³æ³¨ã€‚  
â†’ æƒé‡å°±ä¼šåé«˜ / åä½ï¼Œsoftmax è‡ªåŠ¨å½’ä¸€åŒ–ã€‚
```

---

### ğŸ“Œ ä¸ºä»€ä¹ˆè¦åŠ  softmaxï¼Œè€Œä¸æ˜¯ç›´æ¥ç”¨ trilï¼Ÿ

âœ… softmax å½’ä¸€åŒ–
âœ… softmax å¯å¾®ï¼Œå¯ä»¥è®­ç»ƒ
âœ… softmax è¾“å‡ºçš„ â€œæ³¨æ„åŠ›åˆ†æ•°â€ æ›´ç»†è…»
âœ… softmax æ˜¯ self-attention çš„æ ‡å‡†åšæ³•

---

### ğŸ“Œ self-attention é¢„å‘Š

* åœ¨ Transformer é‡Œï¼Œattention æƒé‡ä¸æ˜¯å…¨ 1ã€ä¸æ˜¯å›ºå®š trilï¼Œ
* è€Œæ˜¯ï¼š

```
attention_scores = Q @ K^T / sqrt(d_k)
attention_weights = softmax(attention_scores + mask)
attention_output = attention_weights @ V
```

è¿™ä¸ª **attention\_weights** å°±å’Œ version 3 çš„ `way` æ˜¯åŒä¸€ä¸ªä¸œè¥¿ï¼

---

### ğŸ“Œ æ€»ç»“ä¸€å¥è¯

ğŸ‘‰ è¿™ä¸€æ®µ version 3 æ•™ä½ ï¼š

**â€œself-attention å…¶å®å°±æ˜¯ç”¨ softmax å½’ä¸€åŒ–åçš„ä¸‹ä¸‰è§’çŸ©é˜µï¼Œä¹˜ embeddingï¼Œ
å®ç° token é—´çš„åŠ æƒä¿¡æ¯èåˆï¼ˆaggregationï¼‰ã€‚â€**

---

### ğŸ“Œ ä¸ºä»€ä¹ˆè¿™ä¸€æ®µå¾ˆé‡è¦ï¼Ÿ

âœ… è¿™æ˜¯ **Transformer self-attention çš„æ•°å­¦æ ¸å¿ƒ**
âœ… ä½ ç†è§£äº†è¿™ä¸ªï¼Œé©¬ä¸Šå°±èƒ½ç†è§£ Transformer çš„ Qã€Kã€V è®¡ç®—
âœ… è¿™å°±æ˜¯ Transformer æ¯” RNN / Bigram å¼ºå¤§çš„åœ°æ–¹ï¼šå¯ä»¥å­¦å‡ºâ€œå…´è¶£å…³ç³»â€ï¼

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥è¡¥å……è®²ï¼š

âœ… softmax ä¸ºä»€ä¹ˆé€‰æ‹© dim=-1ï¼Ÿ
âœ… self-attention é‡Œçš„ QK^T æ€ä¹ˆæ¥ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆå«â€œåŠ æƒæ±‚å’Œâ€è€Œä¸æ˜¯â€œå·ç§¯â€ï¼Ÿ

è¦ä¸è¦æˆ‘å†å¸®ä½ è¯¦ç»†è®²è®²ï¼ŸğŸŒŸ


wei = F.softmax(wei, dim=-1) ä¸­çš„ dim=-1 æ˜¯ä»€ä¹ˆæ„æ€

è¿™ä¸ªé—®é¢˜é—®å¾—ç‰¹åˆ«å¥½ï¼æˆ‘æ¥è¯¦ç»†è§£é‡Šï¼š

---

### ğŸ“Œ å…ˆçœ‹è¯­å¥

```python
wei = F.softmax(wei, dim=-1)
```

---

### ğŸ“Œ ä»€ä¹ˆæ˜¯ `dim=-1` ï¼Ÿ

`dim` å‚æ•°çš„æ„æ€æ˜¯ï¼š

ğŸ‘‰ **åœ¨å“ªä¸€ç»´ä¸Šåš softmax è¿ç®—**ã€‚

---

### ğŸ“Œ ä¸ºä»€ä¹ˆå« `-1` ï¼Ÿ

* åœ¨ PyTorch é‡Œï¼Œ**ç»´åº¦å¯ä»¥ç”¨æ­£æ•°æˆ–è´Ÿæ•°è¡¨ç¤º**ï¼š

  * 0 è¡¨ç¤ºç¬¬ 0 ç»´ï¼ˆæœ€å¤–å±‚ï¼‰
  * 1 è¡¨ç¤ºç¬¬ 1 ç»´
  * 2 è¡¨ç¤ºç¬¬ 2 ç»´
  * ...
  * **-1 è¡¨ç¤ºæœ€åä¸€ç»´**

ä¹Ÿå°±æ˜¯è¯´ï¼š

```
dim = -1 <==> dim = wei.ndim - 1
```

---

### ğŸ“Œ ä¸ºä»€ä¹ˆè¦åœ¨æœ€åä¸€ç»´åš softmaxï¼Ÿ

åœ¨è¿™é‡Œï¼Œ`wei` çš„ shape æ˜¯ `(T, T)`ï¼Œä¹Ÿå°±æ˜¯ï¼š

```
è¡Œ â†’ å½“å‰ tokenï¼ˆtï¼‰  
åˆ— â†’ å¯¹å…¶ä»– token çš„æ³¨æ„åŠ›æƒé‡
```

â†’ æ¯ä¸€è¡Œè¡¨ç¤ºï¼š

**â€œå½“å‰ token å¯¹å…¶ä»– token çš„æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒâ€**

---

æˆ‘ä»¬å¸Œæœ›ï¼š

```
softmax æ¯ä¸€è¡Œï¼Œå½’ä¸€åŒ–æˆ 1  
â†’ æ¯è¡Œ sum = 1ï¼Œå˜æˆâ€œæ¦‚ç‡åˆ†å¸ƒâ€
```

---

æ‰€ä»¥è¦ï¼š

```python
F.softmax(wei, dim=-1)
```

* dim=-1 â†’ å¯¹â€œæ¯ä¸€è¡Œâ€çš„å…ƒç´ åš softmax
* ç»“æœå°±æ˜¯ï¼š

  * æ¯ä¸€è¡Œ sum=1
  * æ¯ä¸€è¡Œæ˜¯â€œå½“å‰ token å¯¹å…¶ä»– token çš„åŠ æƒç³»æ•°â€

---

### ğŸ“Œ ä¸¾ä¾‹

å‡è®¾ï¼š

```python
wei = torch.tensor([[1, 2, 3],
                    [4, 5, 6]])
```

* shape = (2, 3)

å¦‚æœå†™ï¼š

```python
F.softmax(wei, dim=-1)
```

æ•ˆæœæ˜¯ï¼š

* å¯¹ **æ¯ä¸€è¡Œ** `[1,2,3]`ã€`[4,5,6]` å„è‡ªåš softmax
* å½’ä¸€åŒ–æˆï¼š

```
[[softmax(1,2,3)],  
 [softmax(4,5,6)]]
```

---

### ğŸ“Œ ä¸ºä»€ä¹ˆä¸ç”¨ dim=0ï¼Ÿ

å¦‚æœä½ å†™ï¼š

```python
F.softmax(wei, dim=0)
```

é‚£å°±ä¼šå¯¹ â€œæ¯ä¸€åˆ—â€ åš softmaxï¼Œé€»è¾‘å°±é”™äº†ã€‚
å› ä¸ºæˆ‘ä»¬å¸Œæœ› **â€œæ¯ä¸ª token çš„æ³¨æ„åŠ›åˆ†å¸ƒâ€** æ˜¯ä¸€è¡Œï¼Œå½’ä¸€åŒ–ä¸€è¡Œã€‚

---

### ğŸ“Œ æ€»ç»“ä¸€å¥è¯

```python
dim=-1
```

ğŸ‘‰ å°±æ˜¯â€œå¯¹æœ€åä¸€ç»´åš softmaxâ€
ğŸ‘‰ åœ¨ (T,T) é‡Œï¼Œå°±æ˜¯ **æŒ‰è¡Œåš softmax**ï¼Œ
ğŸ‘‰ å¾—åˆ° â€œå½“å‰ token å¯¹å…¶ä»– token çš„æ³¨æ„åŠ›åˆ†å¸ƒâ€ï¼

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥è¡¥å……è®²ï¼š

âœ… ä¸ºä»€ä¹ˆ softmax è¡Œ sum=1 å¾ˆå…³é”®ï¼Ÿ
âœ… å¦‚æœæ˜¯ (B, T, T)ï¼Œdim=-1 ä¼šæ€æ ·ï¼Ÿ
âœ… softmax ä¸ºä»€ä¹ˆé€‰åœ¨ attention ç”¨ï¼Ÿ

è¦ä¸è¦æˆ‘ç»§ç»­å¸®ä½ è®²ï¼ŸğŸŒŸ


# minor code cleanup

some quick preliminaries out of the way first the thing I'm kind of bothered by is that you see how we're passing in
vocap size into the Constructor there's no need to do that because vocap size is already defined uh up top as a global
variable so there's no need to pass this stuff around next what I want to do is I don't
want to actually create I want to create like a level of indirection here where we don't directly go to the embedding
for the um logits but instead we go through this intermediate phase because we're going to start making that bigger
so let me introduce a new variable n embed it shorted for number of embedding
Dimensions so nbed here will be say 32 that was a
suggestion from GitHub co-pilot by the way um it also suest 32 which is a good number so this is an embedding table and
only 32 dimensional embeddings so then here this is not going to give us logits directly instead
this is going to give us token embeddings that's I'm going to call it and then to go from the token Tings to
the logits we're going to need a linear layer so self. LM head let's call it
short for language modeling head is n and linear from n ined up to vocap size
and then when we swing over here we're actually going to get the loits by exactly what the co-pilot says now we
have to be careful here because this C and this C are not equal um this is nmed
C and this is vocap size so let's just say that n ined is equal to
C and then this just creates one spous layer of interaction through a linear layer but uh this should basically
run so we see that this runs and uh this currently looks kind of spous but uh
we're going to build on top of this now next up so far we've taken these indices

ä½ è´´çš„è¿™ä¸€æ®µæ˜¯åœ¨å¯¹åŸæ¥çš„ `BigramLanguageModel` åšä¸€ä¸ª**ç»“æ„ä¸Šçš„å°é‡æ„**ï¼Œä¸»è¦æœ‰ä¸¤ç‚¹å˜åŒ–ï¼š

---

## 1. ä¸å†æŠŠ `vocab_size` å½“ä½œæ„é€ å‡½æ•°å‚æ•°

åŸæœ¬ï¼š

```python
class BigramLanguageModel(nn.Module):
    def __init__(self, vocab_size):
        ...
```

è¿™é‡Œ `vocab_size` å…¶å®æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œç±»é‡Œç›´æ¥ç”¨å°±å¯ä»¥ï¼Œä¸ç”¨ä»å¤–é¢ä¼ è¿›æ¥ã€‚è¿™æ ·æ„é€ å‡½æ•°æ›´ç®€æ´ï¼Œä¹Ÿå°‘ä¸€ä¸ªæ˜¾å¼ä¾èµ–ã€‚

---

## 2. å¼•å…¥ `n_embed`ï¼ˆembedding ç»´åº¦ï¼‰

åŸæœ¬ bigram æ¨¡å‹çš„ `nn.Embedding` æ˜¯ï¼š

```python
nn.Embedding(vocab_size, vocab_size)
```

è¿™ç§æ–¹å¼ç›´æ¥è¾“å‡º `[B, T, vocab_size]` ä½œä¸º logitsã€‚

é‡æ„åå˜æˆï¼š

```python
n_embed = 32
self.token_embedding_table = nn.Embedding(vocab_size, n_embed)
```

ç°åœ¨ `Embedding` è¾“å‡º `[B, T, n_embed]`ï¼Œå®ƒä¸å†æ˜¯ç›´æ¥çš„ logitsï¼Œè€Œæ˜¯ä¸€ä¸ª**ä¸­é—´è¡¨ç¤º**ï¼ˆtoken embedding å‘é‡ï¼‰ã€‚

---

## 3. æ·»åŠ  `lm_head` çº¿æ€§å±‚

ä¸ºäº†ä» embedding å¾—åˆ° logitsï¼Œéœ€è¦å†åŠ ä¸€å±‚å…¨è¿æ¥ï¼š

```python
self.lm_head = nn.Linear(n_embed, vocab_size)
```

è¿™æ ·æ•°æ®æµæ˜¯ï¼š

```
idx â†’ token_embedding_table â†’ [B, T, n_embed] â†’ lm_head â†’ [B, T, vocab_size]
```

* `n_embed` æ˜¯ embedding ç©ºé—´çš„ç»´åº¦ã€‚
* `vocab_size` æ˜¯æœ€ç»ˆè¾“å‡ºçš„ç±»åˆ«æ•°ï¼ˆæ¯ä¸ªä½ç½®é¢„æµ‹ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒï¼‰ã€‚

---

## 4. å¥½å¤„

* **å¯æ‰©å±•æ€§**ï¼šåç»­å¯ä»¥åœ¨ embedding å’Œè¾“å‡ºå±‚ä¹‹é—´æ’æ›´å¤šå±‚ï¼ˆæ¯”å¦‚ Transformer blockï¼‰ï¼Œç°åœ¨è¿™ä¸ªç»“æ„ç›¸å½“äºé¢„ç•™äº†ä½ç½®ã€‚
* **å‚æ•°æ›´å°‘**ï¼šembedding ç»´åº¦é€šå¸¸æ¯” vocab size å°å¾ˆå¤šï¼Œå¯ä»¥å‡è½»è®¡ç®—å’Œå†…å­˜å‹åŠ›ã€‚
* **è¡¨è¾¾èƒ½åŠ›æ›´å¼º**ï¼šembedding å±‚å­¦ä¹ åˆ°çš„æ˜¯ token çš„ç¨ å¯†è¡¨ç¤ºï¼Œå¯ä»¥æ•æ‰è¯­ä¹‰ä¿¡æ¯ï¼Œä¸æ˜¯ç®€å•çš„ one-hot â†’ ç›´æ¥æ˜ å°„ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼ **æ”¹é€ å‰åå¯¹æ¯”çš„ç»“æ„å›¾**ï¼Œç›´è§‚çœ‹å‡ºæ•°æ®æµå’Œç»´åº¦å˜åŒ–ã€‚è¿™æ ·ä½ ä»¥ååœ¨åŠ  Transformer ç»“æ„æ—¶ï¼Œèƒ½ç›´æ¥å¥—ä¸Šå»ã€‚
ä½ è¦æˆ‘å¸®ä½ ç”»å—ï¼Ÿ


ä¸ºä»€ä¹ˆ PyTorch è¦å­˜ (out_features, in_features) è€Œä¸æ˜¯ (in_features, out_features)ï¼Ÿ

PyTorch æŠŠ `nn.Linear` çš„æƒé‡å­˜æˆ `(out_features, in_features)` ä¸»è¦æœ‰ä¸‰ä¸ªåŸå› ï¼Œéƒ½æ˜¯ä¸ºäº†è®¡ç®—æ•ˆç‡å’Œå®ç°æ–¹ä¾¿ï¼š

---

**1. ä¸æ•°å­¦å…¬å¼çš„å®šä¹‰ä¸€è‡´**
åœ¨çº¿æ€§ä»£æ•°é‡Œï¼Œçº¿æ€§å±‚æœ¬è´¨æ˜¯ä¸€ä¸ªä»¿å°„å˜æ¢ï¼š

$$
y = W x + b
$$

å¦‚æœ `x` æ˜¯ `(in_features,)` å‘é‡ï¼Œé‚£ä¹ˆ `W` è‡ªç„¶æ˜¯ `(out_features, in_features)`ï¼Œè¿™æ · `(out_features, in_features) @ (in_features,)` çš„ç»“æœå°±æ˜¯ `(out_features,)`ï¼Œæ­£å¥½è¡¨ç¤ºè¾“å‡ºç»´åº¦ã€‚
PyTorch ç›´æ¥ç”¨è¿™ç§å­˜å‚¨å½¢å¼ï¼Œå°±å’Œæ•°å­¦å®šä¹‰ä¸€è‡´ï¼Œé¿å…å¤šä½™è½¬ç½®å­˜å‚¨ã€‚

---

**2. ä¾¿äºæ‰¹é‡çŸ©é˜µä¹˜æ³•çš„é«˜æ•ˆå®ç°**
åœ¨æ‰¹å¤„ç†ï¼ˆbatchï¼‰æƒ…å†µä¸‹ï¼š

* è¾“å…¥ `X` æ˜¯ `(batch_size, in_features)`
* æƒé‡ `W` æ˜¯ `(out_features, in_features)`

çŸ©é˜µä¹˜æ³•å°±æ˜¯ï¼š

```
Y = X @ W.T + b
```

è¿™é‡Œè°ƒç”¨ `W.T` æ˜¯ä¸ºäº†æŠŠ `(out_features, in_features)` è½¬æˆ `(in_features, out_features)`ï¼Œä¸è¾“å…¥å¯¹é½åç”¨ BLAS/cuBLAS é«˜æ€§èƒ½åº“è®¡ç®—ã€‚
è¿™ç§å¸ƒå±€åœ¨ CPU/GPU å†…å­˜ä¸­æ›´ç¬¦åˆåˆ—ä¸»åºå­˜å‚¨çš„è¿ç»­æ€§ï¼Œè®¿é—®å†…å­˜æ—¶æ•ˆç‡æ›´é«˜ã€‚

---

**3. åå‘ä¼ æ’­æ—¶æ¢¯åº¦è®¡ç®—æ›´æ–¹ä¾¿**
åå‘ä¼ æ’­æ—¶éœ€è¦è®¡ç®—ï¼š

* å¯¹æƒé‡çš„æ¢¯åº¦ï¼š`grad_W = grad_output.T @ input`
* å¯¹è¾“å…¥çš„æ¢¯åº¦ï¼š`grad_input = grad_output @ W`

å¦‚æœ `W` æ˜¯ `(out_features, in_features)`ï¼Œè¿™ä¸¤ä¸ªè®¡ç®—éƒ½èƒ½ç›´æ¥ç”¨é«˜æ•ˆçš„ GEMMï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰å®ç°ï¼Œä¸”ä¸éœ€è¦é¢‘ç¹é‡æ–°å¸ƒå±€å†…å­˜ã€‚

---

ç®€å•è¯´ï¼š
PyTorch ç”¨ `(out_features, in_features)` æ˜¯å› ä¸ºï¼š

1. ç¬¦åˆæ•°å­¦å®šä¹‰
2. èƒ½ç›´æ¥åˆ©ç”¨ BLAS/cuBLAS è¿›è¡Œæ‰¹é‡é«˜æ•ˆçŸ©é˜µä¹˜æ³•
3. åå‘ä¼ æ’­æ¢¯åº¦è®¡ç®—æ›´é«˜æ•ˆã€å†…å­˜è®¿é—®æ¨¡å¼æ›´å‹å¥½

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼ **ä»è¾“å…¥åˆ°è¾“å‡ºçš„ç»´åº¦æµå‘å›¾**ï¼ŒæŠŠ `(B, T, C)` çš„ token embedding å¦‚ä½•é€šè¿‡ `(out_features, in_features)` çš„æƒé‡å˜æˆ logits ç›´è§‚å±•ç¤ºå‡ºæ¥ã€‚è¿™æ ·ä½ ä»¥åçœ‹ Transformer çš„ä»£ç æ—¶ä¼šæ›´å¿«ç†è§£çŸ©é˜µä¹˜æ³•æ–¹å‘ã€‚ä½ éœ€è¦æˆ‘ç”»å—ï¼Ÿ


# positional encoding

and we've encoded them based on the identity of the uh tokens in inside idx
the next thing that people very often do is that we're not just encoding the identity of these tokens but also their
position so we're going to have a second position uh embedding table here so self. position embedding table is an an
embedding of block size by an embed and so each position from zero to block size minus one will also get its own
embedding vector and then here first let me decode B BYT from idx do
shape and then here we're also going to have a pause embedding which is the positional embedding and these are this
is to arrange so this will be basically just integers from Z to T minus one and
all of those integers from 0 to T minus one get embedded through the table to create a t by
C and then here this gets renamed to just say x and x will be the addition of
the token embeddings with the positional embeddings and here the broadcasting note will work out so B by T by C plus T
by C this gets right aligned a new dimension of one gets added and it gets broadcasted across
batch so at this point x holds not just the token identities but the positions
at which these tokens occur and this is currently not that useful because of course we just have a simple byr model
so it doesn't matter if you're in the fifth position the second position or wherever it's all translation invariant at this stage uh so this information
currently wouldn't help uh but as we work on the self attention block we'll see that this starts to matter
okay so now we get the Crux of self attention so this is probably the most important part of this video to

å½“ç„¶å¯ä»¥ï¼Œè¿™ä¸€æ®µè®²çš„å…¶å®æ˜¯ **â€œä¸ºä»€ä¹ˆè¦åŠ  positional encodingï¼Œæ€ä¹ˆåŠ  positional embeddingâ€**ï¼Œæˆ‘æ¥è¯¦ç»†è§£é‡Šä¸€ä¸‹ï¼š

---

### ğŸ“Œ ä¹‹å‰æ¨¡å‹é‡Œï¼Œtokens æ˜¯æ€ä¹ˆç¼–ç çš„ï¼Ÿ

* åŸæ¥æˆ‘ä»¬æ˜¯ï¼š

```text
idx â†’ token_embedding_table â†’ å¾—åˆ° token embedding
```

* ä¹Ÿå°±æ˜¯è¯´ï¼Œ**æ¯ä¸ª token æ˜¯é€šè¿‡ token id â†’ æŸ¥è¡¨ â†’ å¾—åˆ° embedding å‘é‡**ï¼Œ
* è¿™ä¸ª embedding åªç¼–ç äº† **token æ˜¯ä»€ä¹ˆå­—/è¯**ï¼Œ
* ä½†æ˜¯ **â€œå®ƒåœ¨å¥å­é‡Œçš„ä½ç½®â€** æ²¡æœ‰ç¼–ç è¿›å»ï¼

---

### ğŸ“Œ ä¸ºä»€ä¹ˆè¦åŠ  positional encodingï¼Ÿ

ğŸ‘‰ å› ä¸ºï¼š

* Transformer æ˜¯ â€œå…¨å¹¶è¡Œâ€ è®¡ç®—çš„ï¼Œå®ƒ **ä¸å…³å¿ƒ token é¡ºåº**ã€‚
* å¦‚æœä¸ç»™æ¨¡å‹é¢å¤–ä¿¡æ¯ï¼Œå®ƒâ€œçœ‹ä¸å‡ºâ€è¿™ä¸ª token æ˜¯ç¬¬1ä¸ªã€ç¬¬2ä¸ªã€ç¬¬5ä¸ªâ€¦â€¦

**è€Œè¯­è¨€æ¨¡å‹æ˜¯â€œæœ‰é¡ºåºâ€çš„ä»»åŠ¡**ï¼š

* "I love you" å’Œ "You love I" æ˜¯ä¸ä¸€æ ·çš„å¥å­ã€‚

ğŸ‘‰ æ‰€ä»¥éœ€è¦ **å‘Šè¯‰æ¨¡å‹ï¼šæ¯ä¸ª token å¤„åœ¨å“ªä¸ªä½ç½®**ã€‚

---

### ğŸ“Œ æ€ä¹ˆåŠ  positional embeddingï¼Ÿ

```python
self.position_embedding_table = nn.Embedding(block_size, n_embed)
```

* æ–°å¢ä¸€ä¸ª **position embedding è¡¨**ï¼Œshape æ˜¯ï¼š

```
(block_size, n_embed)
```

ä¹Ÿå°±æ˜¯è¯´ï¼š

* ä½ç½® 0 â†’ ä¸€ä¸ª n\_embed ç»´å‘é‡
* ä½ç½® 1 â†’ ä¸€ä¸ª n\_embed ç»´å‘é‡
* ...
* ä½ç½® block\_size-1 â†’ ä¸€ä¸ª n\_embed ç»´å‘é‡

---

### ğŸ“Œ æ€ä¹ˆå–å‡º position embeddingï¼Ÿ

```python
pos_emb = self.position_embedding_table(torch.arange(T))
```

* `T` æ˜¯å½“å‰åºåˆ—é•¿åº¦
* ç”Ÿæˆ 0 \~ T-1 çš„æ•´æ•°åºåˆ— â†’ æ¯ä¸ªä½ç½®ä¸€ä¸ª id
* å–‚è¿› position\_embedding\_tableï¼Œå¾—åˆ°ï¼š

```
(T, n_embed)  â†’ æ¯ä¸ªä½ç½®çš„ embedding
```

---

### ğŸ“Œ æ€ä¹ˆåŠ åˆ° token embedding ä¸Šï¼Ÿ

```python
x = tok_emb + pos_emb  # broadcasting è‡ªåŠ¨å¤„ç†
```

* tok\_emb æ˜¯ï¼š

```
(B, T, n_embed)
```

* pos\_emb æ˜¯ï¼š

```
(T, n_embed)
```

* **é€šè¿‡ broadcasting**ï¼Œpos\_emb ä¼šè‡ªåŠ¨æ‰©å±•åˆ°ï¼š

```
(B, T, n_embed)
```

* ç„¶å **é€å…ƒç´ ç›¸åŠ **ï¼š

```
x[b, t, :] = tok_emb[b, t, :] + pos_emb[t, :]
```

---

### ğŸ“Œ åŠ  positional embedding æœ‰ä»€ä¹ˆç”¨ï¼Ÿ

âœ… è®©æ¨¡å‹çŸ¥é“ â€œå½“å‰ token åœ¨å¥å­ä¸­çš„ä½ç½®â€
âœ… Transformer æœ¬èº«æ˜¯ permutation-invariantï¼ˆæ— åºçš„ï¼‰ï¼Œéœ€è¦ positional embedding æ‰èƒ½å»ºæ¨¡é¡ºåº

---

### ğŸ“Œ ä¸ºä»€ä¹ˆ bigram æ¨¡å‹æš‚æ—¶è¿˜æ²¡ç”¨ä¸Šï¼Ÿ

ğŸ‘‰ ç°åœ¨æ¨¡å‹è¿˜æ˜¯ bigram æ¨¡å‹ â†’ åªçœ‹å½“å‰ tokenï¼Œä¸çœ‹ä¸Šä¸‹æ–‡
ğŸ‘‰ **ä½ç½®ä¿¡æ¯æš‚æ—¶æ²¡ç”¨**ï¼ˆçœ‹ä¸å‡ºæ•ˆæœï¼‰

âœ… ä½†æ˜¯ **é©¬ä¸Šè¦åŠ  self-attention**ï¼Œself-attention æ˜¯å…¨å±€çœ‹çš„ â†’ ä½ç½®å°±å¾ˆé‡è¦äº†ï¼
âœ… æ‰€ä»¥æå‰åŠ å¥½ position embeddingï¼Œå‡†å¤‡åé¢ç”¨ï¼

---

### ğŸ“Œ æ€»ç»“ä¸€å¥è¯

ğŸ‘‰ è¿™ä¸€æ®µè®²çš„æ˜¯ï¼š

**â€œåŠ  positional embeddingï¼Œå‘Šè¯‰æ¨¡å‹ token åœ¨å¥å­é‡Œçš„ä½ç½®ï¼Œ
è¿™æ · Transformer æ‰èƒ½å»ºæ¨¡é¡ºåºï¼â€**

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ è®²è®²ï¼š

âœ… positional encoding å’Œ positional embedding æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆç”¨ nn.Embedding è€Œä¸æ˜¯å›ºå®šçš„ sin/cosï¼Ÿ
âœ… GPT/Transformer é‡Œå®é™…æ˜¯æ€ä¹ˆåŠ çš„ï¼Ÿ

è¦ä¸è¦æˆ‘å¸®ä½ è¯¦ç»†è®²ä¸€è®²ï¼ŸğŸŒŸ

self.position_embedding_tableä¸­çš„æƒé‡å€¼åœ¨è®­ç»ƒæ—¶ä¼šå˜åŒ–å—

ä¼šçš„ã€‚

`self.position_embedding_table` æ˜¯ `nn.Embedding` åˆ›å»ºçš„å¯è®­ç»ƒå‚æ•°çŸ©é˜µï¼Œå®ƒå’Œ `self.token_embedding_table` ä¸€æ ·ï¼Œé»˜è®¤æƒ…å†µä¸‹ `requires_grad=True`ã€‚
è¿™æ„å‘³ç€ï¼š

* åœ¨**å‰å‘ä¼ æ’­**æ—¶ï¼Œå®ƒä¼šæ ¹æ®è¾“å…¥çš„ position indexï¼ˆ0, 1, 2, ... T-1ï¼‰æŸ¥æ‰¾å¯¹åº”çš„å‘é‡ï¼Œå¹¶ä¸ token embedding ç›¸åŠ ã€‚
* åœ¨**åå‘ä¼ æ’­**æ—¶ï¼Œæ¢¯åº¦ä¼šä» loss åä¼ åˆ°è¿™äº›ä½ç½®å‘é‡çš„å¯¹åº”è¡Œï¼Œæ›´æ–°å…¶æƒé‡å€¼ã€‚
* åœ¨**ä¼˜åŒ–å™¨ step** æ—¶ï¼ˆæ¯”å¦‚ Adam æˆ– SGDï¼‰ï¼Œå®ƒçš„æƒé‡ä¼šè¢«æ›´æ–°ï¼Œä»è€Œè®©æ¨¡å‹å­¦ä¼šæ›´åˆé€‚çš„â€œä½ç½®ä¿¡æ¯è¡¨ç¤ºâ€ã€‚

---

### å½¢è±¡ç±»æ¯”

ä½ å¯ä»¥æŠŠ `position_embedding_table` æƒ³è±¡æˆä¸€ç»„â€œä½ç½®æ ‡ç­¾è´´çº¸â€çš„å­—å…¸ï¼š

* ä¸€å¼€å§‹ï¼Œè¿™äº›è´´çº¸çš„å›¾æ¡ˆæ˜¯éšæœºçš„ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰ã€‚
* è®­ç»ƒæ—¶ï¼Œæ¨¡å‹ä¼šä¸æ–­è°ƒæ•´è¿™äº›è´´çº¸ï¼Œè®©å®ƒä»¬ä¸ token embedding é…åˆå¾—æ›´å¥½ï¼Œä»è€Œå¸®åŠ©æ¨¡å‹ç†è§£â€œæŸä¸ªä½ç½®çš„è¯â€åœ¨å¥å­ç»“æ„ä¸­çš„ä½œç”¨ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼ \*\*â€œä½ç½®ç¼–ç æƒé‡éšè®­ç»ƒå˜åŒ–çš„ç¤ºæ„å›¾â€\*\*ï¼Œè®©ä½ æ›´ç›´è§‚åœ°çœ‹åˆ°å®ƒæ˜¯æ€ä¹ˆæ›´æ–°çš„ã€‚è¿™æ ·æ›´å®¹æ˜“ç†è§£ä¸ºä»€ä¹ˆå®ƒä¼šå˜åŒ–ã€‚



# THE CRUX OF THE VIDEO: version 4: self-attention

understand we're going to implement a small self attention for a single individual head as they're called so we
start off with where we were so all of this code is familiar so right now I'm
working with an example where I Chang the number of channels from 2 to 32 so we have a 4x8 arrangement of tokens and
each to and the information each token is currently 32 dimensional but we just are working with random
numbers now we saw here that the code as we had it before does a uh simple weight
simple average of all the past tokens and the current token so it's just the
previous information and current information is just being mixed together in an average and that's what this code currently achieves and it Doo by
creating this lower triangular structure which allows us to mask out this uh we
uh Matrix that we create so we mask it out and then we normalize it and
currently when we initialize the affinities between all the different sort of tokens or nodes I'm going to use
those terms interchangeably so when we initialize the affinities between all the different tokens to be zero then we see that way
gives us this um structure where every single row has these um uniform numbers
and so that's what that's what then uh in this Matrix multiply makes it so that we're doing a simple
average now we don't actually want this to be all uniform because different uh
tokens will find different other tokens more or less interesting and we want that to be data dependent so for example
if I'm a vowel then maybe I'm looking for consonants in my past and maybe I want to know what those consonants are
and I want that information to flow to me and so I want to now gather information from the past but I want to
do it in the data dependent way and this is the problem that self attention solves now the way self attention solves
this is the following every single node or every single token at each position
will emit two vectors it will emit a query and it will emit a
key now the query Vector roughly speaking is what am I looking for and
the key Vector roughly speaking is what do I contain and then the way we get
affinities between these uh tokens now in a sequence is we basically just do a
do product between the keys and the queries so my query dot products with
all the keys of all the other tokens and that dot product now becomes
wayy and so um if the key and the query are sort of aligned they will interact
to a very high amount and then I will get to learn more about that specific token as opposed to any other token in
the sequence so let's implement this
now we're going to implement a single what's called head of self
attention so this is just one head there's a hyper parameter involved with these heads which is the head size and
then here I'm initializing linear modules and I'm using bias equals false so these are just going to apply a
matrix multiply with some fixed weights and now let me produce a key and
q k and Q by forwarding these modules on X so the size of this will now
become B by T by 16 because that is the head size and the same here B by T by
16 so this being the head size so you see here that when I forward this linear
on top of my X all the tokens in all the positions in the B BYT Arrangement all
of them them in parallel and independently produce a key and a query so no communication has happened
yet but the communication comes now all the queries will do product with all the
keys so basically what we want is we want way now or the affinities between these to be query multiplying key but we
have to be careful with uh we can't Matrix multiply this we actually need to transpose uh K but we have to be also
careful because these are when you have The Bash Dimension so in particular we want to transpose uh the last two
dimensions dimension1 and dimension -2 so
-21 and so this Matrix multiply now will basically do the following B by T by
16 Matrix multiplies B by 16 by T to give us B by T by
T right so for every row of B we're now going to
have a t Square Matrix giving us the affinities and these are now the way so
they're not zeros they are now coming from this dot product between the keys and the queries so this can now run I
can I can run this and the weighted aggregation now is a function in a data
Bandon manner between the keys and queries of these nodes so just inspecting what happened
here the way takes on this form and you see that before way was uh just
a constant so it was applied in the same way to all the batch elements but now every single batch elements will have
different sort of we because uh every single batch element contains different uh tokens at different positions and so
this is not data dependent so when we look at just the zeroth uh Row for
example in the input these are the weights that came out and so you can see now that they're not just exactly
uniform um and in particular as an example here for the last row this was the eighth token and the eighth token
knows what content it has and it knows at what position it's in and now the E token based on that uh creates a query
hey I'm looking for this kind of stuff um I'm a vowel I'm on the E position I'm looking for any consonant at positions
up to four and then all the nodes get to emit keys and maybe one of the channels
could be I am a I am a consonant and I am in a position up to four and that that key would have a high number in
that specific Channel and that's how the query and the key when they do product they can find each other and create a
high affinity and when they have a high Affinity like say uh this token was
pretty interesting to uh to this eighth token when they have a high Affinity
then through the softmax I will end up aggregating a lot of its information into my position and so I'll get to
learn a lot about it now just this we're looking at way
after this has already happened um let me erase this operation as well so let
me erase the masking and the softmax just to show you the under the hood internals and how that works so without
the masking in the softmax Whey comes out like this right this is the outputs of the do products um and these are the
raw outputs and they take on values from negative you know two to positive two Etc so that's the raw interactions and
raw affinities between all the nodes but now if I'm going if I'm a fifth node I will not want to aggregate anything from
the sixth node seventh node and the eighth node so actually we use the upper triangular masking so those are not
allowed to communicate and now we actually want to have a nice uh distribution uh so we
don't want to aggregate negative .11 of this node that's crazy so instead we exponentiate and normalize and now we
get a nice distribution that sums to one and this is telling us now in the data dependent manner how much of information
to aggregate from any of these tokens in the past so that's way and it's not zeros
anymore but but it's calculated in this way now there's one more uh part to a
single self attention head and that is that when we do the aggregation we don't actually aggregate the tokens exactly we
aggregate we produce one more value here and we call that the
value so in the same way that we produced p and query we're also going to create a value
and then here we don't aggregate X we calculate a v which is
just achieved by uh propagating this linear on top of X again and then we
output way multiplied by V so V is the elements that we aggregate or the the
vectors that we aggregate instead of the raw X and now of course uh this will make it
so that the output here of this single head will be 16 dimensional because that is the head
size so you can think of X as kind of like private information to this token if you if you think about it that way so
X is kind of private to this token so I'm a fifth token at some and I have some identity and uh my information is
kept in Vector X and now for the purposes of the single head here's what I'm interested in here's what I have and
if you find me interesting here's what I will communicate to you and that's stored in v and so V is the thing that
gets aggregated for the purposes of this single head between the different notes and that's uh basically the self
attention mechanism this is this is what it does there are a few notes that I would make like to make about attention

å½“ç„¶å¯ä»¥ï¼Œè¿™æ®µæ˜¯æ•´æ®µè§†é¢‘çš„**æ ¸å¿ƒ**ï¼ˆCRUXï¼‰ï¼Œè®²è§£äº† **Self-Attention çš„æœ¬è´¨é€»è¾‘**ï¼Œæˆ‘å¸®ä½ è¯¦ç»†è§£é‡Šï¼š

---

### ğŸ“Œ ä¹‹å‰ç‰ˆæœ¬åšäº†ä»€ä¹ˆï¼Ÿ

ä¹‹å‰ version 1/2/3ï¼š

ğŸ‘‰ æ¯ä¸ª token é€šè¿‡ **ç®€å•å¹³å‡/åŠ æƒå¹³å‡** èšåˆå‰é¢çš„ token ä¿¡æ¯ï¼Œ
ğŸ‘‰ ä½†æ˜¯ **åŠ æƒç³»æ•°æ˜¯å›ºå®šçš„**ï¼Œæ²¡æ³•æ ¹æ® token å†…å®¹åŠ¨æ€è°ƒæ•´ã€‚

---

### ğŸ“Œ Self-Attention è¦è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ

**æƒ³è¦åšåˆ°ï¼š**

âœ… æ¯ä¸ª token èƒ½â€œä¸»åŠ¨â€å†³å®šå¯¹å“ªäº›å‰é¢ token æ›´æ„Ÿå…´è¶£ï¼Œ
âœ… æ¯æ¬¡å¤„ç†ä¸åŒçš„è¾“å…¥ï¼ˆä¸åŒ token å†…å®¹ï¼‰æ—¶ï¼Œæƒé‡è¦**åŠ¨æ€å˜åŒ–**ï¼Œ
âœ… ä¸åŒ token å†…å®¹ â†’ ä¸åŒ attention æƒé‡ï¼Œä½“ç°â€œç†è§£â€ä¸Šä¸‹æ–‡ã€‚

---

### ğŸ“Œ Self-Attention æ€ä¹ˆå®ç°è¿™ä¸ªç›®æ ‡ï¼Ÿ

æ ¸å¿ƒæ€æƒ³ï¼š

ğŸ‘‰ æ¯ä¸ª token ç”Ÿæˆä¸¤ä¸ªå‘é‡ï¼š

1ï¸âƒ£ **Queryï¼ˆæŸ¥è¯¢ï¼‰å‘é‡** â€”â€” æˆ‘åœ¨â€œæ‰¾â€ä»€ä¹ˆ
2ï¸âƒ£ **Keyï¼ˆé”®ï¼‰å‘é‡** â€”â€” æˆ‘â€œæ˜¯ä»€ä¹ˆâ€

---

### ğŸ“Œ æ€ä¹ˆç”¨ Query å’Œ Key è®¡ç®—â€œå…´è¶£ç¨‹åº¦â€ï¼Ÿ

ğŸ‘‰ æ¯ä¸ª token çš„ query å‘é‡ï¼Œå’Œ **æ‰€æœ‰ token çš„ key å‘é‡** åš**ç‚¹ç§¯**ï¼Œ
ğŸ‘‰ å¾—åˆ°ä¸€å † **â€œå…´è¶£ç¨‹åº¦ï¼ˆaffinityï¼‰â€**ï¼Œå½¢æˆ attention æƒé‡çŸ©é˜µã€‚

ä¸¾ä¾‹ï¼š

```
token5.query â¨€ token1.key  
token5.query â¨€ token2.key  
token5.query â¨€ token3.key  
...  
token5.query â¨€ token5.key
```

è¿™æ · token5 çŸ¥é“ï¼š
æˆ‘â€œå¤šå¤§ç¨‹åº¦ä¸Šâ€è¦å…³æ³¨å†å² token1ã€2ã€3ã€4ã€5â€¦â€¦

---

### ğŸ“Œ ä¸ºä»€ä¹ˆåŠ  Maskï¼Ÿ

è¯­è¨€æ¨¡å‹æ˜¯**è‡ªå›å½’**çš„ï¼Œtoken5 ä¸èƒ½çœ‹æœªæ¥ï¼ˆtoken6,7,8ï¼‰ï¼Œ
æ‰€ä»¥ç”¨ **tril mask** æŠŠæœªæ¥ token çš„æƒé‡ç½®æˆ -infï¼Œsoftmax å â†’ 0ã€‚

---

### ğŸ“Œ ä¸ºä»€ä¹ˆè¦ softmaxï¼Ÿ

ç‚¹ç§¯ç»“æœæœ‰æ­£æœ‰è´Ÿï¼Œscale ä¸ä¸€ï¼Œ
softmax å½’ä¸€åŒ–ä¸ºæ¦‚ç‡ï¼Œä¿è¯ï¼š

âœ… æƒé‡ sum=1
âœ… ä¸ä¼šå‡ºç°â€œè´Ÿå…³æ³¨â€
âœ… ä¾¿äºè®­ç»ƒä¼˜åŒ–

---

### ğŸ“Œ è¿˜éœ€è¦ Valueï¼ˆVï¼‰ï¼Ÿ

âœ… éœ€è¦ï¼

ğŸ‘‰ æ¯ä¸ª token è¿˜ç”Ÿæˆä¸€ä¸ª **Value å‘é‡**ï¼Œ
ğŸ‘‰ æœ€åä¸æ˜¯èšåˆ Xï¼Œè€Œæ˜¯èšåˆ Vï¼

é€»è¾‘æ˜¯ï¼š

```
query â¨€ key â†’ å¾—åˆ° attention æƒé‡  
attention æƒé‡ â¨€ value â†’ å¾—åˆ°æ–°çš„ token è¡¨ç¤º
```

---

### ğŸ“Œ ä¸ºä»€ä¹ˆè¦ Vï¼Ÿ

å› ä¸º X æ˜¯ token çš„**ç§æœ‰ä¿¡æ¯**ï¼Œ
V æ˜¯ token å‡†å¤‡â€œåˆ†äº«â€å‡ºå»çš„ä¿¡æ¯ï¼Œ
ä¸åŒ headã€ä¸åŒ query å…³æ³¨ä¸åŒ Vï¼Œå¯ä»¥æœ‰ä¸åŒçš„â€œçœ‹æ³•â€ã€‚

---

### ğŸ“Œ è®¡ç®—æµç¨‹æ€»ç»“

1ï¸âƒ£ X â†’ Linear â†’ å¾—åˆ° Qã€Kã€V

```
Q = Linear_Q(X)  
K = Linear_K(X)  
V = Linear_V(X)
```

2ï¸âƒ£ è®¡ç®— attention æƒé‡ï¼š

```
wei = Q @ K^T / sqrt(head_size)  
wei = masked_fill(æœªæ¥ token = -inf)  
wei = softmax(wei, dim=-1)
```

3ï¸âƒ£ åŠ æƒèšåˆ Valueï¼š

```
out = wei @ V
```

---

### ğŸ“Œ ä¸ºä»€ä¹ˆå« Headï¼Ÿ

âœ… è¿™ä¸ªåªæ˜¯ä¸€ä¸ª Headï¼Œ
âœ… GPTã€Transformer é‡Œä¼šæœ‰ **å¤šä¸ª head**ï¼Œå¹¶è¡Œåš attentionï¼Œå­¦ä¹ ä¸åŒå…³æ³¨æ¨¡å¼ã€‚

---

### ğŸ“Œ æ€»ç»“ä¸€å¥è¯

ğŸ‘‰ Self-Attention çš„æœ¬è´¨ï¼š

**â€œæ¯ä¸ª token ç”¨ Q å‘é‡å†³å®šå¯¹å…¶ä»– token K å‘é‡çš„å…´è¶£ç¨‹åº¦ï¼Œ
æ ¹æ®å…´è¶£ç¨‹åº¦åŠ æƒèšåˆå…¶ä»– token çš„ V å‘é‡ï¼Œå¾—åˆ°æ–°çš„ token è¡¨ç¤ºã€‚â€**

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ è¯¦ç»†è¡¥å……ï¼š

âœ… ä¸ºä»€ä¹ˆ QK è¦é™¤ä»¥ sqrt(dk)ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆè¦ç”¨å¤šå¤´ attentionï¼Ÿ
âœ… æœ€åæ€ä¹ˆæ‹¼æ¥å¤šå¤´ï¼Ÿ
âœ… Transformer é‡Œå®Œæ•´ Self-Attention æ˜¯ä»€ä¹ˆæ ·å­ï¼Ÿ

è¦ä¸è¦æˆ‘å†å¸®ä½ è®²è®²ï¼ŸğŸŒŸ


key = nn.Linear(C, head_size, bias=False)
query = nn.Linear(C, head_size, bias=False)
value = nn.Linear(C, head_size, bias=False)

ä¸ºä»€ä¹ˆkey query valueç”¨ nn.Linear æ¥è¡¨ç¤º


ä½ é—®å¾—éå¸¸å¥½ï¼æˆ‘æ¥è¯¦ç»†è§£é‡Šä¸€ä¸‹ï¼š

---

### ğŸ“Œ è¿™ä¸‰å¥è¯åœ¨åšä»€ä¹ˆï¼Ÿ

```python
key   = nn.Linear(C, head_size, bias=False)
query = nn.Linear(C, head_size, bias=False)
value = nn.Linear(C, head_size, bias=False)
```

æ„æ€æ˜¯ï¼š

* æŠŠåŸå§‹çš„ token embeddingï¼ˆ32 ç»´ï¼‰**æ˜ å°„æˆ head\_size ç»´ï¼ˆè¿™é‡Œæ˜¯ 16 ç»´ï¼‰**
* æ¯ä¸ª token é€šè¿‡ä¸€ä¸ª nn.Linear å±‚ç”Ÿæˆï¼š

| åå­—    | å«ä¹‰                   | ç”¨é€”           |
| ----- | -------------------- | ------------ |
| key   | è¿™ä¸ª token â€œæ˜¯ä»€ä¹ˆâ€       | ä¾›å…¶ä»– token å‚è€ƒ |
| query | è¿™ä¸ª token â€œæˆ‘æƒ³æ‰¾ä»€ä¹ˆâ€     | å»æŸ¥è¯¢å…¶ä»– token  |
| value | è¿™ä¸ª token â€œæˆ‘å‡†å¤‡åˆ†äº«ä»€ä¹ˆä¿¡æ¯â€ | ä¾›å…¶ä»– token èšåˆ |

---

### ğŸ“Œ ä¸ºä»€ä¹ˆç”¨ nn.Linear å®ç°ï¼Ÿ

ğŸ‘‰ æœ¬è´¨ä¸Šï¼šQã€Kã€V æ˜¯ token embeddingï¼ˆxï¼‰çš„ **ä¸åŒâ€œæŠ•å½±â€æ–¹å‘**ï¼Œ
ğŸ‘‰ å°±æ˜¯ç”¨ **ä¸åŒå‚æ•°çŸ©é˜µ** åšçº¿æ€§å˜æ¢ï¼Œå­¦å‡ºâ€œåœ¨ä¸åŒè¯­ä¹‰ç©ºé—´ä¸‹çš„è¡¨ç¤ºâ€ã€‚

---

### ğŸ“Œ æ›´å½¢è±¡çš„è§£é‡Š

ä½ å¯ä»¥æŠŠ Qã€Kã€V æƒ³æˆï¼š

```
query = W_q * x  
key   = W_k * x  
value = W_v * x
```

å…¶ä¸­ï¼š

* W\_qã€W\_kã€W\_v æ˜¯å‚æ•°çŸ©é˜µï¼Œå­¦ä¹ å‡ºæ¥çš„
* ä½œç”¨ï¼šæŠŠ xï¼ˆtoken embeddingï¼‰æ˜ å°„åˆ° 3 ä¸ªä¸åŒçš„ç©ºé—´

---

### ğŸ“Œ ä¸ºä»€ä¹ˆæ˜¯ nn.Linearï¼Ÿ

`nn.Linear(C, head_size)` å°±ç›¸å½“äºå®ç°ï¼š

```python
output = x @ W.T
```

* ä¹Ÿå°±æ˜¯ **çŸ©é˜µä¹˜æ³• + å¯å­¦ä¹ å‚æ•° W**
* W ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦å‡ºâ€œä»€ä¹ˆ queryã€ä»€ä¹ˆ keyã€ä»€ä¹ˆ value æ˜¯æœ‰æ•ˆçš„â€

---

### ğŸ“Œ ä¸ºä»€ä¹ˆéœ€è¦ 3 ä¸ªä¸åŒçš„ Linearï¼Ÿ

å› ä¸ºï¼š

* Q æ˜¯â€œè¦å»æŸ¥ä»€ä¹ˆâ€
* K æ˜¯â€œæˆ‘æ˜¯ä»€ä¹ˆâ€
* V æ˜¯â€œæˆ‘è¦åˆ†äº«ä»€ä¹ˆâ€

æ¯ä¸ªä½œç”¨ä¸åŒï¼Œè¯­ä¹‰ç©ºé—´ä¸åŒï¼Œæ‰€ä»¥éœ€è¦ **ç‹¬ç«‹çš„ Linear å±‚ï¼Œå„è‡ªæœ‰å‚æ•° W**ã€‚

---

### ğŸ“Œ æ€»ç»“ä¸€å¥è¯

ğŸ‘‰ ç”¨ `nn.Linear` æ˜¯ä¸ºäº†ï¼š

âœ… æŠŠåŸå§‹ token embedding æŠ•å½±æˆ **Q/K/V 3 ä¸ªç©ºé—´**
âœ… å‚æ•°å¯å­¦ä¹ ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®æ•°æ®è‡ªå·±å†³å®šä»€ä¹ˆæ˜¯â€œæœ‰ç”¨çš„ä¿¡æ¯â€
âœ… æ˜¯ Transformer è®ºæ–‡é‡Œçš„æ ‡å‡†åšæ³•

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥è¡¥å……è®²ï¼š

âœ… ä¸ºä»€ä¹ˆå¯ä»¥å¤šä¸ª headï¼Ÿ
âœ… ä¸ºä»€ä¹ˆ head\_size å¯ä»¥æ¯” C å°ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆ V ä¸ç›´æ¥ç”¨ Xï¼Ÿ

è¦ä¸è¦æˆ‘ç»§ç»­è¯¦ç»†è®²è®²ï¼ŸğŸŒŸ

```python
# version 4: self-attention!
torch.manual_seed(1337)
B,T,C = 4,8,32 # batch, time, channels
x = torch.randn(B,T,C)

# let's see a single Head perform self-attention
head_size = 16
key = nn.Linear(C, head_size, bias=False)
query = nn.Linear(C, head_size, bias=False)
value = nn.Linear(C, head_size, bias=False)
k = key(x)   # (B, T, 16)
q = query(x) # (B, T, 16)
wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)

tril = torch.tril(torch.ones(T, T))
#wei = torch.zeros((T,T))
wei = wei.masked_fill(tril == 0, float('-inf'))
wei = F.softmax(wei, dim=-1)

v = value(x)
out = wei @ v

è§£é‡Šä¸€ä¸‹
```

å½“ç„¶å¯ä»¥ï¼è¿™æ®µä»£ç æ˜¯ **version 4: self-attention!** çš„ PyTorch ä»£ç å®ç°ï¼Œ
æˆ‘æ¥ä¸€æ­¥æ­¥å¸®ä½ è¯¦ç»†è®²è§£ï¼š

---

### ğŸ“Œ æ•´ä½“ç›®æ ‡

ğŸ‘‰ ç”¨ Qã€Kã€V å’ŒçŸ©é˜µä¹˜æ³•ï¼Œ**å®ç°ä¸€ä¸ªå•å¤´ self-attention**ï¼Œ
ğŸ‘‰ å¾—åˆ°æ–°çš„ token è¡¨ç¤º `out`ã€‚

---

### ğŸ“Œ ä»£ç è¯¦ç»†æ‹†è§£

---

#### 1ï¸âƒ£ åˆå§‹åŒ–è¾“å…¥æ•°æ®

```python
torch.manual_seed(1337)
B, T, C = 4, 8, 32
x = torch.randn(B, T, C)
```

* B = 4 â†’ batch size
* T = 8 â†’ æ¯æ¡åºåˆ— 8 ä¸ª token
* C = 32 â†’ æ¯ä¸ª token æ˜¯ 32 ç»´å‘é‡

x shapeï¼š

```
(B, T, C) = (4, 8, 32)
```

---

#### 2ï¸âƒ£ å®šä¹‰ Q, K, V çš„ Linear å±‚

```python
head_size = 16
key   = nn.Linear(C, head_size, bias=False)
query = nn.Linear(C, head_size, bias=False)
value = nn.Linear(C, head_size, bias=False)
```

æ¯ä¸ª token é€šè¿‡ Linear æ˜ å°„æˆï¼š

* Q (query) å‘é‡
* K (key) å‘é‡
* V (value) å‘é‡

ç»´åº¦ï¼š

```
(B, T, 16)
```

---

#### 3ï¸âƒ£ è®¡ç®— Q å’Œ K

```python
k = key(x)   # (B, T, 16)
q = query(x) # (B, T, 16)
```

* å¯¹ batch é‡Œçš„æ¯ä¸ª tokenï¼Œ**ç‹¬ç«‹è®¡ç®— Q å’Œ K å‘é‡**
* ä¸åŒ token å¾—åˆ°ä¸åŒçš„ Q/Kï¼Œè¡¨æ˜å®ƒä»¬â€œæ˜¯è°â€â€œæƒ³æ‰¾ä»€ä¹ˆâ€

---

#### 4ï¸âƒ£ è®¡ç®— Attention æƒé‡

```python
wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)
```

ğŸ‘‰ è®¡ç®—ï¼š

```
q_i â‹… k_j  â†’ è¡¨ç¤º token_i å¯¹ token_j çš„å…´è¶£ç¨‹åº¦
```

ç»“æœæ˜¯ï¼š

```
wei.shape = (B, T, T)
```

æ„æ€æ˜¯ï¼š

```
wei[b, i, j] = batch b é‡Œï¼Œç¬¬ i ä¸ª token å¯¹ ç¬¬ j ä¸ª token çš„å…´è¶£
```

---

#### 5ï¸âƒ£ mask æ‰æœªæ¥ token

```python
tril = torch.tril(torch.ones(T, T))
wei = wei.masked_fill(tril == 0, float('-inf'))
```

ğŸ‘‰ è¯­è¨€æ¨¡å‹æ˜¯ **è‡ªå›å½’**ï¼Œåªèƒ½çœ‹â€œè¿‡å»+ç°åœ¨â€ï¼Œä¸èƒ½çœ‹â€œæœªæ¥â€ã€‚

* ç”¨ tril (ä¸‹ä¸‰è§’çŸ©é˜µ) åš maskï¼Œ
* æŠŠâ€œæœªæ¥ tokenâ€ æƒé‡ç½®æˆ `-inf`ï¼Œ
* softmax å â†’ æƒé‡ = 0

---

#### 6ï¸âƒ£ softmax å½’ä¸€åŒ–

```python
wei = F.softmax(wei, dim=-1)
```

ğŸ‘‰ softmax å¯¹ **æ¯è¡Œ** å½’ä¸€åŒ–æˆæ¦‚ç‡åˆ†å¸ƒ (sum=1)ï¼š

```
wei[b, i, :] = ç¬¬ i ä¸ª token å¯¹å†å² token çš„åŠ æƒç³»æ•°
```

---

#### 7ï¸âƒ£ è®¡ç®— V

```python
v = value(x)  # (B, T, 16)
```

ğŸ‘‰ æ¯ä¸ª token ç”Ÿæˆ **è‡ªå·±è¦åˆ†äº«çš„ value ä¿¡æ¯**ã€‚

---

#### 8ï¸âƒ£ èšåˆ Vï¼Œå¾—åˆ°æ–° token è¡¨ç¤º

```python
out = wei @ v
```

shapeï¼š

```
(B, T, T) @ (B, T, 16) â†’ (B, T, 16)
```

ğŸ‘‰ æ¯ä¸ª tokenï¼š

* å¯¹å†å² token çš„ V åšåŠ æƒå¹³å‡ï¼Œ
* å¾—åˆ° **æ›´æ–°åçš„ token è¡¨ç¤º out**ã€‚

---

### ğŸ“Œ æ•´ä½“æµç¨‹æ€»ç»“

```
x â†’ Q, K, V  
Q @ K^T â†’ attention scores  
mask + softmax â†’ attention weights  
attention weights @ V â†’ out (æ–° token è¡¨ç¤º)
```

---

### ğŸ“Œ è¿™ä¸€æ®µçš„æ„ä¹‰

ğŸ‘‰ è¿™æ®µä»£ç å°±æ˜¯ **å•ä¸ª self-attention head çš„æ ¸å¿ƒå®ç°**ï¼Œ
ğŸ‘‰ è·Ÿ Transformer è®ºæ–‡é‡Œçš„ attention æ•°å­¦å®Œå…¨ä¸€è‡´ï¼

---

### ğŸ“Œ æœ€ç»ˆè¾“å‡º out

```
out.shape = (B, T, head_size)
```

æ¯ä¸ª token æœ‰äº† **å¸¦ä¸Šä¸‹æ–‡æ„ŸçŸ¥** çš„æ–°è¡¨ç¤º outã€‚

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘å¯ä»¥ç»§ç»­è®²è®²ï¼š

âœ… ä¸ºä»€ä¹ˆè¦é™¤ä»¥ sqrt(d\_k)ï¼Ÿï¼ˆè¿™æ®µä»£ç é‡Œè¿˜æ²¡åŠ ï¼‰
âœ… å¤šå¤´ attention æ€ä¹ˆåšï¼Ÿ
âœ… ä¸ºä»€ä¹ˆè¦ç”¨ V è€Œä¸æ˜¯ Xï¼Ÿ

è¦ä¸è¦æˆ‘å†å¸®ä½ è¯¦ç»†è®²è®²ï¼ŸğŸŒŸ


ä½ è´´çš„è¿™æ®µæ˜¯ Andrej Karpathy åœ¨è§£é‡Š **å•å¤´ï¼ˆsingle headï¼‰è‡ªæ³¨æ„åŠ›æœºåˆ¶** æ—¶çš„è¯¦ç»†æ€è·¯ï¼Œæˆ‘å¸®ä½ ç”¨é€šä¿—æ˜“æ‡‚çš„æ–¹å¼æ‹†å¼€æ¥è®²ä¸€ä¸‹ï¼Œå¹¶ç»“åˆä»£ç é€»è¾‘å’Œæ•°å­¦æ„ä¹‰ã€‚

---

## 1. èƒŒæ™¯ï¼šä¸ºä»€ä¹ˆéœ€è¦è‡ªæ³¨æ„åŠ›

ä¹‹å‰çš„ç‰ˆæœ¬åªæ˜¯åšäº†**å¯¹å½“å‰ token å’Œä¹‹å‰ token çš„å‡å€¼èšåˆ**ï¼ˆé€šè¿‡ä¸‹ä¸‰è§’çŸ©é˜µ + å½’ä¸€åŒ–ï¼‰ï¼Œæ¯ä¸ªä½ç½®æ”¶åˆ°çš„å†å²ä¿¡æ¯æƒé‡éƒ½æ˜¯ä¸€æ ·çš„ï¼Œè¿™æ ·ä¸èƒ½æ ¹æ®æ•°æ®æœ¬èº«å†³å®šâ€œæˆ‘æ›´åº”è¯¥å…³æ³¨è°â€ã€‚

è‡ªæ³¨æ„åŠ›çš„ç›®çš„å°±æ˜¯è®©æ¯ä¸ª token **æ ¹æ®å½“å‰ä¸Šä¸‹æ–‡**ï¼ŒåŠ¨æ€å†³å®šè¦ä»å“ªäº› token è·å–æ›´å¤šä¿¡æ¯ã€‚

---

## 2. æ ¸å¿ƒæ€æƒ³ï¼šQueryã€Keyã€Value

åœ¨è‡ªæ³¨æ„åŠ›ä¸­ï¼Œæ¯ä¸ª token ä¼šäº§ç”Ÿä¸‰ä¸ªå‘é‡ï¼š

* **Query (Q)**ï¼šæˆ‘åœ¨æ‰¾ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
* **Key (K)**ï¼šæˆ‘åŒ…å«ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
* **Value (V)**ï¼šå¦‚æœåˆ«äººå¯¹æˆ‘æ„Ÿå…´è¶£ï¼Œæˆ‘æ„¿æ„åˆ†äº«ä»€ä¹ˆä¿¡æ¯ï¼Ÿ

ç”Ÿæˆæ–¹å¼ï¼š

```python
Q = W_q @ X
K = W_k @ X
V = W_v @ X
```

è¿™é‡Œçš„ `W_q, W_k, W_v` æ˜¯çº¿æ€§å±‚çš„æƒé‡çŸ©é˜µï¼Œä¸å¸¦ biasã€‚

---

## 3. è®¡ç®—æ³¨æ„åŠ›æƒé‡

å¯¹åºåˆ—ä¸­æ¯ä¸ª tokenï¼š

1. ç”¨å®ƒçš„ **Query** å’Œæ‰€æœ‰ token çš„ **Key** åšç‚¹ç§¯
   â†’ å¾—åˆ°ä¸æ¯ä¸ª token çš„åŒ¹é…ç¨‹åº¦ï¼ˆäº²å’Œåº¦ï¼‰ã€‚
2. **æ©ç ï¼ˆmaskingï¼‰**ï¼šé˜»æ­¢çœ‹æœªæ¥çš„ä¿¡æ¯ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µå®ç°ï¼‰ã€‚
3. **Softmax å½’ä¸€åŒ–**ï¼šæŠŠäº²å’Œåº¦è½¬æˆæ¦‚ç‡åˆ†å¸ƒï¼Œè¡¨ç¤ºâ€œè¯¥ token å¯¹å…¶ä»–ä½ç½®çš„å…³æ³¨æ¯”ä¾‹â€ã€‚

å…¬å¼ï¼š

$$
\text{AttentionWeights} = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}} + \text{mask}\right)
$$

---

## 4. èšåˆä¿¡æ¯

æ³¨æ„åŠ›æƒé‡ï¼ˆ`[B, T, T]`ï¼‰å’Œ Valueï¼ˆ`[B, T, head_size]`ï¼‰ç›¸ä¹˜ï¼š

```python
output = AttentionWeights @ V
```

è¿™æ ·æ¯ä¸ª token å¾—åˆ°çš„æ˜¯ï¼š

* å¯¹æ‰€æœ‰ token Value çš„åŠ æƒæ±‚å’Œç»“æœ
* æƒé‡å–å†³äº Queryâ€“Key çš„åŒ¹é…ç¨‹åº¦

---

## 5. æ•°æ®æµå½¢çŠ¶å˜åŒ–

å‡è®¾ï¼š

* `B` = batch size
* `T` = åºåˆ—é•¿åº¦
* `C` = embedding ç»´åº¦
* `head_size` = æ¯ä¸ªå¤´çš„è¾“å‡ºç»´åº¦

æµç¨‹ï¼š

1. `X` : `[B, T, C]`
2. Q/K/V : `[B, T, head_size]`ï¼ˆçº¿æ€§å±‚å˜æ¢ï¼‰
3. `Q @ K^T` : `[B, T, T]`ï¼ˆäº²å’Œåº¦çŸ©é˜µï¼‰
4. Mask + Softmax : `[B, T, T]`ï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰
5. `AttentionWeights @ V` : `[B, T, head_size]`ï¼ˆè¾“å‡ºï¼‰

---

## 6. ä¸ºä»€ä¹ˆè¦æœ‰ Valueï¼ˆVï¼‰

* **X æ˜¯ token çš„å…¨éƒ¨ä¿¡æ¯**
* **Q/K æ˜¯â€œåŒ¹é…è§„åˆ™â€**ï¼ˆå†³å®šè°å’Œè°äº¤äº’ï¼‰
* **V æ˜¯â€œå…±äº«å‡ºå»çš„å†…å®¹â€**
  è¿™æ ·å¯ä»¥è®©åŒ¹é…è§„åˆ™å’Œå…±äº«å†…å®¹è§£è€¦ï¼š
* ä½ å¯ä»¥ç”¨ Q/K å†³å®šæ˜¯å¦å…³æ³¨æŸä¸ª token
* ä½†èšåˆæ—¶ï¼Œä¸æ˜¯ç›´æ¥æ‹¿å®ƒçš„åŸå§‹ Xï¼Œè€Œæ˜¯æ‹¿å®ƒé€šè¿‡ V æŠ•å½±åçš„ä¿¡æ¯

---

## 7. æ€»ç»“æµç¨‹

1. **çº¿æ€§å˜æ¢**ï¼šä» X å¾—åˆ° Q/K/V
2. **ç‚¹ç§¯**ï¼šQ å’Œ K è®¡ç®—ç›¸ä¼¼åº¦
3. **æ©ç  + Softmax**ï¼šç”Ÿæˆæ³¨æ„åŠ›åˆ†å¸ƒ
4. **åŠ æƒæ±‚å’Œ**ï¼šç”¨æ³¨æ„åŠ›æƒé‡èšåˆ V
5. å¾—åˆ°æ–°çš„ token è¡¨ç¤ºï¼ˆå«å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼ \*\*â€œå•å¤´è‡ªæ³¨æ„åŠ›æ•°æ®æµåŠ¨å›¾â€\*\*ï¼ŒæŠŠ Qã€Kã€V çš„ç”Ÿæˆã€äº²å’Œåº¦è®¡ç®—ã€æƒé‡å½’ä¸€åŒ–ã€ä¿¡æ¯èšåˆçš„è¿‡ç¨‹ä¸€æ­¥æ­¥ç”»å‡ºæ¥ï¼Œè¿™æ ·ä½ çœ‹ä»£ç çš„æ—¶å€™å°±èƒ½ç›´æ¥å¯¹ç…§æ•°æ®æµç†è§£ã€‚è¿™æ ·ä¼šæ¯”çº¯æ–‡å­—æ›´ç›´è§‚ã€‚



# note 1: attention as communication

number one attention is a communication mechanism you can really think about it as a communication mechanism where you
have a number of nodes in a directed graph where basically you have edges pointed between noes like
this and what happens is every node has some Vector of information and it gets to aggregate information via a weighted
sum from all of the nodes that point to it and this is done in a data dependent manner so depending on whatever data is
actually stored that you should not at any point in time now our graph doesn't
look like this our graph has a different structure we have eight nodes because the block size is eight and there's
always eight to tokens and uh the first node is only pointed to by itself the second node is
pointed to by the first node and itself all the way up to the eighth node which is pointed to by all the previous nodes
and itself and so that's the structure that our directed graph has or happens happens to have in Auto regressive sort
of scenario like language modeling but in principle attention can be applied to any arbitrary directed graph and it's
just a communication mechanism between the nodes the second note is that notice that there is no notion of space so

å½“ç„¶å¯ä»¥ï¼Œè¿™ä¸€æ®µè®²çš„æ˜¯ **â€œæ³¨æ„åŠ›æœºåˆ¶æœ¬è´¨ä¸Šæ˜¯ä¸€ç§é€šä¿¡æœºåˆ¶â€**ï¼Œæˆ‘æ¥å¸®ä½ è¯¦ç»†è§£é‡Šï¼š

---

### ğŸ“Œ 1ï¸âƒ£ æ³¨æ„åŠ›æ˜¯é€šä¿¡æœºåˆ¶ï¼ˆattention as communicationï¼‰

* ä½ å¯ä»¥æŠŠ **Self-Attention** æƒ³è±¡æˆï¼š

ğŸ‘‰ ä¸€å¼  **æœ‰å‘å›¾ï¼ˆdirected graphï¼‰**
ğŸ‘‰ å›¾ä¸­æœ‰è‹¥å¹²ä¸ª â€œèŠ‚ç‚¹ï¼ˆnodeï¼‰â€ï¼Œæ¯ä¸ªèŠ‚ç‚¹ = ä¸€ä¸ª token

---

### ğŸ“Œ 2ï¸âƒ£ èŠ‚ç‚¹ä¹‹é—´æ€ä¹ˆé€šä¿¡ï¼Ÿ

* æ¯ä¸ªèŠ‚ç‚¹æœ‰è‡ªå·±çš„ â€œä¿¡æ¯å‘é‡â€ï¼ˆæ¯”å¦‚ embedding å‘é‡ï¼‰

* èŠ‚ç‚¹ä¹‹é—´é€šè¿‡ â€œæœ‰å‘è¾¹â€ ä¼ é€’ä¿¡æ¯ï¼Œ**è¾¹æœ‰æƒé‡ï¼ˆweightï¼‰**

* æŸä¸ªèŠ‚ç‚¹æ”¶åˆ°çš„ä¿¡æ¯æ˜¯ï¼š

```
æ¥è‡ªæ‰€æœ‰æŒ‡å‘å®ƒçš„èŠ‚ç‚¹ï¼Œä¿¡æ¯çš„åŠ æƒå’Œï¼ˆweighted sumï¼‰
```

* è¿™ä¸ª â€œåŠ æƒâ€ æ˜¯ **æ ¹æ®æ•°æ®å†…å®¹åŠ¨æ€ç®—å‡ºæ¥çš„**
  ï¼ˆç”± QK ç‚¹ç§¯ + softmax è®¡ç®—å‡ºï¼‰

---

### ğŸ“Œ 3ï¸âƒ£ æˆ‘ä»¬çš„è¯­è¨€æ¨¡å‹ï¼ˆè‡ªå›å½’ï¼‰é‡Œå›¾é•¿ä»€ä¹ˆæ ·ï¼Ÿ

* å‡è®¾ block size = 8ï¼Œsequence é•¿åº¦æ˜¯ 8ï¼š

```
token 1ï¼šåªèƒ½çœ‹è‡ªå·±  
token 2ï¼šå¯ä»¥çœ‹ token 1 + è‡ªå·±  
token 3ï¼šå¯ä»¥çœ‹ token 1,2 + è‡ªå·±  
...  
token 8ï¼šå¯ä»¥çœ‹ token 1~7 + è‡ªå·±
```

* ä¹Ÿå°±æ˜¯è¯´ï¼Œå›¾é‡Œè¾¹çš„ â€œç®­å¤´â€ åªæŒ‡å‘ â€œä¹‹å‰ + å½“å‰â€ token
* **ä¸èƒ½çœ‹æœªæ¥ token**ï¼Œè¿™æ˜¯è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’ç‰¹æ€§ã€‚

---

### ğŸ“Œ 4ï¸âƒ£ é€šä¿¡æ˜¯ â€œæ•°æ®ç›¸å…³â€ çš„

* æ¯æ¡è¾¹çš„æƒé‡ **ä¸æ˜¯å›ºå®šçš„**ï¼Œè€Œæ˜¯ **æ ¹æ® Qã€K å‘é‡ç®—å‡ºæ¥çš„**
* æ•°æ®ä¸åŒ â†’ æƒé‡ä¸åŒ â†’ é€šä¿¡å¼ºå¼±ä¸åŒ

---

### ğŸ“Œ 5ï¸âƒ£ ä¸å±€é™äºè¯­è¨€æ¨¡å‹

* è™½ç„¶åœ¨è¯­è¨€æ¨¡å‹é‡Œï¼Œå›¾ç»“æ„æ˜¯ï¼š

```
token 1 â†’ token 1  
token 2 â†’ token 1,2  
token 3 â†’ token 1,2,3  
...  
```

* ä½†å…¶å® **æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥ç”¨åœ¨ä»»ä½•å›¾**
* ä¹Ÿå¯ä»¥ç”¨åœ¨å›¾ç¥ç»ç½‘ç»œï¼ˆGraph Neural Network, GNNï¼‰
* å¯ä»¥å®ç°ä»»æ„å›¾ä¸­çš„ â€œä¿¡æ¯ä¼ é€’â€

---

### ğŸ“Œ 6ï¸âƒ£ æ€»ç»“ä¸€å¥è¯

ğŸ‘‰ Self-Attention æœ¬è´¨æ˜¯ï¼š

**â€œèŠ‚ç‚¹ä¹‹é—´æ ¹æ®æ•°æ®åŠ¨æ€è®¡ç®—æƒé‡ï¼Œè¿›è¡Œä¿¡æ¯ä¼ é€’ï¼ˆé€šä¿¡ï¼‰â€**

* token = èŠ‚ç‚¹
* QK ç‚¹ç§¯ â†’ ç®— â€œè°å¯¹è°æœ‰å…´è¶£â€
* softmax â†’ ç®— â€œé€šä¿¡å¼ºåº¦â€
* V â†’ ä¼ é€’å‡ºå»çš„ä¿¡æ¯
* weighted sum â†’ æ”¶åˆ°çš„ä¿¡æ¯

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘å¯ä»¥æ¥ç€å¸®ä½ è®²ï¼š

âœ… ä¸ºä»€ä¹ˆ attention ä¸éœ€è¦â€œç©ºé—´åæ ‡â€ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆè¿™ç§â€œé€šä¿¡â€æœºåˆ¶è¿™ä¹ˆå¼ºå¤§ï¼Ÿ
âœ… è¿™å’Œ Transformer ä¸ºä»€ä¹ˆè¿™ä¹ˆæœ‰æ•ˆæœ‰å•¥å…³ç³»ï¼Ÿ

è¦ä¸è¦æˆ‘ç»§ç»­å¸®ä½ å±•å¼€è®²ï¼ŸğŸŒŸ

è¿™æ®µè¯ä¸»è¦æ˜¯åœ¨ç”¨\*\*å›¾ï¼ˆgraphï¼‰\*\*çš„ç±»æ¯”è§£é‡Š Attention çš„æœ¬è´¨ä½œç”¨ï¼Œæˆ‘å¸®ä½ åˆ†æˆå‡ ä¸ªé‡ç‚¹è§£é‡Šï¼š

---

## 1. Attention æœ¬è´¨æ˜¯â€œä¿¡æ¯ä¼ é€’æœºåˆ¶â€

å¯ä»¥æŠŠåºåˆ—ä¸­çš„æ¯ä¸ª token çœ‹ä½œä¸€ä¸ª**èŠ‚ç‚¹ï¼ˆnodeï¼‰**ï¼Œè¿™äº›èŠ‚ç‚¹ä¹‹é—´å­˜åœ¨**æœ‰å‘è¾¹ï¼ˆdirected edgesï¼‰**ï¼Œè¾¹çš„æ–¹å‘è¡¨ç¤ºä¿¡æ¯æµåŠ¨çš„æ–¹å‘ã€‚

* æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰ä¸€æ®µ**å‘é‡ä¿¡æ¯**ï¼ˆembedding è¡¨ç¤ºçš„ç‰¹å¾ï¼‰ã€‚
* èŠ‚ç‚¹ä¼šä»æ‰€æœ‰æŒ‡å‘å®ƒçš„èŠ‚ç‚¹é‚£é‡Œæ”¶é›†ä¿¡æ¯ï¼ˆèšåˆï¼‰ï¼Œå¹¶ä¸”æ˜¯**åŠ æƒæ±‚å’Œ**çš„æ–¹å¼ã€‚
* æƒé‡ä¸æ˜¯å›ºå®šçš„ï¼Œè€Œæ˜¯**ç”±æ•°æ®æœ¬èº«åŠ¨æ€è®¡ç®—**å‡ºæ¥çš„ï¼ˆæ•°æ®ä¾èµ–ï¼Œdata-dependentï¼‰ã€‚

è¿™æ ·ï¼ŒAttention å°±ç›¸å½“äºä¸€å¥—è§„åˆ™ï¼šæ ¹æ®èŠ‚ç‚¹ä¹‹é—´çš„â€œå…´è¶£åŒ¹é…åº¦â€ï¼ˆé€šè¿‡ query-key çš„ç‚¹ç§¯è®¡ç®—ï¼‰ï¼Œå†³å®šä¿¡æ¯ä»å“ªé‡Œæ¥ã€æµå¤šå°‘ã€‚

---

## 2. åœ¨è‡ªå›å½’ï¼ˆautoregressiveï¼‰åœºæ™¯ä¸‹çš„å›¾ç»“æ„

æ¯”å¦‚åœ¨è¯­è¨€æ¨¡å‹ï¼ˆLanguage Modelingï¼‰ä¸­ï¼Œå¦‚æœ **block size = 8**ï¼Œå°±æœ‰ 8 ä¸ª tokenï¼ˆèŠ‚ç‚¹ï¼‰ï¼š

* ç¬¬ 1 ä¸ªèŠ‚ç‚¹åªèƒ½çœ‹è‡ªå·±ã€‚
* ç¬¬ 2 ä¸ªèŠ‚ç‚¹å¯ä»¥çœ‹ç¬¬ 1 ä¸ªå’Œè‡ªå·±ã€‚
* ç¬¬ 3 ä¸ªèŠ‚ç‚¹å¯ä»¥çœ‹å‰ä¸¤ä¸ªå’Œè‡ªå·±ã€‚
* â€¦â€¦
* ç¬¬ 8 ä¸ªèŠ‚ç‚¹å¯ä»¥çœ‹å‰ 7 ä¸ªå’Œè‡ªå·±ã€‚

è¿™ä¸ª**ä¸‹ä¸‰è§’å½¢ç»“æ„**æ˜¯ç”± Maskï¼ˆ`tril`ï¼‰å®ç°çš„ï¼Œä¿è¯æ¨¡å‹åªä½¿ç”¨**è¿‡å»å’Œå½“å‰çš„ä¿¡æ¯**ï¼Œè€Œä¸ä¼šâ€œå·çœ‹â€æœªæ¥çš„ tokenã€‚

---

## 3. Attention ä¸å…³å¿ƒâ€œç©ºé—´â€

è¿™é‡Œå¼ºè°ƒ **â€œno notion of spaceâ€** â€”â€” æ³¨æ„åŠ›æœºåˆ¶æœ¬èº«å¹¶ä¸å…³å¿ƒæ•°æ®åœ¨ç‰©ç†ä¸Šæ˜¯äºŒç»´ã€ä¸‰ç»´è¿˜æ˜¯æ—¶é—´åºåˆ—ï¼Œå®ƒåªæ˜¯ä¸€ç»„å‘é‡ä¹‹é—´çš„ä¿¡æ¯æµåŠ¨è§„åˆ™ã€‚

* å› æ­¤ï¼Œåœ¨ NLP ä»»åŠ¡é‡Œï¼Œæˆ‘ä»¬éœ€è¦é¢å¤–çš„\*\*ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰\*\*å‘Šè¯‰æ¨¡å‹ token çš„é¡ºåºï¼Œå¦åˆ™å®ƒä¼šæŠŠè¾“å…¥å½“ä½œä¸€ä¸ªæ— åºé›†åˆã€‚
* åœ¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æˆ–éåºåˆ—æ•°æ®ä¸­ï¼ŒAttention ä¹Ÿå¯ä»¥ç”¨ï¼Œåªè¦å®šä¹‰å¥½èŠ‚ç‚¹å’Œè¾¹çš„å…³ç³»ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼ \*\*â€œè‡ªå›å½’ Attention çš„ä¿¡æ¯æµåŠ¨å›¾â€\*\*ï¼Œç›´è§‚å±•ç¤ºç¬¬ 8 ä¸ªèŠ‚ç‚¹åªèƒ½ä»å‰ 8 ä¸ªä½ç½®æ”¶é›†ä¿¡æ¯ã€è€Œä¸”æ˜¯å¸¦æƒé‡çš„ã€‚è¿™æ ·ä½ åœ¨çœ‹ Transformer çš„ masking æ—¶ä¼šä¸€çœ¼å°±æ˜ç™½ã€‚



# note 2: attention has no notion of space, operates over sets

attention simply acts over like a set of vectors in this graph and so by default
these nodes have no idea where they are positioned in the space and that's why we need to encode them positionally and
sort of give them some information that is anchored to a specific position so that they sort of know where they are
and this is different than for example from convolution because if you're run for example a convolution operation over some input there's a very specific sort
of layout of the information in space and the convolutional filters sort of act in space and so it's it's not like
an attention in ATT ention is just a set of vectors out there in space they communicate and if you want them to have
a notion of space you need to specifically add it which is what we've done when we calculated the um relative
the positional encode encodings and added that information to the vectors the next thing that I hope is very clear

å½“ç„¶å¯ä»¥ï¼è¿™ä¸€æ®µè®²çš„æ˜¯ **Self-Attention å’Œç©ºé—´ä¿¡æ¯çš„å…³ç³»**ï¼Œæˆ‘æ¥å¸®ä½ è¯¦ç»†è§£é‡Šï¼š

---

### ğŸ“Œ 1ï¸âƒ£ Attention æœ¬è´¨ä¸Šæ²¡æœ‰ç©ºé—´æ¦‚å¿µ

* **Attention æ“ä½œçš„æ˜¯â€œä¸€ç»„å‘é‡â€**ï¼ˆset of vectorsï¼‰
* è¿™äº›å‘é‡ä¹‹é—´çš„è®¡ç®—ï¼ˆQK ç‚¹ç§¯ï¼‰**åªè·Ÿå‘é‡æœ¬èº«çš„å†…å®¹æœ‰å…³**
* å®ƒä»¬ **é»˜è®¤ä¸çŸ¥é“è‡ªå·±åœ¨â€œåºåˆ—é‡Œâ€æ˜¯ç¬¬å‡ ä¸ª token**

---

### ğŸ“Œ 2ï¸âƒ£ ä¸ºä»€ä¹ˆè¦åŠ  Positional Encodingï¼Ÿ

* è¯­è¨€æ˜¯æœ‰é¡ºåºçš„ï¼ˆI love you â‰  You love Iï¼‰
* ä½† Attention é»˜è®¤ **ä¸åŒºåˆ†é¡ºåº**ï¼Œåªçœ‹åˆ°ä¸€å † token å‘é‡

æ‰€ä»¥æˆ‘ä»¬è¦ **æ‰‹åŠ¨åŠ  Positional Encoding**ï¼š

```
token_embedding + positional_embedding
```

* è¿™æ · token æ‰çŸ¥é“è‡ªå·±æ˜¯ â€œç¬¬ 3 ä¸ªâ€ è¿˜æ˜¯ â€œç¬¬ 5 ä¸ªâ€

---

### ğŸ“Œ 3ï¸âƒ£ Attention å’Œå·ç§¯ï¼ˆConvolutionï¼‰çš„åŒºåˆ«

ğŸ‘‰ å·ç§¯ï¼ˆConvï¼‰å¤©ç„¶æœ‰â€œç©ºé—´â€æ¦‚å¿µï¼š

* å·ç§¯æ ¸æ˜¯æ»‘åŠ¨çª—å£
* å·ç§¯æ ¸çŸ¥é“â€œæˆ‘åœ¨å›¾ç‰‡ï¼ˆæˆ–åºåˆ—ï¼‰é‡Œçš„ä½ç½®â€
* ä½ç½®å†³å®šäº†å·ç§¯æ ¸æ„ŸçŸ¥çš„ä¿¡æ¯

ğŸ‘‰ Attentionï¼š

* å®Œå…¨æ˜¯å¯¹â€œå‘é‡é›†åˆâ€åšæ“ä½œ
* æ²¡æœ‰â€œæ»‘åŠ¨â€æ¦‚å¿µ
* æ²¡æœ‰å›ºå®šçš„ç©ºé—´é¡ºåº
* å¿…é¡»**æ‰‹åŠ¨åŠ ä½ç½®ç¼–ç **ï¼Œå®ƒæ‰çŸ¥é“â€œè‡ªå·±åœ¨å“ªâ€

---

### ğŸ“Œ 4ï¸âƒ£ æ€»ç»“ä¸€å¥è¯

**Self-Attention æœ¬è´¨å¯¹ä¸€ç»„å‘é‡æ“ä½œï¼Œ**
**é»˜è®¤æ²¡æœ‰ç©ºé—´/ä½ç½®ä¿¡æ¯ï¼Œ**
**éœ€è¦æ˜¾å¼åŠ  Positional Encoding æ‰èƒ½å»ºæ¨¡â€œé¡ºåºâ€æˆ–â€œç©ºé—´â€ã€‚**

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ è®²ï¼š

âœ… Positional Encoding æœ‰å‡ ç§å¸¸ç”¨æ–¹å¼ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆ Transformer ç”¨â€œåŠ æ³•â€è€Œä¸æ˜¯æ‹¼æ¥ï¼Ÿ
âœ… CNN å’Œ Attention åœ¨å¤„ç†ç©ºé—´ä¿¡æ¯æ—¶æœ‰å•¥ä¼˜ç¼ºç‚¹ï¼Ÿ

è¦ä¸è¦æˆ‘ç»§ç»­å±•å¼€è®²ï¼ŸğŸŒŸ


# note 3: there is no communication across batch dimension

is that the elements across the batch Dimension which are independent examples never talk to each other they're always
processed independently and this is a batched matrix multiply that applies basically a matrix multiplication uh
kind of in parallel across the batch dimension so maybe it would be more accurate to say that in this analogy of
a directed graph we really have because the back size is four we really have four separate pools of eight nodes and
those eight nodes only talk to each other but in total there's like 32 nodes that are being processed uh but there's
um sort of four separate pools of eight you can look at it that way the next note is that here in the case of

å½“ç„¶å¯ä»¥ï¼è¿™ä¸€æ®µåœ¨è®² **batch ç»´åº¦ï¼ˆbatch dimensionï¼‰å’Œ attention çš„å…³ç³»**ï¼Œæˆ‘æ¥å¸®ä½ è§£é‡Šä¸€ä¸‹ ğŸ‘‡ï¼š

---

### ğŸ“Œ 1ï¸âƒ£ batch ç»´åº¦é‡Œçš„ä¾‹å­æ˜¯â€œç‹¬ç«‹çš„â€

* åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒæ—¶ï¼Œé€šå¸¸æˆ‘ä»¬ç”¨ **batch è®­ç»ƒ**ï¼š

```
batch_size = 4
```

* æ„æ€æ˜¯ **æ¯æ¬¡è¾“å…¥ 4 ä¸ªæ ·æœ¬ï¼ˆåºåˆ—ï¼‰**ï¼Œä¸€èµ·è®­ç»ƒï¼Œæ•ˆç‡æ›´é«˜ã€‚

---

### ğŸ“Œ 2ï¸âƒ£ Self-Attention é‡Œï¼Œbatch ä¹‹é—´äº’ä¸å½±å“

* æ³¨æ„åŠ›æœºåˆ¶ **ä¸ä¼šè·¨ batch é€šä¿¡**ï¼š

```
batch 0 çš„ token ä»¬  
batch 1 çš„ token ä»¬  
batch 2 çš„ token ä»¬  
batch 3 çš„ token ä»¬
```

* å½¼æ­¤ä¹‹é—´ **å®Œå…¨ç‹¬ç«‹**ï¼Œå®ƒä»¬çš„ Q/K/V è®¡ç®—ï¼Œsoftmax è®¡ç®—ï¼Œ
  å…¨éƒ¨åªåœ¨å„è‡ª batch å†…éƒ¨è¿›è¡Œã€‚

---

### ğŸ“Œ 3ï¸âƒ£ çŸ©é˜µä¹˜æ³•æ€ä¹ˆåšï¼Ÿ

```
wei = q @ k.T  # è¿™é‡Œæ˜¯ batched matrix multiply
```

æ„æ€æ˜¯ï¼š

* PyTorch ä¼šå¯¹ **batch é‡Œçš„æ¯ä¸€æ¡åºåˆ—**
* åˆ†åˆ«åš attention è®¡ç®—ï¼Œ**å¹¶è¡Œæ‰§è¡Œ**ï¼Œä½†äº’ä¸å½±å“

---

### ğŸ“Œ 4ï¸âƒ£ å›¾çš„ç±»æ¯”

è®²è€…æ‰“äº†ä¸ªæ¯”æ–¹ï¼š

* è¿™é‡Œ **batch\_size=4**ï¼Œ**block\_size=8**ï¼š

å°±åƒ **4 ä¸ªç‹¬ç«‹çš„å°å›¾**ï¼Œæ¯ä¸ªå°å›¾é‡Œæœ‰ 8 ä¸ªèŠ‚ç‚¹ï¼š

```
å›¾1ï¼š8ä¸ªtoken  
å›¾2ï¼š8ä¸ªtoken  
å›¾3ï¼š8ä¸ªtoken  
å›¾4ï¼š8ä¸ªtoken
```

â†’ æ¯ä¸ªå›¾å†…éƒ¨ token ä¹‹é—´æœ‰ attentionï¼Œ
â†’ ä¸åŒå›¾ä¹‹é—´å®Œå…¨æ²¡æœ‰é€šä¿¡ã€‚

---

### ğŸ“Œ 5ï¸âƒ£ æ€»ç»“ä¸€å¥è¯

**batch ç»´åº¦é‡Œçš„åºåˆ—æ˜¯ç‹¬ç«‹è®¡ç®— attention çš„ï¼Œ**
**batch åªæ˜¯ä¸ºäº†å¹¶è¡Œè®¡ç®—åŠ é€Ÿï¼Œäº’ä¸é€šä¿¡ã€‚**

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ è®²ï¼š

âœ… ä¸ºä»€ä¹ˆ batch è®¾è®¡æˆè¿™æ ·ï¼Ÿ
âœ… æ€ä¹ˆç†è§£ batched matrix multiplyï¼Ÿ
âœ… æœ‰æ²¡æœ‰åœºæ™¯éœ€è¦è·¨ batch attentionï¼Ÿ

è¦ä¸è¦æˆ‘ç»§ç»­å¸®ä½ è®²ï¼ŸğŸŒŸ


# note 4: encoder blocks vs. decoder blocks

language modeling uh we have this specific uh structure of directed graph where the future tokens will not
communicate to the Past tokens but this doesn't necessarily have to be the constraint in the general case and in
fact in many cases you may want to have all of the uh noes talk to each other uh fully so as an example if you're doing
sentiment analysis or something like that with a Transformer you might have a number of tokens and you may want to
have them all talk to each other fully because later you are predicting for example the sentiment of the sentence
and so it's okay for these NOS to talk to each other and so in those cases you will use an encoder block of self
attention and uh all it means that it's an encoder block is that you will delete this line of code allowing all the noes
to completely talk to each other what we're implementing here is sometimes called a decoder block and it's called a
decoder because it is sort of like a decoding language and it's got this
autor regressive format where you have to mask with the Triangular Matrix so that uh nodes from the future never talk
to the Past because they would give away the answer and so basically in encoder blocks you
would delete this allow all the noes to talk in decoder blocks this will always be present so that you have this
triangular structure uh but both are allowed and attention doesn't care attention supports arbitrary connectivity between nodes the next

å½“ç„¶å¯ä»¥ï¼è¿™ä¸€æ®µåœ¨è®² **encoder block å’Œ decoder block çš„åŒºåˆ«**ï¼Œæˆ‘æ¥å¸®ä½ è¯¦ç»†è§£é‡Š ğŸ‘‡ï¼š

---

### ğŸ“Œ 1ï¸âƒ£ Language Model é‡Œçš„å›¾ç»“æ„æ˜¯â€œå•å‘çš„â€

* åœ¨è¯­è¨€æ¨¡å‹ï¼ˆLanguage Modelingï¼‰ä»»åŠ¡é‡Œï¼š

```
token 1 â†’ token 1  
token 2 â†’ token 1,2  
token 3 â†’ token 1,2,3  
...
```

ğŸ‘‰ æœªæ¥ token **ä¸èƒ½çœ‹åˆ°æœªæ¥**
ğŸ‘‰ åªèƒ½çœ‹åˆ°â€œè¿‡å» + å½“å‰â€

* å› ä¸ºæ˜¯ **è‡ªå›å½’ï¼ˆautoregressiveï¼‰**ï¼Œä¸€æ­¥æ­¥ç”Ÿæˆï¼Œä¸å…è®¸ä½œå¼Šã€‚

---

### ğŸ“Œ 2ï¸âƒ£ mask å°±æ˜¯å®ç°â€œæœªæ¥ä¸èƒ½çœ‹â€

```python
wei = wei.masked_fill(tril == 0, float('-inf'))
```

è¿™ä¸ª tril ä¸‰è§’ mask ä¿è¯ï¼š

```
token_i åªçœ‹ <= i çš„ token  
ä¸ä¼šçœ‹åˆ° i+1 ä»¥å
```

---

### ğŸ“Œ 3ï¸âƒ£ Decoder Block æ˜¯ä»€ä¹ˆï¼Ÿ

ğŸ‘‰ è¿™ç§åŠ äº† mask çš„ blockï¼Œå«ï¼š

```
Decoder Block
```

åŸå› ï¼š

* å®ƒâ€œè§£ç è¯­è¨€â€ï¼Œä¸€æ­¥æ­¥é¢„æµ‹ tokenï¼Œå¿…é¡»é˜»æ­¢æœªæ¥ä¿¡æ¯æ³„éœ²ã€‚

---

### ğŸ“Œ 4ï¸âƒ£ Encoder Block æ˜¯ä»€ä¹ˆï¼Ÿ

å¦‚æœåšçš„æ˜¯ï¼š

âœ… æƒ…æ„Ÿåˆ†æ
âœ… æ–‡æœ¬åˆ†ç±»
âœ… æœºå™¨ç¿»è¯‘ç¼–ç é˜¶æ®µ
âœ… å›¾åƒå¤„ç†

ğŸ‘‰ é€šå¸¸å¸Œæœ› **æ‰€æœ‰ token äº’ç›¸éƒ½èƒ½çœ‹**ï¼Œ
ğŸ‘‰ å°±ä¸ç”¨åŠ  maskï¼Œç›´æ¥è®© token ä¹‹é—´å…¨è¿æ¥ï¼š

```
wei = q @ k.transpose(-2, -1)
# ä¸éœ€è¦ masked_fill
```

â†’ è¿™ç§å«ï¼š

```
Encoder Block
```

---

### ğŸ“Œ 5ï¸âƒ£ æ ¸å¿ƒåŒºåˆ«

| Block ç±»å‹      | æ˜¯å¦ Mask  | é€šä¿¡ç»“æ„       | ç”¨é€”          |
| ------------- | -------- | ---------- | ----------- |
| Encoder Block | ä¸åŠ  Mask  | å…¨ token äº’çœ‹ | æƒ…æ„Ÿåˆ†æã€æœºå™¨ç¿»è¯‘ç¼–ç  |
| Decoder Block | åŠ ä¸‰è§’ Mask | åªèƒ½çœ‹è¿‡å» + å½“å‰ | è¯­è¨€æ¨¡å‹ã€æ–‡æœ¬ç”Ÿæˆ   |

---

### ğŸ“Œ 6ï¸âƒ£ æ€»ç»“ä¸€å¥è¯

ğŸ‘‰ Encoder Blockï¼šæ‰€æœ‰ token äº’çœ‹ï¼Œä¸åŠ  mask
ğŸ‘‰ Decoder Blockï¼šæœªæ¥ token ä¸èƒ½çœ‹ï¼ŒåŠ  maskï¼Œä¿æŒè‡ªå›å½’

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ è®²ï¼š

âœ… Transformer é‡Œçš„ Encoder-Decoder ç»“æ„æ˜¯å•¥ï¼Ÿ
âœ… GPT æ˜¯çº¯ Decoderï¼ŒBERT æ˜¯çº¯ Encoderï¼Œä¸ºä»€ä¹ˆï¼Ÿ
âœ… Encoder å’Œ Decoder æ€ä¹ˆç»„åˆç”¨ï¼Ÿ

è¦ä¸è¦æˆ‘ç»§ç»­å±•å¼€è®²ï¼ŸğŸŒŸ


# note 5: attention vs. self-attention vs. cross-attention

thing I wanted to comment on is you keep me you keep hearing me say attention self attention Etc there's actually also
something called cross attention what is the difference so basically the reason this attention
is self attention is because because the keys queries and the values are all
coming from the same Source from X so the same Source X produces Keys queries
and values so these nodes are self attending but in principle attention is
much more General than that so for example an encoder decoder Transformers uh you can have a case where the queries
are produced from X but the keys and the values come from a whole separate external source and sometimes from uh
encoder blocks that encode some context that we'd like to condition on and so the keys and the values will
actually come from a whole separate Source those are nodes on the side and here we're just producing queries and
we're reading off information from the side so cross attention is used when there's a separate source of nodes we'd
like to pull information from into our nodes and it's self attention if we just have nodes that would like to look at
each other and talk to each other so this attention here happens to be self
attention but in principle um attention is a lot more General okay and the last

å½“ç„¶å¯ä»¥ï¼è¿™ä¸€æ®µè®²çš„æ˜¯ **attention / self-attention / cross-attention æœ‰ä»€ä¹ˆåŒºåˆ«**ï¼Œæˆ‘æ¥å¸®ä½ è¯¦ç»†è§£é‡Š ğŸ‘‡ï¼š

---

### ğŸ“Œ 1ï¸âƒ£ Self-Attention æ˜¯ä»€ä¹ˆï¼Ÿ

ğŸ‘‰ å½“å‰ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å®ç°çš„æ˜¯ **Self-Attention**ï¼Œ
ğŸ‘‰ ä¸ºä»€ä¹ˆå« â€œSelfâ€ï¼Ÿ

å› ä¸ºï¼š

```
Q / K / V å…¨éƒ¨æ¥è‡ªåŒä¸€ä¸ªè¾“å…¥ X
```

ä¹Ÿå°±æ˜¯è¯´ï¼š

```
Q = Linear_Q(X)  
K = Linear_K(X)  
V = Linear_V(X)
```

ä¹Ÿå°±æ˜¯ï¼š

ğŸ‘‰ token ä»¬è‡ªå·±çœ‹è‡ªå·±ã€å½¼æ­¤é€šä¿¡ï¼Œ**è‡ªæˆ‘å…³æ³¨ï¼ˆselfï¼‰**

---

### ğŸ“Œ 2ï¸âƒ£ Cross-Attention æ˜¯ä»€ä¹ˆï¼Ÿ

ğŸ‘‰ â€œCrossâ€ æ„æ€æ˜¯ â€œè·¨æ¥æºâ€

å…·ä½“æ¥è¯´ï¼š

```
Q æ¥è‡ª X  
K / V æ¥è‡ª å¦ä¸€ä¸ª Y
```

---

### ğŸ“Œ 3ï¸âƒ£ ä¸¾ä¾‹åœºæ™¯

âœ… **Encoder-Decoder Transformer**ï¼ˆæ¯”å¦‚æœºå™¨ç¿»è¯‘ï¼‰ï¼š

```
- ç¼–ç å™¨ encoder â†’ å¾—åˆ° Yï¼ˆcontextï¼‰ â†’ K/V  
- è§£ç å™¨ decoder â†’ å½“å‰ X â†’ Q  
- Cross-Attentionï¼šdecoder å½“å‰ token ç”¨ Q å»çœ‹ encoder äº§ç”Ÿçš„ K/V
```

---

### ğŸ“Œ 4ï¸âƒ£ é€šä¿—ç†è§£

| ç±»å‹              | Q æ¥è‡ªå“ªé‡Œ | K/V æ¥è‡ªå“ªé‡Œ | ä¸¾ä¾‹                               |
| --------------- | ------ | -------- | -------------------------------- |
| Self-Attention  | X      | X        | GPT / BERT                       |
| Cross-Attention | X      | Yï¼ˆåˆ«çš„æ¥æºï¼‰  | Encoder-Decoder Transformer æœºå™¨ç¿»è¯‘ |

---

### ğŸ“Œ 5ï¸âƒ£ ä¸ºä»€ä¹ˆéœ€è¦ Cross-Attentionï¼Ÿ

å› ä¸ºæœ‰äº›ä»»åŠ¡ï¼š

âœ… Decoder éœ€è¦ **å‚è€ƒå¤–éƒ¨ä¿¡æ¯**ï¼Œæ¯”å¦‚ï¼š

* Encoder äº§ç”Ÿçš„ä¸Šä¸‹æ–‡
* å›¾åƒç¼–ç çš„ç»“æœ
* å…¶ä»– modality ä¿¡æ¯ï¼ˆå¤šæ¨¡æ€ï¼‰

è¿™æ—¶å€™ï¼Œ**Q å’Œ K/V ä¸åŒæº**ï¼Œå°±æ˜¯ Cross-Attentionã€‚

---

### ğŸ“Œ 6ï¸âƒ£ æ€»ç»“ä¸€å¥è¯

**Self-Attention**ï¼š
ğŸ‘‰ token è‡ªå·±çœ‹è‡ªå·±ï¼ŒQ/K/V åŒæº

**Cross-Attention**ï¼š
ğŸ‘‰ token ä»â€œå¤–éƒ¨â€æä¿¡æ¯ï¼ŒQ â‰  K/V

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ ç»§ç»­è®²ï¼š

âœ… ä¸ºä»€ä¹ˆ Transformer å¤§æ¨¡å‹ï¼ˆGPT-4V ç­‰ï¼‰ä¼šç”¨ Cross-Attentionï¼Ÿ
âœ… BERT å’Œ GPT ä¸ºä»€ä¹ˆåªç”¨ Self-Attentionï¼Ÿ
âœ… Cross-Attention æ€ä¹ˆç”¨æ¥åšâ€œå›¾æ–‡ç»“åˆâ€ï¼Ÿ

è¦ä¸è¦æˆ‘ç»§ç»­å±•å¼€è®²ï¼ŸğŸŒŸ


# note 6: "scaled" self-attention. why divide by sqrt(head_size)

note at this stage is if we come to the attention is all need paper here we've already implemented attention so given
query key and value we've U multiplied the query and a key we've soft maxed it
and then we are aggregating the values there's one more thing that we're missing here which is the dividing by one / square root of the head size the
DK here is the head size why are they doing this finds this important so they call it the scaled attention and it's
kind of like an important normalization to basically have the problem is if you have unit gsh
and inputs so zero mean unit variance K and Q are unit gashin then if you just do we naively then you see that your we
actually will be uh the variance will be on the order of head size which in our case is 16 but if you multiply by one
over head size square root so this is square root and this is one over then the variance of we will be one
so it will be preserved now why is this important you'll not notice that way
here will feed into softmax and so it's really important especially at initialization that we be
fairly diffuse so in our case here we sort of locked out here and we had a
fairly diffuse numbers here so um like this now the problem is that because of
softmax if weight takes on very positive and very negative numbers inside it softmax will actually converge towards
one hot vectors and so I can illustrate that here um say we are applying softmax
to a tensor of values that are very close to zero then we're going to get a diffuse thing out of softmax but the moment I take the exact
same thing and I start sharpening it making it bigger by multiplying these numbers by eight for example you'll see
that the softmax will start to sharpen and in fact it will sharpen towards the max so it will sharpen towards whatever
number here is the highest and so um basically we don't want these values to be too extreme especially at
initialization otherwise softmax will be way too peaky and um you're basically aggregating um information from like a
single node every node just agregates information from a single other node that's not what we want especially at
initialization and so the scaling is used just to control the variance at initialization okay so having said all

å½“ç„¶å¯ä»¥ï¼è¿™æ®µåœ¨è®² **ä¸ºä»€ä¹ˆ Self-Attention è¦é™¤ä»¥ âˆšhead\_sizeï¼Œå«åšâ€œscaled attentionâ€**ï¼Œæˆ‘æ¥å¸®ä½ è¯¦ç»†è§£é‡Š ğŸ‘‡ï¼š

---

### ğŸ“Œ 1ï¸âƒ£ åŸå§‹ Attention å…¬å¼

åˆ°ç›®å‰ä¸ºæ­¢æˆ‘ä»¬å®ç°çš„ attention æ˜¯ï¼š

```
wei = q @ k.T   â†’ (B, T, T)  
wei = softmax(wei)  
out = wei @ v
```

---

### ğŸ“Œ 2ï¸âƒ£ è®ºæ–‡é‡Œå¤šäº†ä¸€ä¸ªé™¤æ³•ï¼š

```
wei = (q @ k.T) / sqrt(head_size)
```

è¿™é‡Œçš„ **head\_size = d\_k**
ä¹Ÿå°±æ˜¯ q å’Œ k çš„ç»´åº¦ï¼ˆæ¯”å¦‚ 16ï¼‰

---

### ğŸ“Œ 3ï¸âƒ£ ä¸ºä»€ä¹ˆè¦é™¤ï¼Ÿ

æ ¸å¿ƒé—®é¢˜ï¼š

ğŸ‘‰ å¦‚æœ **q / k** å‘é‡æ˜¯ â€œé›¶å‡å€¼ï¼Œå•ä½æ–¹å·®ï¼ˆunit Gaussianï¼‰â€ï¼Œ
ğŸ‘‰ é‚£ä¹ˆ **q @ k.T** çš„è¾“å‡ºï¼Œæ–¹å·®ä¼šæ˜¯ **head\_size å¤§å°**ã€‚

å› ä¸ºï¼š

```
Var(q â‹… k) ~ head_size
```

---

### ğŸ“Œ 4ï¸âƒ£ å¦‚æœä¸é™¤ï¼Œsoftmax ä¼šå‡ºä»€ä¹ˆé—®é¢˜ï¼Ÿ

* softmax å¯¹ wei åšæŒ‡æ•°è¿ç®—ï¼š

```
softmax(x) = exp(x) / sum(exp(x))
```

* å¦‚æœ wei å¾ˆå¤§ï¼ˆæ¯”å¦‚ mean=0ï¼Œä½† variance å¾ˆå¤§ï¼‰ï¼š

ğŸ‘‰ softmax ä¼šå˜å¾—éå¸¸â€œå°–é”ï¼ˆpeakyï¼‰â€ï¼š

```
softmax â‰ˆ one-hot  
â†’ åªçœ‹ä¸€ä¸ª tokenï¼Œä¸¢å¤±å…¶ä½™ä¿¡æ¯
```

---

### ğŸ“Œ 5ï¸âƒ£ ä¸ºä»€ä¹ˆ variance å¤§ï¼Ÿ

q @ k.Tï¼š

```
q.shape = (T, head_size)  
k.shape = (T, head_size)
```

q\_i â‹… k\_j æ˜¯ **head\_size ä¸ªæ•°çš„ä¹˜ç§¯æ±‚å’Œ**ï¼Œ
â†’ æ–¹å·® â‰ˆ head\_size

---

### ğŸ“Œ 6ï¸âƒ£ é™¤ä»¥ sqrt(head\_size) æœ‰ä»€ä¹ˆç”¨ï¼Ÿ

ğŸ‘‰ è¿™æ ·å¯ä»¥æŠŠ variance æ§åˆ¶åˆ° â‰ˆ 1ï¼Œ
ğŸ‘‰ softmax è¾“å‡ºä¼šâ€œå¹³æ»‘ã€æ‰©æ•£â€ä¸€äº›ï¼Œä¸ä¼šä¸€ä¸Šæ¥å°±å˜æˆ one-hot

---

### ğŸ“Œ 7ï¸âƒ£ ä¸¾ä¾‹è¯´æ˜

å‡è®¾ï¼š

```
wei = [0.1, 0.2, 0.3]
softmax(wei) â†’ æ¯”è¾ƒå‡åŒ€
```

ä½†å¦‚æœï¼š

```
wei = [10, 20, 30]
softmax(wei) â†’ å˜å¾—éå¸¸ one-hot
```

---

### ğŸ“Œ 8ï¸âƒ£ æ€»ç»“ä¸€å¥è¯

ğŸ‘‰ é™¤ä»¥ âˆšhead\_size æ˜¯ä¸ºäº† **æ§åˆ¶ qâ‹…k.T çš„æ–¹å·®**ï¼Œ
ğŸ‘‰ é˜²æ­¢ softmax è¾“å‡ºè¿‡äºå°–é”ï¼Œé¿å…ä¸€å¼€å§‹å°± over-confidentï¼Œ
ğŸ‘‰ è¿™æ ·æ¨¡å‹åˆå§‹åŒ–æ—¶æ›´å®¹æ˜“æ”¶æ•›ã€æ›´ç¨³å®šã€‚

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ ç»§ç»­è®²ï¼š

âœ… ä¸ºä»€ä¹ˆ head\_size è¶Šå¤§è¶Šè¦é™¤ï¼Ÿ
âœ… å¦‚æœä¸ç”¨è¿™ä¸ª scale ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
âœ… æœ‰æ²¡æœ‰åˆ«çš„æ–¹æ³•æ¥ normalize attentionï¼Ÿ

è¦ä¸è¦æˆ‘ç»§ç»­å±•å¼€è®²ï¼ŸğŸŒŸ


# inserting a single self-attention block to our network

that let's now take our self attention knowledge and let's uh take it for a spin so here in the code I created this
head module and it implements a single head of self attention so you give it a head size and then here it creates the
key query and the value linear layers typically people don't use biases in these uh so those are the linear
projections that we're going to apply to all of our nodes now here I'm creating this Trill variable Trill is not a
parameter of the module so in sort of pytorch naming conventions uh this is called a buffer it's not a parameter and
you have to call it you have to assign it to the module using a register buffer so that creates the trill uh the triang
lower triangular Matrix and we're given the input X this should look very familiar now we calculate the keys the
queries we C calculate the attention scores inside way uh we normalize it so
we're using scaled attention here then we make sure that uh future doesn't communicate with the past so this makes
it a decoder block and then softmax and then aggregate the value and
output then here in the language model I'm creating a head in the Constructor and I'm calling it self attention head
and the head size I'm going to keep as the same and embed just for
now and then here once we've encoded the information with the token embeddings
and the position embeddings we're simply going to feed it into the self attention head and then the output of that is
going to go into uh the decoder language modeling head and create the logits so
this the sort of the simplest way to plug in a self attention component uh into our Network right now I had to make
one more change which is that here in the generate uh we have to make sure
that our idx that we feed into the model because now we're using positional embeddings we can never have more than
block size coming in because if idx is more than block size then our position
embedding table is going to run out of scope because it only has embeddings for up to block size and so therefore I
added some uh code here to crop the context that we're going to feed into
self um so that uh we never pass in more than block siiz elements
so those are the changes and let's Now train the network okay so I also came up to the script here and I decreased the
learning rate because uh the self attention can't tolerate very very high learning rates and then I also increased
number of iterations because the learning rate is lower and then I trained it and previously we were only able to get to up to 2.5 and now we are
down to 2.4 so we definitely see a little bit of an improvement from 2.5 to 2.4 roughly uh but the text is still not
amazing so clearly the self attention head is doing some useful communication
but um we still have a long way to go okay so now we've implemented the scale. product attention now next up and the

å½“ç„¶å¯ä»¥ï¼è¿™ä¸€æ®µåœ¨è®² **å¦‚ä½•æŠŠ self-attention block æ’å…¥åˆ°ç°æœ‰çš„ç½‘ç»œé‡Œ**ï¼Œæˆ‘æ¥å¸®ä½ è¯¦ç»†è§£é‡Š ğŸ‘‡ï¼š

---

### ğŸ“Œ 1ï¸âƒ£ æ–°å»ºäº†ä¸€ä¸ª Head æ¨¡å—

```python
class Head(nn.Module):
    ...
```

è¿™ä¸ªæ¨¡å—å®ç°çš„æ˜¯ï¼š

ğŸ‘‰ ä¸€ä¸ª **å•å¤´ self-attention**ï¼ˆsingle-head attentionï¼‰

---

### ğŸ“Œ 2ï¸âƒ£ Head é‡Œæœ‰ä»€ä¹ˆï¼Ÿ

* åˆ›å»ºäº† 3 ä¸ª `nn.Linear` å±‚ï¼š

```
key = nn.Linear(...)  
query = nn.Linear(...)  
value = nn.Linear(...)  
```

â†’ ç”¨æ¥åš Q/K/V çš„ projection

ğŸ‘‰ é€šå¸¸è¿™äº› projection **ä¸ç”¨ bias**ï¼Œä¿æŒçº¯çº¿æ€§æ˜ å°„

---

### ğŸ“Œ 3ï¸âƒ£ åŠ å…¥ Trillï¼ˆä¸‰è§’ maskï¼‰

* åˆ›å»ºäº†ä¸€ä¸ª **lower triangular mask**ï¼ˆtrilï¼‰ï¼š

```
tril = torch.tril(torch.ones(T, T))
```

ä½œç”¨ï¼š

ğŸ‘‰ ä¿è¯ decoder block ä¸çœ‹æœªæ¥
ğŸ‘‰ å®ç° **è‡ªå›å½’**ï¼Œé˜²æ­¢ token çœ‹åˆ°æœªæ¥ token

---

### ğŸ“Œ 4ï¸âƒ£ tril æ˜¯ Buffer

ä¸ºä»€ä¹ˆå« â€œbufferâ€ è€Œä¸æ˜¯ parameterï¼Ÿ

ğŸ‘‰ å› ä¸º tril æ˜¯ **å›ºå®šçš„ï¼Œä¸éœ€è¦è®­ç»ƒ**
ğŸ‘‰ ç”¨ `register_buffer` æŠŠ tril æ³¨å†Œè¿›æ¨¡å—ï¼Œä¿å­˜æ¨¡å‹æ—¶ä¸ä¼šæ¼æ‰

---

### ğŸ“Œ 5ï¸âƒ£ Forward æµç¨‹

* ç»™å®šè¾“å…¥ `x`ï¼Œæµç¨‹æ˜¯ï¼š

```
key = Linear(x)  
query = Linear(x)  
value = Linear(x)
```

* è®¡ç®— Attention Scoreï¼š

```
wei = (q @ k.T) / sqrt(head_size)
```

* åŠ  maskï¼Œsoftmaxï¼Œæ±‚ weighted sumï¼š

```
wei = softmax(masked wei)  
out = wei @ value
```

---

### ğŸ“Œ 6ï¸âƒ£ å¦‚ä½•æ’å…¥åˆ°è¯­è¨€æ¨¡å‹ï¼Ÿ

åœ¨ language model é‡Œï¼š

```
self.sa_head = Head(head_size)
```

---

### ğŸ“Œ 7ï¸âƒ£ Forward è°ƒç”¨

åŸæ¥ embedding + positional embedding â†’ ç›´æ¥ç”¨åˆ° output
ç°åœ¨ï¼š

```
x = token_embedding + positional_embedding  
x = self.sa_head(x)  
x â†’ decoder head â†’ logits
```

---

### ğŸ“Œ 8ï¸âƒ£ ç”Ÿæˆæ—¶æ³¨æ„äº‹é¡¹

å› ä¸ºç°åœ¨ç”¨äº† positional embeddingï¼š

ğŸ‘‰ å¿…é¡»ä¿è¯ **ç”Ÿæˆæ—¶ context <= block\_size**
ğŸ‘‰ æ‰€ä»¥ `generate` é‡ŒåŠ äº†æˆªæ–­é€»è¾‘ï¼Œé˜²æ­¢è¶…èŒƒå›´è®¿é—® position embedding

---

### ğŸ“Œ 9ï¸âƒ£ è®­ç»ƒç»“æœ

* learning rate è¦è°ƒå°
* iteration è¦å˜å¤š

ç»“æœï¼š

* loss ä» 2.5 â†’ 2.4
* æ–‡æœ¬è´¨é‡ **ç¨å¾®æå‡**ï¼Œä½†è¿˜æ²¡åˆ°â€œå¾ˆèªæ˜â€

---

### ğŸ“Œ 10ï¸âƒ£ æ€»ç»“ä¸€å¥è¯

ğŸ‘‰ è¿™ä¸€æ®µè®²çš„å°±æ˜¯ï¼š

âœ… å¦‚ä½•å°è£… self-attention æˆæ¨¡å—
âœ… å¦‚ä½•æ’å…¥åˆ° language model
âœ… å¦‚ä½•è®­ç»ƒ / ç”Ÿæˆæ—¶å¤„ç†ä½ç½®
âœ… è§‚å¯Ÿæ•ˆæœï¼šèƒ½æ˜æ˜¾æå‡ï¼Œä½†è¿˜éœ€è¦æ›´å¤æ‚ç½‘ç»œ

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ ç»§ç»­è®²ï¼š

âœ… ä¸ºä»€ä¹ˆå•å¤´ attention æ•ˆæœæœ‰é™ï¼Ÿ
âœ… ä¸‹ä¸€æ­¥å¤šå¤´ attention æ€ä¹ˆåšï¼Ÿ
âœ… ä¸ºä»€ä¹ˆ self-attention head ä¸éœ€è¦ biasï¼Ÿ

è¦ä¸è¦æˆ‘ç»§ç»­å¸®ä½ è¯¦ç»†è®²ï¼ŸğŸŒŸ


# multi-headed self-attention

attention is all you need paper there's something called multi-head attention and what is multi-head attention it's
just applying multiple attentions in parallel and concatenating their results so they have a little bit of diagram
here I don't know if this is super clear it's really just multiple attentions in
parallel so let's Implement that fairly straightforward if we want a multi-head attention then
we want multiple heads of self attention running in parallel so in pytorch we can
do this by simply creating multiple heads so however heads how however many
heads you want and then what is the head size of each and then we run all of them
in parallel into a list and simply concatenate all of the outputs and we're
concatenating over the channel Dimension so the way this looks now is we don't have just a single ATT
that uh has a hit size of 32 because remember n Ed is 32 instead of having one Communication
channel we now have four communication channels in parallel and each one of these communication channels typically
will be uh smaller uh correspondingly so because we have four communication
channels we want eight dimensional self attention and so from each Communication channel we're going to together eight
dimensional vectors and then we have four of them and that concatenates to give us 32 which is the original and
embed and so this is kind of similar to um if you're familiar with convolutions this is kind of like a group convolution
uh because basically instead of having one large convolution we do convolution in groups and uh that's multi-headed
self attention and so then here we just use essay heads self attention heads instead
now I actually ran it and uh scrolling down I ran the same thing and then we
now get this down to 2.28 roughly and the output is still the generation is
still not amazing but clearly the validation loss is improving because we were at 2.4 just now and so it helps to
have multiple communication channels because obviously these tokens have a lot to talk about they want to find the
consonants the vowels they want to find the vowels just from certain positions uh they want to find any kinds of
different things and so it helps to create multiple independent channels of communication gather lots of different
types of data and then uh decode the output now going back to the paper for a second of course I didn't explain this

å½“ç„¶å¯ä»¥ï¼è¿™æ®µåœ¨è®² **Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰**ï¼Œæˆ‘æ¥å¸®ä½ è¯¦ç»†è§£é‡Š ğŸ‘‡ï¼š

---

### ğŸ“Œ 1ï¸âƒ£ ä»€ä¹ˆæ˜¯ Multi-Head Attentionï¼Ÿ

ğŸ‘‰ å°±æ˜¯ **å¤šä¸ª Self-Attention å¤´å¹¶è¡Œè¿è¡Œ**ï¼Œ
ğŸ‘‰ ç„¶å **æŠŠå®ƒä»¬çš„è¾“å‡ºæ‹¼æ¥èµ·æ¥**

---

### ğŸ“Œ 2ï¸âƒ£ ä¸ºä»€ä¹ˆè¦å¤šä¸ª Headï¼Ÿ

ä¸€ä¸ªå•ç‹¬çš„ attention headï¼š

```
åªèƒ½å­¦ä¹ ä¸€ç§â€œå…³æ³¨æ¨¡å¼â€
```

æ¯”å¦‚ï¼š

* æ‰¾å…ƒéŸ³
* æ‰¾åè¯
* æ‰¾æŸç§è¯­æ³•ä¾èµ–

---

å¤šä¸ª headï¼š

```
æ¯ä¸ª head å­¦ä¸€ç§ä¸åŒçš„â€œå…³æ³¨æ¨¡å¼â€  
å¯ä»¥æ•æ‰åˆ°æ›´å¤šä¸åŒç±»å‹çš„ä¿¡æ¯
```

---

### ğŸ“Œ 3ï¸âƒ£ ç»“æ„å›¾ç¤ºæ„

è®ºæ–‡å›¾ï¼š

```
å¤šä¸ª parallel attention head â†’ æ‹¼æ¥ concat â†’ æœ€åç»Ÿä¸€ projection
```

---

### ğŸ“Œ 4ï¸âƒ£ ä»£ç æ€ä¹ˆå®ç°ï¼Ÿ

åœ¨ PyTorch é‡Œï¼š

```
self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
```

* ä¾‹å¦‚ï¼š

```
embed_size = 32  
num_heads = 4  
head_size = 8  # å› ä¸º 32 / 4 = 8
```

â†’ æ¯ä¸ª head è¾“å‡º 8 ç»´ï¼Œæ€»å…±æ‹¼æˆ 32 ç»´ã€‚

---

### ğŸ“Œ 5ï¸âƒ£ ä¸ºä»€ä¹ˆåƒ Group Convolutionï¼Ÿ

* æ™®é€š conv æ˜¯ï¼š

```
æ‰€æœ‰é€šé“å…¨å·ç§¯
```

* Group Conv æ˜¯ï¼š

```
æ¯ç»„å·ç§¯å™¨å•ç‹¬å·ä¸€éƒ¨åˆ†é€šé“ï¼Œæœ€åæ‹¼æ¥
```

Multi-Head Attention ç±»ä¼¼ï¼š

```
å¤šä¸ª head å„è‡ªæ³¨æ„ä¸€éƒ¨åˆ†â€œä¿¡æ¯å…³ç³»â€ â†’ æ‹¼æ¥
```

---

### ğŸ“Œ 6ï¸âƒ£ å®éªŒç»“æœ

è®­ç»ƒç»“æœï¼š

* åŸæ¥å•å¤´ Attentionï¼Œval loss \~ 2.4
* ç°åœ¨ 4 å¤´ Multi-Head Attentionï¼Œval loss \~ 2.28

---

### ğŸ“Œ 7ï¸âƒ£ ä¸ºä»€ä¹ˆ Multi-Head æœ‰ç”¨ï¼Ÿ

å› ä¸º token ä¹‹é—´çš„å…³ç³» **å¾ˆå¤æ‚**ï¼š

* æœ‰æ—¶å…³æ³¨æ‹¼å†™æ¨¡å¼
* æœ‰æ—¶å…³æ³¨è¯­æ³•
* æœ‰æ—¶å…³æ³¨è¯æ€§
* æœ‰æ—¶å…³æ³¨ä¸Šä¸‹æ–‡æ„ä¹‰

**å¤šä¸ª head = å¤šç§å…³æ³¨èƒ½åŠ›**

---

### ğŸ“Œ 8ï¸âƒ£ æ€»ç»“ä¸€å¥è¯

ğŸ‘‰ Multi-Head Attention å°±æ˜¯ï¼š

âœ… å¤šä¸ª Attention Head å¹¶è¡Œè¿è¡Œ
âœ… æ¯ä¸ª Head å­¦ä¸åŒå…³æ³¨æ¨¡å¼
âœ… è¾“å‡ºæ‹¼æ¥åæ›´ä¸°å¯Œï¼Œæ•ˆæœæ›´å¥½

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ è®²ï¼š

âœ… Multi-Head Attention æœ€ç»ˆè¿˜è¦ä¸è¦ projectionï¼Ÿ
âœ… å¤šå¤´æ˜¯â€œåŠ æ³•â€è¿˜æ˜¯â€œæ‹¼æ¥â€ï¼Ÿ
âœ… ä¸ºä»€ä¹ˆ GPT ç”¨å¾ˆå¤š headï¼ŒBERT ç”¨æ›´å¤š headï¼Ÿ

è¦ä¸è¦æˆ‘ç»§ç»­è®²ï¼ŸğŸŒŸ

è¿™æ®µè®²çš„æ˜¯ **å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆmulti-headed self-attentionï¼‰** çš„æ ¸å¿ƒæ€æƒ³å’Œå®ç°é€»è¾‘ï¼Œæˆ‘å¸®ä½ åˆ†æˆå‡ ä¸ªéƒ¨åˆ†è§£é‡Šä¸€ä¸‹ï¼š

---

## 1. èƒŒæ™¯

åœ¨ä¹‹å‰çš„å•å¤´è‡ªæ³¨æ„åŠ›ï¼ˆsingle-head self-attentionï¼‰ä¸­ï¼Œæˆ‘ä»¬åªæœ‰**ä¸€ä¸ªæ³¨æ„åŠ›é€šé“**ï¼Œæ‰€æœ‰ä¿¡æ¯çš„äº¤äº’éƒ½åœ¨è¿™ä¸€ä¸ªé€šé“ä¸­å®Œæˆã€‚

* ç¼ºç‚¹ï¼šä¸€æ¡é€šé“å¾ˆå¯èƒ½æ— æ³•åŒæ—¶æ•æ‰åˆ°åºåˆ—ä¸­å¤šç§ä¸åŒç±»å‹çš„ä¾èµ–å…³ç³»ï¼ˆä¾‹å¦‚è¾…éŸ³æ‰¾å…ƒéŸ³ã€ä¸»è¯­æ‰¾è°“è¯­ã€è¿œè·ç¦»ä¾èµ–ç­‰ï¼‰ã€‚
* å¤šå¤´çš„æƒ³æ³•ï¼šåŒæ—¶å¼€å¤šä¸ª**ç‹¬ç«‹çš„æ³¨æ„åŠ›é€šé“**ï¼ˆheadsï¼‰ï¼Œæ¯ä¸ªé€šé“å¯ä»¥å…³æ³¨ä¸åŒçš„ç‰¹å¾æ¨¡å¼ã€‚

---

## 2. å¤šå¤´è‡ªæ³¨æ„åŠ›çš„åŸç†

å‡è®¾åŸå§‹åµŒå…¥ç»´åº¦æ˜¯ `n_embd = 32`ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ 4 ä¸ªå¤´ï¼ˆheadsï¼‰ï¼Œé‚£ä¹ˆï¼š

* æ¯ä¸ªå¤´çš„**head\_size** = `n_embd / num_heads = 8`
* æ¯ä¸ªå¤´ç‹¬ç«‹è¿è¡Œä¸€å¥— Q/K/V çº¿æ€§å˜æ¢ + æ³¨æ„åŠ›è®¡ç®—
* å¾—åˆ°çš„æ¯ä¸ªå¤´è¾“å‡ºç»´åº¦æ˜¯ `(B, T, head_size)`
* **æœ€åå°†æ‰€æœ‰å¤´çš„ç»“æœåœ¨é€šé“ç»´åº¦ä¸Šæ‹¼æ¥**ï¼Œå¾—åˆ° `(B, T, n_embd)` çš„æ•´ä½“è¾“å‡ºã€‚

è¿™æ ·åšçš„å¥½å¤„ï¼š

* æ¯ä¸ªå¤´å¯ä»¥å­¦ä¹ åˆ°ä¸åŒçš„æ¨¡å¼
* å¤šä¸ªå¤´å¹¶è¡Œè®¡ç®—ï¼Œæé«˜ä¿¡æ¯æ•æ‰èƒ½åŠ›

---

## 3. å®ç°ç»†èŠ‚ï¼ˆç±»æ¯”å·ç§¯ï¼‰

ä½œè€…æåˆ°ï¼Œè¿™æœ‰ç‚¹åƒå·ç§¯é‡Œçš„**group convolution**ï¼š

* æ™®é€šå·ç§¯ï¼šæ‰€æœ‰é€šé“ä¸€èµ·å·ç§¯
* group convolutionï¼šæŠŠé€šé“åˆ†ç»„ï¼Œæ¯ç»„å•ç‹¬å·ç§¯ï¼Œæœ€åæ‹¼èµ·æ¥
* å¤šå¤´æ³¨æ„åŠ›ï¼šæŠŠåµŒå…¥åˆ†æˆå¤šä¸ªå­ç©ºé—´ï¼Œæ¯ä¸ªå­ç©ºé—´ç‹¬ç«‹åšæ³¨æ„åŠ›ï¼Œç„¶åæ‹¼èµ·æ¥

åœ¨ PyTorch ä¸­å®ç°å¾ˆç®€å•ï¼š

1. åˆ›å»ºå¤šä¸ª `Head` å®ä¾‹ï¼ˆæ¯ä¸ªæ˜¯ä¸€ä¸ªè‡ªæ³¨æ„åŠ›å¤´ï¼‰
2. å¹¶è¡Œè¿è¡Œè¿™äº›å¤´
3. ç”¨ `torch.cat(outputs, dim=-1)` æŠŠç»“æœæ‹¼æ¥å›å»

---

## 4. æ•ˆæœ

åœ¨å®éªŒä¸­ï¼š

* å•å¤´æ—¶éªŒè¯é›† loss å¯èƒ½æ˜¯ 2.4
* å¤šå¤´ï¼ˆæ¯”å¦‚ 4 ä¸ªå¤´ï¼‰ä¹‹åï¼Œloss é™åˆ° 2.28
* ç”Ÿæˆè´¨é‡è™½æœªæ˜¾è‘—æå‡ï¼Œä½†æ¨¡å‹æ›´æœ‰æ½œåŠ›ï¼Œå› ä¸ºå®ƒèƒ½å¹¶è¡Œæ•æ‰æ›´å¤šç±»å‹çš„ä¾èµ–ä¿¡æ¯

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼ **å¤šå¤´è‡ªæ³¨æ„åŠ›çš„æ•°æ®æµåŠ¨å›¾**ï¼ŒæŠŠè¾“å…¥å¦‚ä½•è¢«æ‹†æˆå¤šå¤´ã€å„è‡ªè®¡ç®—æ³¨æ„åŠ›ã€å†æ‹¼æ¥çš„è¿‡ç¨‹å¯è§†åŒ–ï¼Œè¿™æ ·ä¼šç›´è§‚å¾ˆå¤šã€‚
ä½ è¦æˆ‘ç”»å—ï¼Ÿ


```
class MultiHeadAttention(nn.Module):
    """ multiple heads of self-attention in parallel """

    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])

    def forward(self, x):
        return torch.cat([h(x) for h in self.heads], dim=-1)
```

è¿™ä¸ª `MultiHeadAttention` ç±»å°±æ˜¯æŠŠ\*\*å¤šä¸ªè‡ªæ³¨æ„åŠ›å¤´ï¼ˆHeadï¼‰\*\*å¹¶è¡Œè¿è¡Œï¼Œç„¶åæŠŠç»“æœæ‹¼æ¥åœ¨ä¸€èµ·ã€‚æˆ‘ä»¬æ‹†å¼€æ¥çœ‹ï¼š

---

## 1. `__init__` éƒ¨åˆ†

```python
self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
```

* `num_heads`ï¼šè¦åˆ›å»ºçš„æ³¨æ„åŠ›å¤´çš„æ•°é‡
* `head_size`ï¼šæ¯ä¸ªå¤´è¾“å‡ºçš„å‘é‡ç»´åº¦
* è¿™é‡Œç”¨ `nn.ModuleList` ä¿å­˜å¤šä¸ª `Head` å®ä¾‹ï¼Œæ¯ä¸ª `Head` éƒ½æ˜¯ä¹‹å‰å®šä¹‰çš„å•å¤´è‡ªæ³¨æ„åŠ›ç»“æ„ã€‚
* ä¾‹å¦‚ï¼š

  * å¦‚æœæ€»çš„ embedding ç»´åº¦ `n_embd = 32`
  * `num_heads = 4`
  * é‚£ä¹ˆæ¯ä¸ª `head_size = 8`
  * æœ€ç»ˆ 4 ä¸ªå¤´çš„ç»“æœæ‹¼èµ·æ¥è¿˜æ˜¯ 32 ç»´ã€‚

---

## 2. `forward` éƒ¨åˆ†

```python
return torch.cat([h(x) for h in self.heads], dim=-1)
```

* `[h(x) for h in self.heads]` ä¼šä¾æ¬¡æŠŠè¾“å…¥ `x` é€åˆ°æ¯ä¸ªæ³¨æ„åŠ›å¤´ä¸­è®¡ç®—ï¼Œå¾—åˆ°å¤šä¸ª `(B, T, head_size)` å¼ é‡ã€‚
* `torch.cat(..., dim=-1)` ä¼šåœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼ˆé€šé“ç»´åº¦ Cï¼‰ä¸Šæ‹¼æ¥å®ƒä»¬ï¼Œå¾—åˆ° `(B, T, num_heads * head_size)`ã€‚
* è¿™æ ·å¤šå¤´çš„ç»“æœå°±åˆå¹¶æˆåŸå§‹çš„ embedding ç»´åº¦ï¼Œæ–¹ä¾¿é€åˆ°åç»­ç½‘ç»œå±‚ã€‚

---

## 3. æ•°æ®æµç¤ºæ„

å¯ä»¥æŠŠè¿™ä¸ªè¿‡ç¨‹æƒ³æˆï¼š

1. **å¤åˆ¶è¾“å…¥** â†’ æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›ï¼ˆQ/K/V ä¸åŒï¼‰
2. **å¾—åˆ°å¤šä»½ä¸åŒçš„ä¿¡æ¯è§†è§’**ï¼ˆæ¯ä¸ªå¤´å…³æ³¨çš„æ¨¡å¼ä¸åŒï¼‰
3. **æ‹¼æ¥ç»“æœ** â†’ å¾—åˆ°ä¸€ä¸ªåŒ…å«å¤šç§ç‰¹å¾çš„ç»¼åˆè¡¨ç¤º

å›¾å½¢åŒ–çš„è¯å°±åƒæœ‰ `num_heads` æ¡å¹³è¡Œçš„â€œä¿¡æ¯é€šé“â€ï¼Œæœ€ååœ¨é€šé“ç»´åº¦ä¸Šåˆå¹¶ã€‚

---

å¦‚æœä½ éœ€è¦çš„è¯ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼ **å¤šå¤´è‡ªæ³¨æ„åŠ›ç»“æ„å›¾**ï¼Œç›´è§‚å±•ç¤º `x` å¦‚ä½•è¢«é€è¿›å¤šä¸ªå¤´ã€å¹¶è¡Œè®¡ç®—ã€å†æ‹¼æ¥çš„è¿‡ç¨‹ã€‚è¿™æ ·æ›´å®¹æ˜“å’Œ Transformer çš„æ•´ä½“ç»“æ„å¯¹åº”èµ·æ¥ã€‚
ä½ è¦æˆ‘ç”»å—ï¼Ÿ



# feedforward layers of transformer block

figure in full detail but we are starting to see some components of what we've already implemented we have the positional encodings the token encodings
that add we have the masked multi-headed attention implemented now here's another
multi-headed attention which is a cross attention to an encoder which we haven't we're not going to implement in this
case I'm going to come back to that later but I want you to notice that there's a feed forward part here and
then this is grouped into a block that gets repeat it again and again now the feedforward part here is just a simple
uh multi-layer perceptron um so the multi-headed so here position
wise feed forward networks is just a simple little MLP so I want to start basically in a similar fashion also
adding computation into the network and this computation is on a per node level
so I've already implemented it and you can see the diff highlighted on the left here when I've added or changed things
now before we had the self multi-headed self attention that did the communication but we went way too fast
to calculate the logits so the tokens looked at each other but didn't really have a lot of time to think on what they
found from the other tokens and so what I've implemented here is a little feet
forward single layer and this little layer is just a linear followed by a Rel nonlinearity and that's that's it so
it's just a little layer and then I call it feed forward um and embed
and then this feed forward is just called sequentially right after the self attention so we self attend then we feed
forward and you'll notice that the feet forward here when it's applying linear this is on a per token level all the
tokens do this independently so the self attention is the communication and then once they've gathered all the data now
they need to think on that data individually and so that's what feed forward is doing and that's why I've
added it here now when I train this the validation LW actually continues to go down now to 2. 24 which is down from
2.28 uh the output still look kind of terrible but at least we've improved the situation and so as a preview we're
going to now start to intersperse the communication with the computation and
that's also what the Transformer does when it has blocks that communicate and then compute and it groups them and
replicates them okay so let me show you what we'd like to do we'd like to do

è¿™æ®µè¯è®²çš„æ˜¯ Transformer ä¸­çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ï¼š**å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Layerï¼‰**ï¼Œå®ƒåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¹‹åè¢«æ’å…¥è¿›æ¥ï¼Œæä¾›æ¯ä¸ª token å•ç‹¬çš„å¤„ç†èƒ½åŠ›ã€‚ä»¥ä¸‹æ˜¯é€šä¿—æ˜“æ‡‚çš„ä¸­æ–‡è§£é‡Šï¼š

---

## ğŸ’¡ è¿™æ®µè®²äº†ä»€ä¹ˆï¼Ÿ

åœ¨ Transformer ç»“æ„ä¸­ï¼Œé™¤äº†å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆmulti-head self-attentionï¼‰ï¼Œè¿˜æœ‰ä¸€ä¸ªå…³é”®éƒ¨åˆ†å«åš **å‰é¦ˆç½‘ç»œï¼ˆFeedforward Layerï¼‰**ã€‚

* å¤šå¤´æ³¨æ„åŠ›ï¼šè®©æ¯ä¸ª token å¯ä»¥å’Œå…¶å®ƒ token äº¤äº’ã€äº¤æµä¿¡æ¯ï¼ˆ"æ²Ÿé€š"ï¼‰
* å‰é¦ˆç½‘ç»œï¼šè®©æ¯ä¸ª token å•ç‹¬åšæ€è€ƒå’Œè®¡ç®—ï¼ˆ"æ€è€ƒ"ï¼‰

---

## ğŸ§  ä½œè€…ä¸ºä»€ä¹ˆè¦åŠ å…¥ Feedforward å±‚ï¼Ÿ

åŸæ¥æ¨¡å‹ç»“æ„æ˜¯è¿™æ ·çš„ï¼š

```text
TokenEmbedding + PositionalEmbedding
        â†“
Multi-Head Self Attentionï¼ˆtoken é—´æ²Ÿé€šï¼‰
        â†“
Linear è¾“å‡º logitsï¼ˆå¤ªå¿«äº†ï¼Œè¿˜æ²¡æ€è€ƒå°±è¾“å‡ºï¼‰
```

ä½œè€…è§‰å¾—ï¼štoken ä¹‹é—´è™½ç„¶æ²Ÿé€šäº†ï¼Œä½†**è¿˜æ²¡æ¥å¾—åŠæ€è€ƒ**ï¼Œå°±ç›´æ¥åšé¢„æµ‹äº†ï¼Œè¿™ä¸åˆç†ï¼

äºæ˜¯ä»–åŠ äº†ä¸€ä¸ªå‰é¦ˆå±‚ï¼ˆFeedforwardï¼‰è®©æ¯ä¸ª token **è‡ªå·±æ€è€ƒ**åˆšåˆšæ”¶åˆ°çš„ä¿¡æ¯ï¼š

```text
Self Attentionï¼ˆæ²Ÿé€šï¼‰
â†’ FeedForwardï¼ˆç‹¬ç«‹æ€è€ƒï¼‰
â†’ è¾“å‡º logitsï¼ˆæœ€ç»ˆé¢„æµ‹ï¼‰
```

---

## âš™ï¸ Feedforward å±‚æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ

åœ¨ä»£ç ä¸­ï¼Œå…¶å®å°±æ˜¯éå¸¸ç®€å•çš„ä¸€ä¸ªä¸¤å±‚ç¥ç»ç½‘ç»œï¼š

```python
self.ffwd = nn.Sequential(
    nn.Linear(n_embd, n_embd),
    nn.ReLU()
)
```

ç„¶ååœ¨å‰å‘ä¼ æ’­ä¸­åŠ å…¥ï¼š

```python
x = self.sa_head(x)  # è‡ªæ³¨æ„åŠ›ï¼Œtoken ä¹‹é—´äº’ç›¸äº¤æµ
x = self.ffwd(x)     # å‰é¦ˆç½‘ç»œï¼Œæ¯ä¸ª token è‡ªå·±å¤„ç†æ•°æ®
```

æ³¨æ„ï¼šè¿™ä¸ªå‰é¦ˆæ˜¯ **é€ä¸ª token ç‹¬ç«‹è¿›è¡Œçš„**ï¼Œä¸åƒ self-attention ä¼šè·¨ tokenã€‚

---

## ğŸ¯ ä¸ºä»€ä¹ˆè¿™ä¹ˆè®¾è®¡ï¼Ÿ

* è‡ªæ³¨æ„åŠ›æ˜¯**æ¨ªå‘ä¿¡æ¯æµ**ï¼ˆtoken ä¹‹é—´æ²Ÿé€šï¼‰
* å‰é¦ˆç½‘ç»œæ˜¯**çºµå‘ä¿¡æ¯å¤„ç†**ï¼ˆæ¯ä¸ª token å†…éƒ¨åˆ†æï¼‰

è¿™ä¸¤ä¸ªç»“åˆèµ·æ¥ï¼Œå½¢æˆå®Œæ•´çš„å¤„ç†æµç¨‹ï¼š

> â€œå¬å®Œåˆ«äººè¯´è¯â€ï¼ˆself-attentionï¼‰ â†’ â€œè‡ªå·±æƒ³ä¸€æƒ³â€ï¼ˆfeedforwardï¼‰ â†’ â€œå†³å®šæ€ä¹ˆåšâ€ï¼ˆè¾“å‡ºé¢„æµ‹ï¼‰

---

## âœ… æ•ˆæœå¦‚ä½•ï¼Ÿ

* æ¨¡å‹è®­ç»ƒåçš„æŸå¤±ï¼ˆlossï¼‰ä» 2.28 é™åˆ°äº† 2.24
* è¾“å‡ºæ–‡æœ¬è™½ç„¶è¿˜ä¸å®Œç¾ï¼Œä½†ç¡®å®æœ‰æ‰€æå‡
* è¡¨æ˜ feedforward å±‚ç¡®å®èµ·äº†ä½œç”¨

---

## ğŸ§± æœ€åä¸€å¥è¯æ€»ç»“

> Feedforward å±‚è®©æ¯ä¸ª token åœ¨æ”¶åˆ°å…¶å®ƒ token ä¿¡æ¯åï¼Œ**è¿›è¡Œç‹¬ç«‹æ€è€ƒå’Œå¤„ç†**ï¼Œè¿™ä¸€æ­¥å¯¹æœ€ç»ˆé¢„æµ‹éå¸¸å…³é”®ã€‚

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥ç”»ä¸€å¼ å›¾å¸®ä½ å¯è§†åŒ– self-attention å’Œ feedforward çš„å…³ç³»ã€‚éœ€è¦å—ï¼Ÿ


ä½ åˆšæ‰è´´çš„å†…å®¹å…¶å®æ˜¯åœ¨è§£é‡Š **Transformer Block ä¸­çš„ Feed Forward å±‚** çš„ä½œç”¨å’Œä½ç½®ï¼Œæˆ‘å¸®ä½ åˆ†æˆå‡ éƒ¨åˆ†é€šä¿—è§£é‡Šä¸€ä¸‹ï¼š

---

### 1. Transformer Block çš„åŸºæœ¬ç»“æ„

ä¸€ä¸ªå…¸å‹çš„ Transformer Block ä¸»è¦åˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼š

1. **å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attentionï¼‰**

   * è¿™æ˜¯â€œæ²Ÿé€šâ€é˜¶æ®µï¼Œæ‰€æœ‰ tokenï¼ˆèŠ‚ç‚¹ï¼‰ä¹‹é—´äº¤æ¢ä¿¡æ¯ã€‚
2. **å‰é¦ˆç½‘ç»œï¼ˆFeed Forward Network, FFNï¼‰**

   * è¿™æ˜¯â€œæ€è€ƒâ€é˜¶æ®µï¼Œæ¯ä¸ª token ç‹¬ç«‹å¯¹æ”¶é›†åˆ°çš„ä¿¡æ¯è¿›è¡Œå¤„ç†ã€‚

è€Œä¸”è¿™ä¸¤ä¸ªé˜¶æ®µä¼š **äº¤æ›¿å †å ** å¤šæ¬¡ï¼Œæ¯”å¦‚ï¼š

```
[è‡ªæ³¨æ„åŠ›] â†’ [å‰é¦ˆç½‘ç»œ] â†’ [è‡ªæ³¨æ„åŠ›] â†’ [å‰é¦ˆç½‘ç»œ] â†’ ...
```

---

### 2. ä¸ºä»€ä¹ˆè¦åŠ  Feed Forward å±‚ï¼Ÿ

ä½œè€…çš„æ¯”å–»æ˜¯ï¼š

* **è‡ªæ³¨æ„åŠ›** è®© token ä¹‹é—´äº’ç›¸ä¼ é€’ä¿¡æ¯ï¼ˆâ€œå¤§å®¶å¼€ä¼šäº¤æµâ€ï¼‰ã€‚
* **Feed Forward å±‚** è®©æ¯ä¸ª token ç‹¬ç«‹åœ°å¤„ç†æ”¶åˆ°çš„ä¿¡æ¯ï¼ˆâ€œä¼šåè‡ªå·±æ¶ˆåŒ–â€ï¼‰ã€‚

å¦‚æœåªæœ‰è‡ªæ³¨æ„åŠ›ï¼Œæ¨¡å‹åªæ˜¯ä¿¡æ¯ä¼ é€’ï¼Œä½†ç¼ºå°‘ä¿¡æ¯åŠ å·¥ï¼›åŠ äº† Feed Forward å±‚ï¼Œå°±è®© token èƒ½åœ¨è‡ªèº«çš„ä¸Šä¸‹æ–‡é‡Œåšéçº¿æ€§å˜æ¢ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£å’Œç¼–ç ä¿¡æ¯ã€‚

---

### 3. Feed Forward å±‚çš„å®ç°

åœ¨ Transformer é‡Œï¼Œè¿™ä¸ªéƒ¨åˆ†å¾ˆç®€å•ï¼š

1. **çº¿æ€§å˜æ¢ï¼ˆLinearï¼‰**ï¼šç›¸å½“äºä¸€ä¸ªå…¨è¿æ¥å±‚ï¼Œç‹¬ç«‹ä½œç”¨åœ¨æ¯ä¸ª token ä¸Šã€‚
2. **æ¿€æ´»å‡½æ•°ï¼ˆReLU/GELUï¼‰**ï¼šå¢åŠ éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚
3. **å†æ¥ä¸€æ¬¡çº¿æ€§å˜æ¢ï¼ˆLinearï¼‰**ï¼šæŠ•å°„å›åŸæ¥çš„ç»´åº¦ã€‚

å…¬å¼ä¸Šå¯ä»¥å†™æˆï¼š

```
FFN(x) = Linear2( ReLU( Linear1(x) ) )
```

å…¶ä¸­ï¼Œ`x` çš„å½¢çŠ¶æ˜¯ `(B, T, C)`ï¼ˆæ‰¹å¤§å° Ã— åºåˆ—é•¿åº¦ Ã— é€šé“æ•°ï¼‰ï¼Œ**æ¯ä¸ª token çš„ C ç»´å‘é‡å•ç‹¬åšè®¡ç®—**ï¼Œäº’ä¸å¹²æ‰°ã€‚

---

### 4. è®­ç»ƒæ•ˆæœ

ä½œè€…å®éªŒå‘ç°ï¼ŒåŠ äº†è¿™ä¸ª Feed Forward å±‚åï¼š

* éªŒè¯é›†çš„ loss ä» **2.28 é™åˆ° 2.24**ï¼Œè¯´æ˜æ¨¡å‹æ€§èƒ½æå‡äº†ã€‚
* è¾“å‡ºè™½ç„¶è¿˜ä¸å®Œç¾ï¼Œä½†æ˜¾ç„¶æ›´å¥½ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼  **Transformer Block æ•°æ®æµå›¾**ï¼ŒæŠŠâ€œæ²Ÿé€šé˜¶æ®µâ€å’Œâ€œæ€è€ƒé˜¶æ®µâ€å¯è§†åŒ–ï¼Œè¿™æ ·ä½ å¯ä»¥æ›´ç›´è§‚åœ°ç†è§£ä¸ºä»€ä¹ˆè¦äº¤æ›¿ä½¿ç”¨ Attention å’Œ Feed Forwardã€‚è¿™æ ·è¦ç”»å—ï¼Ÿ


ä¸ºä»€ä¹ˆè¯´æ˜¯æ¯ä¸ª token ç‹¬ç«‹å¯¹æ”¶é›†åˆ°çš„ä¿¡æ¯è¿›è¡Œå¤„ç†ï¼Ÿä»å“ªé‡Œå¯ä»¥çœ‹å‡ºæ¥

è¿™é‡Œè¯´â€œæ¯ä¸ª token ç‹¬ç«‹å¯¹æ”¶é›†åˆ°çš„ä¿¡æ¯è¿›è¡Œå¤„ç†â€ï¼Œä¸»è¦æ˜¯å› ä¸º **Feed Forward Networkï¼ˆå‰é¦ˆç½‘ç»œï¼‰** çš„å®ç°æ–¹å¼å†³å®šäº†å®ƒä¸ä¼šè®© token ä¹‹é—´å†ç›¸äº’é€šä¿¡ï¼Œè€Œæ˜¯é€ä¸ª token å•ç‹¬è®¡ç®—ã€‚

### ä»ä»£ç ä¸Šçœ‹

åœ¨ Transformer ä¸­ï¼ŒFeed Forward å±‚ä¸€èˆ¬æ˜¯è¿™æ ·å†™çš„ï¼š

```python
self.ffwd = nn.Sequential(
    nn.Linear(n_embd, hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, n_embd),
)
```

å¦‚æœè¾“å…¥ `x` çš„å½¢çŠ¶æ˜¯ `(B, T, C)`ï¼š

* `B` = batch size
* `T` = åºåˆ—é•¿åº¦ï¼ˆtoken æ•°ï¼‰
* `C` = embedding ç»´åº¦

å½“æ‰§è¡Œï¼š

```python
out = self.ffwd(x)
```

æ—¶ï¼Œ`nn.Linear` ä¼š**ç‹¬ç«‹ä½œç”¨åœ¨æœ€åä¸€ä¸ªç»´åº¦ C** ä¸Šï¼Œä¸ä¼šåœ¨ T ç»´ï¼ˆtoken ç»´åº¦ï¼‰ä¸Šåšäº¤äº’ã€‚è¿™æ„å‘³ç€ï¼š

* å¯¹äºæ¯ä¸ª batch ä¸­çš„æ¯ä¸ª tokenï¼ŒLinear éƒ½ä¼šå•ç‹¬è¯»å–å®ƒçš„ C ç»´å‘é‡
* è®¡ç®—æ–¹å¼æ˜¯ **çŸ©é˜µä¹˜æ³• + åç½®**ï¼Œä¸åŒä¸€åºåˆ—ä¸­å…¶ä»– token çš„å‘é‡æ— å…³

PyTorch çš„ `nn.Linear` æ–‡æ¡£ä¸­ä¹Ÿæ˜ç¡®è¯´æ˜ï¼šè¾“å…¥å¯ä»¥æ˜¯ `(N, *, in_features)`ï¼ŒLinear ä¼šåœ¨ `in_features` ç»´åº¦ä¸Šè®¡ç®—ï¼Œå…¶å®ƒç»´åº¦ä¿æŒç‹¬ç«‹ã€‚

---

### æ•°æ®æµçš„ç›´è§‚ç†è§£

ä½ å¯ä»¥è¿™æ ·ç†è§£æ•´ä¸ª Transformer å—ï¼š

1. **Self-Attention é˜¶æ®µ**ï¼šæ‰€æœ‰ token ä¹‹é—´é€šä¿¡ï¼Œæ”¶é›†ä¿¡æ¯ã€‚
2. **Feed Forward é˜¶æ®µ**ï¼šæ¯ä¸ª token æŠŠåˆšåˆšæ”¶é›†åˆ°çš„ä¿¡æ¯ï¼Œæ”¾è¿›è‡ªå·±çš„â€œå°è„‘è¢‹â€é‡Œï¼ˆå‰é¦ˆç½‘ç»œï¼‰ï¼Œè‡ªå·±æ¶ˆåŒ–ã€æç‚¼ï¼Œ**ä¸ä¼šå†å»çœ‹åˆ«çš„ token**ã€‚

æ‰€ä»¥è¯´ï¼Œè¿™ä¸€æ­¥æ˜¯ â€œç‹¬ç«‹æ€è€ƒâ€ é˜¶æ®µï¼Œå› ä¸ºè®¡ç®—æ˜¯é€ token ç‹¬ç«‹å®Œæˆçš„ã€‚

---

### ä»å½¢çŠ¶å˜åŒ–çœ‹ç‹¬ç«‹æ€§

å‡è®¾è¾“å…¥ï¼š

```
x.shape = (B=2, T=4, C=8)
```

Feed Forward ç¬¬ä¸€å±‚ Linearï¼š

```
nn.Linear(8, 16)
```

å¤„ç†åï¼š

```
shape -> (B=2, T=4, 16)
```

è¿™é‡Œ `(B, T)` éƒ¨åˆ†æ²¡å˜ï¼Œåªæ˜¯ C ç»´åº¦å˜äº†
â†’ è¯´æ˜ token ä¸ token ä¹‹é—´æ²¡å‘ç”Ÿæ··åˆï¼Œè®¡ç®—æ˜¯**é€ token ç‹¬ç«‹**çš„ã€‚

---

å¦‚æœä½ éœ€è¦çš„è¯ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼  **â€œTransformer Block æ•°æ®æµåŠ¨å›¾â€**ï¼ŒæŠŠ Self-Attention çš„â€œé€šä¿¡â€å’Œ Feed Forward çš„â€œç‹¬ç«‹æ€è€ƒâ€å¯¹æ¯”æ¸…æ¥šã€‚è¿™æ ·ä¼šæ›´ç›´è§‚ã€‚


# residual connections

something like this we have a block and this block is is basically this part here except for the cross
attention now the block basically intersperses communication and then computation the computation the
communication is done using multi-headed selfelf attention and then the computation is done using a feed forward
Network on all the tokens independently now what I've added here
also is you'll notice this takes the number of embeddings in the embedding Dimension
and number of heads that we would like which is kind of like group size in group convolution and and I'm saying
that number of heads we'd like is four and so because this is 32 we calculate that because this is 32 the number of
heads should be four um the head size should be eight so that everything sort
of works out Channel wise um so this is how the Transformer structures uh sort of the uh the sizes typically so the
head size will become eight and then this is how we want to intersperse them and then here I'm trying to create
blocks which is just a sequential application of block block block so that we're interspersing communication feed
forward many many times and then finally we decode now I actually tried to run
this and the problem is this doesn't actually give a very good uh answer and very good result and the reason for that
is we're start starting to actually get like a pretty deep neural net and deep neural Nets uh suffer from optimization
issues and I think that's what we're kind of like slightly starting to run into so we need one more idea that we can borrow from the um Transformer paper
to resolve those difficulties now there are two optimizations that dramatically help with the depth of these networks
and make sure that the networks remain optimizable let's talk about the first one the first one in this diagram is you
see this Arrow here and then this arrow and this Arrow those are skip connections or sometimes called residual
connections they come from this paper uh the presidual learning for image recognition from about
2015 uh that introduced the concept now these are basically what it means is you
transform data but then you have a skip connection with addition from the previous features now the way I like to
visualize it uh that I prefer is the following here the computation happens
from the top to bottom and basically you have this uh residual pathway and you
are free to Fork off from the residual pathway perform some computation and then project back to the residual
pathway via addition and so you go from the the uh inputs to the targets only
via plus and plus plus and the reason this is useful is because during back propagation remember from our microG
grad video earlier addition distributes gradients equally to both of its branches that that fed as the input and
so the supervision or the gradients from the loss basically hop through every
addition node all the way to the input and then also Fork off into the residual
blocks but basically you have this gradient Super Highway that goes directly from the supervision all the
way to the input unimpeded and then these viral blocks are usually initialized in the beginning so they
contribute very very little if anything to the residual pathway they they are initialized that way so in the beginning
they are sort of almost kind of like not there but then during the optimization they come online over time and they uh
start to contribute but at least at the initialization you can go from directly supervision to the input gradient is
unimpeded and just flows and then the blocks over time kick in and so that dramatically helps
with the optimization so let's implement this so coming back to our block here basically what we want to do is we want
to do xal X+ self attention and xal X+ self. feed
forward so this is X and then we Fork off and do some communication and come
back and we Fork off and we do some computation and come back so those are residual connections and then swinging
back up here we also have to introd use this projection so nn.
linear and uh this is going to be from after we concatenate this this is
the prze and embed so this is the output of the self tension itself but then we
actually want the uh to apply the projection and that's the result so the projection is just a
linear transformation of the outcome of this layer so that's the projection back into the virual pathway and then here in a
feet forward it's going to be the same same thing I could have a a self doot projection here as well but let me just
simplify it and let me uh couple it inside the same sequential container and
so this is the projection layer going back into the residual pathway and
so that's uh well that's it so now we can train this so I implemented one more small change when you look into the
paper again you see that the dimensionality of input and output is 512 for them and they're saying that the
inner layer here in the feet forward has dimensionality of 248 so there's a multiplier of four and so the inner
layer of the feet forward Network should be multiplied by four in terms of Channel sizes so I came here and I
multiplied four times embed here for the feed forward and then from four times nmed coming back down to nmed when we go
back to the pro uh to the projection so adding a bit of computation here and growing that layer that is in the
residual block on the side of the residual pathway and then I train this and we
actually get down all the way to uh 2.08 validation loss and we also see that network is starting to get big enough
that our train loss is getting ahead of validation loss so we're starting to see like a little bit of overfitting and um our our
um uh Generations here are still not amazing but at least you see that we can see like is here this now grief syn like
this starts to almost look like English so um yeah we're starting to really get there okay and the second Innovation

è¿™æ®µè®²çš„æ˜¯ Transformer ä¸­ä¸€ä¸ªéå¸¸é‡è¦çš„ç»“æ„è®¾è®¡ â€”â€” **æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰**ï¼Œå®ƒçš„ä½œç”¨æ˜¯å¸®åŠ©æ·±å±‚ç¥ç»ç½‘ç»œæ›´å®¹æ˜“è®­ç»ƒã€‚ä¸‹é¢æˆ‘ä¼šé€šä¿—åœ°è®²è§£è¿™æ®µå†…å®¹ã€‚

---

## ğŸŒ‰ ä¸€ã€æ®‹å·®è¿æ¥æ˜¯ä»€ä¹ˆï¼Ÿ

æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰ï¼Œä¹Ÿå« **è·³è·ƒè¿æ¥ï¼ˆSkip Connectionï¼‰**ï¼Œå°±æ˜¯åœ¨ç¥ç»ç½‘ç»œä¸­çš„æŸä¸€å±‚åšå®Œå˜æ¢åï¼ŒæŠŠè¾“å…¥ **åŠ å›å»**ã€‚

ç”¨ä¸€å¥è¯æ€»ç»“ï¼š

> "ä½ å˜æ¢äº†ä¸€äº›ä¿¡æ¯ï¼Œæˆ‘æŠŠå˜æ¢åçš„ç»“æœï¼Œå’ŒåŸå§‹ä¿¡æ¯ç›´æ¥åŠ èµ·æ¥ã€‚"

ä¸¾ä¸ªä¾‹å­ï¼ˆå‡è®¾æˆ‘ä»¬è¦å¤„ç†ä¸€ä¸ªå‘é‡ `x`ï¼‰ï¼š

```python
out = x + f(x)
```

* `f(x)` æ˜¯æŸå±‚ç½‘ç»œï¼ˆæ¯”å¦‚ self-attention æˆ– MLPï¼‰è®¡ç®—çš„ç»“æœã€‚
* `x` æ˜¯è¾“å…¥ã€‚
* `out` æ˜¯â€œæ®‹å·®è¿æ¥â€åçš„ç»“æœã€‚

---

## ğŸ§  äºŒã€ä¸ºä»€ä¹ˆæ®‹å·®è¿æ¥æœ‰ç”¨ï¼Ÿ

å› ä¸ºæ·±å±‚ç½‘ç»œå¾ˆéš¾è®­ç»ƒï¼Œæ®‹å·®è¿æ¥è§£å†³äº†å‡ ä¸ªå¤§é—®é¢˜ï¼š

### âœ… é—®é¢˜1ï¼šæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

åœ¨æ·±å±‚ç½‘ç»œä¸­ï¼Œ**æ¢¯åº¦ä¼šè¶Šæ¥è¶Šå°æˆ–è¶Šæ¥è¶Šå¤§**ï¼Œå¯¼è‡´å­¦ä¹ åœæ»æˆ–ä¸ç¨³å®šã€‚

ğŸ‘‰ æ®‹å·®è¿æ¥é€šè¿‡â€œåŠ æ³•â€æ–¹å¼ï¼Œè®©æ¢¯åº¦å¯ä»¥**é¡ºç€åŠ æ³•ç›´æ¥æµå‘å‰é¢å±‚**ï¼Œè€Œä¸éœ€è¦ç©¿è¶Šå¤šä¸ªå±‚çš„å˜æ¢ï¼Œåƒé«˜é€Ÿå…¬è·¯ä¸€æ ·ã€‚

### âœ… é—®é¢˜2ï¼šç½‘ç»œå¤ªæ·±ï¼Œåè€Œå˜å·®

åŸå§‹ç½‘ç»œï¼ˆæ²¡æœ‰æ®‹å·®è¿æ¥ï¼‰è¶Šæ·±ï¼Œæ€§èƒ½å¯èƒ½åè€Œå˜å·®ï¼ˆå‡†ç¡®ç‡ä¸‹é™ï¼‰ã€‚

ğŸ‘‰ åŠ ä¸Šæ®‹å·®è¿æ¥åï¼Œå¦‚æœæŸä¸€å±‚å­¦ä¸å¥½ï¼Œå®ƒå¯ä»¥é€‰æ‹©â€œä»€ä¹ˆéƒ½ä¸åšâ€ï¼Œå› ä¸º `x + 0 = x`ï¼Œè¿™å°±å…è®¸ç½‘ç»œ**è·³è¿‡é‚£ä¸€å±‚çš„å½±å“**ï¼Œæ›´å®¹æ˜“ä¼˜åŒ–ã€‚

---

## ğŸ”§ ä¸‰ã€ä»£ç å®ç°é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼‰

```python
x = ... # è¾“å…¥å‘é‡
x = x + self_attention(x)   # ç¬¬ä¸€æ¬¡æ®‹å·®è¿æ¥
x = x + feed_forward(x)     # ç¬¬äºŒæ¬¡æ®‹å·®è¿æ¥
```

æ¯ä¸€å—æ¨¡å—ï¼ˆæ¯”å¦‚ Self-Attention å’Œ FeedForwardï¼‰éƒ½æ’åœ¨æ®‹å·®è·¯å¾„çš„æ—è¾¹ï¼Œå…ˆâ€œåˆ†å‰å‡ºå»â€ï¼Œç„¶åå†åŠ å›æ¥ã€‚

---

## ğŸ§ª å››ã€æ³¨æ„å‡ ä¸ªæŠ€æœ¯ç»†èŠ‚

1. **æ®‹å·®è·¯å¾„ä¸­é€šå¸¸ä¼šåŠ ä¸€ä¸ª `Linear` å±‚**ï¼Œç”¨äºæŠŠå˜æ¢ç»“æœçš„ç»´åº¦â€œæŠ•å½±â€å›æ¥ï¼Œä½¿å¾—å¯ä»¥å’ŒåŸå§‹è¾“å…¥ç›¸åŠ ï¼ˆç»´åº¦å¿…é¡»åŒ¹é…ï¼‰ã€‚

2. **MLP é€šå¸¸ä¼šåŠ å®½ 4 å€**ï¼ˆä¾‹å¦‚ `nn.Linear(n_embd, 4*n_embd)`ï¼‰ï¼Œå†ç”¨ ReLUï¼Œç„¶åå†å‹ç¼©å›æ¥ï¼ˆ`nn.Linear(4*n_embd, n_embd)`ï¼‰ã€‚è¿™æ ·æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚

---

## ğŸ—ï¸ äº”ã€æœ€ç»ˆç»“æ„

ä¸€ä¸ªå…¸å‹çš„ Transformer Blockï¼ˆåªè€ƒè™‘ decoder blockï¼‰ï¼š

```
x â€”â€”â¤ [Multi-head Self-Attention] â€”â€”â¤ +
|                                    |
+â€”â€”â€”â€”â€”â€”â€”â€”â€”â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤

(æ¥ç€å†æ¥ä¸€æ¬¡...)

x â€”â€”â¤ [FeedForwardï¼ˆMLPï¼‰] â€”â€”â¤ +
|                              |
+â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤â¤
```

---

## âœ¨ æ€»ç»“ä¸€å¥è¯ï¼š

> æ®‹å·®è¿æ¥å°±åƒæ˜¯ä¿¡æ¯é«˜é€Ÿå…¬è·¯ï¼Œè®©ä¿¡æ¯å’Œæ¢¯åº¦éƒ½å¯ä»¥**ç›´æ¥ç©¿è¿‡ç½‘ç»œå±‚**ï¼Œæ—¢ä¸ä¸¢ä¿¡æ¯ï¼Œåˆåˆ©äºè®­ç»ƒï¼Œæ˜¯æ·±å±‚ Transformer èƒ½æˆåŠŸçš„æ ¸å¿ƒè®¾è®¡ä¹‹ä¸€ã€‚

éœ€è¦æˆ‘ç”»ä¸ªå›¾æˆ–è€…ç”¨åŠ¨ç”»å¸®ä½ æ›´æ¸…æ¥šç†è§£è¿™ä¸ªâ€œåˆ†å‰å†åˆå¹¶â€çš„æµç¨‹å—ï¼Ÿ

ä½ è´´çš„è¿™ä¸€æ®µä¸»è¦æ˜¯åœ¨è®² **Transformer ä¸­ Residual Connectionsï¼ˆæ®‹å·®è¿æ¥ï¼‰** çš„ä½œç”¨ã€åŸç†ï¼Œä»¥åŠåœ¨å®ç°æ—¶æ€ä¹ˆç»“åˆ Self-Attention å’Œ Feed Forward ä½¿ç”¨ã€‚
æˆ‘å¸®ä½ åˆ†æˆå‡ éƒ¨åˆ†è§£é‡Šã€‚

---

## 1. ä¸ºä»€ä¹ˆéœ€è¦ Residual Connection

* **é—®é¢˜èƒŒæ™¯**
  Transformer Block é‡Œæˆ‘ä»¬ä¼šå¤šæ¬¡å †å  â€œé€šä¿¡ï¼ˆSelf-Attentionï¼‰+ è®¡ç®—ï¼ˆFeed Forwardï¼‰â€ çš„å­æ¨¡å—ã€‚
  è¿™æ ·ç½‘ç»œä¼šå¾ˆæ·±ï¼Œæ·±åº¦ç½‘ç»œè®­ç»ƒå®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±ã€ä¼˜åŒ–å›°éš¾ç­‰é—®é¢˜ã€‚

* **è§£å†³åŠæ³•**
  2015 å¹´ ResNet è®ºæ–‡æå‡ºäº† Residual Connectionï¼šè®©è¾“å…¥ `x` åœ¨è®¡ç®—åçš„ç»“æœ `F(x)` ä¸ŠåŠ å›å»ï¼š

  ```
  out = x + F(x)
  ```

  è¿™æ ·ï¼Œæ¢¯åº¦åœ¨åå‘ä¼ æ’­æ—¶å¯ä»¥ç›´æ¥æ²¿ç€è¿™æ¡ â€œæ·å¾„â€ æµå›è¾“å…¥å±‚ï¼Œå½¢æˆä¸€ä¸ª **æ¢¯åº¦é«˜é€Ÿé€šé“ï¼ˆGradient Super Highwayï¼‰**ï¼Œé¿å…æ¢¯åº¦åœ¨æ·±å±‚æ¶ˆå¤±ã€‚

---

## 2. å½¢è±¡ç±»æ¯”

ä½œè€…ç»™çš„ç±»æ¯”æ˜¯ï¼š

* æŠŠä¸»é€šé“æƒ³è±¡æˆä¸€æ¡é«˜é€Ÿå…¬è·¯ï¼ˆResidual Pathï¼‰ã€‚
* ä½ å¯ä»¥ä»é«˜é€Ÿå…¬è·¯â€œåˆ†å‰â€å‡ºå»ï¼Œèµ°ä¸€æ®µå°è·¯åšä¸€äº›è®¡ç®—ï¼ˆæ¯”å¦‚ Self-Attention æˆ– Feed Forwardï¼‰ï¼Œå†å›åˆ°é«˜é€Ÿå…¬è·¯ã€‚
* **åŠ æ³•èŠ‚ç‚¹ï¼ˆ`+`ï¼‰** æ˜¯å…³é”®ï¼Œå› ä¸ºæ¢¯åº¦ä¼šåœ¨åŠ æ³•æ—¶åŒæ—¶åˆ†é…ç»™ä¸¤æ¡è·¯å¾„ï¼Œä»è€Œä¿è¯æ¢¯åº¦å¯ä»¥æ— éšœç¢åœ°ä»è¾“å‡ºå›åˆ°è¾“å…¥ã€‚

---

## 3. åœ¨ Transformer Block ä¸­çš„åº”ç”¨

ä¸€ä¸ªæ ‡å‡†çš„ Block ä¸­ä¼šæœ‰ä¸¤ä¸ªæ®‹å·®åŠ æ³•ï¼š

1. **Self-Attention ä¹‹ååŠ å›è¾“å…¥**ï¼š

   ```
   x = x + self_attention(x)
   ```
2. **Feed Forward ä¹‹ååŠ å›è¾“å…¥**ï¼š

   ```
   x = x + feed_forward(x)
   ```

è¿™æ ·æ¯ä¸ªå­æ¨¡å—éƒ½æ˜¯ â€œåˆ†å‰ â†’ å¤„ç† â†’ åŠ å›â€ çš„ç»“æ„ã€‚

---

## 4. æŠ•å½±å±‚ï¼ˆProjectionï¼‰

* å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰ä¼šæŠŠå¤šä¸ª head çš„è¾“å‡ºåœ¨ channel ç»´åº¦æ‹¼æ¥èµ·æ¥ï¼ˆ`torch.cat`ï¼‰ï¼Œæ‹¼æ¥åçš„ç»´åº¦æ˜¯ `num_heads * head_size`ã€‚
* ä¸ºäº†ä¿è¯å’Œ residual path çš„ç»´åº¦ä¸€è‡´ï¼Œéœ€è¦å†ç”¨ä¸€ä¸ª `nn.Linear` æŠ•å½±å›åŸæ¥çš„ `n_embd` å¤§å°ï¼Œè¿™ä¸ªå°±æ˜¯ projection layerã€‚

---

## 5. Feed Forward çš„ç»´åº¦å˜åŒ–

* æŒ‰ Transformer è®ºæ–‡çš„å»ºè®®ï¼ŒFeed Forward å†…éƒ¨çš„éšè—å±‚ç»´åº¦æ˜¯ `n_embd * 4`ï¼ˆè®¡ç®—æ›´å¤šã€æ›´å……åˆ†ï¼‰ï¼Œç„¶åå†é™å› `n_embd`ã€‚
* æ‰€ä»¥ï¼š

  ```
  Linear(n_embd, 4*n_embd) â†’ ReLU â†’ Linear(4*n_embd, n_embd)
  ```

---

## 6. ä¼˜åŒ–æ•ˆæœ

* æœ‰äº†æ®‹å·®è¿æ¥ï¼Œæ·±å±‚ Transformer è®­ç»ƒä¼šæ›´ç¨³å®šã€‚
* åœ¨è¿™æ®µæè¿°ä¸­ï¼ŒåŠ äº† Residual Connection åéªŒè¯é›† loss ä» 2.28 é™åˆ° 2.08ï¼Œè¯´æ˜æ¨¡å‹æ›´å®¹æ˜“æ”¶æ•›ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼  **Transformer Block çš„æ•°æ®æµå›¾**ï¼ŒæŠŠ **æ®‹å·®è¿æ¥ï¼ˆResidualï¼‰** åœ¨ Self-Attention å’Œ Feed Forward ä¸­çš„åˆ†å‰ & å›è·¯ç”¨ç®­å¤´æ ‡å‡ºæ¥ï¼Œè¿™æ ·æ¯”æ–‡å­—ç›´è§‚å¾—å¤šã€‚
è¿™æ ·ä½ ä¸€çœ¼å°±èƒ½çœ‹å‡ºæ¢¯åº¦ä¸ºä»€ä¹ˆèƒ½ç•…é€šæ— é˜»åœ°å›åˆ°è¾“å…¥ã€‚

ä½ å…³å¿ƒçš„æ˜¯è¿™ä¸€è¡Œï¼š`self.proj(out)` åˆ°åº•åœ¨åšä»€ä¹ˆã€ä¸ºä»€ä¹ˆéœ€è¦å®ƒã€å½¢çŠ¶æ€ä¹ˆå¯¹ã€å°‘äº†ä¼šæ€æ ·ã€‚ä¸‹é¢æŠŠå®ƒæ°å¼€è®²æ¸…æ¥šã€‚

# å®ƒåœ¨åšä»€ä¹ˆ

`self.proj` æ˜¯ä¸€ä¸ª `nn.Linear(n_embd, n_embd)`ã€‚å½“ä½ å…ˆæŠŠæ‰€æœ‰ head çš„è¾“å‡ºåœ¨æœ€åä¸€ç»´æ‹¼èµ·æ¥ï¼š

```
out = torch.cat([h(x) for h in self.heads], dim=-1)   # å½¢çŠ¶: (B, T, num_heads * head_size)
```

åœ¨å¸¸è§è®¾ç½®é‡Œ `num_heads * head_size == n_embd`ï¼Œæ‰€ä»¥æ­¤æ—¶ `out` çš„å½¢çŠ¶æ˜¯ `(B, T, n_embd)`ã€‚æ¥ç€

```
out = self.proj(out)   # é€ token çš„çº¿æ€§å˜æ¢
```

è¿™ä¸€æ­¥ä¼šå¯¹æœ€åä¸€ç»´åšä»¿å°„å˜æ¢ï¼ˆçŸ©é˜µä¹˜æ³• + åç½®ï¼‰ï¼Œç­‰ä»·äºå¯¹æ¯ä¸ªä½ç½® `(b, t)` çš„å‘é‡æ‰§è¡Œï¼š

```
y[b,t] = out[b,t] @ W^T + b
```

å…¶ä¸­ `W` çš„å½¢çŠ¶æ˜¯ `(n_embd, n_embd)`ï¼ˆPyTorch å†…éƒ¨å­˜ `(out_features, in_features)`ï¼‰ï¼Œ`b` æ˜¯åç½® `(n_embd,)`ã€‚æœ€ç»ˆå½¢çŠ¶ä»æ˜¯ `(B, T, n_embd)`ã€‚

# ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼ˆä¸‰ä¸ªå…³é”®åŸå› ï¼‰

1. æ®‹å·®ç›¸åŠ éœ€è¦ç»´åº¦å¯¹é½
   Transformer çš„æ ‡å‡†æ®‹å·®å†™æ³•æ˜¯

```
x = x + MHA(x)         # è¿™é‡Œ MHA è¾“å‡ºå¿…é¡»æ˜¯ n_embd
x = x + FFN(x)
```

å³ä½¿æ‹¼æ¥åå·²ç»æ˜¯ `n_embd`ï¼Œä»ä¿ç•™è¿™ä¸ªè¾“å‡ºæŠ•å½±ï¼ˆè®ºæ–‡é‡Œçš„ W\_oï¼‰ï¼Œä¿è¯ç»´åº¦æ’ç­‰ä¸”å¯å­¦ä¹ ã€‚

2. è·¨ head çš„ä¿¡æ¯â€œå†æ··åˆâ€
   æ‹¼æ¥åªæ˜¯æŠŠå„ä¸ª head çš„ç‰¹å¾**å¹¶æ’æ‘†åœ¨ä¸€èµ·**ï¼›`self.proj` è®©æ¨¡å‹èƒ½å­¦åˆ°â€œè¯¥å¦‚ä½•çº¿æ€§ç»„åˆè¿™äº› head çš„è¾“å‡ºâ€ï¼Œå³è®©ä¸åŒ head ä¹‹é—´å‘ç”Ÿ**é€šé“é—´äº¤äº’**ã€‚æ²¡æœ‰è¿™ä¸ªæŠ•å½±ï¼Œhead ä¹‹é—´å°±åƒå½¼æ­¤ç‹¬ç«‹çš„åˆ†ç»„ç‰¹å¾ï¼Œè¡¨è¾¾åŠ›ä¼šæ‰“æŠ˜ã€‚

3. ä½œä¸ºå®¹é‡ä¸ç¨³å®šæ€§çš„â€œé—¨â€
   è¾“å‡ºæŠ•å½±ç›¸å½“äºç»™å¤šå¤´æ³¨æ„åŠ›åé¢å†åŠ ä¸€å±‚å¯å­¦ä¹ çš„çº¿æ€§å±‚ï¼Œæ—¢å¢åŠ å®¹é‡ï¼Œä¹Ÿèƒ½é…åˆ dropout/æ®‹å·®/å½’ä¸€åŒ–ç¨³å®šè®­ç»ƒï¼ˆè®ºæ–‡é‡Œå¸¸åœ¨è¿™ä¸€æ­¥åæ¥ dropoutï¼‰ã€‚

# å½¢çŠ¶ä¸å¹¿æ’­æ€ä¹ˆå¯¹

* è¾“å…¥ `x`: `(B, T, n_embd)`
* æ¯ä¸ª head è¾“å‡º: `(B, T, head_size)`
* æ‹¼æ¥å: `(B, T, num_heads * head_size)`ï¼Œé€šå¸¸ç­‰äº `(B, T, n_embd)`
* `self.proj`: `Linear(n_embd â†’ n_embd)`ï¼Œé€ token ä½œç”¨åœ¨æœ€åä¸€ç»´ï¼Œå› æ­¤æ‰¹ç»´/æ—¶é—´ç»´ä¸å˜

# å¦‚æœæŠŠå®ƒå»æ‰ä¼šæ€æ ·

* ç»´åº¦ä¸Šï¼šè‹¥ `num_heads * head_size == n_embd`ï¼Œå»æ‰ä¹Ÿèƒ½åšæ®‹å·®ç›¸åŠ ï¼Œä¸ä¼šæŠ¥é”™ã€‚
* æ€§èƒ½ä¸Šï¼šé€šå¸¸ä¼šä¸‹é™ã€‚å› ä¸ºç¼ºå°‘è·¨ head çš„çº¿æ€§æ··åˆï¼Œæ¨¡å‹åªèƒ½â€œæŠŠæ¯ä¸ª head å½“ç‹¬ç«‹é€šé“ç”¨â€ï¼Œè¡¨è¾¾ä¸å¦‚å¸¦ `W_o` çµæ´»ï¼Œè¿™åœ¨å®è·µé‡Œå‡ ä¹æ€»æ˜¯å˜å·®ã€‚

# å‚æ•°é‡ä¸åˆå§‹åŒ–

* å‚æ•°é‡çº¦ä¸º `n_embd * n_embd + n_embd`ï¼ˆå«åç½®ï¼‰ã€‚
* `nn.Linear` é»˜è®¤ç”¨ Kaiming/Uniform é£æ ¼åˆå§‹åŒ–ï¼Œè®­ç»ƒæ—¶ç”±ä¼˜åŒ–å™¨æ›´æ–°ã€‚è¯¥å±‚æƒé‡åœ¨æ‰€æœ‰ batch ä¸æ—¶é—´æ­¥ä¸Šå…±äº«ï¼Œé€ token åº”ç”¨ã€‚

# å’Œè®ºæ–‡çš„ä¸€è‡´æ€§

è¿™ä¸€æ­¥å°±æ˜¯ Attention è®ºæ–‡ä¸­çš„è¾“å‡ºæŠ•å½±çŸ©é˜µ **W\_o**ï¼š
MultiHead(Q,K,V) = Concat(headâ‚,â€¦,head\_h) Â· W\_o

# å°ç»“

`self.proj(out)` ä¸æ˜¯â€œå‡‘ç»´åº¦â€çš„æ‘†è®¾ï¼Œè€Œæ˜¯å¤šå¤´æ³¨æ„åŠ›åçš„å¿…å¤‡è¾“å‡ºæŠ•å½±ï¼šè®©å„ä¸ª head çš„ä¿¡æ¯è¢«å†æ¬¡æ··åˆã€è¾“å‡ºç»´åº¦å¯¹é½ä»¥ä¾¿åšæ®‹å·®ã€å¹¶æå‡æ¨¡å‹å®¹é‡ä¸è®­ç»ƒç¨³å®šæ€§ã€‚


# layernorm (and its relationship to our previous batchnorm)

that is very helpful for optimizing very deep neural networks is right here so we have this addition now that's the
residual part but this Norm is referring to something called layer Norm so layer Norm is implemented in pytorch it's a
paper that came out a while back here um and layer Norm is very very similar
to bash Norm so remember back to our make more series part three we implemented bash
normalization and uh bash normalization basically just made sure that um Across
The Bash dimension any individual neuron had unit uh Gan um distribution so it
was zero mean and unit standard deviation one standard deviation output
so what I did here is I'm copy pasting the bashor 1D that we developed in our make more series and see here we can
initialize for example this module and we can have a batch of 32 100 dimensional vectors feeding through the
bachor layer so what this does is it guarantees that when we look at just the
zeroth column it's a zero mean one standard deviation so it's normalizing
every single column of this uh input now the rows are not uh going to be
normalized by default because we're just normalizing columns so let's now Implement layer Norm uh it's very
complicated look we come here we change this from zero to one so we don't
normalize The Columns we normalize the rows and now we've implemented layer
Norm so now the columns are not going to be normalized um but the rows are going to
be normalized for every individual example it's 100 dimensional Vector is normalized uh in this way and because
our computation Now does not span across examples we can delete all of this
buffers stuff uh because uh we can always apply this operation and don't
need to maintain any running buffers so we don't need the buffers uh we
don't There's no distinction between training and test time uh and we don't need these running
buffers we do keep gamma and beta we don't need the momentum we don't care if it's training or not and this is now a
layer norm and it normalizes the rows instead of the columns and this here is
identical to basically this here so let's now Implement layer Norm in our
Transformer before I incorporate the layer Norm I just wanted to note that as I said very few details about the
Transformer have changed in the last 5 years but this is actually something that slightly departs from the original paper you see that the ADD and Norm is
applied after the transformation but um in now it is a bit
more uh basically common to apply the layer Norm before the transformation so there's a reshuffling of the layer Norms
uh so this is called the prorm formulation and that's the one that we're going to implement as well so select deviation from the original paper
basically we need two layer Norms layer Norm one is uh NN do layer norm and we
tell it how many um what is the embedding Dimension and we need the second layer norm and then here the
layer Norms are applied immediately on X so self. layer Norm one applied on X and
self. layer Norm two applied on X before it goes into self attention and feed
forward and uh the size of the layer Norm here is an ed so 32 so when the
layer Norm is normalizing our features it is uh the normalization here uh
happens the mean and the variance are taken over 32 numbers so the batch and the time act as batch Dimensions both of
them so this is kind of like a per token um transformation that just normalizes
the features and makes them a unit mean uh unit Gan at
initialization but of course because these layer Norms inside it have these gamma and beta training
parameters uh the layer Norm will U eventually create outputs that might not
be unit gion but the optimization will determine that so for now this is the uh
this is incorporating the layer norms and let's train them on okay so I let it run and we see that we get down to 2.06
which is better than the previous 2.08 so a slight Improvement by adding the layer norms and I'd expect that they
help uh even more if we had bigger and deeper Network one more thing I forgot to add is that there should be a layer
Norm here also typically as at the end of the Transformer and right before the
final uh linear layer that decodes into vocabulary so I added that as well so at
this stage we actually have a pretty complete uh Transformer according to the original paper and it's a decoder only
Transformer I'll I'll talk about that in a second uh but at this stage uh the major pieces are in place so we can try
to scale this up and see how well we can push this number now in order to scale out the model I had to perform some

è¿™æ®µå†…å®¹è®²çš„æ˜¯ Transformer ä¸­çš„å¦ä¸€ä¸ªæ ¸å¿ƒæŠ€å·§ â€”â€” **LayerNormï¼ˆå±‚å½’ä¸€åŒ–ï¼‰**ï¼Œå®ƒçš„ä½œç”¨æ˜¯è®©æ·±å±‚ç½‘ç»œæ›´å®¹æ˜“è®­ç»ƒï¼Œå°¤å…¶æ˜¯åœ¨åŠ äº†æ®‹å·®è¿æ¥ä¹‹åã€‚ä¸‹é¢æˆ‘ç”¨é€šä¿—è¯­è¨€è§£é‡Šè¿™ä¸ªå†…å®¹ã€‚

---

## ğŸ§  ä¸€ã€ä»€ä¹ˆæ˜¯ LayerNormï¼Ÿ

**LayerNormï¼ˆå±‚å½’ä¸€åŒ–ï¼‰** æ˜¯ä¸€ç§å¯¹æ¯ä¸€ä¸ªæ ·æœ¬ï¼ˆtokenï¼‰**å•ç‹¬åšå½’ä¸€åŒ–**çš„æ“ä½œã€‚

> å®ƒå’Œ BatchNorm ç±»ä¼¼ï¼ŒåŒºåˆ«æ˜¯ï¼š
>
> * **BatchNorm** æ˜¯å¯¹â€œåŒä¸€ä¸ªä½ç½®çš„ç¥ç»å…ƒâ€åœ¨ä¸€ä¸ª batch ä¸­è·¨æ ·æœ¬åšå½’ä¸€åŒ–ã€‚
> * **LayerNorm** æ˜¯å¯¹â€œåŒä¸€ä¸ªæ ·æœ¬ï¼ˆtokenï¼‰â€çš„æ‰€æœ‰ç‰¹å¾åšå½’ä¸€åŒ–ã€‚

---

### âœ… ä¸¾ä¸ªä¾‹å­ï¼š

å‡è®¾ä¸€ä¸ª token çš„å‘é‡æ˜¯è¿™æ ·ï¼ˆæœ‰ 4 ä¸ªç»´åº¦ï¼‰ï¼š

```python
x = [2.0, 4.0, 6.0, 8.0]
```

**LayerNorm** ä¼šåšä»¥ä¸‹å‡ æ­¥ï¼š

1. è®¡ç®—å‡å€¼ mean = (2 + 4 + 6 + 8) / 4 = 5.0
2. è®¡ç®—æ ‡å‡†å·® std = âˆš((å·®å¹³æ–¹çš„å¹³å‡)) = 2.236
3. å¯¹æ¯ä¸ªå€¼åšå½’ä¸€åŒ–ï¼š

   ```
   x_norm = [(2-5)/2.236, (4-5)/2.236, (6-5)/2.236, (8-5)/2.236]
          â‰ˆ [-1.34, -0.45, 0.45, 1.34]
   ```

è¿™æ ·ï¼Œè¾“å‡ºçš„æ¯ä¸ª token å‘é‡çš„å‡å€¼æ˜¯ 0ï¼Œæ ‡å‡†å·®æ˜¯ 1ã€‚

> **å®ƒæ˜¯å¯¹æ¯ä¸ª token ç‹¬ç«‹å¤„ç†çš„ï¼ä¸å— batch size çš„å½±å“ã€‚**

---

## ğŸ—ï¸ äºŒã€ä¸ºä»€ä¹ˆ LayerNorm å¯¹ Transformer ç‰¹åˆ«é‡è¦ï¼Ÿ

1. **ç»“åˆæ®‹å·®è¿æ¥ä½¿ç”¨**ï¼š

   * Transformer ä¸­ä½¿ç”¨äº†æ®‹å·®è¿æ¥ï¼š`x + f(x)`ï¼Œé˜²æ­¢ç½‘ç»œå¤ªæ·±å¯¼è‡´è®­ç»ƒå›°éš¾ã€‚
   * ä½†æ®‹å·®ä¼šå¸¦æ¥â€œæ•°å€¼ä¸ç¨³å®šâ€ï¼šå åŠ çš„ç»“æœå¯èƒ½è¶Šæ¥è¶Šå¤§ã€‚
   * LayerNorm åœ¨è¿™é‡Œèµ·åˆ°äº†â€œç¨³å®šè¾“å‡ºâ€çš„ä½œç”¨ï¼Œè®©è®­ç»ƒæ›´å®¹æ˜“æ”¶æ•›ã€‚

2. **é€‚åˆåºåˆ—ä»»åŠ¡**ï¼š

   * BatchNorm åœ¨ NLP ä¸­ä¸å¤ªå¥½ç”¨ï¼Œå› ä¸ºå¥å­é•¿åº¦ä¸åŒï¼Œbatch size å°ã€‚
   * LayerNorm ä¸ä¾èµ– batch å¤§å°æˆ–ä½ç½®ï¼Œæ˜¯å¯¹æ¯ä¸ª token å‘é‡å•ç‹¬å¤„ç†ï¼Œéå¸¸é€‚åˆ Transformerã€‚

---

## ğŸ§ª ä¸‰ã€åœ¨ä»£ç ä¸­å¦‚ä½•ç”¨ LayerNormï¼Ÿ

```python
self.ln1 = nn.LayerNorm(n_embd)  # å¯¹æ¯ä¸ª token çš„ n_embd ç»´å‘é‡åšå½’ä¸€åŒ–
self.ln2 = nn.LayerNorm(n_embd)
```

ç„¶ååœ¨å‰å‘ä¼ æ’­ä¸­è¿™æ ·ä½¿ç”¨ï¼š

```python
x = x + self.attn(self.ln1(x))     # æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
x = x + self.ffwd(self.ln2(x))     # å†æ¥ä¸€æ¬¡
```

è¿™ä¸ªæ–¹å¼å«åš **Pre-LNï¼ˆå‰å½’ä¸€åŒ–ï¼‰**ï¼Œç›¸æ¯”è®ºæ–‡ä¸­çš„ Post-LN æ›´ç¨³å®šã€æ›´å¸¸ç”¨ã€‚

---

## ğŸ§© å››ã€LayerNorm è¿˜ä¿ç•™å¯å­¦ä¹ å‚æ•°

è™½ç„¶å®ƒæŠŠè¾“å‡ºå˜æˆäº†â€œå‡å€¼ä¸º 0ã€æ–¹å·®ä¸º 1â€çš„å½¢å¼ï¼Œä½†å®ƒè¿˜ä¿ç•™äº†ä¸¤ä¸ªå‚æ•°ï¼š

```python
output = Î³ * x_norm + Î²
```

* Î³ï¼ˆgammaï¼‰ï¼šæ¯ä¸ªç‰¹å¾çš„ç¼©æ”¾ç³»æ•°
* Î²ï¼ˆbetaï¼‰ï¼šæ¯ä¸ªç‰¹å¾çš„åç§»é‡

è¿™äº›å‚æ•°æ˜¯å¯ä»¥å­¦ä¹ çš„ï¼Œæœ€ç»ˆç½‘ç»œä¼šå­¦ä¼šæ˜¯å¦éœ€è¦â€œæ‹‰ä¼¸â€æˆ–â€œå¹³ç§»â€æ¯ä¸ªç»´åº¦ã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯ï¼š

> **LayerNorm æ˜¯å¯¹æ¯ä¸ª token çš„å‘é‡åšæ ‡å‡†åŒ–å¤„ç†ï¼Œä½¿è®­ç»ƒæ›´åŠ ç¨³å®šã€æ”¶æ•›æ›´å¿«ï¼Œæ˜¯ Transformer æˆåŠŸçš„æ ¸å¿ƒæŠ€å·§ä¹‹ä¸€ã€‚**

éœ€è¦æˆ‘é…å›¾è¯´æ˜â€œLayerNorm ä¸ BatchNorm çš„åŒºåˆ«â€ï¼Œæˆ–è€…â€œæ®‹å·® + LayerNorm çš„ç»“æ„å›¾â€å—ï¼Ÿ


æˆ‘ç»™ä½ åˆ†æˆå‡ ä¸ªéƒ¨åˆ†è§£é‡Šä¸€ä¸‹ LayerNorm ä»¥åŠå®ƒå’Œä¹‹å‰ BatchNorm çš„å…³ç³»ï¼Œè¿˜æœ‰å®ƒåœ¨ Transformer é‡Œçš„ç”¨æ³•å’Œä½œç”¨ã€‚

---

## 1. LayerNorm å’Œ BatchNorm çš„æ ¸å¿ƒåŒºåˆ«

* **BatchNormï¼ˆæ‰¹å½’ä¸€åŒ–ï¼‰**

  * å¯¹**åŒä¸€ç¥ç»å…ƒä½ç½®**åœ¨ä¸€ä¸ª batch å†…æ‰€æœ‰æ ·æœ¬çš„å€¼è¿›è¡Œå½’ä¸€åŒ–ã€‚
  * å½’ä¸€åŒ–ç»´åº¦ï¼š**æ‰¹ç»´åº¦ (B)**ã€‚
  * ç‰¹ç‚¹ï¼šä¾èµ– batch å¤§å°ï¼Œè®­ç»ƒ/æ¨ç†æ—¶éœ€è¦åŒºåˆ†ï¼ˆå› ä¸ºæ¨ç†ç”¨çš„æ˜¯ running mean/varï¼‰ã€‚
  * é€‚åˆ CNNï¼Œå› ä¸ºå·ç§¯ç‰¹å¾åœ¨ batch å†…å¯ä»¥å‡è®¾ç»Ÿè®¡ç‰¹æ€§ç›¸ä¼¼ã€‚

* **LayerNormï¼ˆå±‚å½’ä¸€åŒ–ï¼‰**

  * å¯¹**å•ä¸ªæ ·æœ¬çš„æ‰€æœ‰ç‰¹å¾**è¿›è¡Œå½’ä¸€åŒ–ã€‚
  * å½’ä¸€åŒ–ç»´åº¦ï¼š**ç‰¹å¾ç»´åº¦ (C)**ï¼ˆæœ‰æ—¶ä¹Ÿä¼šåŒ…å«åºåˆ—ç»´ Tï¼Œä½†é€šå¸¸æŒ‰ token ç‹¬ç«‹åšï¼‰ã€‚
  * ç‰¹ç‚¹ï¼šå’Œ batch å¤§å°æ— å…³ï¼Œè®­ç»ƒ/æ¨ç†æ—¶æ²¡æœ‰åŒºåˆ«ã€‚
  * é€‚åˆ RNNã€Transformerï¼Œå› ä¸ºåºåˆ—ä¸­æ¯ä¸ª token çš„ç‰¹å¾åˆ†å¸ƒå¯èƒ½å·®å¼‚å¤§ï¼Œä¸é€‚åˆè·¨æ ·æœ¬å½’ä¸€åŒ–ã€‚

---

## 2. å½’ä¸€åŒ–çš„è®¡ç®—è¿‡ç¨‹

å‡è®¾è¾“å…¥ `x` å½¢çŠ¶æ˜¯ `(B, T, C)`ï¼ŒLayerNorm å¯¹æ¯ä¸ª `(B, T)` ä½ç½®çš„å‘é‡ `x[b,t,:]` è¿›è¡Œï¼š

```
Î¼ = mean(x[b,t,:])
Ïƒ = std(x[b,t,:])
x_norm = (x[b,t,:] - Î¼) / Ïƒ
y = Î³ * x_norm + Î²
```

å…¶ä¸­ Î³ å’Œ Î² æ˜¯å¯è®­ç»ƒå‚æ•°ï¼ˆç¼©æ”¾å’Œå¹³ç§»ï¼‰ã€‚

---

## 3. ä¸ºä»€ä¹ˆ Transformer è¦ç”¨ LayerNorm

* **é€‚é…åºåˆ—å¤„ç†**ï¼šTransformer æ¯ä¸ª token çš„ embedding åˆ†å¸ƒå¯èƒ½ä¸åŒï¼ŒBatchNorm è·¨ batch ç»Ÿè®¡ä¼šç ´åè¿™ç§ç‹¬ç«‹æ€§ã€‚
* **ç¨³å®šæ¢¯åº¦**ï¼šæ·±å±‚ç½‘ç»œï¼ˆå‡ åä¸ª blockï¼‰ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼ŒLayerNorm èƒ½ç¨³å®šå„å±‚æ¿€æ´»çš„å°ºåº¦ï¼Œè®©è®­ç»ƒæ›´ç¨³å®šã€‚
* **ä¸æ®‹å·®è¿æ¥é…åˆ**ï¼šTransformer é‡Œæ¯ä¸ªå­å±‚éƒ½æœ‰æ®‹å·®è¿æ¥ï¼ŒLayerNorm åœ¨æ®‹å·®å‰ï¼ˆPre-Normï¼‰æˆ–åï¼ˆPost-Normï¼‰èƒ½ä¿è¯ä¿¡å·å°ºåº¦å¯æ§ï¼Œæ¢¯åº¦æµåŠ¨æ›´é¡ºç•…ã€‚

---

## 4. Pre-Norm vs Post-Norm

* **åŸè®ºæ–‡ï¼ˆPost-Normï¼‰**ï¼š`x = x + SubLayer(LayerNorm(x))`ï¼Œå½’ä¸€åŒ–åœ¨å­å±‚ä¹‹åã€‚
* **ç°åœ¨æ›´å¸¸ç”¨ï¼ˆPre-Normï¼‰**ï¼š`x = x + SubLayer(LayerNorm(x))`ï¼Œå½’ä¸€åŒ–åœ¨å­å±‚ä¹‹å‰ï¼ˆæ³¨æ„åªæ˜¯é¡ºåºå˜äº†ï¼Œæ•°å­¦ç»“æ„ä¸ä¸€æ ·ï¼‰ã€‚

  * Pre-Norm æ›´å®¹æ˜“è®­ç»ƒæ·±å±‚æ¨¡å‹ï¼Œå› ä¸ºæ¢¯åº¦èƒ½æ›´ç›´æ¥ä¼ é€’åˆ°è¾“å…¥å±‚ã€‚
  * ä½ ç»™çš„å®ç°é‡Œå°±æ˜¯ Pre-Normï¼šå…ˆå¯¹ `x` åš LayerNormï¼Œå†é€åˆ° Self-Attention æˆ– FeedForwardã€‚

---

## 5. åœ¨ Transformer é‡Œçš„ä½ç½®

åœ¨ä¸€ä¸ªå…¸å‹ Decoder Blockï¼ˆç®€åŒ–ï¼‰ä¸­ï¼š

```
x = x + MHA(LayerNorm1(x))
x = x + FFN(LayerNorm2(x))
```

å¹¶ä¸”åœ¨æ•´ä¸ª Transformer æœ€åçš„è¾“å‡ºå±‚å‰ï¼Œé€šå¸¸ä¹Ÿä¼šåŠ ä¸€ä¸ª `LayerNorm`ï¼Œä¿è¯è¾“å‡ºåˆ†å¸ƒç¨³å®šåå†é€è¿› `Linear` æ˜ å°„åˆ°è¯è¡¨ã€‚

---

## 6. æ€»ç»“

* LayerNorm æ˜¯â€œæŒ‰ token ç‹¬ç«‹â€çš„ç‰¹å¾å½’ä¸€åŒ–ï¼Œä¸ä¾èµ– batch ç»Ÿè®¡ï¼Œé€‚åˆ Transformerã€‚
* å®ƒå’Œæ®‹å·®è¿æ¥é…åˆï¼Œèƒ½ä¿æŒä¿¡å·å°ºåº¦ç¨³å®šï¼Œè®©æ¢¯åº¦é«˜æ•ˆå›ä¼ ã€‚
* Pre-Norm æ˜¯ç°åœ¨çš„ä¸»æµåšæ³•ï¼Œå°¤å…¶é€‚åˆæ›´æ·±çš„ Transformer ç»“æ„ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼  **Transformer ä¸­ LayerNorm æ•°æ®æµçš„ç¤ºæ„å›¾**ï¼Œè®©ä½ ç›´è§‚çœ‹åˆ°å®ƒåœ¨æ®‹å·®å’Œå­å±‚ä¹‹é—´çš„ä½ç½®ã€‚è¿™æ ·ä½ ä¼šæ›´å®¹æ˜“è®°ä½ Pre-Norm å’Œ Post-Norm çš„åŒºåˆ«ã€‚ä½ è¦æˆ‘ç”»å—ï¼Ÿ


# scaling up the model! creating a few variables. adding dropout

cosmetic changes here to make it nicer so I introduced this variable called n layer which just specifies how many
layers of the blocks we're going to have I created a bunch of blocks and we have a new variable number of heads as well I
pulled out the layer Norm here and uh so this is identical now one thing that I did briefly change is I added a Dropout
so Dropout is something that you can add right before the residual connection back right before the connection back
into the residual pathway so we can drop out that as l layer here we can drop out
uh here at the end of the multi-headed exension as well and we can also drop out here uh when we calculate the um
basically affinities and after the softmax we can drop out some of those so we can randomly prevent some of the
nodes from communicating and so Dropout uh comes from this paper from 2014 or so and
basically it takes your neural nut and it randomly every forward backward pass shuts off some subset of
uh neurons so randomly drops them to zero and trains without them and what
this does effectively is because the mask of what's being dropped out is changed every single forward backward
pass it ends up kind of uh training an ensemble of sub networks and then at
test time everything is fully enabled and kind of all of those sub networks are merged into a single Ensemble if you
can if you want to think about it that way so I would read the paper to get the full detail for now we're just going to
stay on the level of this is a regularization technique and I added it because I'm about to scale up the model
quite a bit and I was concerned about overfitting so now when we scroll up to the top uh we'll see that I changed a
number of hyper parameters here about our neural nut so I made the batch size be much larger now it's 64 I changed the
block size to be 256 so previously it was just eight eight characters of context now it is 256 characters of
context to predict the 257th uh I brought down the learning rate a
little bit because the neural net is now much bigger so I brought down the learning rate the embedding Dimension is
now 384 and there are six heads so 384 divide 6 means that every head is 64
dimensional as it as a standard and then there's going to be six layers of that
and the Dropout will be at 02 so every forward backward pass 20% of all of these um intermediate calculations are
disabled and dropped to zero and then I already trained this and I ran it so uh drum roll how well does it
perform so let me just scroll up here we get a validation loss of
1.48 which is actually quite a bit of an improvement on what we had before which I think was 2.07 so it went from 2.07
all the way down to 1.48 just by scaling up this neural nut with the code that we have and this of course ran for a lot
longer this maybe trained for I want to say about 15 minutes on my a100 GPU so
that's a pretty a GPU and if you don't have a GPU you're not going to be able to reproduce this uh on a CPU this would
be um I would not run this on a CPU or MacBook or something like that you'll have to Brak down the number of uh
layers and the embedding Dimension and so on uh but in about 15 minutes we can get this kind of a result and um I'm
printing some of the Shakespeare here but what I did also is I printed 10,000 characters so a lot more and I wrote
them to a file and so here we see some of the outputs
so it's a lot more recognizable as the input text file so the input text file just for reference looked like this so
there's always like someone speaking in this manner and uh our predictions now
take on that form except of course they're they're nonsensical when you actually read them
so it is every crimp tap be a house oh those
prepation we give heed um you know
Oho sent me you mighty Lord anyway so you can read through this
um it's nonsensical of course but this is just a Transformer trained on a character level for 1 million characters
that come from Shakespeare so there's sort of like blabbers on in Shakespeare like manner but it doesn't of course
make sense at this scale uh but I think I think still a pretty good demonstration of what's
possible so now I think uh that kind of like concludes
the programming section of this video we basically kind of uh did a pretty good job and um of implementing this
Transformer uh but the picture doesn't exactly match up to what we've done so what's going on with all these digital
Parts here so let me finish explaining this architecture and why it looks so funky basically what's happening here is

è¿™æ®µå†…å®¹è®²è§£äº†å¦‚ä½•å¯¹ Transformer æ¨¡å‹è¿›è¡Œ**æ‰©å±•**ï¼Œä»¥åŠåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚ä½•åŠ å…¥ **Dropout** å’Œä¸€äº› **è¶…å‚æ•°è°ƒæ•´**ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆå¹¶æé«˜æ€§èƒ½ã€‚ä¸‹é¢æˆ‘å°†é€šä¿—æ˜“æ‡‚åœ°è®²è§£ã€‚

---

## ğŸ§‘â€ğŸ’» ä¸€ã€å¢åŠ  Dropout

### âœ… ä»€ä¹ˆæ˜¯ Dropoutï¼Ÿ

**Dropout** æ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä½œç”¨æ˜¯**åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºä¸¢å¼ƒä¸€äº›ç¥ç»å…ƒï¼ˆæˆ–è¿æ¥ï¼‰**ã€‚
è¿™å¯ä»¥å¸®åŠ©æ¨¡å‹å‡å°‘å¯¹æŸäº›ç‰¹å®šç¥ç»å…ƒçš„ä¾èµ–ï¼Œä»è€Œé¿å…è¿‡æ‹Ÿåˆã€‚

* æ¯æ¬¡è®­ç»ƒæ—¶ï¼Œç½‘ç»œä¸­ä¸€äº›ç¥ç»å…ƒï¼ˆæˆ–è¿æ¥ï¼‰ä¼šè¢«**éšæœºâ€œå…³æ‰â€**ï¼ˆå³è¾“å‡ºè®¾ä¸ºé›¶ï¼‰ã€‚
* è¿™æ ·å¯ä»¥æ¨¡æ‹Ÿå¤šä¸ªå­ç½‘ç»œçš„è®­ç»ƒæ•ˆæœï¼Œæœ€ç»ˆæµ‹è¯•æ—¶ç½‘ç»œæ˜¯å®Œå…¨å¯ç”¨çš„ã€‚

### âœ… ä¸ºä»€ä¹ˆåœ¨ Transformer ä¸­ä½¿ç”¨ Dropoutï¼Ÿ

åœ¨æ‰©å±•æ¨¡å‹æ—¶ï¼Œ**æ¨¡å‹ä¼šå˜å¾—æ›´å¤§ã€æ›´å¤æ‚**ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆã€‚
é€šè¿‡ **Dropout**ï¼Œæ¨¡å‹åœ¨æ¯æ¬¡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­æ—¶éƒ½ä¼šä½¿ç”¨ä¸åŒçš„ç¥ç»å…ƒç»„åˆï¼Œé˜²æ­¢æ¨¡å‹â€œè®°ä½â€è®­ç»ƒæ•°æ®çš„å™ªå£°ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚

---

## ğŸ› ï¸ äºŒã€æ‰©å±•æ¨¡å‹çš„è¶…å‚æ•°è°ƒæ•´

ä¸ºäº†è®©æ¨¡å‹å˜å¾—æ›´å¼ºå¤§ï¼Œä½œè€…è°ƒæ•´äº†å‡ ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼š

### 1. **å¢å¤§æ‰¹å¤„ç†å¤§å°ï¼ˆBatch Sizeï¼‰**

* ä¹‹å‰çš„æ‰¹å¤„ç†å¤§å°æ˜¯ 4ï¼Œç°åœ¨å¢å¤§åˆ° **64**ï¼Œæ„å‘³ç€æ¯æ¬¡è®­ç»ƒä¼šç”¨æ›´å¤šçš„æ ·æœ¬ã€‚

### 2. **å¢å¤§ä¸Šä¸‹æ–‡å—å¤§å°ï¼ˆBlock Sizeï¼‰**

* ä¹‹å‰æ˜¯æ¯æ¬¡å¤„ç† **8** ä¸ªå­—ç¬¦çš„ä¸Šä¸‹æ–‡ï¼Œç°åœ¨æ‰©å±•åˆ° **256** ä¸ªå­—ç¬¦ï¼Œè¿™æ ·æ¨¡å‹èƒ½ç†è§£æ›´é•¿çš„ä¸Šä¸‹æ–‡ã€‚

### 3. **å‡å°å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰**

* æ¨¡å‹å˜å¤§åï¼Œå­¦ä¹ ç‡éœ€è¦**è°ƒå°**ï¼Œå¦åˆ™å®¹æ˜“éœ‡è¡ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚

### 4. **å¢åŠ åµŒå…¥ç»´åº¦ï¼ˆEmbedding Dimensionï¼‰**

* åµŒå…¥ç»´åº¦ä» 32 å¢åŠ åˆ° **384**ï¼Œæ¯ä¸ªå¤´çš„å¤§å°ä» 32 é™åˆ° 64ï¼Œè¿™æ ·å¯ä»¥å¤„ç†æ›´å¤šçš„ç‰¹å¾ã€‚

### 5. **å¢åŠ  Transformer å±‚æ•°ï¼ˆNumber of Layersï¼‰**

* ä¹‹å‰åªæœ‰å‡ å±‚ï¼Œç°åœ¨å¢åŠ äº† **6 å±‚**ï¼Œè®©æ¨¡å‹æ›´å¼ºå¤§ã€‚

### 6. **è®¾ç½® Dropout æ¯”ä¾‹**

* è®¾ç½® **0.2 çš„ Dropout**ï¼Œæ„å‘³ç€åœ¨æ¯æ¬¡è®­ç»ƒæ—¶ï¼Œ20% çš„ç¥ç»å…ƒä¼šè¢«éšæœºä¸¢å¼ƒã€‚

---

## ğŸ“ˆ ä¸‰ã€è®­ç»ƒæ•ˆæœ

ç»è¿‡è¿™äº›è°ƒæ•´ï¼Œè®­ç»ƒåçš„ **éªŒè¯æŸå¤±ï¼ˆvalidation lossï¼‰** ä» **2.07** é™åˆ° **1.48**ï¼Œè¡¨æ˜æ¨¡å‹æ€§èƒ½å¤§å¹…æå‡ã€‚
è™½ç„¶è¾“å‡ºæ–‡æœ¬è¿˜æ˜¯ä¸å®Œç¾ï¼Œä½†å·²ç»æ›´åƒèå£«æ¯”äºšçš„é£æ ¼äº†ï¼Œèƒ½å¤Ÿçœ‹åˆ°ä¸€äº›å¯è¯†åˆ«çš„å•è¯å’Œç»“æ„ã€‚

---

## ğŸ­ å››ã€ç”Ÿæˆçš„æ–‡æœ¬

æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬çœ‹èµ·æ¥åƒæ˜¯èå£«æ¯”äºšçš„é£æ ¼ï¼Œä½†å…¶å®æ˜¯**æ— æ„ä¹‰çš„**ã€‚ä¾‹å¦‚ï¼š

> "every crimp tap be a house oh those prepation we give heed..."

è¿™äº›æ–‡æœ¬è™½ç„¶çœ‹ä¼¼æœ‰ç‚¹é“ç†ï¼Œä½†å…¶å®æ˜¯å®Œå…¨æ— æ„ä¹‰çš„ï¼Œæ¯•ç«Ÿæ¨¡å‹è®­ç»ƒçš„åªæ˜¯å­—ç¬¦çº§åˆ«çš„è¯­è¨€æ¨¡å‹ã€‚

---

## ğŸ§© äº”ã€æ€»ç»“

1. **Dropout** å¸®åŠ©æ¨¡å‹é¿å…è¿‡æ‹Ÿåˆï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. å¢å¤§äº†æ‰¹å¤„ç†å¤§å°ã€ä¸Šä¸‹æ–‡å¤§å°ç­‰è¶…å‚æ•°ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥å­¦ä¹ æ›´ä¸°å¯Œçš„ç‰¹å¾ã€‚
3. è®­ç»ƒåçš„ **éªŒè¯æŸå¤±æ˜¾è‘—ä¸‹é™**ï¼Œå°½ç®¡ç”Ÿæˆçš„æ–‡æœ¬ä¸å®Œç¾ï¼Œä½†å·²ç»æ˜¾ç¤ºå‡ºèå£«æ¯”äºšé£æ ¼çš„ç‰¹å¾ã€‚

è¿™äº›è°ƒæ•´ä½¿å¾—æ¨¡å‹æ›´åŠ å¼ºå¤§ï¼Œä¸”èƒ½å¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ã€‚

å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œæˆ–è€…æƒ³æ›´æ·±å…¥åœ°ç†è§£æ¨¡å‹çš„ç»†èŠ‚ï¼Œéšæ—¶å¯ä»¥é—®æˆ‘ï¼

æˆ‘å¸®ä½ æŠŠè¿™ä¸€æ®µæ‹†å¼€è§£é‡Šä¸€ä¸‹ï¼Œè¿™é‡Œä¸»è¦è®²äº† **æ¨¡å‹æ”¾å¤§ (Scaling up)** å’Œ **Dropout æ­£åˆ™åŒ–** çš„ä½œç”¨ï¼Œä»¥åŠå…·ä½“æ€ä¹ˆåšçš„ã€‚

---

## 1. æ”¾å¤§ Transformer æ¨¡å‹çš„ç»“æ„

ä½œè€…åšäº†å‡ ä¸ªè¶…å‚è°ƒæ•´ï¼Œè®©æ¨¡å‹æ›´å¤§ã€æ›´æœ‰èƒ½åŠ›ï¼š

* **n\_layer**ï¼šå®šä¹‰æœ‰å¤šå°‘ä¸ª Transformer blockï¼ˆä»¥å‰å¯èƒ½æ˜¯ 3-4 å±‚ï¼Œç°åœ¨å˜å¤šï¼‰ã€‚
* **n\_head**ï¼šæ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œæ¯”å¦‚ 6 ä¸ªå¤´ã€‚
* **n\_embd**ï¼šembedding ç»´åº¦ï¼Œä¾‹å¦‚ 384ã€‚å› ä¸ºå¤šå¤´æ³¨æ„åŠ›è¦å¹³å‡åˆ†é…ç»´åº¦ï¼Œæ‰€ä»¥æ¯ä¸ªå¤´çš„ç»´åº¦ = `n_embd / n_head` = 384 / 6 = 64ã€‚
* **block\_size**ï¼šä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä» 8 æåˆ° 256ï¼Œè¿™æ ·æ¨¡å‹èƒ½â€œçœ‹åˆ°â€æ›´é•¿çš„å†å²ä¿¡æ¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚
* **batch\_size**ï¼šä»å°æ‰¹é‡æ”¹ä¸º 64ï¼Œæ›´é«˜çš„è®­ç»ƒå¹¶è¡Œåº¦ã€‚
* **å­¦ä¹ ç‡**ï¼šé™ä½ä¸€ç‚¹ï¼Œé˜²æ­¢å¤§æ¨¡å‹æ›´æ–°è¿‡çŒ›å¯¼è‡´ä¸ç¨³å®šã€‚

è¿™äº›è°ƒæ•´è®©æ¨¡å‹å®¹é‡æ›´å¤§ã€ä¸Šä¸‹æ–‡æ›´é•¿ï¼Œç†è®ºä¸Šå¯ä»¥å­¦åˆ°æ›´å¤æ‚çš„æ¨¡å¼ã€‚

---

## 2. Dropout çš„ä½œç”¨å’Œä½ç½®

**Dropout** æ˜¯ 2014 å¹´çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œç”¨æ¥å‡å°‘è¿‡æ‹Ÿåˆï¼š

* è®­ç»ƒæ—¶éšæœºæŠŠéƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºç½®ä¸º 0ï¼ˆæ¯”å¦‚ 20%ï¼‰ï¼Œæ¯æ¬¡å‰å‘/åå‘ä¼ æ’­éƒ½ä¼šé‡æ–°éšæœºé€‰æ‹©ã€‚
* è¿™æ ·ç›¸å½“äºè®­ç»ƒäº†å¾ˆå¤šâ€œå­ç½‘ç»œâ€çš„é›†åˆï¼Œæ¨ç†æ—¶ç”¨å…¨éƒ¨ç¥ç»å…ƒï¼Œç›¸å½“äºâ€œæ¨¡å‹é›†æˆâ€ã€‚
* åœ¨ Transformer é‡Œé€šå¸¸åŠ åœ¨ï¼š

  1. **Multi-Head Attention è¾“å‡ºå›æ®‹å·®å‰**ï¼ˆout projection åï¼‰ã€‚
  2. **FeedForward è¾“å‡ºå›æ®‹å·®å‰**ã€‚
  3. **Softmax åçš„æ³¨æ„åŠ›æƒé‡**ï¼ˆé˜²æ­¢æ³¨æ„åŠ›å›¾è¿‡äºä¾èµ–ç‰¹å®šè¿æ¥ï¼‰ã€‚

åœ¨è¿™é‡Œ dropout=0.2ï¼Œæ„å‘³ç€æ¯æ¬¡è®­ç»ƒæœ‰ 20% çš„æ¿€æ´»å€¼è¢«éšæœºç½®é›¶ã€‚

---

## 3. æ”¾å¤§åæ•ˆæœ

* åŸæ¥çš„éªŒè¯é›† loss å¤§çº¦ **2.07**ã€‚
* æ”¾å¤§æ¨¡å‹ + åŠ å…¥ dropout åï¼Œloss é™åˆ° **1.48**ï¼Œè¯´æ˜æ¨¡å‹ç¡®å®æ›´å¼ºã€‚
* è¾“å‡ºè™½ç„¶ä¾ç„¶æ˜¯â€œèå£«æ¯”äºšé£æ ¼çš„èƒ¡è¨€ä¹±è¯­â€ï¼Œä½†ç»“æ„ä¸Šæ›´æ¥è¿‘åŸå§‹æ–‡æœ¬æ ¼å¼ï¼ˆäººç‰©å¯¹ç™½ã€å°è¯æ ¼å¼ç­‰ï¼‰ã€‚

---

## 4. å…³é”®åŸå› æ€»ç»“

* **æ›´å¤§ä¸Šä¸‹æ–‡ (block\_size)** â†’ èƒ½æ•æ‰é•¿ç¨‹ä¾èµ–ã€‚
* **æ›´å¤šå±‚ (n\_layer) å’Œå¤´æ•° (n\_head)** â†’ å¢å¼ºè¡¨è¾¾èƒ½åŠ›ã€‚
* **æ›´å¤§ embedding ç»´åº¦ (n\_embd)** â†’ æ¯ä¸ª token å‘é‡æ›´ä¸°å¯Œã€‚
* **Dropout** â†’ é˜²æ­¢å¤§æ¨¡å‹è¿‡æ‹Ÿåˆã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼  **æ”¾å¤§å‰å Transformer æ•°æ®æµ + Dropout æ’å…¥ä½ç½®** çš„ç¤ºæ„å›¾ï¼Œè®©ä½ ç›´è§‚çœ‹å‡º Dropout åœ¨æ®‹å·®è·¯å¾„å’Œæ³¨æ„åŠ›é‡Œçš„ä½ç½®ï¼Œè¿™æ ·è®°èµ·æ¥ä¼šå¾ˆæ¸…æ¥šã€‚ä½ è¦æˆ‘ç”»å—ï¼Ÿ



è¿™ä¸€è¡Œä»£ç 

```python
self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])
```

æ˜¯åœ¨ **Transformer æ¨¡å‹åˆå§‹åŒ–é˜¶æ®µ** åˆ›å»º **å¤šå±‚ Transformer Block** çš„åœ°æ–¹ï¼Œæˆ‘å¸®ä½ è¯¦ç»†æ‹†å¼€è§£é‡Šä¸€ä¸‹ï¼š

---

## 1. `[Block(n_embd, n_head=n_head) for _ in range(n_layer)]`

* è¿™æ˜¯ä¸€ä¸ª **åˆ—è¡¨ç”Ÿæˆå¼**ï¼Œä¼šç”Ÿæˆä¸€ä¸ªé•¿åº¦ä¸º `n_layer` çš„åˆ—è¡¨ã€‚
* æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ª **`Block` å®ä¾‹**ï¼Œå‚æ•°æ˜¯ï¼š

  * `n_embd`ï¼šembedding ç»´åº¦ï¼ˆè¾“å…¥/è¾“å‡ºçš„ç‰¹å¾ç»´æ•°ï¼‰ã€‚
  * `n_head`ï¼šå¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°ã€‚
* ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœ `n_layer=6`ï¼Œè¿™ä¸ªåˆ—è¡¨å°±æ˜¯ï¼š

  ```python
  [Block(...), Block(...), Block(...), Block(...), Block(...), Block(...)]
  ```

  ç›¸å½“äºå‡†å¤‡äº† 6 å±‚ Transformer blockã€‚

---

## 2. `*`ï¼ˆæ˜Ÿå·è¿ç®—ç¬¦ï¼‰

* è¿™æ˜¯ **å‚æ•°è§£åŒ…** è¯­æ³•ã€‚
* `[a, b, c]` ç›´æ¥ä¼ ç»™å‡½æ•°ä¼šè¢«å½“æˆä¸€ä¸ªåˆ—è¡¨å‚æ•°ï¼›
* `*[a, b, c]` åˆ™ä¼šè¢«â€œæ‰“æ•£â€æˆ `a, b, c` ä¸‰ä¸ªä½ç½®å‚æ•°ã€‚
* æ‰€ä»¥è¿™é‡Œ `*[Block(...) for _ in range(n_layer)]` çš„ä½œç”¨å°±æ˜¯ï¼š
  æŠŠé‚£ 6 ä¸ª `Block` ä½œä¸ºå•ç‹¬çš„å‚æ•°ä¼ ç»™ `nn.Sequential(...)`ã€‚

---

## 3. `nn.Sequential(...)`

* `nn.Sequential` æ˜¯ PyTorch çš„ä¸€ä¸ªæ¨¡å—å®¹å™¨ï¼Œå¯ä»¥é¡ºåºæ‰§è¡Œä¸€ç³»åˆ—å­æ¨¡å—ã€‚
* åˆ›å»ºçš„ `self.blocks` ä¼šæŒ‰é¡ºåºæ‰§è¡Œæ¯ä¸ª Blockï¼š

  ```
  è¾“å…¥ â†’ Block1 â†’ Block2 â†’ ... â†’ BlockN â†’ è¾“å‡º
  ```
* å› ä¸º Transformer éœ€è¦é‡å¤å †å ç›¸åŒç»“æ„çš„ blockï¼ˆæ®‹å·® + å¤šå¤´æ³¨æ„åŠ› + FeedForwardï¼‰ï¼Œæ‰€ä»¥ç”¨ `nn.Sequential` æ˜¯å¾ˆè‡ªç„¶çš„å†™æ³•ã€‚

---

## 4. ç›´è§‚ç†è§£

å‡è®¾ï¼š

```python
n_layer = 3
n_embd = 384
n_head = 6
```

é‚£ä¹ˆ `self.blocks` çš„ç»“æ„å°±åƒè¿™æ ·ï¼š

```
Sequential(
  (0): Block(n_embd=384, n_head=6)
  (1): Block(n_embd=384, n_head=6)
  (2): Block(n_embd=384, n_head=6)
)
```

è¾“å…¥æ•°æ®ä¼šä¾æ¬¡ç»è¿‡ **3 å±‚ Transformer Block**ï¼Œæ¯å±‚éƒ½ä¼šåšï¼š

1. LayerNorm
2. Multi-Head Attention
3. æ®‹å·®è¿æ¥
4. FeedForward
5. å†æ®‹å·®è¿æ¥

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼  **`nn.Sequential` åŒ…å«å¤šä¸ª Transformer Block çš„æ•°æ®æµå›¾**ï¼Œè®©ä½ æ›´ç›´è§‚çœ‹å‡ºè¾“å…¥æ˜¯æ€ä¹ˆé€å±‚ä¼ é€’çš„ã€‚è¿™æ ·ä¼šæ›´å®¹æ˜“è®°ä½è¿™ä¸ªç»“æ„ã€‚ä½ è¦æˆ‘ç”»å—ï¼Ÿ



# encoder vs. decoder vs. both (?) Transformers

what we implemented here is a decoder only Transformer so there's no component here this part is called the encoder and
there's no cross attention block here our block only has a self attention and
the feet forward so it is missing this third in between piece here this piece
does cross attention so we don't have it and we don't have the encoder we just have the decoder and the reason we have
a decoder only uh is because we are just uh generating text and it's unconditioned on anything we're just
we're just blabbering on according to a given data set what makes it a decoder is that we are using the Triangular mask
in our uh trans former so it has this Auto regressive property where we can just uh go and sample from it so the
fact that it's using the Triangular triangular mask to mask out the attention makes it a decoder and it can
be used for language modeling now the reason that the original paper had an incoder decoder architecture is because
it is a machine translation paper so it is concerned with a different setting in particular it expects some uh tokens
that encode say for example French and then it is expecting to decode the translation in English so so you
typically these here are special tokens so you are expected to read in this and
condition on it and then you start off the generation with a special token called start so this is a special new
token um that you introduce and always place in the beginning and then the network is expected to Output neural
networks are awesome and then a special end token to finish the generation so this part here will be
decoded exactly as we we've done it neural networks are awesome will be identical to what we did but unlike what
we did they wanton to condition the generation on some additional information and in that case this
additional information is the French sentence that they should be translating so what they do now is they
bring in the encoder now the encoder reads this part here so we're only going
to take the part of French and we're going to uh create tokens from it exactly as we've seen in our video and
we're going to put a Transformer on it but there's going to be no triangular mask and so all the tokens are allowed
to talk to each other as much as they want and they're just encoding whatever's the content of this French uh
sentence once they've encoded it they they basically come out in the top here
and then what happens here is in our decoder which does the uh language modeling there's an additional
connection here to the outputs of the encoder and that is brought in through a cross
attention so the queries are still generated from X but now the keys and the values are coming from the side the
keys and the values are coming from the top generated by the nodes that came outside of the de the encoder and those
tops the keys and the values there the top of it feed in on a side into every
single block of the decoder and so that's why there's an additional cross attention and really what it's doing is
it's conditioning the decoding not just on the past of this current decoding but also on having seen the
full fully encoded French um prompt sort of and so it's an encoder decoder model
which is why we have those two Transformers an additional block and so on so we did not do this because we have
no we have nothing to encode there's no conditioning we just have a text file and we just want to imitate it and that's why we are using a decoder only
Transformer exactly as done in GPT okay okay so now I wanted to do a

è¿™æ®µå†…å®¹è®²è§£äº† **Encoder-Decoder Transformer** å’Œ **Decoder-only Transformer** çš„åŒºåˆ«ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬åœ¨ **æœºå™¨ç¿»è¯‘** å’Œ **è¯­è¨€å»ºæ¨¡** ä¸­çš„ä¸åŒåº”ç”¨ã€‚

---

## ğŸ§  ä¸€ã€æˆ‘ä»¬å®ç°çš„æ˜¯ **Decoder-only Transformer**

### âœ… ä»€ä¹ˆæ˜¯ **Decoder-only Transformer**ï¼Ÿ

æˆ‘ä»¬å®ç°çš„æ¨¡å‹æ˜¯ **Decoder-only Transformer**ï¼Œè¿™æ„å‘³ç€ï¼š

* **æ²¡æœ‰ Encoder éƒ¨åˆ†**ï¼Œæ¨¡å‹æ²¡æœ‰ç”¨äºå¤„ç†è¾“å…¥çš„éƒ¨åˆ†ï¼ŒåªåŒ…å«äº† **è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰** å’Œ **å‰é¦ˆç½‘ç»œï¼ˆfeed-forwardï¼‰**ã€‚
* æˆ‘ä»¬ä½¿ç”¨ **ä¸‰è§’å½¢æ©ç ï¼ˆtriangular maskï¼‰**ï¼Œè¿™æ ·æ¨¡å‹åªèƒ½çœ‹åˆ°å½“å‰å’Œè¿‡å»çš„ tokenï¼Œè€Œä¸èƒ½çœ‹åˆ°æœªæ¥çš„ tokenï¼Œè¿™ç§æ–¹å¼æ˜¯**è‡ªå›å½’**çš„ï¼ˆautogressiveï¼‰ï¼Œå³å®ƒä¼šæ ¹æ®ä¹‹å‰ç”Ÿæˆçš„å†…å®¹é¢„æµ‹ä¸‹ä¸€ä¸ªå†…å®¹ã€‚

### âœ… ä¸ºä»€ä¹ˆæ˜¯ **Decoder-only**ï¼Ÿ

* è¿™ä¸ªæ¨¡å‹çš„ä»»åŠ¡æ˜¯ **ç”Ÿæˆæ–‡æœ¬**ï¼Œä¸éœ€è¦å¤–éƒ¨çš„æ¡ä»¶ä¿¡æ¯ã€‚å®ƒåªæ˜¯æ ¹æ®ç»™å®šçš„æ•°æ®é›† **è‡ªå‘åœ°ç”Ÿæˆæ–‡æœ¬**ï¼Œä¾‹å¦‚åŸºäºèå£«æ¯”äºšçš„è¯­æ–™ç”Ÿæˆèå£«æ¯”äºšé£æ ¼çš„æ–‡æœ¬ã€‚
* è¿™ç§æ–¹å¼ä¸‹ï¼Œåªéœ€è¦ä½¿ç”¨ Decoder æ¥ç”Ÿæˆæ–‡æœ¬ï¼Œä¸”æ¨¡å‹é€šè¿‡ **ä¸‰è§’å½¢æ©ç ** æ¥ç¡®ä¿æ¯æ¬¡ç”Ÿæˆéƒ½ä»…ä¾èµ–äºå½“å‰æˆ–è¿‡å»çš„å†…å®¹ã€‚

---

## ğŸ—ï¸ äºŒã€**Encoder-Decoder Transformer**

åœ¨åŸå§‹çš„ Transformer è®ºæ–‡ä¸­ï¼Œæå‡ºäº† **Encoder-Decoder ç»“æ„**ï¼Œè¿™æ˜¯å› ä¸ºå®ƒçš„åº”ç”¨åœºæ™¯æ˜¯ **æœºå™¨ç¿»è¯‘**ï¼Œä¾‹å¦‚ä»æ³•è¯­ç¿»è¯‘æˆè‹±è¯­ã€‚è¿™ä¸ªç»“æ„çš„ç‰¹ç‚¹å¦‚ä¸‹ï¼š

1. **Encoderï¼ˆç¼–ç å™¨ï¼‰**ï¼š

   * è¯»å–è¾“å…¥ï¼ˆæ¯”å¦‚æ³•è¯­å¥å­ï¼‰ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºä¸€ç»„ tokenï¼ˆå•è¯çš„è¡¨ç¤ºï¼‰ã€‚
   * åœ¨ Encoder ä¸­ï¼Œæ‰€æœ‰çš„ token éƒ½å¯ä»¥è‡ªç”±äº¤æµï¼ˆæ²¡æœ‰ä¸‰è§’å½¢æ©ç ï¼‰ï¼Œè¿™æ„å‘³ç€æ¯ä¸ª token å¯ä»¥çœ‹åˆ°æ‰€æœ‰å…¶ä»–çš„ tokenã€‚
2. **Decoderï¼ˆè§£ç å™¨ï¼‰**ï¼š

   * è§£ç å™¨åŸºäºè¾“å…¥çš„ **ç¼–ç ä¿¡æ¯** æ¥ç”Ÿæˆè¾“å‡ºï¼ˆä¾‹å¦‚ç¿»è¯‘åçš„è‹±è¯­å¥å­ï¼‰ã€‚
   * è§£ç å™¨ä¸ä»…ä¾èµ–äºä¹‹å‰ç”Ÿæˆçš„ tokenï¼ˆå°±åƒæˆ‘ä»¬åœ¨ Decoder-only æ¨¡å‹ä¸­åšçš„é‚£æ ·ï¼‰ï¼Œè¿˜è¦**ä¾èµ–ç¼–ç å™¨è¾“å‡ºçš„è¡¨ç¤º**ã€‚è¿™éƒ¨åˆ†é€šè¿‡ **Cross-attentionï¼ˆäº¤å‰æ³¨æ„åŠ›ï¼‰** æ¥å®ç°ã€‚

     * åœ¨ Cross-attention ä¸­ï¼Œè§£ç å™¨çš„ **æŸ¥è¯¢ï¼ˆqueryï¼‰** ä¾ç„¶æ¥è‡ªè¾“å…¥ï¼ˆä¾‹å¦‚å½“å‰æ­£åœ¨ç”Ÿæˆçš„å†…å®¹ï¼‰ï¼Œä½† **é”®ï¼ˆkeyï¼‰** å’Œ **å€¼ï¼ˆvalueï¼‰** æ¥è‡ª Encoder çš„è¾“å‡ºï¼Œå³ç¼–ç åçš„æ³•è¯­å¥å­ã€‚

### âœ… **Cross-attention** çš„ä½œç”¨ï¼š

* é€šè¿‡ Cross-attentionï¼Œè§£ç å™¨ä¸ä»…æ ¹æ®å½“å‰ç”Ÿæˆçš„å†…å®¹æ¥ç”Ÿæˆä¸‹ä¸€ä¸ª tokenï¼Œè¿˜èƒ½å¤Ÿåˆ©ç”¨ç¼–ç å™¨ç”Ÿæˆçš„å®Œæ•´æ³•è¯­å¥å­çš„ä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆæ›´ç¬¦åˆç¿»è¯‘è¦æ±‚çš„å†…å®¹ã€‚

---

## ğŸ“– ä¸‰ã€ä¸ºä»€ä¹ˆæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨ **Encoder** éƒ¨åˆ†ï¼Ÿ

* æˆ‘ä»¬çš„ä»»åŠ¡ä»…ä»…æ˜¯ **æ¨¡ä»¿** ä¸€ä¸ªç»™å®šçš„æ•°æ®é›†ï¼ˆæ¯”å¦‚èå£«æ¯”äºšçš„æ–‡æœ¬ï¼‰ï¼Œå¹¶æ²¡æœ‰å¤–éƒ¨æ¡ä»¶ä¿¡æ¯éœ€è¦è€ƒè™‘ï¼ˆæ¯”å¦‚ç¿»è¯‘çš„ç›®æ ‡è¯­è¨€ï¼‰ã€‚
* å› æ­¤ï¼Œæˆ‘ä»¬ä¸éœ€è¦ Encoderï¼Œåªéœ€è¦ Decoder æ¥æ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆæ–‡æœ¬ï¼Œä¸”è¿™ä¸ªæ¨¡å‹æ˜¯è‡ªå›å½’çš„ï¼ˆæ¯æ¬¡ç”Ÿæˆä¾èµ–äºä¹‹å‰çš„å†…å®¹ï¼‰ã€‚

---

## ğŸ§© å››ã€æ€»ç»“

> * **Decoder-only Transformer**ï¼šç”¨æ¥åšè¯­è¨€å»ºæ¨¡æˆ–æ–‡æœ¬ç”Ÿæˆï¼Œæ¨¡å‹åªéœ€è¦å¤„ç†è‡ªå›å½’çš„ç”Ÿæˆä»»åŠ¡ï¼Œæ²¡æœ‰å¤–éƒ¨è¾“å…¥çš„æ¡ä»¶ã€‚
> * **Encoder-Decoder Transformer**ï¼šç”¨æ¥åšæœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ï¼Œç¼–ç å™¨ç”¨äºè¯»å–è¾“å…¥ï¼ˆå¦‚æ³•è¯­ï¼‰ï¼Œè§£ç å™¨ç”¨äºç”Ÿæˆè¾“å‡ºï¼ˆå¦‚è‹±è¯­ï¼‰ï¼Œå¹¶é€šè¿‡ Cross-attention å°†è§£ç è¿‡ç¨‹ä¸ç¼–ç ç»“æœç›¸ç»“åˆã€‚

å¦‚æœä½ æœ‰æ›´å¤šé—®é¢˜æˆ–æƒ³äº†è§£ **Encoder-Decoder ç»“æ„** åœ¨å…¶ä»–ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼

æˆ‘å¸®ä½ æŠŠè¿™ä¸€æ®µå†…å®¹æ‹†å¼€è®²æ¸…æ¥šï¼Œä½ å°±èƒ½æ˜ç™½ **encoder-only / decoder-only / encoder-decoder** Transformer çš„åŒºåˆ«ï¼Œä»¥åŠä¸ºä»€ä¹ˆæˆ‘ä»¬ç°åœ¨çš„å®ç°æ˜¯ **decoder-only**ã€‚

---

## 1. ä¸‰ç§ Transformer ç»“æ„

### **(1) Encoder-only Transformer**

* **ä»£è¡¨æ¨¡å‹**ï¼šBERT
* **ç‰¹ç‚¹**ï¼š

  * åªæœ‰ç¼–ç å™¨éƒ¨åˆ†ï¼ˆEncoder stackï¼‰ã€‚
  * è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰**ä¸ä½¿ç”¨å› æœæ©ç **ï¼ˆno triangular maskï¼‰ï¼Œæ‰€ä»¥æ¯ä¸ª token éƒ½å¯ä»¥çœ‹åˆ°å¥å­é‡Œå…¶å®ƒæ‰€æœ‰ tokenã€‚
  * ä¸»è¦ç”¨äºç†è§£ä»»åŠ¡ï¼ˆclassificationã€span extractionï¼‰ï¼Œä¸æ˜¯ç”Ÿæˆã€‚
* **æµç¨‹**ï¼š

  ```
  è¾“å…¥åºåˆ— â†’ Token Embedding â†’ å¤šå±‚ Encoder â†’ è¾“å‡ºå‘é‡è¡¨ç¤º
  ```

---

### **(2) Decoder-only Transformer**

* **ä»£è¡¨æ¨¡å‹**ï¼šGPT ç³»åˆ—
* **ç‰¹ç‚¹**ï¼š

  * åªæœ‰è§£ç å™¨éƒ¨åˆ†ï¼ˆDecoder stackï¼‰ã€‚
  * è‡ªæ³¨æ„åŠ›ä½¿ç”¨**ä¸‰è§’æ©ç **ï¼ˆtriangular maskï¼Œä¹Ÿå« causal maskï¼‰ï¼Œç¡®ä¿ä¸€ä¸ªä½ç½®åªèƒ½çœ‹åˆ°è‡ªå·±å’Œä¹‹å‰çš„ tokenï¼Œä¸èƒ½çœ‹æœªæ¥çš„ tokenã€‚
  * ç”¨äºè‡ªå›å½’ç”Ÿæˆï¼ˆauto-regressive generationï¼‰ï¼Œä¸€è¾¹ç”Ÿæˆä¸€è¾¹é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚
* **æµç¨‹**ï¼š

  ```
  è¿‡å»ç”Ÿæˆçš„ token â†’ Token Embedding â†’ å¤šå±‚ Decoderï¼ˆmasked self-attentionï¼‰ â†’ é¢„æµ‹ä¸‹ä¸€ä¸ª token
  ```

---

### **(3) Encoder-Decoder Transformer**

* **ä»£è¡¨æ¨¡å‹**ï¼šåŸå§‹ Transformerï¼ˆæœºå™¨ç¿»è¯‘ï¼‰ã€T5
* **ç‰¹ç‚¹**ï¼š

  * å·¦è¾¹æ˜¯ Encoderï¼šè¯»å®Œæ•´ä¸ªè¾“å…¥ï¼ˆæ— æ©ç ï¼‰ï¼Œå¾—åˆ°ç¼–ç åçš„è¡¨ç¤ºã€‚
  * å³è¾¹æ˜¯ Decoderï¼šè§£ç æ—¶ä½¿ç”¨ masked self-attention **+ cross-attention**ã€‚

    * **masked self-attention**ï¼šåªèƒ½çœ‹å†å²çš„ target tokenã€‚
    * **cross-attention**ï¼šQuery æ¥è‡ª decoder å½“å‰ tokenï¼ŒKey/Value æ¥è‡ª encoder è¾“å‡ºï¼Œä¿è¯è§£ç æ—¶å¯ä»¥â€œå‚è€ƒâ€æ•´ä¸ªè¾“å…¥ã€‚
  * å¸¸ç”¨äºæœºå™¨ç¿»è¯‘ã€æ‘˜è¦ã€æ¡ä»¶ç”Ÿæˆç­‰ã€‚
* **æµç¨‹**ï¼š

  ```
  Encoder: è¾“å…¥æºè¯­è¨€åºåˆ— â†’ æ— æ©ç  self-attention â†’ ç¼–ç è¡¨ç¤º
  Decoder: ä¸Šä¸‹æ–‡ç›®æ ‡è¯­è¨€ + cross-attention è¯»å–ç¼–ç è¡¨ç¤º â†’ ç”Ÿæˆç¿»è¯‘
  ```

---

## 2. æˆ‘ä»¬ç°åœ¨å®ç°çš„æ˜¯ä»€ä¹ˆï¼Ÿ

* æˆ‘ä»¬åªå®ç°äº† **Decoder stack**ï¼Œè€Œä¸”ä½¿ç”¨äº† **triangular mask**ã€‚
* æ²¡æœ‰ï¼š

  * Encoder éƒ¨åˆ†ï¼ˆæ‰€ä»¥ä¸èƒ½å¤„ç†å¤–éƒ¨æ¡ä»¶è¾“å…¥ï¼‰
  * Cross-Attentionï¼ˆæ‰€ä»¥è§£ç å®Œå…¨ä¾èµ–å·²ç”Ÿæˆçš„ tokenï¼‰
* **é€‚åˆåœºæ™¯**ï¼š

  * çº¯è¯­è¨€å»ºæ¨¡ï¼ˆLanguage Modelingï¼‰
  * ChatGPT / GPT-2/3 ç±»çš„æ–‡æœ¬ç”Ÿæˆ
* è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒæ˜¯ **decoder-only** Transformerã€‚

---

## 3. å…³é”®åˆ¤æ–­æ ‡å‡†

ä½ å¯ä»¥ç”¨ä¸¤ä¸ªé—®é¢˜æ¥åˆ¤æ–­ Transformer çš„ç±»å‹ï¼š

1. **æ˜¯å¦æœ‰ cross-attentionï¼Ÿ**

   * æœ‰ â†’ encoder-decoder
   * æ²¡æœ‰ â†’ å¯èƒ½æ˜¯ encoder-only æˆ– decoder-only
2. **self-attention æ˜¯å¦æœ‰ä¸‰è§’æ©ç ï¼Ÿ**

   * æœ‰ â†’ decoder-only
   * æ²¡æœ‰ â†’ encoder-only

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼  **å¯¹æ¯”å›¾**ï¼Œç›´è§‚å±•ç¤ºä¸‰ç§ Transformer çš„ç»“æ„å·®å¼‚ï¼ŒåŒ…æ‹¬æ•°æ®æµå’Œæ©ç æ–¹å¼ï¼Œè¿™æ ·ä½ ä¸€çœ¼å°±èƒ½çœ‹å‡ºæ¥ã€‚
ä½ è¦æˆ‘ç”»å—ï¼Ÿè¿™æ ·è®°å¿†ä¼šæ›´ç‰¢ã€‚



# super quick walkthrough of nanoGPT, batched multi-headed self-attention

very brief walkthrough of nanog GPT which you can find in my GitHub and uh nanog GPT is basically two files of
Interest there's train.py and model.py train.py is all the boilerplate code for
training the network it is basically all the stuff that we had here it's the training loop it's just that it's a lot
more complicated because we're saving and loading checkpoints and pre-trained weights and we are uh decaying the
learning rate and compiling the model and using distributed training across multiple nodes or GP use so the training
Pi gets a little bit more hairy complicated uh there's more options Etc
but the model.py should look very very um similar to what we've done here in fact the model is is almost identical so
first here we have the causal self attention block and all of this should look very very recognizable to you we're
producing queries Keys values we're doing Dot products we're masking applying soft Maxs optionally dropping
out and here we are pulling the wi the values what is different here is that in
our code I have separated out the multi-headed detention into just a
single individual head and then here I have multiple heads and I explicitly concatenate them whereas here uh all of
it is implemented in a batched manner inside a single causal self attention and so we don't just have a b and a T
and A C Dimension we also end up with a fourth dimension which is the heads and so it just gets a lot more sort of hairy
because we have four dimensional array um tensors now but it is um equivalent
mathematically so the exact same thing is happening as what we have it's just it's a bit more efficient because all
the heads are now treated as a batch Dimension as well then we have the multier perceptron
it's using the Galu nonlinearity which is defined here except instead of Ru and
this is done just because opening I used it and I want to be able to load their checkpoints uh the blocks of the
Transformer are identical to communicate in the compute phase as we saw and then the GPT will be identical we have the
position encodings token encodings the blocks the layer Norm at the end uh the
final linear layer and this should look all very recognizable and there's a bit
more here because I'm loading checkpoints and stuff like that I'm separating out the parameters into those that should be weight decayed and those
that shouldn't um but the generate function should also be very very similar so a
few details are different but you should definitely be able to look at this uh file and be able to understand little
the pieces now so let's now bring things back to chat GPT what would it look like if we wanted to train chat GPT ourselves

è¿™æ®µå†…å®¹è®²è§£äº† **nanoGPT** çš„å®ç°ï¼Œå¹¶ç®€è¦ä»‹ç»äº†å…¶ä¸­çš„æ ¸å¿ƒæ¨¡å—å’Œä¸ä¹‹å‰å®ç°çš„ Transformer ç›¸ä¼¼ä¹‹å¤„ã€‚ä¸‹é¢æˆ‘å°†é€šä¿—æ˜“æ‡‚åœ°è§£é‡Šä¸€ä¸‹ï¼š

---

## ğŸ§‘â€ğŸ’» **nanoGPT å¿«é€Ÿæ¦‚è§ˆ**

**nanoGPT** æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ GPT æ¨¡å‹å®ç°ï¼Œå®ƒæœ‰ä¸¤ä¸ªä¸»è¦æ–‡ä»¶ï¼š

* **train.py**ï¼šè´Ÿè´£æ¨¡å‹çš„è®­ç»ƒéƒ¨åˆ†ï¼ŒåŒ…å«è®­ç»ƒå¾ªç¯ã€ä¿å­˜å’ŒåŠ è½½æ£€æŸ¥ç‚¹ã€å­¦ä¹ ç‡è¡°å‡ã€åˆ†å¸ƒå¼è®­ç»ƒç­‰ã€‚
* **model.py**ï¼šå®šä¹‰äº† GPT æ¨¡å‹çš„ç»“æ„ï¼ŒåŒ…æ‹¬è‡ªæ³¨æ„åŠ›ã€å‰é¦ˆç½‘ç»œç­‰ã€‚

### âœ… **train.py æ–‡ä»¶**

* **train.py** ä¸»è¦ç”¨äºè®­ç»ƒæ¨¡å‹ï¼ŒåŒ…å«å¾ˆå¤šè®­ç»ƒçš„ç»†èŠ‚ï¼Œå¦‚ï¼š

  * **ä¿å­˜å’ŒåŠ è½½æ£€æŸ¥ç‚¹**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå®šæœŸä¿å­˜æ¨¡å‹çŠ¶æ€ï¼Œå¹¶å¯ä»¥åœ¨ä¹‹ååŠ è½½é¢„è®­ç»ƒçš„æƒé‡ã€‚
  * **å­¦ä¹ ç‡è¡°å‡**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ®ä¸€å®šè§„åˆ™è°ƒæ•´å­¦ä¹ ç‡ã€‚
  * **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šå¦‚æœæœ‰å¤šä¸ª GPU æˆ–è®¡ç®—èŠ‚ç‚¹ï¼Œå¯ä»¥è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚

### âœ… **model.py æ–‡ä»¶**

* **model.py** æ˜¯å®ç° GPT æ¨¡å‹çš„æ ¸å¿ƒæ–‡ä»¶ã€‚å®ƒåŒ…å«äº†æ¨¡å‹çš„å…·ä½“ç»“æ„ï¼š

  * **è‡ªæ³¨æ„åŠ›ï¼ˆSelf-attentionï¼‰**ï¼šåŒ…æ‹¬è®¡ç®—æŸ¥è¯¢ï¼ˆQueryï¼‰ã€é”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰ï¼Œå¹¶è¿›è¡ŒçŸ©é˜µä¹˜æ³•ã€æ©ç ã€Softmax å’Œ Dropout ç­‰æ“ä½œã€‚
  * **å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-headed Self-attentionï¼‰**ï¼šå°†å¤šä¸ªè‡ªæ³¨æ„åŠ›å¤´è¿›è¡Œæ‰¹å¤„ç†ï¼Œæ‰€æœ‰çš„å¤´å¹¶è¡Œå¤„ç†ï¼Œæœ€ç»ˆå°†å®ƒä»¬çš„è¾“å‡ºè¿›è¡Œæ‹¼æ¥ã€‚è¿™ç§æ–¹æ³•æ¯”å•ç‹¬å¤„ç†æ¯ä¸ªå¤´æ›´é«˜æ•ˆã€‚
  * **å‰é¦ˆç½‘ç»œï¼ˆFeedforwardï¼‰**ï¼šä½¿ç”¨ Galu æ¿€æ´»å‡½æ•°ï¼ˆä¸ä¹‹å‰çš„ ReLU æ¿€æ´»å‡½æ•°ä¸åŒï¼‰ï¼Œè¿™æ˜¯å› ä¸º OpenAI åœ¨ GPT çš„å®ç°ä¸­ä½¿ç”¨äº† Galu æ¿€æ´»å‡½æ•°ã€‚
  * **ä½ç½®ç¼–ç å’Œæ ‡è®°ç¼–ç **ï¼šä¸ä¹‹å‰å®ç°çš„ä¸€æ ·ï¼Œç”¨äºä¸ºæ¯ä¸ªè¾“å…¥æ ‡è®°æä¾›ä½ç½®ä¿¡æ¯ã€‚

### âœ… **ä¸ä¹‹å‰å®ç°çš„åŒºåˆ«**

* **å¤šå¤´è‡ªæ³¨æ„åŠ›**ï¼šä¸ä¹‹å‰çš„å®ç°ä¸åŒï¼ŒnanoGPT ä¸­çš„å¤šå¤´è‡ªæ³¨æ„åŠ›æ“ä½œæ˜¯åœ¨ä¸€ä¸ªæ‰¹æ¬¡ä¸­å¹¶è¡Œè¿›è¡Œçš„ï¼Œè¿™ä½¿å¾—æ“ä½œæ›´åŠ é«˜æ•ˆï¼Œå› ä¸ºæ‰€æœ‰çš„å¤´éƒ½è¢«è§†ä¸ºä¸€ä¸ªæ‰¹æ¬¡çš„ç»´åº¦ã€‚è¿™æ ·å¯ä»¥é¿å…ç¹ççš„æ“ä½œï¼Œå¹¶æå‡è®¡ç®—æ•ˆç‡ã€‚
* **å‰é¦ˆç½‘ç»œ**ï¼šä½¿ç”¨äº† Galu æ¿€æ´»å‡½æ•°ï¼ˆåœ¨ä¹‹å‰çš„å®ç°ä¸­æˆ‘ä»¬ç”¨çš„æ˜¯ ReLUï¼‰ï¼Œè¿™æ˜¯ä¸ºäº†å…¼å®¹ OpenAI çš„ GPT æ£€æŸ¥ç‚¹æƒé‡ã€‚

### âœ… **ç”Ÿæˆï¼ˆGenerateï¼‰å‡½æ•°**

* `generate` å‡½æ•°çš„å®ç°ä¸æˆ‘ä»¬ä¹‹å‰çš„å®ç°éå¸¸ç›¸ä¼¼ï¼Œç”¨äºåœ¨è®­ç»ƒåæ ¹æ®ç»™å®šçš„è¾“å…¥ç”Ÿæˆæ–‡æœ¬ã€‚

---

## ğŸ§© **æ€»ç»“**

* **nanoGPT** é€šè¿‡ç®€åŒ– GPT æ¨¡å‹ï¼Œå»é™¤äº†è®¸å¤šå¤æ‚çš„é…ç½®ï¼Œåšäº†è®¸å¤šä¼˜åŒ–ï¼Œä½¿å¾—æ¨¡å‹è®­ç»ƒå˜å¾—æ›´åŠ é«˜æ•ˆã€‚
* å®ƒä½¿ç”¨ **å¤šå¤´è‡ªæ³¨æ„åŠ›**ã€**ä½ç½®ç¼–ç **ã€**æ ‡è®°ç¼–ç **ã€**å‰é¦ˆç½‘ç»œ** ç­‰ä¸ä¼ ç»Ÿ GPT æ¨¡å‹ç›¸åŒçš„æ¶æ„ï¼Œä½†åœ¨å®ç°ä¸Šåšäº†æ€§èƒ½ä¼˜åŒ–ï¼Œèƒ½æ›´é«˜æ•ˆåœ°è¿›è¡Œå¹¶è¡Œè®¡ç®—ã€‚
* **train.py** è´Ÿè´£è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç»†èŠ‚ï¼Œå¦‚å­¦ä¹ ç‡è¡°å‡ã€æ£€æŸ¥ç‚¹ä¿å­˜ç­‰ï¼Œ**model.py** åˆ™å®šä¹‰äº†æ ¸å¿ƒçš„ Transformer æ¶æ„ã€‚

é€šè¿‡è¿™äº›ä¼˜åŒ–ï¼ŒnanoGPT åœ¨ä¿æŒè¾ƒç®€æ´ä»£ç çš„åŒæ—¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œè®­ç»ƒå’Œç”Ÿæˆã€‚


# back to ChatGPT, GPT-3, pretraining vs. finetuning, RLHF

and how does it relate to what we learned today well to train in chat GPT there are roughly two stages first is
the pre-training stage and then the fine-tuning stage in the pre-training stage uh we are training on a large
chunk of internet and just trying to get a first decoder only Transformer to
babble text so it's very very similar to what we've done ourselves except we've
done like a tiny little baby pre-training step um and so in our case
uh this is how you print a number of parameters I printed it and it's about 10 million so this Transformer that I
created here to create little Shakespeare um Transformer was about 10
million parameters our data set is roughly 1 million uh characters so roughly 1 million tokens but you have to
remember that opening I is different vocabulary they're not on the Character level they use these um subword chunks
of words and so they have a vocabulary of 50,000 roughly elements and so their sequences are a bit more condensed so
our data set the Shakespeare data set would be probably around 300,000 uh tokens in the open AI vocabulary roughly
so we trained about 10 million parameter model on roughly 300,000 tokens now when
you go to the gpt3 paper and you look at the Transformers
that they trained they trained a number of trans Transformers of different sizes but the biggest Transformer here has 175
billion parameters uh so ours is again 10 million they used this number of layers in the Transformer this is the
nmed this is the number of heads and this is the head size and then this is
the batch size uh so ours was 65 and the learning rate is similar now
when they train this Transformer they trained on 300 billion tokens so again remember ours is about 300,000
so this is uh about a millionfold increase and this number would not be even that large by today's standards
you'd be going up uh 1 trillion and above so they are training a
significantly larger model on uh a good chunk of the internet
and that is the pre-training stage but otherwise these hyper parameters should be fairly recognizable to you and the
architecture is actually like nearly identical to what we implemented ourselves but of course it's a massive
infrastructure challenge to train this you're talking about typically thousands of gpus having to you know talk to each
other to train models of this size so that's just a pre-training stage now after you complete the pre-training
stage uh you don't get something that responds to your questions with answers and is not helpful and Etc you get a
document completer right so it babbles but it doesn't Babble Shakespeare it babbles
internet it will create arbitrary news articles and documents and it will try to complete documents because that's
what it's trained for it's trying to complete the sequence so when you give it a question it would just uh potentially just give you more questions
it would follow with more questions it will do whatever it looks like the some close document would do in the training
data on the internet and so who knows you're getting kind of like undefined Behavior it might basically answer with
to questions with other questions it might ignore your question it might just try to complete some news article it's
totally unineed as we say so the second fine-tuning stage is to actually align
it to be an assistant and uh this is the second stage and so this chat GPT block
post from openi talks a little bit about how the stage is achieved we basically
um there's roughly three steps to to this stage uh so what they do here is they start to collect training data that
looks specifically like what an assistant would do so these are documents that have to format where the question is on top and then an answer is
below and they have a large number of these but probably not on the order of the internet uh this is probably on the
of maybe thousands of examples and so they they then fine-tune the model to
basically only focus on documents that look like that and so you're starting to slowly align it so it's going to expect
a question at the top and it's going to expect to complete the answer and uh these very very large models are very
sample efficient during their fine-tuning so this actually somehow works but that's just step one that's
just fine tuning so then they actually have more steps where okay the second step is you let the model respond and
then different Raiders look at the different responses and rank them for their preference as to which one is
better than the other they use that to train a reward model so they can predict uh basically using a different network
how much of any candidate response would be desirable and then
once they have a reward model they run po which is a form of polic policy gradient um reinforcement learning
Optimizer to uh fine-tune this sampling policy uh so that the answers that the
GP chat GPT now generates are expected to score a high reward according to the
reward model and so basically there's a whole aligning stage here or fine-tuning stage it's got multiple steps in between
there as well and it takes the model from being a document completer to a
question answerer and that's like a whole separate stage a lot of this data is not available publicly it is internal
to open AI and uh it's much harder to replicate this stage um and so that's
roughly what would give you a chat GPT and nanog GPT focuses on the pre-training stage okay and that's

è¿™æ®µå†…å®¹ä»‹ç»äº† **ChatGPT** å’Œ **GPT-3** çš„è®­ç»ƒè¿‡ç¨‹ï¼Œé‡ç‚¹è®²è§£äº† **é¢„è®­ç»ƒ** å’Œ **å¾®è°ƒ** é˜¶æ®µçš„ä¸åŒï¼Œä»¥åŠ **å¼ºåŒ–å­¦ä¹ ** åœ¨å¾®è°ƒä¸­çš„åº”ç”¨ã€‚ä¸‹é¢æ˜¯é€šä¿—æ˜“æ‡‚çš„è§£é‡Šï¼š

---

## ğŸ§‘â€ğŸ’» **ChatGPT å’Œ GPT-3 çš„è®­ç»ƒæµç¨‹**

### âœ… **é¢„è®­ç»ƒé˜¶æ®µ**ï¼š

* **é¢„è®­ç»ƒ** ä¸»è¦ç›®çš„æ˜¯è®©æ¨¡å‹å­¦ä¹ è¯­è¨€çš„åŸºæœ¬ç»“æ„å’Œæ¨¡å¼ã€‚è¿™ä¸ªé˜¶æ®µï¼Œæ¨¡å‹è®­ç»ƒçš„å†…å®¹æ¥è‡ªäº’è”ç½‘ä¸Šçš„å¤§é‡æ–‡æœ¬æ•°æ®ï¼ˆæ¯”å¦‚æ–°é—»æ–‡ç« ã€åšå®¢ã€ç»´åŸºç™¾ç§‘ç­‰ï¼‰ã€‚è®­ç»ƒæ—¶ï¼Œæ¨¡å‹åªå…³æ³¨ç”Ÿæˆæ–‡æœ¬ï¼Œæ²¡æœ‰ä¸“é—¨çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚å›ç­”é—®é¢˜ï¼‰ã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡è§‚å¯Ÿå¤§é‡æ–‡æœ¬æ•°æ®ï¼Œå­¦ä¹ å¦‚ä½•ç”Ÿæˆä¸è¾“å…¥ç›¸ä¼¼çš„æ–‡æœ¬ã€‚

* å¯¹äºæˆ‘ä»¬è‡ªå·±å®ç°çš„æ¨¡å‹æ¥è¯´ï¼Œ**é¢„è®­ç»ƒé˜¶æ®µ** å°±åƒæ˜¯è®­ç»ƒä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®å¤§çº¦æ˜¯ **100ä¸‡å­—ç¬¦**ï¼Œæ¨¡å‹å‚æ•°å¤§çº¦æ˜¯ **1000ä¸‡ä¸ª**ã€‚ç„¶è€Œï¼ŒOpenAI çš„ GPT-3 æ¨¡å‹åˆ™è®­ç»ƒäº† **1750äº¿ä¸ªå‚æ•°**ï¼Œæ•°æ®é‡æ˜¯ **3000äº¿ä¸ª token**ï¼ˆç›¸å½“äº 1000 å¤šä¸‡å€çš„è§„æ¨¡ï¼‰ã€‚è¿™éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºï¼Œæ¯”å¦‚æˆåƒä¸Šä¸‡çš„ GPU æ¥è¿›è¡Œè®­ç»ƒã€‚

* é¢„è®­ç»ƒé˜¶æ®µçš„ç›®æ ‡æ˜¯è®©æ¨¡å‹èƒ½å¤Ÿâ€œç”Ÿæˆæ–‡æœ¬â€ï¼Œæ¯”å¦‚ç»™å®šä¸€ä¸ªå¼€å§‹çš„å¥å­ï¼Œæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ¥ä¸‹æ¥çš„å†…å®¹ã€‚è¿™æ˜¯ä¸€ä¸ª **æ— ç›‘ç£** çš„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹å­¦ä¼šäº†è¯­è¨€æ¨¡å¼ï¼Œä½†å®ƒå¹¶ä¸æ‡‚å¾—å¦‚ä½•å›ç­”é—®é¢˜ã€‚

### âœ… **å¾®è°ƒé˜¶æ®µ**ï¼š

* åœ¨å®Œæˆ **é¢„è®­ç»ƒ** åï¼Œæ¨¡å‹å¹¶ä¸èƒ½ç›´æ¥å›ç­”é—®é¢˜ã€‚å®ƒè™½ç„¶å¯ä»¥ç”Ÿæˆæ–‡æœ¬ï¼Œä½†æœ‰æ—¶ä¼šå‡ºç°æ— æ„ä¹‰çš„å›ç­”ï¼Œæˆ–è€…æ— æ³•ç†è§£å¤æ‚çš„æŒ‡ä»¤ã€‚è¿™æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œ **å¾®è°ƒ**ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆç‰¹å®šçš„ä»»åŠ¡ï¼Œä¾‹å¦‚é—®ç­”ã€å¯¹è¯ç­‰ã€‚

* **å¾®è°ƒ** é€šå¸¸åˆ†ä¸ºå‡ ä¸ªæ­¥éª¤ï¼š

  1. **æ•°æ®æ”¶é›†**ï¼šæ”¶é›†ä¸€äº›å…¸å‹çš„å¯¹è¯æ•°æ®ï¼Œè¿™äº›æ•°æ®åŒ…æ‹¬ **é—®é¢˜** å’Œ **ç­”æ¡ˆ** çš„é…å¯¹ï¼Œæ¨¡æ‹Ÿç”¨æˆ·ä¸åŠ©æ‰‹çš„äº’åŠ¨ã€‚
  2. **å¾®è°ƒè®­ç»ƒ**ï¼šå°†æ¨¡å‹é‡æ–°è®­ç»ƒï¼Œä¸“é—¨è®©å®ƒå­¦ä¹ è¿™ç§æ ¼å¼çš„é—®é¢˜å’Œå›ç­”ã€‚æ­¤æ—¶ï¼Œæ¨¡å‹å¼€å§‹é€‚åº”å›ç­”é—®é¢˜çš„ä»»åŠ¡ï¼Œä½†å®ƒè¿˜ä¸å®Œç¾ã€‚

* åœ¨å¾®è°ƒé˜¶æ®µï¼Œ**å¤§æ¨¡å‹**ï¼ˆä¾‹å¦‚ GPT-3ï¼‰ä¼šæ˜¾å¾—æ›´åŠ é«˜æ•ˆï¼Œå®ƒèƒ½å¤Ÿ **ä»å°‘é‡æ•°æ®ä¸­å­¦ä¹ **ï¼Œè¿™å°±æ˜¯ **å¾®è°ƒ** çš„ä¼˜åŠ¿ã€‚

### âœ… **ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–**ï¼š

* å¾®è°ƒå®Œæˆåï¼Œæ¨¡å‹å¯èƒ½ä¾ç„¶ä¸æ˜¯å®Œç¾çš„å›ç­”è€…ã€‚å› æ­¤ï¼ŒOpenAI é‡‡ç”¨äº† **å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰** æ¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„è¾“å‡ºã€‚

  * **ç¬¬ä¸€æ­¥**ï¼šè®©æ¨¡å‹ç”Ÿæˆä¸€äº›å›ç­”ï¼Œç„¶åè®©äººå·¥è¯„å®¡äººå‘˜å¯¹è¿™äº›å›ç­”è¿›è¡Œæ’åï¼Œé€‰å‡ºæœ€å¥½çš„å›ç­”ã€‚
  * **ç¬¬äºŒæ­¥**ï¼šåŸºäºè¿™äº›è¯„å®¡ç»“æœï¼Œè®­ç»ƒä¸€ä¸ª **å¥–åŠ±æ¨¡å‹**ï¼Œè®©æ¨¡å‹çŸ¥é“å“ªäº›å›ç­”æ˜¯å¥½çš„ï¼Œå“ªäº›å›ç­”ä¸å¥½ã€‚
  * **ç¬¬ä¸‰æ­¥**ï¼šä½¿ç”¨ **å¼ºåŒ–å­¦ä¹ ** æ¥ä¼˜åŒ–æ¨¡å‹çš„è¾“å‡ºï¼Œä½¿å¾—æ¨¡å‹ç”Ÿæˆçš„å›ç­”èƒ½å¤Ÿæœ€å¤§åŒ–è·å¾—çš„å¥–åŠ±ï¼ˆå³æœ€ç¬¦åˆè¯„å®¡æ ‡å‡†çš„å›ç­”ï¼‰ã€‚

---

## ğŸ§© **æ€»ç»“**ï¼š

* **é¢„è®­ç»ƒé˜¶æ®µ**ï¼šæ¨¡å‹åœ¨è¿™ä¸ªé˜¶æ®µå­¦ä¹ è¯­è¨€çš„åŸºæœ¬è§„å¾‹ï¼Œé€šè¿‡å¤§è§„æ¨¡çš„æ–‡æœ¬æ•°æ®è®­ç»ƒï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ–‡æœ¬ã€‚
* **å¾®è°ƒé˜¶æ®µ**ï¼šé€šè¿‡å°‘é‡é’ˆå¯¹æ€§çš„æ•°æ®ï¼Œè°ƒæ•´æ¨¡å‹è®©å®ƒèƒ½å¤Ÿæ‰§è¡Œç‰¹å®šä»»åŠ¡ï¼Œä¾‹å¦‚é—®ç­”ã€‚
* **å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰**ï¼šé€šè¿‡äººå·¥è¯„å®¡å’Œå¥–åŠ±æ¨¡å‹ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„è¾“å‡ºï¼Œä½¿å…¶ç”Ÿæˆé«˜è´¨é‡çš„ç­”æ¡ˆã€‚

è¿™äº›æ­¥éª¤çš„æœ€ç»ˆç›®çš„æ˜¯è®©æ¨¡å‹ä»ä¸€ä¸ªç®€å•çš„æ–‡æœ¬ç”Ÿæˆå™¨ï¼Œå‘å±•æˆä¸ºä¸€ä¸ªèƒ½å¤Ÿç†è§£é—®é¢˜å¹¶ç»™å‡ºåˆç†å›ç­”çš„ **èŠå¤©åŠ©æ‰‹**ï¼Œè¿™ä¹Ÿæ˜¯ **ChatGPT** çš„æ ¸å¿ƒã€‚


# conclusions

everything that I wanted to cover today so we trained to summarize a decoder
only Transformer following this famous paper attention is all you need from
2017 and so that's basically a GPT we trained it on Tiny Shakespeare and got
sensible results all of the training code is roughly 200 lines of code I will be
releasing this um code base so also it comes with all the git log commits along
the way as we built it up in addition to this code I'm going to release the um notebook of course the
Google collab and I hope that gave you a sense for how you can train um these
models like say gpt3 that will be um architecturally basically identical to what we have but they are somewhere
between 10,000 and 1 million times bigger depending on how you count and so
uh that's all I have for now uh we did not talk about any of the fine-tuning stages that would typically go on top of
this so if you're interested in something that's not just language modeling but you actually want to you know say perform tasks um or you want
them to be aligned in a specific way or you want um to detect sentiment or anything like that basically anytime you
don't want something that's just a document completer you have to complete further stages of fine tuning which did
not cover uh and that could be simple supervised fine tuning or it can be something more fancy like we see in chat
jpt where we actually train a reward model and then do rounds of Po to uh align it with respect to the reward
model so there's a lot more that can be done on top of it I think for now we're starting to get to about two hours Mark
uh so I'm going to um kind of finish here uh I hope you enjoyed the lecture
uh and uh yeah go forth and transform see you later

è¿™æ®µè¯å…¶å®æ˜¯æ•´ä¸ªå®ç°è¿‡ç¨‹çš„æ€»ç»“ï¼Œæ„æ€æ˜¯ï¼š

---

## 1. æœ¬æ¬¡å®ç°çš„å†…å®¹

* æˆ‘ä»¬ä»é›¶å®ç°äº†ä¸€ä¸ª **decoder-only Transformer**ï¼Œå‚è€ƒçš„æ˜¯ 2017 å¹´çš„è®ºæ–‡ *Attention is All You Need*ã€‚
* è¿™ä¸ªæ¶æ„åŸºæœ¬å°±æ˜¯ GPTï¼ˆå°¤å…¶æ˜¯ GPT-1ã€GPT-2 è¿™ç§çº¯è§£ç å™¨ç»“æ„ï¼‰ã€‚
* åœ¨ **Tiny Shakespeare** æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå¹¶å¾—åˆ°äº†åˆç†çš„ç»“æœã€‚
* å…¨éƒ¨è®­ç»ƒä»£ç å¤§çº¦ 200 è¡Œï¼Œå¹¶ä¸”ä½œè€…ä¼šå‘å¸ƒå®Œæ•´çš„ä»£ç å’Œæ„å»ºè¿‡ç¨‹çš„ git commit è®°å½•ã€‚
* è¿˜ä¼šæä¾› Google Colab ç‰ˆæœ¬çš„ notebookï¼Œæ–¹ä¾¿ç›´æ¥è¿è¡Œã€‚

---

## 2. ä¸å¤§å‹æ¨¡å‹çš„å…³ç³»

* è™½ç„¶æˆ‘ä»¬å®ç°çš„æ˜¯ä¸€ä¸ªå°æ¨¡å‹ï¼Œä½†æ¶æ„ä¸Šä¸ GPT-3 æ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯è§„æ¨¡å·®å¼‚æå¤§ã€‚
* GPT-3 å¯èƒ½æ¯”æˆ‘ä»¬å®ç°çš„ç‰ˆæœ¬å¤§ **1 ä¸‡åˆ° 100 ä¸‡å€**ï¼ˆå‚æ•°æ•°é‡ä¸Šï¼‰ã€‚

---

## 3. è¿˜æ²¡åšçš„éƒ¨åˆ†

* æˆ‘ä»¬åªåšäº† **çº¯è¯­è¨€å»ºæ¨¡ï¼ˆLanguage Modelingï¼‰**ï¼Œå³ç»™å‡ºä¸€æ®µæ–‡æœ¬é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚
* å¦‚æœæƒ³è®©æ¨¡å‹åšç‰¹å®šä»»åŠ¡ï¼ˆæ¯”å¦‚æƒ…æ„Ÿåˆ†æã€é—®ç­”ã€ä»»åŠ¡å¯¹é½ç­‰ï¼‰ï¼Œéœ€è¦é¢å¤–çš„ **å¾®è°ƒï¼ˆFine-tuningï¼‰** é˜¶æ®µã€‚
* å¾®è°ƒæ–¹å¼åŒ…æ‹¬ï¼š

  * **ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-tuning, SFTï¼‰**
  * **å¥–åŠ±æ¨¡å‹ + å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰**ï¼Œåƒ ChatGPT é‚£æ ·ï¼Œé€šè¿‡å¥–åŠ±æ¨¡å‹å¯¹é½è¾“å‡ºã€‚

---

## 4. æ€»ç»“ä¸ç»“è¯­

* ç”¨è¾ƒçŸ­çš„ä»£ç å®ç°äº† Transformer çš„æ ¸å¿ƒé€»è¾‘ã€‚
* å±•ç¤ºäº†å¦‚ä½•ä»é›¶æ„å»ºä¸€ä¸ª GPT æ¶æ„çš„æ¨¡å‹ï¼Œå¹¶éªŒè¯å…¶åŸºæœ¬å¯è¡Œæ€§ã€‚
* åç»­å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šæ‰©å±•è§„æ¨¡ã€åŠ ä¸Šä»»åŠ¡å¾®è°ƒã€å¯¹é½ç­‰æ­¥éª¤ï¼Œä»è€Œåšå‡ºçœŸæ­£å¯ç”¨çš„èŠå¤©æˆ–ä»»åŠ¡æ¨¡å‹ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼  **GPT å°æ¨¡å‹åˆ°å¤§æ¨¡å‹çš„â€œè¿›åŒ–è·¯çº¿å›¾â€**ï¼ŒæŠŠä» decoder-only åˆ° ChatGPT çš„æ¶æ„æ¼”è¿›å’Œé¢å¤–è®­ç»ƒé˜¶æ®µç›´è§‚ç”»å‡ºæ¥ï¼Œè¿™æ ·ä¸€çœ¼å°±èƒ½çœ‹å‡ºå·®åˆ«ã€‚ä½ è¦æˆ‘ç”»å—ï¼Ÿ



