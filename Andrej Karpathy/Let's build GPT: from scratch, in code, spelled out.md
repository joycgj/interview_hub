We build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video.

Links:
- Google colab for the video: https://colab.research.google.com/dri...
- GitHub repo for the video: https://github.com/karpathy/ng-video-...
- Playlist of the whole Zero to Hero series so far:    â€¢ The spelled-out intro to neural networks a...  
- nanoGPT repo: https://github.com/karpathy/nanoGPT
- my website: https://karpathy.ai
- my twitter:   / karpathy  
- our Discord channel:   / discord  

Supplementary links:
- Attention is All You Need paper: https://arxiv.org/abs/1706.03762
- OpenAI GPT-3 paper: https://arxiv.org/abs/2005.14165 
- OpenAI ChatGPT blog post: https://openai.com/blog/chatgpt/
- The GPU I'm training the model on is from Lambda GPU Cloud, I think the best and easiest way to spin up an on-demand GPU instance in the cloud that you can ssh to: https://lambdalabs.com . If you prefer to work in notebooks, I think the easiest path today is Google Colab.

Suggested exercises:
- EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).
- EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)
- EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?
- EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?

```
Chapters:
00:00:00 intro: ChatGPT, Transformers, nanoGPT, Shakespeare
baseline language modeling, code setup
00:07:52 reading and exploring the data
00:09:28 tokenization, train/val split
00:14:27 data loader: batches of chunks of data
00:22:11 simplest baseline: bigram language model, loss, generation
00:34:53 training the bigram model
00:38:00 port our code to a script
Building the "self-attention"
00:42:13 version 1: averaging past context with for loops, the weakest form of aggregation
00:47:11 the trick in self-attention: matrix multiply as weighted aggregation
00:51:54 version 2: using matrix multiply
00:54:42 version 3: adding softmax
00:58:26 minor code cleanup
01:00:18 positional encoding
01:02:00 THE CRUX OF THE VIDEO: version 4: self-attention
01:11:38 note 1: attention as communication
01:12:46 note 2: attention has no notion of space, operates over sets
01:13:40 note 3: there is no communication across batch dimension
01:14:14 note 4: encoder blocks vs. decoder blocks
01:15:39 note 5: attention vs. self-attention vs. cross-attention
01:16:56 note 6: "scaled" self-attention. why divide by sqrt(head_size)
Building the Transformer
01:19:11 inserting a single self-attention block to our network
01:21:59 multi-headed self-attention
01:24:25 feedforward layers of transformer block
01:26:48 residual connections
01:32:51 layernorm (and its relationship to our previous batchnorm)
01:37:49 scaling up the model! creating a few variables. adding dropout
Notes on Transformer
01:42:39 encoder vs. decoder vs. both (?) Transformers
01:46:22 super quick walkthrough of nanoGPT, batched multi-headed self-attention
01:48:53 back to ChatGPT, GPT-3, pretraining vs. finetuning, RLHF
01:54:32 conclusions

Corrections: 
00:57:00 Oops "tokens from the future cannot communicate", not "past". Sorry! :)
01:20:05 Oops I should be using the head_size for the normalization, not C
```

å½“ç„¶å¯ä»¥ï¼Œæˆ‘å¸®ä½ æŠŠè¿™æ®µå†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼Œæ–¹ä¾¿ç†è§£ï¼š

---

æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ª**ç”Ÿæˆå¼é¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆGPTï¼‰**ï¼Œå‚è€ƒã€ŠAttention is All You Needã€‹è®ºæ–‡å’Œ OpenAI çš„ GPT-2 / GPT-3ã€‚æˆ‘ä»¬è¿˜ä¼šè®¨è®ºå®ƒå’Œå¦‚ä»Šå¸­å·å…¨çƒçš„ ChatGPT ä¹‹é—´çš„è”ç³»ã€‚æˆ‘ä»¬è¿˜ä¼šæ¼”ç¤ºå¦‚ä½•ç”¨ GitHub Copilotï¼ˆå®ƒæœ¬èº«å°±æ˜¯ GPTï¼‰æ¥è¾…åŠ©æˆ‘ä»¬ç¼–å†™ GPT ä»£ç ï¼ˆå¾ˆæœ‰â€œå…ƒâ€æ„Ÿ ğŸ˜„ï¼‰ã€‚
æˆ‘å»ºè®®å¤§å®¶å…ˆå»çœ‹ä¹‹å‰çš„ "makemore" ç³»åˆ—è§†é¢‘ï¼Œç†Ÿæ‚‰**è‡ªå›å½’è¯­è¨€å»ºæ¨¡**æ¡†æ¶ã€å¼ é‡ï¼ˆtensorï¼‰çš„åŸºæœ¬æ“ä½œï¼Œä»¥åŠ PyTorch çš„ `nn` æ¨¡å—ï¼Œè¿™äº›åœ¨æœ¬è§†é¢‘ä¸­æ˜¯é»˜è®¤å·²æŒæ¡çš„ã€‚

ğŸ”— ç›¸å…³é“¾æ¥ï¼š

* æœ¬è§†é¢‘ç”¨åˆ°çš„ Google Colab: [https://colab.research.google.com/dri](https://colab.research.google.com/dri)...
* æœ¬è§†é¢‘çš„ GitHub ä»£ç åº“: [https://github.com/karpathy/ng-video-](https://github.com/karpathy/ng-video-)...
* â€œä»é›¶åˆ°é«˜æ‰‹â€ç³»åˆ—æ’­æ”¾åˆ—è¡¨:    â€¢ The spelled-out intro to neural networks a...
* nanoGPT ä»“åº“: [https://github.com/karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)
* æˆ‘çš„ä¸ªäººç½‘ç«™: [https://karpathy.ai](https://karpathy.ai)
* æˆ‘çš„ Twitter:   / karpathy
* æˆ‘ä»¬çš„ Discord é¢‘é“:   / discord

ğŸ“„ å‚è€ƒèµ„æ–™ï¼š

* ã€ŠAttention is All You Needã€‹è®ºæ–‡: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
* OpenAI GPT-3 è®ºæ–‡: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
* OpenAI ChatGPT åšå®¢: [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)
* æœ¬è§†é¢‘è®­ç»ƒæ¨¡å‹ç”¨çš„ GPU æ¥è‡ª Lambda GPU Cloudï¼Œæ˜¯ç›®å‰æˆ‘è§‰å¾—æœ€å¿«æ·å¼€å¯äº‘ç«¯ GPU å®ä¾‹çš„æ–¹å¼ï¼Œå¯ä»¥é€šè¿‡ ssh è¿œç¨‹è¿æ¥: [https://lambdalabs.com](https://lambdalabs.com)
* å¦‚æœå–œæ¬¢ç”¨ notebookï¼ŒGoogle Colab æ˜¯ç›®å‰æœ€ç®€å•çš„å…¥é—¨æ–¹å¼ã€‚

ğŸ’» æ¨èç»ƒä¹ é¢˜ï¼š
**EX1**ï¼šæŒæ¡ N ç»´å¼ é‡ï¼ŒæŒ‘æˆ˜é¢˜ï¼šæŠŠ `Head` å’Œ `MultiHeadAttention` åˆå¹¶æˆä¸€ä¸ªç±»ï¼Œè®©å¤šä¸ªå¤´å¹¶è¡Œå¤„ç†ï¼ŒæŠŠâ€œheadâ€ä½œä¸ºé¢å¤– batch ç»´åº¦å¤„ç†ï¼ˆç­”æ¡ˆè§ nanoGPTï¼‰
**EX2**ï¼šç”¨ä½ å–œæ¬¢çš„æ•°æ®é›†è®­ç»ƒ GPTï¼å¯ä»¥è®­ç»ƒ GPT æ¥åšåŠ æ³•ï¼Œä¾‹å¦‚ a+b=cï¼Œå»ºè®®è®©æ¨¡å‹é¢„æµ‹ c çš„æ•°å­—ï¼ŒæŒ‰é€†åºé¢„æµ‹ï¼ˆå› ä¸ºåŠ æ³•é€šå¸¸æ˜¯ä»ä½ä½å¼€å§‹çš„ï¼‰ï¼Œæ•°æ® loader éœ€è¦è°ƒæ•´ï¼Œä¸ç”¨ç”Ÿæˆ train.bin å’Œ val.binï¼Œè¾“å…¥ a+b è¿™éƒ¨åˆ†çš„ loss å¯ä»¥ç”¨ `y=-1` å±è”½ï¼ˆå‚è€ƒ CrossEntropyLoss çš„ ignore\_indexï¼‰ã€‚èƒ½å­¦ä¼šåŠ æ³•å—ï¼Ÿå¦‚æœèƒ½ï¼Œè¿›ä¸€æ­¥æŒ‘æˆ˜ï¼šåšä¸€ä¸ª GPT è®¡ç®—å™¨ï¼Œæ”¯æŒ + - \* /ã€‚è¿™æ˜¯é«˜é˜¶æŒ‘æˆ˜ï¼Œå¯èƒ½éœ€è¦ Chain of Thought æŠ€æœ¯ã€‚
**EX3**ï¼šæ‰¾ä¸€ä¸ªè¶…å¤§æ•°æ®é›†ï¼Œè®© train å’Œ val loss ä¹‹é—´çœ‹ä¸å‡ºå·®è·ï¼Œå…ˆç”¨è¿™ä¸ªå¤§æ•°æ®é›†é¢„è®­ç»ƒ Transformerï¼Œç„¶åç”¨è¿™ä¸ªæ¨¡å‹åˆå§‹åŒ–ï¼Œfinetune åœ¨ tiny shakespeare æ•°æ®é›†ä¸Šï¼Œçœ‹çœ‹èƒ½ä¸èƒ½é€šè¿‡é¢„è®­ç»ƒè·å¾—æ›´ä½çš„ val lossã€‚
**EX4**ï¼šè¯» transformer çš„è®ºæ–‡ï¼Œè‡ªå·±å®ç°ä¸€ä¸ªé¢å¤–çš„æ”¹è¿›ï¼Œçœ‹èƒ½å¦æå‡ GPT æ€§èƒ½ã€‚

```
ğŸ“… è§†é¢‘ç« èŠ‚æ—¶é—´è½´ï¼š
00:00:00 ä»‹ç»ï¼šChatGPTã€Transformerã€nanoGPTã€Shakespeareï¼ŒåŸºç¡€è¯­è¨€å»ºæ¨¡ï¼Œä»£ç å‡†å¤‡
00:07:52 è¯»å–å’Œæ¢ç´¢æ•°æ®
00:09:28 åˆ†è¯ï¼Œè®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†
00:14:27 æ•°æ® loaderï¼šæ•°æ®å— batch ç”Ÿæˆ
00:22:11 æœ€ç®€å•çš„ bigram è¯­è¨€æ¨¡å‹ï¼Œlossï¼Œæ–‡æœ¬ç”Ÿæˆ
00:34:53 è®­ç»ƒ bigram æ¨¡å‹
00:38:00 æŠŠä»£ç æ”¹æˆè„šæœ¬
ğŸ”¨ å®ç° self-attention
00:42:13 ç‰ˆæœ¬ 1ï¼šç”¨ for å¾ªç¯åšå¹³å‡èšåˆï¼Œæœ€å¼±å½¢å¼
00:47:11 self-attention æ ¸å¿ƒ trickï¼šçŸ©é˜µä¹˜æ³•åšåŠ æƒèšåˆ
00:51:54 ç‰ˆæœ¬ 2ï¼šç”¨çŸ©é˜µä¹˜æ³•
00:54:42 ç‰ˆæœ¬ 3ï¼šåŠ ä¸Š softmax
00:58:26 ä»£ç æ•´ç†
01:00:18 ä½ç½®ç¼–ç  (positional encoding)
01:02:00 æ ¸å¿ƒéƒ¨åˆ†ï¼šç‰ˆæœ¬ 4ï¼Œå®Œæ•´ self-attention
01:11:38 æ³¨è§£ 1ï¼šattention æ˜¯ä¸€ç§ä¿¡æ¯äº¤æµæœºåˆ¶
01:12:46 æ³¨è§£ 2ï¼šattention æ²¡æœ‰ç©ºé—´æ¦‚å¿µï¼Œå¤„ç†çš„æ˜¯é›†åˆ
01:13:40 æ³¨è§£ 3ï¼šbatch ä¹‹é—´ä¸é€šä¿¡
01:14:14 æ³¨è§£ 4ï¼šencoder block vs decoder block
01:15:39 æ³¨è§£ 5ï¼šattention / self-attention / cross-attention åŒºåˆ«
01:16:56 æ³¨è§£ 6ï¼šâ€œscaledâ€ self-attentionï¼Œä¸ºä»€ä¹ˆè¦é™¤ä»¥ sqrt(head\_size)
ğŸ§± æ­å»º Transformer
01:19:11 ç½‘ç»œä¸­æ’å…¥ self-attention block
01:21:59 å¤šå¤´ self-attention
01:24:25 Transformer block ä¸­çš„å‰é¦ˆå±‚ feedforward
01:26:48 æ®‹å·®è¿æ¥
01:32:51 LayerNorm åŠå…¶å’Œä¹‹å‰ BatchNorm çš„å…³ç³»
01:37:49 æ¨¡å‹æ‰©å±•ï¼å¼•å…¥å˜é‡ï¼ŒåŠ ä¸Š dropout
ğŸ“š Transformer ç»“æ„æ€»ç»“
01:42:39 encoder / decoder / encoder-decoder Transformer
01:46:22 å¿«é€Ÿè¿‡ä¸€é nanoGPTï¼Œå¤šå¤´ self-attention æ‰¹å¤„ç†
01:48:53 å›åˆ° ChatGPTã€GPT-3ã€é¢„è®­ç»ƒ vs å¾®è°ƒã€RLHF
01:54:32 æ€»ç»“

âš ï¸ å‹˜è¯¯ï¼š
00:57:00 é”™è¯´æˆâ€œtokens from the future cannot communicateâ€ï¼Œåº”æ˜¯â€œtokens from the past cannot communicateâ€
01:20:05 åº”è¯¥ç”¨ head\_size å½’ä¸€åŒ–ï¼Œä¸æ˜¯ C
```

---

å¦‚æœä½ è¿˜éœ€è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ **æ€»ç»“ä¸€ä¸‹è¿™ä¸ªè§†é¢‘çš„å­¦ä¹ é‡ç‚¹**ï¼Œæˆ–è€…**åˆ—å‡ºä¸€ä¸ªé€‚åˆåˆå­¦è€…çš„å­¦ä¹ è·¯çº¿**ï¼Œè¦ä¸è¦æˆ‘é¡ºä¾¿æ•´ç†ä¸€ä¸‹ï¼Ÿ ğŸš€
