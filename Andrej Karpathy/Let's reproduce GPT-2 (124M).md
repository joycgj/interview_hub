We reproduce the GPT-2 (124M) from scratch. This video covers the whole process: First we build the GPT-2 network, then we optimize its training to be really fast, then we set up the training run following the GPT-2 and GPT-3 paper and their hyperparameters, then we hit run, and come back the next morning to see our results, and enjoy some amusing model generations. Keep in mind that in some places this video builds on the knowledge from earlier videos in the Zero to Hero Playlist (see my channel). You could also see this video as building my nanoGPT repo, which by the end is about 90% similar.

Links:
- build-nanogpt GitHub repo, with all the changes in this video as individual commits: https://github.com/karpathy/- build-nan...
- nanoGPT repo: https://github.com/karpathy/nanoGPT
- llm.c repo: https://github.com/karpathy/llm.c
- my website: https://karpathy.ai
- my twitter:   / karpathy  
- our Discord channel:   / discord  

Supplementary links:
- Attention is All You Need paper: https://arxiv.org/abs/1706.03762
- OpenAI GPT-3 paper: https://arxiv.org/abs/2005.14165 - OpenAI GPT-2 paper: https://d4mucfpksywv.cloudfront.net/b... - The GPU I'm training the model on is from Lambda GPU Cloud, I think the best and easiest way to spin up an on-demand - GPU instance in the cloud that you can ssh to: https://lambdalabs.com 

```
Chapters:
00:00:00 intro: Letâ€™s reproduce GPT-2 (124M)
00:03:39 exploring the GPT-2 (124M) OpenAI checkpoint
00:13:47 SECTION 1: implementing the GPT-2 nn.Module
00:28:08 loading the huggingface/GPT-2 parameters
00:31:00 implementing the forward pass to get logits
00:33:31 sampling init, prefix tokens, tokenization
00:37:02 sampling loop
00:41:47 sample, auto-detect the device
00:45:50 letâ€™s train: data batches (B,T) â†’ logits (B,T,C)
00:52:53 cross entropy loss
00:56:42 optimization loop: overfit a single batch
01:02:00 data loader lite
01:06:14 parameter sharing wte and lm_head
01:13:47 model initialization: std 0.02, residual init
01:22:18 SECTION 2: Letâ€™s make it fast. GPUs, mixed precision, 1000ms
01:28:14 Tensor Cores, timing the code, TF32 precision, 333ms
01:39:38 float16, gradient scalers, bfloat16, 300ms
01:48:15 torch.compile, Python overhead, kernel fusion, 130ms
02:00:18 flash attention, 96ms
02:06:54 nice/ugly numbers. vocab size 50257 â†’ 50304, 93ms
02:14:55 SECTION 3: hyperpamaters, AdamW, gradient clipping
02:21:06 learning rate scheduler: warmup + cosine decay
02:26:21 batch size schedule, weight decay, FusedAdamW, 90ms
02:34:09 gradient accumulation
02:46:52 distributed data parallel (DDP)
03:10:21 datasets used in GPT-2, GPT-3, FineWeb (EDU)
03:23:10 validation data split, validation loss, sampling revive
03:28:23 evaluation: HellaSwag, starting the run
03:43:05 SECTION 4: results in the morning! GPT-2, GPT-3 repro
03:56:21 shoutout to llm.c, equivalent but faster code in raw C/CUDA
03:59:39 summary, phew, build-nanogpt github repo
```

Corrections:
I will post all errata and followups to the build-nanogpt GitHub repo (link above)

SuperThanks:
I experimentally enabled them on my channel yesterday. Totally optional and only use if rich. All revenue goes to to supporting my work in AI + Education.

ä»¥ä¸‹æ˜¯ä½ æä¾›å†…å®¹çš„ä¸­æ–‡ç¿»è¯‘ï¼š

---

**æˆ‘ä»¬ä»é›¶å¼€å§‹å¤ç° GPT-2ï¼ˆ124Mï¼‰æ¨¡å‹ã€‚**
è¿™ä¸ªè§†é¢‘å±•ç¤ºäº†å®Œæ•´çš„è¿‡ç¨‹ï¼šé¦–å…ˆæˆ‘ä»¬æ„å»º GPT-2 ç½‘ç»œï¼Œç„¶åä¼˜åŒ–è®­ç»ƒæµç¨‹è®©å®ƒå˜å¾—éå¸¸å¿«ï¼Œæ¥ç€æ ¹æ® GPT-2 å’Œ GPT-3 çš„è®ºæ–‡åŠå…¶è¶…å‚æ•°è®¾ç½®è®­ç»ƒæµç¨‹ï¼Œç‚¹å‡»è¿è¡ŒæŒ‰é’®ï¼Œç¬¬äºŒå¤©æ—©ä¸Šå›æ¥çœ‹çœ‹ç»“æœï¼Œå¹¶æ¬£èµä¸€äº›æœ‰è¶£çš„æ¨¡å‹ç”Ÿæˆå†…å®¹ã€‚è¯·æ³¨æ„ï¼Œè§†é¢‘ä¸­çš„ä¸€äº›å†…å®¹åŸºäºâ€œZero to Heroâ€ç³»åˆ—å‰é¢çš„è§†é¢‘ï¼ˆè¯·çœ‹æˆ‘çš„é¢‘é“ï¼‰ã€‚ä½ ä¹Ÿå¯ä»¥æŠŠè¿™æ”¯è§†é¢‘çœ‹ä½œæ˜¯æˆ‘ [nanoGPT](https://github.com/karpathy/nanoGPT) ä»“åº“çš„æ„å»ºè¿‡ç¨‹ï¼Œæœ€ç»ˆå¤§çº¦æœ‰ 90% çš„å†…å®¹æ˜¯ä¸€æ ·çš„ã€‚

ğŸ”— **é“¾æ¥**ï¼š

* build-nanogpt GitHub ä»“åº“ï¼ˆæœ¬è§†é¢‘ä¸­çš„æ‰€æœ‰æ›´æ”¹éƒ½ä»¥å•ç‹¬çš„æäº¤ä¿å­˜ï¼‰ï¼š
  [https://github.com/karpathy/build-nanogpt](https://github.com/karpathy/build-nanogpt)
* nanoGPT ä»“åº“ï¼š
  [https://github.com/karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)
* llm.c ä»“åº“ï¼ˆç”¨ C/CUDA å†™çš„ç­‰æ•ˆä½†æ›´å¿«çš„ä»£ç ï¼‰ï¼š
  [https://github.com/karpathy/llm.c](https://github.com/karpathy/llm.c)
* æˆ‘çš„ä¸ªäººç½‘ç«™ï¼š
  [https://karpathy.ai](https://karpathy.ai)
* æˆ‘çš„ Twitterï¼š[@karpathy](https://twitter.com/karpathy)
* æˆ‘ä»¬çš„ Discord é¢‘é“ï¼š[åŠ å…¥](https://discord.gg/karpathy)

ğŸ“š **è¡¥å……é“¾æ¥**ï¼š

* ã€ŠAttention is All You Needã€‹è®ºæ–‡ï¼š[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
* OpenAI GPT-3 è®ºæ–‡ï¼š[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
* OpenAI GPT-2 è®ºæ–‡ï¼š[ç‚¹å‡»æŸ¥çœ‹](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
* GPU è®­ç»ƒæ˜¯åœ¨ Lambda GPU Cloud ä¸Šè¿›è¡Œçš„ï¼Œä»–ä»¬æä¾›æœ€ä¾¿æ·çš„æŒ‰éœ€ GPU äº‘å®ä¾‹ï¼Œæ”¯æŒ SSH ç™»å½•ï¼š[https://lambdalabs.com](https://lambdalabs.com)

ğŸ“¼ **è§†é¢‘ç« èŠ‚ç›®å½•**ï¼š

* 00:00:00 å¼€åœºï¼šæˆ‘ä»¬è¦å¤ç° GPT-2ï¼ˆ124Mï¼‰
* 00:03:39 æ¢ç´¢ OpenAI çš„ GPT-2ï¼ˆ124Mï¼‰æ¨¡å‹
* 00:13:47 ç¬¬1éƒ¨åˆ†ï¼šå®ç° GPT-2 çš„ `nn.Module`
* 00:28:08 åŠ è½½ huggingface/GPT-2 å‚æ•°
* 00:31:00 å®ç°å‰å‘ä¼ æ’­ï¼Œå¾—åˆ° logits
* 00:33:31 å¼€å§‹é‡‡æ ·ã€å‰ç¼€ tokenã€åˆ†è¯å¤„ç†
* 00:37:02 é‡‡æ ·å¾ªç¯
* 00:41:47 é‡‡æ ·ï¼Œè‡ªåŠ¨æ£€æµ‹è®¾å¤‡
* 00:45:50 å¼€å§‹è®­ç»ƒï¼šæ•°æ®æ‰¹ (B,T) â†’ logits (B,T,C)
* 00:52:53 äº¤å‰ç†µæŸå¤±
* 00:56:42 ä¼˜åŒ–å¾ªç¯ï¼šåœ¨ä¸€ä¸ª batch ä¸Šè¿‡æ‹Ÿåˆ
* 01:02:00 ç²¾ç®€æ•°æ®åŠ è½½å™¨
* 01:06:14 å‚æ•°å…±äº«ï¼š`wte` ä¸ `lm_head`
* 01:13:47 æ¨¡å‹åˆå§‹åŒ–ï¼šæ ‡å‡†å·® 0.02ã€æ®‹å·®åˆå§‹åŒ–
* 01:22:18 ç¬¬2éƒ¨åˆ†ï¼šæé€Ÿï¼GPUã€æ··åˆç²¾åº¦ã€1000ms
* 01:28:14 Tensor Coresã€è®¡æ—¶ã€TF32 ç²¾åº¦ï¼Œé™åˆ° 333ms
* 01:39:38 float16ã€æ¢¯åº¦ç¼©æ”¾å™¨ã€bfloat16ï¼Œé™åˆ° 300ms
* 01:48:15 torch.compileã€Pythonå¼€é”€ã€èåˆå†…æ ¸ï¼Œé™åˆ° 130ms
* 02:00:18 flash attentionï¼Œ96ms
* 02:06:54 å¾®è°ƒ vocab sizeï¼š50257 â†’ 50304ï¼Œ93ms
* 02:14:55 ç¬¬3éƒ¨åˆ†ï¼šè¶…å‚æ•°ï¼ŒAdamWã€æ¢¯åº¦è£å‰ª
* 02:21:06 å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šwarmup + ä½™å¼¦è¡°å‡
* 02:26:21 æ‰¹é‡å¤§å°è°ƒåº¦ã€æƒé‡è¡°å‡ã€FusedAdamWï¼Œ90ms
* 02:34:09 æ¢¯åº¦ç´¯ç§¯
* 02:46:52 åˆ†å¸ƒå¼è®­ç»ƒï¼šDDP
* 03:10:21 GPT-2ã€GPT-3 æ‰€ç”¨çš„æ•°æ®é›†ï¼šFineWebï¼ˆæ•™è‚²ç±»ï¼‰
* 03:23:10 éªŒè¯é›†åˆ’åˆ†ã€éªŒè¯æŸå¤±ã€é‡‡æ ·æ¼”ç¤º
* 03:28:23 è¯„ä¼°ï¼šHellaSwagã€å¯åŠ¨è®­ç»ƒ
* 03:43:05 ç¬¬4éƒ¨åˆ†ï¼šç¬¬äºŒå¤©æ—©ä¸Šçœ‹ç»“æœï¼GPT-2ã€GPT-3 å¤ç°
* 03:56:21 æ¨è llm.cï¼šC/CUDA å®ç°çš„ç­‰æ•ˆç‰ˆæœ¬ï¼Œæ›´å¿«
* 03:59:39 æ€»ç»“ï¼šå‘¼ï¼Œä»£ç å…¨åœ¨ build-nanogpt GitHub ä»“åº“

ğŸ›  **æ›´æ­£è¯´æ˜**ï¼š
æ‰€æœ‰å‹˜è¯¯å’Œåç»­è¡¥å……å°†ä¼šå‘å¸ƒåœ¨ [build-nanogpt GitHub ä»“åº“](https://github.com/karpathy/build-nanogpt)

ğŸ’¸ **SuperThanks**ï¼š
æˆ‘æ˜¨å¤©åœ¨é¢‘é“ä¸Šå®éªŒæ€§åœ°å¼€å¯äº†â€œSuperThanksâ€åŠŸèƒ½ï¼Œå®Œå…¨å¯é€‰ï¼Œä»…åœ¨ä½ çœŸçš„å¯Œæœ‰çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚æ‰€æœ‰æ”¶å…¥éƒ½ç”¨äºæ”¯æŒæˆ‘åœ¨ AI å’Œæ•™è‚²æ–¹é¢çš„å·¥ä½œã€‚

---

å¦‚æœä½ æƒ³æˆ‘è¿›ä¸€æ­¥å½’çº³è§†é¢‘é‡ç‚¹æˆ–æ‹†è§£æŸä¸€éƒ¨åˆ†å†…å®¹ï¼ˆæ¯”å¦‚â€œå‚æ•°å…±äº«â€ã€â€œFlash Attentionâ€ã€â€œåˆ†å¸ƒå¼è®­ç»ƒâ€ç­‰ï¼‰ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘ã€‚
