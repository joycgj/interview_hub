We reproduce the GPT-2 (124M) from scratch. This video covers the whole process: First we build the GPT-2 network, then we optimize its training to be really fast, then we set up the training run following the GPT-2 and GPT-3 paper and their hyperparameters, then we hit run, and come back the next morning to see our results, and enjoy some amusing model generations. Keep in mind that in some places this video builds on the knowledge from earlier videos in the Zero to Hero Playlist (see my channel). You could also see this video as building my nanoGPT repo, which by the end is about 90% similar.

Links:
- build-nanogpt GitHub repo, with all the changes in this video as individual commits: https://github.com/karpathy/- build-nan...
- nanoGPT repo: https://github.com/karpathy/nanoGPT
- llm.c repo: https://github.com/karpathy/llm.c
- my website: https://karpathy.ai
- my twitter:   / karpathy  
- our Discord channel:   / discord  

Supplementary links:
- Attention is All You Need paper: https://arxiv.org/abs/1706.03762
- OpenAI GPT-3 paper: https://arxiv.org/abs/2005.14165 
- OpenAI GPT-2 paper: https://d4mucfpksywv.cloudfront.net/b... 
- The GPU I'm training the model on is from Lambda GPU Cloud, I think the best and easiest way to spin up an on-demand - GPU instance in the cloud that you can ssh to: https://lambdalabs.com 

```
Chapters:
00:00:00 intro: Letâ€™s reproduce GPT-2 (124M)
00:03:39 exploring the GPT-2 (124M) OpenAI checkpoint
00:13:47 SECTION 1: implementing the GPT-2 nn.Module
00:28:08 loading the huggingface/GPT-2 parameters
00:31:00 implementing the forward pass to get logits
00:33:31 sampling init, prefix tokens, tokenization
00:37:02 sampling loop
00:41:47 sample, auto-detect the device
00:45:50 letâ€™s train: data batches (B,T) â†’ logits (B,T,C)
00:52:53 cross entropy loss
00:56:42 optimization loop: overfit a single batch
01:02:00 data loader lite
01:06:14 parameter sharing wte and lm_head
01:13:47 model initialization: std 0.02, residual init
01:22:18 SECTION 2: Letâ€™s make it fast. GPUs, mixed precision, 1000ms
01:28:14 Tensor Cores, timing the code, TF32 precision, 333ms
01:39:38 float16, gradient scalers, bfloat16, 300ms
01:48:15 torch.compile, Python overhead, kernel fusion, 130ms
02:00:18 flash attention, 96ms
02:06:54 nice/ugly numbers. vocab size 50257 â†’ 50304, 93ms
02:14:55 SECTION 3: hyperpamaters, AdamW, gradient clipping
02:21:06 learning rate scheduler: warmup + cosine decay
02:26:21 batch size schedule, weight decay, FusedAdamW, 90ms
02:34:09 gradient accumulation
02:46:52 distributed data parallel (DDP)
03:10:21 datasets used in GPT-2, GPT-3, FineWeb (EDU)
03:23:10 validation data split, validation loss, sampling revive
03:28:23 evaluation: HellaSwag, starting the run
03:43:05 SECTION 4: results in the morning! GPT-2, GPT-3 repro
03:56:21 shoutout to llm.c, equivalent but faster code in raw C/CUDA
03:59:39 summary, phew, build-nanogpt github repo
```

Corrections:
I will post all errata and followups to the build-nanogpt GitHub repo (link above)

SuperThanks:
I experimentally enabled them on my channel yesterday. Totally optional and only use if rich. All revenue goes to to supporting my work in AI + Education.

ä»¥ä¸‹æ˜¯ä½ æä¾›å†…å®¹çš„ä¸­æ–‡ç¿»è¯‘ï¼š

---

**æˆ‘ä»¬ä»é›¶å¼€å§‹å¤ç° GPT-2ï¼ˆ124Mï¼‰æ¨¡å‹ã€‚**
è¿™ä¸ªè§†é¢‘å±•ç¤ºäº†å®Œæ•´çš„è¿‡ç¨‹ï¼šé¦–å…ˆæˆ‘ä»¬æ„å»º GPT-2 ç½‘ç»œï¼Œç„¶åä¼˜åŒ–è®­ç»ƒæµç¨‹è®©å®ƒå˜å¾—éå¸¸å¿«ï¼Œæ¥ç€æ ¹æ® GPT-2 å’Œ GPT-3 çš„è®ºæ–‡åŠå…¶è¶…å‚æ•°è®¾ç½®è®­ç»ƒæµç¨‹ï¼Œç‚¹å‡»è¿è¡ŒæŒ‰é’®ï¼Œç¬¬äºŒå¤©æ—©ä¸Šå›æ¥çœ‹çœ‹ç»“æœï¼Œå¹¶æ¬£èµä¸€äº›æœ‰è¶£çš„æ¨¡å‹ç”Ÿæˆå†…å®¹ã€‚è¯·æ³¨æ„ï¼Œè§†é¢‘ä¸­çš„ä¸€äº›å†…å®¹åŸºäºâ€œZero to Heroâ€ç³»åˆ—å‰é¢çš„è§†é¢‘ï¼ˆè¯·çœ‹æˆ‘çš„é¢‘é“ï¼‰ã€‚ä½ ä¹Ÿå¯ä»¥æŠŠè¿™æ”¯è§†é¢‘çœ‹ä½œæ˜¯æˆ‘ [nanoGPT](https://github.com/karpathy/nanoGPT) ä»“åº“çš„æ„å»ºè¿‡ç¨‹ï¼Œæœ€ç»ˆå¤§çº¦æœ‰ 90% çš„å†…å®¹æ˜¯ä¸€æ ·çš„ã€‚

ğŸ”— **é“¾æ¥**ï¼š

* build-nanogpt GitHub ä»“åº“ï¼ˆæœ¬è§†é¢‘ä¸­çš„æ‰€æœ‰æ›´æ”¹éƒ½ä»¥å•ç‹¬çš„æäº¤ä¿å­˜ï¼‰ï¼š
  [https://github.com/karpathy/build-nanogpt](https://github.com/karpathy/build-nanogpt)
* nanoGPT ä»“åº“ï¼š
  [https://github.com/karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)
* llm.c ä»“åº“ï¼ˆç”¨ C/CUDA å†™çš„ç­‰æ•ˆä½†æ›´å¿«çš„ä»£ç ï¼‰ï¼š
  [https://github.com/karpathy/llm.c](https://github.com/karpathy/llm.c)
* æˆ‘çš„ä¸ªäººç½‘ç«™ï¼š
  [https://karpathy.ai](https://karpathy.ai)
* æˆ‘çš„ Twitterï¼š[@karpathy](https://twitter.com/karpathy)
* æˆ‘ä»¬çš„ Discord é¢‘é“ï¼š[åŠ å…¥](https://discord.gg/karpathy)

ğŸ“š **è¡¥å……é“¾æ¥**ï¼š

* ã€ŠAttention is All You Needã€‹è®ºæ–‡ï¼š[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
* OpenAI GPT-3 è®ºæ–‡ï¼š[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
* OpenAI GPT-2 è®ºæ–‡ï¼š[ç‚¹å‡»æŸ¥çœ‹](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
* GPU è®­ç»ƒæ˜¯åœ¨ Lambda GPU Cloud ä¸Šè¿›è¡Œçš„ï¼Œä»–ä»¬æä¾›æœ€ä¾¿æ·çš„æŒ‰éœ€ GPU äº‘å®ä¾‹ï¼Œæ”¯æŒ SSH ç™»å½•ï¼š[https://lambdalabs.com](https://lambdalabs.com)

ğŸ“¼ **è§†é¢‘ç« èŠ‚ç›®å½•**ï¼š

* 00:00:00 å¼€åœºï¼šæˆ‘ä»¬è¦å¤ç° GPT-2ï¼ˆ124Mï¼‰
* 00:03:39 æ¢ç´¢ OpenAI çš„ GPT-2ï¼ˆ124Mï¼‰æ¨¡å‹
* 00:13:47 ç¬¬1éƒ¨åˆ†ï¼šå®ç° GPT-2 çš„ `nn.Module`
* 00:28:08 åŠ è½½ huggingface/GPT-2 å‚æ•°
* 00:31:00 å®ç°å‰å‘ä¼ æ’­ï¼Œå¾—åˆ° logits
* 00:33:31 å¼€å§‹é‡‡æ ·ã€å‰ç¼€ tokenã€åˆ†è¯å¤„ç†
* 00:37:02 é‡‡æ ·å¾ªç¯
* 00:41:47 é‡‡æ ·ï¼Œè‡ªåŠ¨æ£€æµ‹è®¾å¤‡
* 00:45:50 å¼€å§‹è®­ç»ƒï¼šæ•°æ®æ‰¹ (B,T) â†’ logits (B,T,C)
* 00:52:53 äº¤å‰ç†µæŸå¤±
* 00:56:42 ä¼˜åŒ–å¾ªç¯ï¼šåœ¨ä¸€ä¸ª batch ä¸Šè¿‡æ‹Ÿåˆ
* 01:02:00 ç²¾ç®€æ•°æ®åŠ è½½å™¨
* 01:06:14 å‚æ•°å…±äº«ï¼š`wte` ä¸ `lm_head`
* 01:13:47 æ¨¡å‹åˆå§‹åŒ–ï¼šæ ‡å‡†å·® 0.02ã€æ®‹å·®åˆå§‹åŒ–
* 01:22:18 ç¬¬2éƒ¨åˆ†ï¼šæé€Ÿï¼GPUã€æ··åˆç²¾åº¦ã€1000ms
* 01:28:14 Tensor Coresã€è®¡æ—¶ã€TF32 ç²¾åº¦ï¼Œé™åˆ° 333ms
* 01:39:38 float16ã€æ¢¯åº¦ç¼©æ”¾å™¨ã€bfloat16ï¼Œé™åˆ° 300ms
* 01:48:15 torch.compileã€Pythonå¼€é”€ã€èåˆå†…æ ¸ï¼Œé™åˆ° 130ms
* 02:00:18 flash attentionï¼Œ96ms
* 02:06:54 å¾®è°ƒ vocab sizeï¼š50257 â†’ 50304ï¼Œ93ms
* 02:14:55 ç¬¬3éƒ¨åˆ†ï¼šè¶…å‚æ•°ï¼ŒAdamWã€æ¢¯åº¦è£å‰ª
* 02:21:06 å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šwarmup + ä½™å¼¦è¡°å‡
* 02:26:21 æ‰¹é‡å¤§å°è°ƒåº¦ã€æƒé‡è¡°å‡ã€FusedAdamWï¼Œ90ms
* 02:34:09 æ¢¯åº¦ç´¯ç§¯
* 02:46:52 åˆ†å¸ƒå¼è®­ç»ƒï¼šDDP
* 03:10:21 GPT-2ã€GPT-3 æ‰€ç”¨çš„æ•°æ®é›†ï¼šFineWebï¼ˆæ•™è‚²ç±»ï¼‰
* 03:23:10 éªŒè¯é›†åˆ’åˆ†ã€éªŒè¯æŸå¤±ã€é‡‡æ ·æ¼”ç¤º
* 03:28:23 è¯„ä¼°ï¼šHellaSwagã€å¯åŠ¨è®­ç»ƒ
* 03:43:05 ç¬¬4éƒ¨åˆ†ï¼šç¬¬äºŒå¤©æ—©ä¸Šçœ‹ç»“æœï¼GPT-2ã€GPT-3 å¤ç°
* 03:56:21 æ¨è llm.cï¼šC/CUDA å®ç°çš„ç­‰æ•ˆç‰ˆæœ¬ï¼Œæ›´å¿«
* 03:59:39 æ€»ç»“ï¼šå‘¼ï¼Œä»£ç å…¨åœ¨ build-nanogpt GitHub ä»“åº“

ğŸ›  **æ›´æ­£è¯´æ˜**ï¼š
æ‰€æœ‰å‹˜è¯¯å’Œåç»­è¡¥å……å°†ä¼šå‘å¸ƒåœ¨ [build-nanogpt GitHub ä»“åº“](https://github.com/karpathy/build-nanogpt)

ğŸ’¸ **SuperThanks**ï¼š
æˆ‘æ˜¨å¤©åœ¨é¢‘é“ä¸Šå®éªŒæ€§åœ°å¼€å¯äº†â€œSuperThanksâ€åŠŸèƒ½ï¼Œå®Œå…¨å¯é€‰ï¼Œä»…åœ¨ä½ çœŸçš„å¯Œæœ‰çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚æ‰€æœ‰æ”¶å…¥éƒ½ç”¨äºæ”¯æŒæˆ‘åœ¨ AI å’Œæ•™è‚²æ–¹é¢çš„å·¥ä½œã€‚

---

å¦‚æœä½ æƒ³æˆ‘è¿›ä¸€æ­¥å½’çº³è§†é¢‘é‡ç‚¹æˆ–æ‹†è§£æŸä¸€éƒ¨åˆ†å†…å®¹ï¼ˆæ¯”å¦‚â€œå‚æ•°å…±äº«â€ã€â€œFlash Attentionâ€ã€â€œåˆ†å¸ƒå¼è®­ç»ƒâ€ç­‰ï¼‰ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘ã€‚

# build nanoGPT

This repo holds the from-scratch reproduction of nanoGPT. The git commits were specifically kept step by step and clean so that one can easily walk through the git commit history to see it built slowly. Additionally, there is an accompanying video lecture on YouTube where you can see me introduce each commit and explain the pieces along the way.

We basically start from an empty file and work our way to a reproduction of the GPT-2 (124M) model. If you have more patience or money, the code can also reproduce the GPT-3 models. While the GPT-2 (124M) model probably trained for quite some time back in the day (2019, ~5 years ago), today, reproducing it is a matter of ~1hr and ~$10. You'll need a cloud GPU box if you don't have enough, for that I recommend Lambda.

Note that GPT-2 and GPT-3 and both simple language models, trained on internet documents, and all they do is "dream" internet documents. So this repo/video this does not cover Chat finetuning, and you can't talk to it like you can talk to ChatGPT. The finetuning process (while quite simple conceptually - SFT is just about swapping out the dataset and continuing the training) comes after this part and will be covered at a later time. For now this is the kind of stuff that the 124M model says if you prompt it with "Hello, I'm a language model," after 10B tokens of training:

```
Hello, I'm a language model, and my goal is to make English as easy and fun as possible for everyone, and to find out the different grammar rules
Hello, I'm a language model, so the next time I go, I'll just say, I like this stuff.
Hello, I'm a language model, and the question is, what should I do if I want to be a teacher?
Hello, I'm a language model, and I'm an English person. In languages, "speak" is really speaking. Because for most people, there's
```

And after 40B tokens of training:

```
Hello, I'm a language model, a model of computer science, and it's a way (in mathematics) to program computer programs to do things like write
Hello, I'm a language model, not a human. This means that I believe in my language model, as I have no experience with it yet.
Hello, I'm a language model, but I'm talking about data. You've got to create an array of data: you've got to create that.
Hello, I'm a language model, and all of this is about modeling and learning Python. I'm very good in syntax, however I struggle with Python due
```

Lol. Anyway, once the video comes out, this will also be a place for FAQ, and a place for fixes and errata, of which I am sure there will be a number :)

For discussions and questions, please use Discussions tab, and for faster communication, have a look at my Zero To Hero Discord, channel #nanoGPT:

# Video

Let's reproduce GPT-2 (124M) YouTube lecture

# Errata

Minor cleanup, we forgot to delete register_buffer of the bias once we switched to flash attention, fixed with a recent PR.

Earlier version of PyTorch may have difficulty converting from uint16 to long. Inside load_tokens, we added npt = npt.astype(np.int32) to use numpy to convert uint16 to int32 before converting to torch tensor and then converting to long.

The torch.autocast function takes an arg device_type, to which I tried to stubbornly just pass device hoping it works ok, but PyTorch actually really wants just the type and creates errors in some version of PyTorch. So we want e.g. the device cuda:3 to get stripped to cuda. Currently, device mps (Apple Silicon) would become device_type CPU, I'm not 100% sure this is the intended PyTorch way.

Confusingly, model.require_backward_grad_sync is actually used by both the forward and backward pass. Moved up the line so that it also gets applied to the forward pass.

# Prod

For more production-grade runs that are very similar to nanoGPT, I recommend looking at the following repos:

- litGPT
- TinyLlama

# FAQ

# License

MIT

ä»¥ä¸‹æ˜¯ä½ æä¾›å†…å®¹çš„ä¸­æ–‡ç¿»è¯‘ï¼š

---

## build-nanoGPT

è¿™ä¸ªä»“åº“æ˜¯å¯¹ **nanoGPT** ä»é›¶å®ç°çš„å¤ç°ã€‚Git æäº¤å†å²è¢«ç²¾å¿ƒä¿ç•™ä¸ºä¸€æ­¥æ­¥ã€å¹²å‡€æ•´æ´çš„è®°å½•ï¼Œæ–¹ä¾¿å¤§å®¶é€æ­¥æŸ¥çœ‹æ•´ä¸ªæ„å»ºè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘è¿˜åœ¨ YouTube ä¸Šå½•åˆ¶äº†ä¸€æ®µè§†é¢‘è®²è§£ï¼Œä½ å¯ä»¥çœ‹åˆ°æˆ‘ä¾æ¬¡ä»‹ç»æ¯ä¸ªæäº¤ï¼Œå¹¶è§£é‡Šå…¶ä¸­çš„æ¯ä¸ªéƒ¨åˆ†ã€‚

æˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯ä»ä¸€ä¸ªç©ºæ–‡ä»¶å¼€å§‹ï¼Œæœ€ç»ˆå¤ç°äº† GPT-2ï¼ˆ124Mï¼‰æ¨¡å‹ã€‚å¦‚æœä½ æœ‰æ›´å¤šè€å¿ƒæˆ–é¢„ç®—ï¼Œä¹Ÿå¯ä»¥ç”¨è¿™äº›ä»£ç å¤ç° GPT-3 æ¨¡å‹ã€‚è™½ç„¶å½“å¹´ï¼ˆ2019 å¹´ï¼Œä¹Ÿå°±æ˜¯å¤§çº¦ 5 å¹´å‰ï¼‰è®­ç»ƒ GPT-2ï¼ˆ124Mï¼‰å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼Œä½†å¦‚ä»Šå¤ç°å®ƒåªéœ€è¦å¤§çº¦ 1 å°æ—¶å’Œçº¦ 10 ç¾å…ƒçš„æˆæœ¬ã€‚å¦‚æœä½ æœ¬åœ°æ²¡æœ‰è¶³å¤Ÿçš„ GPUï¼Œå»ºè®®ä½¿ç”¨äº‘ç«¯ GPUï¼Œæ¯”å¦‚ [Lambda](https://lambdalabs.com)ã€‚

æ³¨æ„ï¼šGPT-2 å’Œ GPT-3 éƒ½æ˜¯ç®€å•çš„è¯­è¨€æ¨¡å‹ï¼Œå®ƒä»¬çš„è®­ç»ƒæ•°æ®æ¥è‡ªäº’è”ç½‘æ–‡æ¡£ï¼Œè€Œå®ƒä»¬çš„ä½œç”¨åªæ˜¯â€œæ¢¦æƒ³â€äº’è”ç½‘æ–‡æ¡£ï¼ˆå³æ¨¡æ‹Ÿäº’è”ç½‘å†…å®¹çš„ç”Ÿæˆï¼‰ã€‚å› æ­¤ï¼Œè¿™ä¸ªä»“åº“/è§†é¢‘å¹¶ä¸æ¶‰åŠ Chat å¾®è°ƒï¼Œæ‰€ä»¥ä½ æ— æ³•åƒä½¿ç”¨ ChatGPT ä¸€æ ·â€œä¸å®ƒå¯¹è¯â€ã€‚å¾®è°ƒæµç¨‹ï¼ˆè™½ç„¶æ¦‚å¿µä¸Šå¾ˆç®€å•â€”â€”æ¯”å¦‚ç›‘ç£å¼å¾®è°ƒ SFT å°±æ˜¯æ›´æ¢æ•°æ®é›†å¹¶ç»§ç»­è®­ç»ƒï¼‰å±äºåç»­å†…å®¹ï¼Œå°†åœ¨ä»¥åè®²è§£ã€‚

ç›®å‰ï¼Œä»¥ä¸‹æ˜¯æˆ‘ä»¬ç”¨â€œHello, I'm a language modelâ€ä½œä¸ºæç¤ºè¯ï¼Œåœ¨è®­ç»ƒäº† **100 äº¿ä¸ª token** åï¼Œæ¨¡å‹çš„è¾“å‡ºç¤ºä¾‹ï¼š

> Hello, I'm a language model, and my goal is to make English as easy and fun as possible for everyone, and to find out the different grammar rules
> Hello, I'm a language model, so the next time I go, I'll just say, I like this stuff.
> Hello, I'm a language model, and the question is, what should I do if I want to be a teacher?
> Hello, I'm a language model, and I'm an English person. In languages, "speak" is really speaking. Because for most people, there's

è€Œåœ¨è®­ç»ƒäº† **400 äº¿ä¸ª token** åçš„è¾“å‡ºæ˜¯ï¼š

> Hello, I'm a language model, a model of computer science, and it's a way (in mathematics) to program computer programs to do things like write
> Hello, I'm a language model, not a human. This means that I believe in my language model, as I have no experience with it yet.
> Hello, I'm a language model, but I'm talking about data. You've got to create an array of data: you've got to create that.
> Hello, I'm a language model, and all of this is about modeling and learning Python. I'm very good in syntax, however I struggle with Python due

å“ˆå“ˆï¼Œæ€»ä¹‹ï¼Œè§†é¢‘å‘å¸ƒåï¼Œè¿™é‡Œä¹Ÿä¼šæˆä¸º FAQ çš„æ•´ç†åœ°ï¼Œä»¥åŠå‘å¸ƒä¿®å¤å’Œå‹˜è¯¯ï¼ˆåº”è¯¥ä¼šæœ‰ä¸å°‘ :)ï¼‰ã€‚

å¦‚æœ‰é—®é¢˜æˆ–è®¨è®ºï¼Œè¯·ä½¿ç”¨ GitHub çš„ Discussions é¡µé¢ã€‚è‹¥æƒ³æ›´å¿«äº¤æµï¼Œä¹Ÿå¯ä»¥åŠ å…¥æˆ‘åœ¨ Zero To Hero ç³»åˆ—ä¸­çš„ Discordï¼Œé¢‘é“æ˜¯ **#nanoGPT**ã€‚

---

### ğŸ¬ è§†é¢‘

* [Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=kCc8FmEb1nY) YouTube è®²è§£è§†é¢‘

---

### âš ï¸ å‹˜è¯¯ï¼ˆErrataï¼‰

* æœ‰ä¸ªå°é—®é¢˜æ˜¯æˆ‘ä»¬åœ¨å¯ç”¨ flash attention åï¼Œå¿˜è®°åˆ é™¤ `bias` çš„ `register_buffer`ï¼Œå·²é€šè¿‡è¿‘æœŸçš„ PR ä¿®å¤ã€‚
* è¾ƒæ—§ç‰ˆæœ¬çš„ PyTorch åœ¨å°† `uint16` è½¬ä¸º `long` æ—¶å¯èƒ½æœ‰é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨ `load_tokens` å‡½æ•°ä¸­å¢åŠ äº†ä¸€è¡Œï¼š
  `npt = npt.astype(np.int32)`
  å…ˆç”¨ NumPy è½¬ä¸º int32ï¼Œå†è½¬ä¸º torch tensorï¼Œæœ€åè½¬ä¸º longã€‚
* `torch.autocast` å‡½æ•°éœ€è¦ä¸€ä¸ª `device_type` å‚æ•°ï¼Œæˆ‘æœ€åˆç›´æ¥ä¼ äº† `device`ï¼Œç»“æœåœ¨æŸäº› PyTorch ç‰ˆæœ¬ä¸­æŠ¥é”™ã€‚æ­£ç¡®æ–¹å¼æ˜¯å°† `cuda:3` è¿™æ ·çš„è®¾å¤‡ç®€åŒ–ä¸º `cuda`ã€‚
* Apple Silicon ä¸Šçš„ `mps` è®¾å¤‡ä¼šè¢«è¯†åˆ«ä¸º CPU çš„ `device_type`ï¼Œæˆ‘ä¸å¤ªç¡®å®š PyTorch æ˜¯å¦å°±æ˜¯è¿™ä¹ˆè®¾è®¡çš„ã€‚
* `model.require_backward_grad_sync` å®é™…ä¸Šåœ¨æ­£å‘å’Œåå‘ä¼ æ’­ä¸­éƒ½ä¼šç”¨åˆ°ï¼Œå› æ­¤æˆ‘ä»¬å°†è¿™è¡Œä»£ç ä¸Šç§»ï¼Œä»¥ç¡®ä¿åœ¨æ­£å‘ä¼ æ’­ä¸­ä¹Ÿç”Ÿæ•ˆã€‚

---

### ğŸš€ ç”Ÿäº§çº§å»ºè®®

å¦‚æœä½ æƒ³è¿›è¡Œæ›´åŠ â€œç”Ÿäº§çº§â€çš„è®­ç»ƒï¼Œè¿™äº›ä»“åº“ä¸ nanoGPT éå¸¸ç±»ä¼¼ï¼Œæ¨èæŸ¥çœ‹ï¼š

* [litGPT](https://github.com/Lightning-AI/litgpt)
* [TinyLlama](https://github.com/jzhang38/TinyLlama)

---

### ğŸ“„ License

MIT è®¸å¯è¯

---

å¦‚æœä½ éœ€è¦æˆ‘å¸®ä½ å½’çº³é‡ç‚¹æ­¥éª¤ï¼Œæˆ–è€…ç¿»è¯‘ç›¸å…³ä»£ç ç»“æ„ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®å¿™ã€‚

# intro: Letâ€™s reproduce GPT-2 (124M)

hi everyone so today we are going to be continuing our Zero to Hero series and in particular today we are going to
reproduce the gpt2 model the 124 million version of it so when openi released
gpt2 this was 2019 and they released it with this blog post on top of that they
released this paper and on top of that they released this code on GitHub so open a/
gpt2 now when we talk about reproducing gpt2 we have to be careful because in particular in this video we're going to
be reproducing the 124 million parameter model so the thing to realize is that there's always a miniseries when these
are releases are made so there are the gpt2 miniseries made up of models at
different sizes and usually the biggest model is called the gpt2 but basically the reason we do that
is because you can put the model sizes on the x-axis of plots like this and on the Y AIS you put a lot of uh Downstream
metrics that you're interested in like translation summarization question answering and so on and you can chart out these scaling laws so basically as
the model size increases you're getting better and better at Downstream metrics and so in particular for
gpt2 if we scroll down in paper there are four models in the gpt2 miniseries
starting at 124 million all the way up to 1558 million now the reason my
numbers the way I say them disagree with this table is that this table is wrong if you actually go to the uh gpt2 uh
GitHub repo they sort of say that um there was an error in how they added up the parameters but basically this is the
124 million parameter model Etc so the 124 million parameter had 12 layers in
the Transformer and it had 768 channels in the Transformer 768 dimensions and
I'm going to be assuming some familiarity with what these terms mean because I covered all of this in my previous video let's build gpt2 uh let's
build GPT from scratch so I covered that in the previous video in this playlist now if we do everything correctly and
everything works out well by the end of this video we're going to see something like this where we're looking at the
validation loss which basically um measures how good we are at predicting
the next token in a sequence on some validation data that the model has not seen during training and we see that we
go from doing that task not very well because we're initializing from scratch all the way to doing that task quite
well um by the end of the training and hopefully we're going to beat the gpt2 uh 124 M model
now previously when they were working on this this is already 5 years ago so this was probably a fairly complicated
optimization at the time and the gpus and the compute was a lot smaller today you can reproduce this model in roughly
an hour or probably less even and it will cost you about 10 bucks if you want to do this on the cloud uh Cloud Compu a
sort of computer that you can all rent and if you pay $10 for that computer you wait about an hour or less you can
actually achieve a model that is as good as this model that open ey released and
uh one more thing to mention is unlike many other models open ey did release the weights for gpt2 so those weights
are all available in this repository but the gpt2 paper is not always as good
with all of the details of training so in addition to the gpt2 paper we're going to be referencing the gpt3 paper
which is a lot more Concrete in a lot of the hyp parameters and optimization settings and so on um and it's not a
huge departure in the architecture from the GPT 2 uh version of the model so we're going to be referencing both gpt2
and gpt3 as we try to reproduce gpt2 124 M uh so let's go so the first thing I

å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯ä½ è¿™æ®µè§†é¢‘å†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

# å¯¼è¯­ï¼šæˆ‘ä»¬æ¥å¤ç° GPT-2ï¼ˆ124Mï¼‰

å¤§å®¶å¥½ï¼Œä»Šå¤©æˆ‘ä»¬å°†ç»§ç»­ã€ŠZero to Heroï¼ˆä»é›¶åˆ°é«˜æ‰‹ï¼‰ã€‹ç³»åˆ—è¯¾ç¨‹ï¼Œè¿™ä¸€èŠ‚çš„ä¸»é¢˜æ˜¯ï¼š**å¤ç° GPT-2 æ¨¡å‹ï¼ˆ124M å‚æ•°ç‰ˆæœ¬ï¼‰**ã€‚

### ä»€ä¹ˆæ˜¯ GPT-2 124Mï¼Ÿ

åœ¨ 2019 å¹´ï¼ŒOpenAI å‘å¸ƒäº† GPT-2ã€‚ä»–ä»¬åŒæ—¶å‘å¸ƒäº†ï¼š

* ä¸€ç¯‡åšå®¢æ–‡ç« ä»‹ç»æ¨¡å‹ï¼›
* ä¸€ç¯‡è®ºæ–‡è®²è§£æ¶æ„ï¼›
* ä»¥åŠæ¨¡å‹ä»£ç ï¼ˆå¼€æºåœ¨ GitHub ä¸Šï¼Œåœ°å€æ˜¯ `openai/gpt-2`ï¼‰ã€‚

GPT-2 å®é™…ä¸Šå¹¶ä¸åªæŒ‡ä¸€ä¸ªæ¨¡å‹ï¼Œè€Œæ˜¯ä¸€æ•´ä¸ªâ€œè¿·ä½ ç³»åˆ—â€ï¼ˆminiseriesï¼‰â€”â€”å®ƒåŒ…å«äº†å¤šä¸ªè§„æ¨¡çš„æ¨¡å‹ã€‚æœ€å¤§çš„ä¸€ä¸ªé€šå¸¸æ‰è¢«ç®€ç§°ä¸ºâ€œGPT-2â€ï¼Œä½†å®é™…ä¸Šæ˜¯å¤šç§ä¸åŒè§„æ¨¡çš„ç‰ˆæœ¬ç»„åˆã€‚

### ä¸ºä»€ä¹ˆè¦å¤ç° GPT-2ï¼Ÿ

å¤ç° GPT-2 æ˜¯å› ä¸ºå®ƒæ˜¯ Transformer æ¶æ„ä¸­éå¸¸ç»å…¸ã€å½±å“æ·±è¿œçš„æ¨¡å‹ä¹‹ä¸€ã€‚è€Œä¸”è¿™äº›æ¨¡å‹éƒ½éµå¾ªæŸç§â€œ**æ‰©å±•è§„å¾‹**â€ï¼ˆScaling Lawsï¼‰â€”â€”ä½ å¯ä»¥æŠŠæ¨¡å‹çš„å‚æ•°é‡æ”¾åœ¨ X è½´ï¼Œç„¶ååœ¨ Y è½´ä¸Šæ”¾å„ç§ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½æŒ‡æ ‡ï¼Œæ¯”å¦‚ç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”ç­‰ç­‰ã€‚ä½ ä¼šå‘ç°ï¼š**æ¨¡å‹è¶Šå¤§ï¼Œæ•ˆæœè¶Šå¥½**ã€‚

æ‰€ä»¥ï¼Œåœ¨ GPT-2 ç³»åˆ—ä¸­ï¼Œæœ‰å››ä¸ªæ¨¡å‹ï¼Œä» 124M å‚æ•°èµ·æ­¥ï¼Œä¸€ç›´åˆ° 1558M å‚æ•°ï¼ˆçº¦ 15 äº¿ï¼‰ã€‚ä¸è¿‡ï¼Œè§†é¢‘ä¸­ä¹Ÿæåˆ°ä¸€ä¸ªå°é—®é¢˜ï¼šGPT-2 è®ºæ–‡ä¸­çš„å‚æ•°è¡¨æ˜¯é”™çš„ï¼ŒOpenAI åæ¥åœ¨ GitHub ä¸Šæ›´æ­£äº†è¿™äº›æ•°å€¼ã€‚æˆ‘ä»¬è¿™æ¬¡å¤ç°çš„æ˜¯æœ€å°çš„é‚£ä¸ªï¼š**124M å‚æ•°çš„ GPT-2 æ¨¡å‹**ã€‚

è¿™ä¸ª 124M æ¨¡å‹çš„æ¶æ„åŒ…æ‹¬ï¼š

* 12 å±‚ Transformer å±‚ï¼›
* æ¯å±‚çš„ç»´åº¦æ˜¯ 768ï¼›
  è¿™äº›æŠ€æœ¯ç»†èŠ‚åœ¨å‰ä¸€èŠ‚è§†é¢‘ï¼ˆä»é›¶å®ç° GPTï¼‰ä¸­å·²ç»è®²è§£è¿‡äº†ï¼Œè¿™é‡Œé»˜è®¤å¤§å®¶å·²ç»æœ‰ä¸€å®šçš„åŸºç¡€ã€‚

### ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ

æˆ‘ä»¬å¸Œæœ›ä» **å®Œå…¨ç©ºç™½** å¼€å§‹å®ç° GPT-2 æ¨¡å‹ï¼Œç„¶åè®­ç»ƒå®ƒï¼Œæœ€ååœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°ä¸€ä¸ªä¸é”™çš„æ•ˆæœã€‚éªŒè¯æŸå¤±ï¼ˆvalidation lossï¼‰æ˜¯æˆ‘ä»¬è¯„ä¼°æ¨¡å‹çš„æ ‡å‡†ï¼Œä»£è¡¨å®ƒé¢„æµ‹ä¸‹ä¸€ä¸ª token çš„èƒ½åŠ›ã€‚è®­ç»ƒå¼€å§‹æ—¶ï¼Œæ¨¡å‹å‡ ä¹ä»€ä¹ˆéƒ½ä¸ä¼šï¼Œä½†è®­ç»ƒåˆ°æœ€åï¼Œåº”è¯¥èƒ½æ¯”è‚©ç”šè‡³è¶…è¿‡ OpenAI æä¾›çš„åŸå§‹ GPT-2 124M æ¨¡å‹çš„æ•ˆæœã€‚

### æ‰€éœ€èµ„æºå’ŒèŠ±è´¹

è™½ç„¶ OpenAI å½“å¹´è®­ç»ƒè¿™ä¸ªæ¨¡å‹å¯èƒ½èŠ±äº†ä¸å°‘æ—¶é—´å’Œè®¡ç®—èµ„æºï¼ˆé‚£æ˜¯ 5 å¹´å‰çš„äº‹äº†ï¼‰ï¼Œä½† **ç°åœ¨åªéœ€çº¦ 1 å°æ—¶å’Œå¤§çº¦ 10 ç¾å…ƒ**ï¼ˆä½¿ç”¨äº‘ç«¯ GPUï¼‰å°±å¯ä»¥è®­ç»ƒå‡ºä¸€ä¸ªåŒç­‰æ°´å¹³çš„æ¨¡å‹äº†ã€‚å¦‚æœä½ æ²¡æœ‰æœ¬åœ° GPUï¼Œå¯ä»¥ç”¨åƒ Lambda è¿™æ ·çš„äº‘æœåŠ¡ç§Ÿç”¨è®¡ç®—èµ„æºã€‚

### å…³äºæƒé‡å’Œå‚è€ƒèµ„æ–™

OpenAI å‘å¸ƒäº† GPT-2 çš„æ¨¡å‹æƒé‡ï¼ˆweightsï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç›´æ¥åŠ è½½å¹¶å‚è€ƒè¿™äº›æƒé‡ã€‚ä½†é—®é¢˜æ˜¯ï¼ŒGPT-2 çš„è®ºæ–‡é‡Œæ²¡æœ‰è¯¦ç»†å†™å‡ºæ‰€æœ‰çš„è®­ç»ƒç»†èŠ‚ã€‚å› æ­¤æˆ‘ä»¬è¿˜ä¼šå‚è€ƒ **GPT-3 çš„è®ºæ–‡**ï¼Œå› ä¸ºå®ƒåœ¨è¶…å‚æ•°è®¾ç½®å’Œä¼˜åŒ–æ–¹æ³•ä¸Šæè¿°å¾—æ›´å…·ä½“ã€‚è€Œä¸” GPT-3 çš„æ¶æ„ä¸ GPT-2 å¹¶æ²¡æœ‰æœ¬è´¨åŒºåˆ«ã€‚

---

æ€»ä¹‹ï¼Œè¿™èŠ‚è§†é¢‘çš„ç›®æ ‡æ˜¯ï¼š**ä»é›¶å¼€å§‹å®ç° GPT-2ï¼ˆ124Mï¼‰æ¨¡å‹ï¼Œå¹¶æˆåŠŸè®­ç»ƒå®ƒ**ï¼ŒæœŸé—´è¿˜ä¼šå‚è€ƒ GPT-2 å’Œ GPT-3 çš„è®­ç»ƒç»éªŒã€‚

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥ç»§ç»­è§£é‡Šè§†é¢‘ä¸­çš„å…¶ä»–ç« èŠ‚ï¼Œæ¯”å¦‚æ¨¡å‹æ„å»ºã€è®­ç»ƒè¿‡ç¨‹ã€æ€§èƒ½ä¼˜åŒ–ç­‰ç­‰ã€‚


# exploring the GPT-2 (124M) OpenAI checkpoint

would like to do is actually start at the end or at the Target so in other words let's load the GPT to 124 M model
as it was released by openi and maybe take it for a spin let's sample some tokens from it now the issue with that
is when you go into the code base of gpt2 and you go into the source and you click in on the model. pi you'll realize
that actually this is using tensorflow so the original gpt2 code here was written in tensor flow which is
um you know not let's just say not used as much anymore um so we'd like to use
pytorch uh because it's a lot friendlier easier and I just personally like a lot more the problem with that is the
initial code is intenser flow we'd like to use pytorch so instead uh to get the target we're going to use the hugging
face Transformers um code which I like a lot more so when you go into the
Transformers source Transformers models gpt2 modeling gpt2 Pi you will see that
they have the gpt2 implementation of that Transformer here in this file um and it's like medium readable
but not fully readable um but what it does is it did all the work of converting all those weights uh from
tensor flow to pytorch Friendly and so it's much easier to load and work with so in particular we can look at the
gpt2 um model here and we can load it using hugging face Transformers so swinging over this is what that looks
like from Transformers import the DP GT2 LM head model and then from pre-train
gpt2 uh now one awkward thing about this is that when you do gpt2 as the model that we're loading this actually is the
124 million parameter model if you want the actual the gpt2 the 1.5 billion then
you actually want to do- XL so this is the 12 4 M our Target now what we're
doing is when we actually get this we're initializing the uh pytorch NN module as defined here in this
class from it I want to get just the state dict which is just a raw tensors
so we just have um the tensors of that file and by the way here this is a jupyter notebook uh but this is jupyter
notebook running inside vs code uh so I like to work with it all in a single sort of interface so I like to use vs
code so this is the jupyter notebook extension inside the es
code so when we get the state dick this is just a dict so we can print the key
and the value which is the tensor and let's just look at the shapes so these are sort of
the uh different parameters inside the gbt2 model and their shape so the W
weight for token embedding is of size
50257 by 768 where this is coming from is that we have 50257 tokens in the gpt2 vocabulary um
and the tokens by the way these are exactly the tokens that we spoken about in the previous video on my tokenization
Series so the previous videos just before this I go into a ton of detail on tokenization gpt2 tokenizer happens to
have this many tokens for each token we have a 768 dimensional
embedding that is the distributed representation that stands in for that token so each token is a little string
piece and then the 768 numbers are the vector that represents that
token and so this is just our lookup table for tokens and then here we have the lookup table for the positions so
because gbt2 has a maximum sequence length of 1024 we have up to 1,24 positions that
each token can be attending to in the past and every one of those positions in gpd2 has a fixed Vector of
768 that is learned by optimization um and so this is the
position embedding and the token embedding um and then everything here is just the other weights and biases and
everything else of this Transformer so when you just take for example the positional embeddings and
flatten it out and take just the 20 elements you can see that these are just the parameters these are weights floats
just we can take and we can plot them so these are the position embeddings and we
get something like this and you can see that this has structure and it has structure because what we what we have
here really is every Row in this visualization is a different position a
fixed absolute position in um the range from 0 to 1024 and each row here is the
representation of that position and so it has structure because these positional embeddings end up learning
these sinusoids and cosiness um that sort of like represent each of these
positions and uh each row here stands in for that position and is processed by the Transformer to recover all the
relative positions and uh sort of realize which token is where and um
attend to them depending on their position not just their content so when we actually just look
into an individual column inside these and I just grabbed three random columns
you'll see that for example here we are focusing on every every single um
Channel and we're looking at what that channel is doing as a
function of uh position from one from Z to 1223
really and we can see that some of these channels basically like respond more or less to different parts of the position
Spectrum so this green channel uh really likes to fire for everything after 200
uh up to 800 but not less a lot less and has a sharp drop off here near zero so
who knows what these embeddings are doing and why they are the way they are you can tell for example that because they're a bit more Jagged and they're
kind of noisy you can tell that this model was not fully trained and the more trained this model was the more you
would expect to smooth this out and so this is telling you that this is a little bit of an undertrained model um
but in principle actually these curves don't even have to be smooth this should just be totally random noise and in fact
in the beginning of the optimization it is complete random noise because this position embedding table is initialized
completely at random so in the beginning you have jaggedness and the fact that you end up with something smooth is
already kind of impressive um that that just falls out of the optimization because in principle you shouldn't even
be able to get any single graph out of this that makes sense but we actually get something that looks a little bit
noisy but for the most part looks sinusoidal like um in the original Transformer um in the original
Transformer paper the attention is all you need paper the positional embeddings are actually initialized and fixed if I
remember correctly to sinusoids and cosiness of uh different frequencies and that's the positional coding and it's
fixed but in gpt2 these are just parameters and they're trained from scratch just like any other parameter uh
and that seems to work about as well and so what they do is they kind of like recover these sinusoidal like features
during the optimization we can also look at any of the other matrices here so here I took
the first layer of the Transformer and looking at like one of
its weights and just the first block of 300 by 300 and you see some structure
but like again like who knows what any of this is if you're into mechanistic interpretability you might get a real
kick out of trying to figure out like what is going on what is this structure and what does this all mean but we're
not going to be doing that in this video but we definitely see that there's some interesting structure and that's kind of cool what we're mostly interested in is
we've loaded the weights of this model that was released by open Ai and now using the hogging face Transformers we
can not just get all the raw weights but we can also get the um what they call
Pipeline and sample from it so this is the prefix hello I'm a language model
comma and then we're sampling uh 30 tokens and we getting five sequences and
I ran this and this is what it produced um hell language model but what I'm really doing is
making a human readable document there are other languages but those are dot dot dot so you can read through these if
you like but basically these are five different completions of the same prefix from this uh gbt
2124m now uh if I go here I took this example from here and sadly even though
we are fixing the seed we are getting different Generations from the snippet than what they got so presumably the
code changed um but what we see though at this stage that's important is that
we are getting coherent text so we've loaded the model successfully we can look at all its parameters and the keys
tell us where in the model these come from and we want to actually write our
own gpt2 class so that we have full understanding of what's happening there we don't want to be working with something like uh the modeling gpt2 Pi
because it's just too complicated we want to write this from scratch ourselves so we're going to be implementing the GPT model here in
parallel and as our first task let's load the gpt2 124 M into the class that
we're going to develop here from scratch that's going to give us confidence that we can load the open ey model and
therefore there's a setting of Weights that exactly is the 124 model but then of course what we're going to do is
we're going to initialize the model from scratch instead and try try to train it ourselves um on a bunch of documents
that we're going to get and we're going to try to surpass that model so we're going to get different weights and everything's going to look different
hopefully better even um but uh we're going to have a lot of confidence that because we can load the
openi model we are in the same model family and model class and we just have to ReDiscover a good setting of the
weights uh but from scratch so let's now write the gbt2 model and let's load the
weights and make sure that we can also generate text that looks coherent okay so let's now swing over to the attention

å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µè§†é¢‘å†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## æ¢ç´¢ OpenAI çš„ GPT-2ï¼ˆ124Mï¼‰æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆCheckpointï¼‰

æˆ‘ä»¬è¿™æ¬¡çš„åšæ³•æœ‰ç‚¹â€œåå‘â€ï¼Œ**ä»ç»ˆç‚¹å¼€å§‹**ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å…ˆæ¥åŠ è½½ OpenAI å‘å¸ƒçš„ GPT-2 124M æ¨¡å‹ï¼Œç„¶åå°è¯•ç”¨å®ƒæ¥ç”Ÿæˆä¸€äº›æ–‡æœ¬ï¼Œçœ‹çœ‹æ•ˆæœã€‚

### OpenAI çš„åŸå§‹ GPT-2 ä»£ç ç”¨çš„æ˜¯ TensorFlow

å½“ä½ å»æŸ¥çœ‹ OpenAI æä¾›çš„ GPT-2 ä»“åº“æ—¶ï¼Œä¼šå‘ç°ä»–ä»¬æœ€åˆæ˜¯ç”¨ **TensorFlow** å†™çš„ï¼ˆæ¯”å¦‚ `model.py` æ–‡ä»¶ï¼‰ã€‚
ä½†ç°åœ¨ TensorFlow å·²ç»ä¸å¤ªå¸¸ç”¨äº†ï¼Œ**PyTorch æ›´å—æ¬¢è¿ã€æ˜“ç”¨æ€§æ›´å¥½**ï¼Œæˆ‘ä¸ªäººä¹Ÿæ›´å–œæ¬¢ PyTorchã€‚

### è½¬è€Œä½¿ç”¨ Hugging Face Transformers å®ç°

ä¸ºäº†è§£å†³ TensorFlow ä¸æ–¹ä¾¿çš„é—®é¢˜ï¼Œæˆ‘ä»¬é€‰æ‹©ä½¿ç”¨ Hugging Face çš„ Transformers åº“ï¼Œå®ƒé‡Œé¢å·²ç»å¸®æˆ‘ä»¬æŠŠ GPT-2 çš„ TensorFlow æƒé‡**è½¬æ¢æˆäº† PyTorch å¯ç”¨æ ¼å¼**ã€‚

å…·ä½“åœ°ï¼Œæˆ‘ä»¬æŸ¥çœ‹ Transformers æºç ä¸­çš„ï¼š

```
transformers/models/gpt2/modeling_gpt2.py
```

è¿™ä¸ªæ–‡ä»¶ä¸­å®ç°äº† GPT-2 çš„ PyTorch ç‰ˆæœ¬ï¼ˆè™½ç„¶ä»£ç æœ‰ç‚¹å¤æ‚ï¼Œä½†å¯è¯»æ€§è¿˜è¡Œï¼‰ã€‚

æˆ‘ä»¬å¯ä»¥åƒè¿™æ ·ä½¿ç”¨å®ƒï¼š

```python
from transformers import GPT2LMHeadModel

model = GPT2LMHeadModel.from_pretrained("gpt2")
```

> è¿™é‡Œçš„ `"gpt2"` å°±æ˜¯ 124M å‚æ•°ç‰ˆæœ¬ï¼ˆæ³¨æ„ä¸æ˜¯æœ€å¤§é‚£ä¸ª 1.5B æ¨¡å‹ï¼Œè¦åŠ è½½ 1.5B çš„è¯ç”¨ `"gpt2-xl"`ï¼‰ã€‚

åŠ è½½ä¹‹åï¼Œè¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch çš„ `nn.Module` ç±»å‹ï¼Œæˆ‘ä»¬å¯ä»¥æå–å®ƒçš„ **`state_dict`ï¼ˆçŠ¶æ€å­—å…¸ï¼‰**ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹æ‰€æœ‰çš„å‚æ•°å¼ é‡ï¼ˆtensorï¼‰ã€‚
åœ¨ Jupyter Notebookï¼ˆæˆ‘æ˜¯åœ¨ VSCode é‡Œè¿è¡Œçš„ï¼‰ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè¿™äº›å¼ é‡çš„é”®ï¼ˆå‚æ•°åï¼‰å’Œå€¼ï¼ˆå½¢çŠ¶ï¼‰æ‰“å°å‡ºæ¥çœ‹çœ‹ã€‚

### æ¨¡å‹çš„å‚æ•°ç»„æˆ

ä¸¾ä¸ªä¾‹å­ï¼š

```python
wte.weight.shape  # è¾“å‡º: [50257, 768]
```

* `50257` æ˜¯ GPT-2 çš„è¯è¡¨å¤§å°ï¼›
* `768` æ˜¯æ¯ä¸ª token çš„åµŒå…¥ç»´åº¦ï¼ˆembedding sizeï¼‰ï¼›
* æ‰€ä»¥è¿™å¼ é‡å°±æ˜¯è¯åµŒå…¥çŸ©é˜µï¼ˆtoken embedding tableï¼‰ã€‚

åŒç†ï¼Œè¿˜æœ‰ä½ç½®åµŒå…¥ï¼ˆpositional embeddingï¼‰ï¼Œå› ä¸º GPT-2 æœ€é•¿åºåˆ—é•¿åº¦æ˜¯ 1024ï¼Œæ‰€ä»¥å®ƒè¿˜æœ‰ä¸€ä¸ªå¤§å°ä¸º `[1024, 768]` çš„ä½ç½®åµŒå…¥çŸ©é˜µã€‚

è¿™ä¸¤ä¸ªåµŒå…¥æ˜¯æ¨¡å‹è¾“å…¥é˜¶æ®µéå¸¸å…³é”®çš„éƒ¨åˆ†ã€‚

### å¯è§†åŒ–ä½ç½®åµŒå…¥

æˆ‘ä»¬å¯ä»¥æŠŠä½ç½®åµŒå…¥çš„å‰ 20 ä¸ªå‘é‡ç”»æˆå›¾ï¼Œå¯ä»¥çœ‹åˆ°å®ƒä»¬**å…·æœ‰ä¸€å®šçš„ç»“æ„**ï¼Œè€Œä¸æ˜¯å®Œå…¨çš„éšæœºå™ªå£°ã€‚

æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªä½ç½®ï¼ˆä» 0 åˆ° 1023ï¼‰ï¼Œæ¯è¡Œçš„ 768 ä¸ªæ•°ä»£è¡¨è¿™ä¸ªä½ç½®çš„å‘é‡è¡¨ç¤ºã€‚
è¿™äº›å‘é‡è¢«æ¨¡å‹ç”¨æ¥åˆ¤æ–­æ¯ä¸ª token çš„ç›¸å¯¹ä½ç½®ã€‚

è¿›ä¸€æ­¥åœ°ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€‰å‡ºå…¶ä¸­çš„å‡ åˆ—ï¼ˆå‡ ä¸ªç»´åº¦ï¼‰æ¥çœ‹å®ƒä»¬åœ¨ä¸åŒä½ç½®ä¸Šçš„å“åº”æƒ…å†µã€‚ä½ ä¼šå‘ç°å®ƒä»¬åœ¨æŸäº›ä½ç½®èŒƒå›´å†…â€œæ¿€æ´»â€æ›´å¤šâ€”â€”è¯´æ˜å®ƒä»¬**å­¦ä¹ åˆ°äº†ä¸€å®šçš„ä½ç½®æ„ŸçŸ¥èƒ½åŠ›**ã€‚

> è¿™å¾ˆåƒæœ€åˆ Transformer è®ºæ–‡ä¸­ç”¨çš„ `sin`/`cos` ä½ç½®ç¼–ç ï¼Œä½† GPT-2 æ˜¯**å­¦ä¹ å‡ºæ¥çš„**è€Œä¸æ˜¯å›ºå®šå‡½æ•°ã€‚

è€Œä¸”å¯ä»¥çœ‹å‡ºè¿™äº›å‘é‡æ¯”è¾ƒâ€œæŠ–åŠ¨â€æˆ–â€œä¸å…‰æ»‘â€ï¼Œè¯´æ˜è¿™ä¸ªæ¨¡å‹æ²¡æœ‰å®Œå…¨è®­ç»ƒå¥½ï¼ˆå¯èƒ½åªæ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å¸ƒçš„æ¨¡å‹ï¼‰ã€‚è®­ç»ƒè¶Šå……åˆ†ï¼Œè¿™äº›å‘é‡çš„ç»“æ„å°±è¶Šå¹³æ»‘ã€‚

### æŸ¥çœ‹å…¶ä»–æƒé‡çŸ©é˜µ

æˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹æ¯”å¦‚ Transformer ç¬¬ä¸€å±‚ä¸­æŸä¸ªçº¿æ€§å˜æ¢æƒé‡çŸ©é˜µçš„ä¸€éƒ¨åˆ†ï¼Œæ¯”å¦‚ `300x300` çš„ç‰‡æ®µï¼Œå‘ç°é‡Œé¢ä¹Ÿæœ‰äº›è§„å¾‹æ€§ç»“æ„ï¼Œä½†å¾ˆéš¾ä¸€çœ¼çœ‹æ‡‚ã€‚è¿™å±äº â€œå¯è§£é‡Šæ€§â€ çš„ç ”ç©¶èŒƒç•´ï¼ˆmechanistic interpretabilityï¼‰ã€‚

ä¸è¿‡æœ¬è§†é¢‘ä¸æ‰“ç®—æ·±æŒ–è¿™äº›ï¼Œæˆ‘ä»¬åªæ˜¯éªŒè¯ï¼š

1. æˆåŠŸåŠ è½½äº† OpenAI å‘å¸ƒçš„ GPT-2ï¼ˆ124Mï¼‰æ¨¡å‹ï¼›
2. èƒ½è®¿é—®æ‰€æœ‰å‚æ•°ï¼Œå¹¶ç†è§£å®ƒä»¬çš„ç»“æ„å’Œå«ä¹‰ï¼›
3. **å¯ä»¥ä»ä¸­é‡‡æ ·ç”Ÿæˆæ–‡æœ¬**ã€‚

### ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ç¤ºä¾‹

æˆ‘ä»¬ç»™æ¨¡å‹ä¸€ä¸ªå‰ç¼€ï¼Œä¾‹å¦‚ï¼š

```text
"Hello, I'm a language model,"
```

ç„¶åé‡‡æ ·ç”Ÿæˆ 30 ä¸ª tokenï¼Œé‡å¤ 5 æ¬¡ï¼Œå¾—åˆ°ä¸åŒçš„è¾“å‡ºï¼š

ä¾‹å¦‚ï¼š

> "Hello, I'm a language model, but what I'm really doing is making a human readable document..."
> "Hello, I'm a language model, and there are other languages but those are..."
> ï¼ˆè¿™äº›éƒ½æ˜¯çœŸå®è¾“å‡ºï¼‰

å³ä½¿æˆ‘ä»¬è®¾ç½®äº†éšæœºç§å­ï¼ˆ`seed`ï¼‰ï¼Œç”Ÿæˆçš„æ–‡æœ¬å¯èƒ½è¿˜æ˜¯è·Ÿå®˜æ–¹ä»£ç ç•¥æœ‰ä¸åŒï¼ˆå¯èƒ½æ˜¯ä»£ç æœ‰æ›´æ–°ï¼‰ã€‚ä½†å…³é”®æ˜¯ï¼š**æ–‡æœ¬æ˜¯è¿è´¯çš„**ï¼Œè¯´æ˜æ¨¡å‹åŠ è½½å’Œæ¨ç†æ˜¯æˆåŠŸçš„ã€‚

---

## ç›®æ ‡å°ç»“

æˆ‘ä»¬å¸Œæœ›åœ¨åç»­åšä¸¤ä»¶äº‹ï¼š

1. **è‡ªå·±æ‰‹åŠ¨ä»é›¶å†™ä¸€ä¸ª GPT-2 ç±»**ï¼ˆä¸ä¾èµ– Hugging Face çš„å°è£…ï¼‰ï¼›
2. ç„¶åå°è¯•åŠ è½½ OpenAI å‘å¸ƒçš„ 124M æƒé‡è¿›æˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹ç±»ï¼›
3. æœ€ç»ˆï¼Œåœ¨æˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹ä¸Šè¿›è¡Œä»é›¶è®­ç»ƒï¼Œçœ‹æ˜¯å¦èƒ½è®­ç»ƒå‡º**æ¯” OpenAI å®˜æ–¹æ¨¡å‹æ›´å¥½çš„ç»“æœ**ã€‚

è¿™æ ·æˆ‘ä»¬å°±ä¸ä»…ç†è§£äº†æ¨¡å‹ç»“æ„ï¼Œè¿˜èƒ½ä»å¤´æŒæ¡è®­ç»ƒæµç¨‹å’Œå‚æ•°ç»†èŠ‚ã€‚

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥å¸®ä½ è¿›ä¸€æ­¥æ¢³ç†å‚æ•°ç»“æ„æˆ–å†™ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬çš„ `GPT2` ç±»ã€‚æ˜¯å¦éœ€è¦ï¼Ÿ


# SECTION 1: implementing the GPT-2 nn.Module

is all un need paper that started everything and let's scroll over to the model architecture the original
Transformer now remember that gpt2 is slightly modified from the or or Transformer in particular we do not have
uh the encoder gpt2 is a decoder only Transformer as we call it so this entire encoder here is missing in addition to
that this cross attention here that was using that encoder is also missing so we delete this entire part everything else
stays almost the same but there are some differences that we're going to uh sort of look at here so there are two main
differences when we go to the gb2 page under 2.3 model we notice that first
there's a reshuffling of the layer Norms so they change place and second an
additional layer normalization was added here to the final self detention block
so basically all the layer Norms here instead of being after the MLP or after the attention they SN before it and an
additional layer Norm gets added here right before the final classifier so now let's Implement some
of the first sort of skeleton NN module modules here in our GPT NN module and in
particular we're going to try to match up this schema here that is used by hugging face Transformers because that
will make it much easier to load these weights from this state dict so we want something that reflects uh this schema
here so here's what I came up with um basically we see that the main
container here that has all the modules is called Transformer so I'm reflecting that with an NN module dict and this is
basically a module that allows you to index into the subm modules using keys just like a dictionary uh
strings within it we have the weights of the token embeddings WT and that's an N
embedding and the weights of the position embeddings which is also just an N embedding and if you remember n
embedding is really just a fancy little wrapper module around just a single um
single array of numbers a single uh block of numbers just like this it's a
single tensor and an embedding is a glorified um wrapper around a tensor
that allows you to access its elements uh by indexing into the rows now in addition to that we see here
that we have a h and then there's a this is index using numbers instead of
indexed using strings so there's a h. 0 1 2 Etc all the way up till h. 11 and
that's because there are 12 layers here in this Transformer so to reflect that I'm creating also an H I think that
probably stands for hidden and instead of a module dict this is a model list so we can index it using integers exactly
as we see here 01 2 Etc and the modular list has a n layer blocks and the blocks
are yet to be defined in a module in a bit in addition to that following the gpt2 paper we have we need an additional
final layer Norm that we're going to put in there and then we have the final classifier uh the language model head
which um projects from 768 the number of embedding dimensions in this GPT all the
way to the vocab size which is 50257 and gpt2 uses no bias for this
final uh sort of projection so this is the skeleton and you can see that it
reflects this so the wte is the token embeddings here it's called output
embedding but it's really the token embeddings the PE is the positional codings uh those two pieces of
information as we saw previously are going to add and then go into the Transformer the H is the all the blocks
in Gray and the LNF is this new layer that gets added here by the gpt2 model
and LM head is this linear part here so that's the skeleton of the gpt2 we now
have to implement the block okay so let's now recurse to the block itself so
we want to define the block um so I'll start putting them here so the block I
like to write out like this uh these are some of the initializations and then this is the
actual forward pass of what this block computes and notice here that there's a change from the Transformer again that
is mentioned in the gpt2 paper so here the layer normalizations are after the
application of attention or feed forward in addition to that note that the normalizations are inside the residual
stream you see how feed forward is applied and this arrow goes through and through the normalization so that means
that your residual pathway has normalizations inside them and this is not very good or desirable uh you
actually prefer to have a single uh clean residual stream all the way from supervision all the way down to the
inputs the tokens and this is very desirable and nice because the gradients
that flow from the top if you remember from your microad addition just distributes gradients during the
backwards state to both of its branches equally so addition is a branch in the
gradients and so that means that the gradients from the top flows straight to the inputs the tokens through the
residual Pathways unchanged but then in addition to that the gradient also flows through the blocks and the blocks you
know contribute their own contribution over time and kick in and change the optimization over time but basically
clean residual pathway is desirable from an optimization perspective and then the
this is the pre-normalization version where you see that RX first goes through the layer normalization and then the
attention and then goes uh back out to go to the L ration number two and the
multia perceptron sometimes also referred to as a feed forward Network or an FFN and then that goes into the
residual stream again and the one more thing that is kind of interesting to note is that recall that attention is a
communication operation it is where all the tokens and there's 1,24 tokens lined up in a sequence and this is where the
tokens communicate this is where they exchange information so attention is a
um aggregation function it's a pooling function it's a weighted sum function it
is a reduce operation whereas MLP this uh MLP here happens at every single
token individually there's no information being collected or exchanged between the tokens so the attention is
the reduce and the MLP is the map and what you end up with is that the Transformer just ends up just being a
repeated application of map produce if you want to think about it that way so
um this is where they communicate and this is where they think individually about the information that they gathered
and every one of these blocks uh iteratively refines the um representation is at the residual stream
so this is our block um slightly modified from this picture Okay so let's now move on to the MLP so the MLP block
uh I implemented as follows it is relatively straightforward we basically have two linear projections
here that are sandwiched in between the G nonlinearity so nn. G approximate is 10h
now when we swing on uh swing over to the Pyro documentation this is n.g and
it has this format and it has two versions the original version of G which we'll step into into in a bit and the
approximate version of Galo which we can request using 10 so as you can see just as a preview
here G is a basically like a reu except there's no flat exactly Flat Tail here
at exactly zero but otherwise it looks very much like a slightly smoother reu
it comes from this paper here Gan error linear units and uh you can step through
this paper and there's some mathematical calac reasoning that leads to an interpretation that leads to the specific formulation it has to do with
stochastic radial risers and the expectation of a modification to Adaptive dropout so you can read through
all of that if you'd like here and there's a little bit of history as to why there is an an approximate version
of G and that comes from this issue here as far as I can tell and in this issue
Daniel Hendrix mentions that at the time when they developed this nonlinearity
the Earth function which you need to evaluate the exact G was very slow in tensor flow so they ended up basically
developing this approximation and this approximation that then ended up being picked up by Bert and by GP P2 Etc but
today there's no real good reason to use the approximate version you'd prefer to just use the exact version um because I
my expectation is that there's no big difference anymore and this is kind of like a historical um kind of Quirk um
but we are trying to reproduce gpt2 exactly and gpt2 used the 10h
approximate version so we prefer to stick with that um now one other reason to actually
just intuitively use G instead of veru is previously in the in videos in the past we've spoken about the dead reu
neuron problem where in this tale of a reu if it's exactly flat at zero any
activations that fall there will get exactly zero gradient there's no change there's no adaptation there's no
development of the network if any of these activations end in this flat region but the G always contributes a
local gradient and so there's always going to be a change always going to be an adaptation and sort of smoothing it
out ends up empirically working better in practice as demonstrated in this paper and also as demonstrated by it
being picked up by the bird paper gbt2 paper and so on so for that reason we adopt this nonlinearity uh here in the
10 in the gbt2 reproduction now in more modern networks also like llama 3 and so on this nonlinearity also further
changes uh to swiglo and other variants like that uh but for gpt2 they Ed this
approximate G okay and finally we have the attention operation so let me paste in my
attention so I know this is a lot so I'm going to go through this a bit quickly a bit
slowly but not too slowly because we have covered this in the previous video and I would just point you there um so
this is the attention operation now in the previous video you will remember this is not just attention this is um
multi-headed attention right and so in the previous video we had this multi-headed attention module and this
implementation made it obvious that these heads are not actually that complicated uh there's basically
in parallel inside every attention block there's multiple heads and they're all functioning in parallel and uh their
outputs are just being concatenated and that becomes the output of the multi-headed attention so the heads are
just kind of like parallel streams and their outputs get concatenated and so it was very simple
and made the head be kind of like U fairly straightforward in terms of its
implementation what happens here is that instead of having two separate modules and indeed many more modules that get
concatenated all of that is just put into a single uh self attention uh
module and instead I'm being very careful and doing a bunch of transpose
split um tensor gymnastics to make this very efficient in pych but fundamentally
and algorithmically nothing is different from the implementation we saw before um in this uh give
repository so to remind you very briefly and I don't want to go in this uh into
this in too many in too much time but we have these tokens lined up in a sequence and there's 1,20 of them and then each
token at this stage of the attention emits three vectors the query key and the value and first what happens here um
is that the queries and the keys have to multiply each other to get sort of the attention um amount like how interesting
they find each other so they have to interact multiplicatively so what we're doing here is we're calculating the qkv
we splitting it and then there's a bunch of gymnastics as I mentioned here and the way this works is that we're
basically making the number of heads and H into a batch Dimension and so it's a
batch Dimension just like B so that in these operations that follow pytorch treats B and NH as batches and it
applies all the operations on all of them in parallel in both the batch and the
heads and the operations that get applied are number one the queries and the keys intera to give us her attention
this is the autoaggressive mask that makes sure that the tokens only attend to tokens before them and never to
tokens in the future the softmax here normalizes the attention so it sums to one always and
then recall from the previous video that doing the attention Matrix multiply with the values is basically a way to do a
weighted sum of the values of the tokens that we found interesting at every single token and then the final
transpose conf VI and view is just reassembling all of that again and this actually performs the concatenation
operation so you can step through this uh slowly if you'd like um but it is equivalent mathematically to our
previous implementation is just more efficient in P torch so that's why I chose this implementation
instead now in addition to that I'm being careful with how I name my variables so for example cattin is the
same as seaten and so actually our keys should basically exactly follow the schema of the hugging face train
Transformers code and that will make it very easy for us to now Port over all the weights from exactly this sort of
naming conventions because all of our variables are named the same thing but um at this point we have finished the
gpt2 implementation and what that allows us to do is we don't have to basically use uh this file from hugging face which
is fairly long um this is uh 2,000 lines of code um instead we
just have a less than 100 lines of code and this is the complete uh gpd2 implementation so at this stage we
should just be able to take over all the weights set them and then do generation so let's see what that looks like okay

å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯è¿™ä¸€æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## ç¬¬ 1 éƒ¨åˆ†ï¼šå®ç° GPT-2 çš„ `nn.Module`ï¼ˆæ¨¡å‹ç»“æ„ï¼‰

æˆ‘ä»¬ä» Transformer æœ€æ—©çš„è®ºæ–‡ã€ŠAttention is All You Needã€‹å¼€å§‹è®²èµ·ã€‚GPT-2 æ˜¯å¯¹åŸå§‹ Transformer çš„è½»å¾®ä¿®æ”¹ç‰ˆæœ¬ï¼š

### GPT-2 æ˜¯ **Decoder-only** çš„ Transformer

ä¸åŸå§‹ Transformer ä¸åŒï¼š

* **GPT-2 åˆ é™¤äº† Encoder éƒ¨åˆ†**ï¼›
* åŒæ—¶ä¹Ÿåˆ é™¤äº† Cross-Attentionï¼ˆå› ä¸ºæ²¡æœ‰ Encoderï¼ŒCross-Attention ä¹Ÿä¸éœ€è¦ï¼‰ï¼›
* å‰©ä¸‹çš„ç»“æ„åŸºæœ¬ä¿ç•™ï¼Œä½†æœ‰å‡ ä¸ªå…³é”®çš„ä¸åŒç‚¹ï¼š

---

### GPT-2 çš„ç»“æ„æ”¹åŠ¨ï¼ˆä¸åŸå§‹ Transformer ç›¸æ¯”ï¼‰

æ ¹æ® GPT-2 è®ºæ–‡ç¬¬ 2.3 èŠ‚ï¼š

1. **LayerNorm çš„ä½ç½®åšäº†è°ƒæ•´**ï¼Œæ”¾åœ¨ Attention å’Œ MLPï¼ˆå‰é¦ˆç½‘ç»œï¼‰**ä¹‹å‰**ï¼›
2. **å¢åŠ äº†ä¸€ä¸ªé¢å¤–çš„ LayerNorm**ï¼Œæ”¾åœ¨ Transformer çš„è¾“å‡ºå’Œåˆ†ç±»å™¨ï¼ˆLanguage Model Headï¼‰ä¹‹é—´ã€‚

è¿™ç§ç»“æ„ä¹Ÿç§°ä¸º **Pre-LayerNorm æ¶æ„**ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„ Post-LayerNormï¼Œæ›´ç¨³å®šã€ä¾¿äºè®­ç»ƒï¼Œæ¢¯åº¦ä¼ æ’­ä¹Ÿæ›´æµç•…ã€‚

---

### å®ç° GPT-2 çš„æ¨¡å‹éª¨æ¶ï¼ˆSkeletonï¼‰

æˆ‘ä»¬è¦ä»é›¶å®ç°è¿™ä¸ªæ¨¡å‹ï¼Œæ‰€ä»¥é¦–å…ˆæ­å»ºæ¨¡å‹çš„å¤§æ¡†æ¶ï¼Œå¹¶æ¨¡ä»¿ Hugging Face çš„å‘½åå’Œç»“æ„ï¼Œä»¥ä¾¿åç»­èƒ½ç›´æ¥åŠ è½½å®˜æ–¹çš„æƒé‡ã€‚

ä¸»è¦ç»„ä»¶åŒ…æ‹¬ï¼š

* `wte`: token embeddingï¼ˆè¯åµŒå…¥ï¼‰
* `wpe`: position embeddingï¼ˆä½ç½®åµŒå…¥ï¼‰
* `h`: æ¨¡å‹çš„ Transformer å±‚åˆ—è¡¨ï¼Œå…± 12 å±‚ï¼Œæ¯å±‚æ˜¯ä¸€ä¸ª `Block`ï¼ˆè‡ªå®šä¹‰ç±»ï¼‰
* `ln_f`: æœ€åçš„ LayerNormï¼ˆGPT-2 ç‰¹æœ‰ï¼‰
* `lm_head`: æœ€åçš„çº¿æ€§å±‚ï¼ˆä» 768 æ˜ å°„åˆ°è¯è¡¨å¤§å° 50257ï¼‰

å…¶ä¸­ï¼š

* `wte` å’Œ `wpe` æ˜¯ PyTorch çš„ `nn.Embedding`ï¼Œåˆ†åˆ«æ˜¯è¯åµŒå…¥å’Œä½ç½®åµŒå…¥ï¼›
* `h` æ˜¯ä¸€ä¸ª `nn.ModuleList`ï¼ŒåŒ…å« 12 ä¸ª Blockï¼›
* `lm_head` æ˜¯ä¸€ä¸ª `nn.Linear(768, 50257, bias=False)`ï¼Œç”¨äºç”Ÿæˆè¯çš„æ¦‚ç‡åˆ†å¸ƒï¼›
* æ‰€æœ‰è¿™äº›æ¨¡å—ç»Ÿä¸€å°è£…åœ¨ä¸€ä¸ª `Transformer` å®¹å™¨ä¸­ã€‚

---

### å®ç° Transformer Blockï¼ˆæ¯ä¸€å±‚çš„æ ¸å¿ƒç»“æ„ï¼‰

æ¯ä¸ª Block ç”±ä»¥ä¸‹éƒ¨åˆ†ç»„æˆï¼š

1. LayerNorm
2. å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-head Self Attentionï¼‰
3. LayerNorm
4. å‰é¦ˆç½‘ç»œï¼ˆMLPï¼‰

ç»“æ„ä¸ºï¼š

```text
Input
 â†’ LayerNorm â†’ Attention â†’ åŠ æ®‹å·® â†’ 
 â†’ LayerNorm â†’ MLP â†’ åŠ æ®‹å·® â†’ è¾“å‡º
```

æ³¨æ„ï¼š

* æ®‹å·®è¿æ¥ç›´æ¥ç»•è¿‡æ¯ä¸ªå­æ¨¡å—ï¼ˆAttention / MLPï¼‰ï¼Œä¿è¯æ¢¯åº¦æµåŠ¨é€šç•…ï¼›
* é‡‡ç”¨äº† Pre-LN æ¶æ„ï¼Œæœ‰åŠ©äºè®­ç»ƒç¨³å®šæ€§ï¼›
* Attention ç”¨äºåœ¨æ‰€æœ‰ token ä¹‹é—´é€šä¿¡ï¼ˆæ˜¯ä¸€ç§èšåˆæ“ä½œï¼‰ï¼›
* MLP åˆ™æ˜¯å¯¹æ¯ä¸ª token ç‹¬ç«‹å¤„ç†ï¼ˆæ˜¯ä¸€ç§é€å…ƒç´ æ˜ å°„ï¼‰ï¼›
* å› æ­¤ Transformer å¯ä»¥ç†è§£ä¸ºâ€œMapï¼ˆMLPï¼‰ + Reduceï¼ˆAttentionï¼‰â€çš„åå¤å †å ã€‚

---

### MLP éƒ¨åˆ†ç»†èŠ‚ï¼šGELU æ¿€æ´»å‡½æ•°

GPT-2 ä½¿ç”¨çš„æ˜¯ **GELUï¼ˆGaussian Error Linear Unitï¼‰**ï¼Œæ›´å‡†ç¡®åœ°è¯´ï¼Œæ˜¯å®ƒçš„ **è¿‘ä¼¼ç‰ˆæœ¬ `gelu_approx`ï¼ˆä¹Ÿå« `gelu_tanh`ï¼‰**ã€‚

ä¸ºä»€ä¹ˆä½¿ç”¨è¿‘ä¼¼ç‰ˆæœ¬ï¼Ÿ

* åŸå› æ˜¯æ—©æœŸ TensorFlow ä¸­ç²¾ç¡®ç‰ˆæœ¬ `gelu` çš„ `erf` å‡½æ•°å¾ˆæ…¢ï¼›
* æ‰€ä»¥å½“æ—¶ç”¨äº†ä¸€ä¸ªå¿«é€Ÿè¿‘ä¼¼ç‰ˆï¼›
* GPT-2 æ²¿ç”¨äº†è¿™ä¸ªè¿‘ä¼¼ç‰ˆæœ¬ï¼Œä¸ºäº†å¿ å®å¤ç°ï¼Œæˆ‘ä»¬ä¹Ÿåº”ä½¿ç”¨å®ƒï¼›
* GELU æ¯” ReLU æ›´å¹³æ»‘ï¼Œæ²¡æœ‰â€œæ­»äº¡ç¥ç»å…ƒâ€é—®é¢˜ï¼Œå› æ­¤åœ¨ BERTã€GPT ä¸­å¹¿æ³›ä½¿ç”¨ã€‚

---

### Attention éƒ¨åˆ†ç»†èŠ‚ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶

GPT-2 ä½¿ç”¨æ ‡å‡†çš„ **Multi-head Self Attention**ï¼Œå…¶å·¥ä½œè¿‡ç¨‹ä¸ºï¼š

1. æ¯ä¸ª token äº§ç”Ÿä¸‰ä¸ªå‘é‡ï¼šQueryã€Keyã€Valueï¼›
2. Query å’Œ Key è¿›è¡Œç‚¹ç§¯ï¼Œè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼›
3. åº”ç”¨ Maskï¼ˆè‡ªå›å½’ç»“æ„ï¼Œåªèƒ½çœ‹å‰é¢çš„ tokenï¼‰ï¼›
4. å¯¹æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œ Softmaxï¼Œå¾—åˆ°æƒé‡ï¼›
5. ç”¨è¿™äº›æƒé‡å¯¹ Value åšåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°æ³¨æ„åŠ›è¾“å‡ºï¼›
6. æ‰€æœ‰å¤´çš„è¾“å‡ºæ‹¼æ¥èµ·æ¥ï¼Œå¹¶è¿‡ä¸€ä¸ªçº¿æ€§å±‚ã€‚

è¿™éƒ¨åˆ†è™½ç„¶å®ç°èµ·æ¥æ¯”è¾ƒå¤æ‚ï¼ˆæ¶‰åŠå¾ˆå¤šè½¬ç½®ã€reshape æ“ä½œï¼‰ï¼Œä½†ç®—æ³•æœ¬è´¨å¹¶æ²¡æœ‰å˜åŒ–ï¼Œåªæ˜¯ç”¨ PyTorch çš„å‘é‡åŒ–æ–¹å¼å®ç°æ›´é«˜æ•ˆçš„ç‰ˆæœ¬ã€‚

---

### å‘½åä¸€è‡´æ€§ä¸å‚æ•°å…¼å®¹æ€§

ä½œè€…ç‰¹åˆ«æ³¨æ„äº†å‘½åæ–¹å¼ï¼Œæ¯”å¦‚ï¼š

* `c_attn`, `c_proj` ç­‰å˜é‡åéƒ½ä¸ Hugging Face ä¿æŒä¸€è‡´ï¼›
* è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ç›´æ¥åŠ è½½å®ƒä»¬çš„é¢„è®­ç»ƒå‚æ•°ï¼Œæ— éœ€ä¿®æ”¹å˜é‡åã€‚

---

### æœ€ç»ˆæ•ˆæœ

æ•´ä¸ª GPT-2 æ¨¡å‹åœ¨ä¸åˆ° 100 è¡Œä»£ç ä¸­å°±å®ç°äº†ï¼ˆç›¸æ¯” Hugging Face çš„ 2000 å¤šè¡Œï¼‰ï¼Œç»“æ„æ¸…æ™°ï¼Œé€»è¾‘ä¸€è‡´ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥ï¼š

* åŠ è½½ Hugging Face çš„é¢„è®­ç»ƒæƒé‡ï¼›
* ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ï¼›
* ä¹‹åæˆ‘ä»¬è¿˜å¯ä»¥ç”¨è¿™ä¸ªç»“æ„è®­ç»ƒè‡ªå·±çš„ GPT æ¨¡å‹ã€‚

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥ä¸ºä½ æ•´ç†è¿™ä¸ª GPT-2 æ¨¡å‹çš„ç®€åŒ–ç‰ˆä»£ç ç»“æ„ï¼Œå¹¶é€å±‚è®²è§£æ¯ä¸€éƒ¨åˆ†ã€‚æ˜¯å¦éœ€è¦æˆ‘è¿™ä¹ˆåšï¼Ÿ


ä»‹ç»ä¸€ä¸‹ GELU

å¥½çš„ ğŸ‘ æˆ‘æ¥è¯¦ç»†ä»‹ç»ä¸€ä¸‹ **GELU (Gaussian Error Linear Unitï¼Œé«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒ)**ã€‚

---

## 1. å®šä¹‰

GELU æ˜¯ä¸€ç§ **æ¿€æ´»å‡½æ•°**ï¼Œå¸¸ç”¨äº Transformerï¼ˆæ¯”å¦‚ GPTã€BERTï¼‰é‡Œé¢çš„ MLP å±‚ã€‚
å®ƒçš„æ•°å­¦å®šä¹‰æ˜¯ï¼š

$$
\text{GELU}(x) = x \cdot \Phi(x)
$$

å…¶ä¸­ $\Phi(x)$ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯è®¡åˆ†å¸ƒå‡½æ•° (CDF)ã€‚

å±•å¼€å½¢å¼ä¸ºï¼š

$$
\text{GELU}(x) = \frac{1}{2}x \left(1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right)
$$

* $\text{erf}$ æ˜¯è¯¯å·®å‡½æ•°ï¼ˆerror functionï¼‰ã€‚
* ç›´è§‚ç†è§£ï¼šGELU æŠŠè¾“å…¥ $x$ æŒ‰æ¦‚ç‡ã€Œé€šè¿‡ã€æˆ–ã€ŒæŠ‘åˆ¶ã€ï¼Œè€Œä¸æ˜¯åƒ ReLU é‚£æ ·ç¡¬æ€§æˆªæ–­ã€‚

---

## 2. è¿‘ä¼¼å…¬å¼

åœ¨æ·±åº¦å­¦ä¹ åº“é‡Œï¼Œå¸¸ç”¨ä¸€ä¸ªè¿‘ä¼¼å…¬å¼ï¼ˆè®¡ç®—æ›´å¿«ï¼‰ï¼š

$$
\text{GELU}(x) \approx 0.5x \left(1 + \tanh\Big(\sqrt{\frac{2}{\pi}} \big(x + 0.044715x^3\big)\Big)\right)
$$

è¿™ä¸ªå…¬å¼èƒ½å¾ˆå¥½é€¼è¿‘åŸå§‹å®šä¹‰ï¼Œå¹¶ä¸”ä¾¿äº GPU è®¡ç®—ã€‚

---

## 3. ç›´è§‚ç†è§£

* **ReLU**ï¼šå°äº 0 çš„è¾“å…¥ç›´æ¥å˜æˆ 0ï¼Œå¤§äº 0 çš„ä¿ç•™åŸå€¼ã€‚
* **Sigmoid**ï¼šæŠŠè¾“å…¥å‹ç¼©åˆ° $[0,1]$ï¼Œä½†æ˜¯ä¼šæœ‰é¥±å’Œé—®é¢˜ã€‚
* **GELU**ï¼šç»“åˆäº† sigmoid çš„å¹³æ»‘æ€§å’Œ ReLU çš„ç¨€ç–æ€§ã€‚

  * å¯¹äºè´Ÿå€¼ï¼Œè¾“å‡ºæ¥è¿‘ 0ï¼ˆä½†ä¸æ˜¯å®Œå…¨ä¸º 0ï¼‰ã€‚
  * å¯¹äºå¤§æ­£å€¼ï¼Œè¾“å‡ºæ¥è¿‘ $x$ã€‚
  * åœ¨ 0 é™„è¿‘æœ‰ä¸ªå¹³æ»‘çš„è¿‡æ¸¡ã€‚

æ‰€ä»¥å®ƒçš„æ•ˆæœä»‹äº **ReLU å’Œ Sigmoid** ä¹‹é—´ï¼Œæ›´è‡ªç„¶åœ°å¯¹å°è¾“å…¥è¿›è¡Œã€Œéƒ¨åˆ†æŠ‘åˆ¶ã€ã€‚

---

## 4. ç‰¹ç‚¹å’Œä¼˜ç‚¹

1. **å¹³æ»‘**ï¼šä¸åƒ ReLU é‚£æ ·æœ‰ä¸è¿ç»­ç‚¹ï¼ˆåœ¨ 0 å¤„ï¼‰ã€‚
2. **æ¦‚ç‡æ€§è§£é‡Š**ï¼š$\Phi(x)$ æ˜¯è¾“å…¥æœä»é«˜æ–¯åˆ†å¸ƒæ—¶å¤§äº 0 çš„æ¦‚ç‡ï¼Œæ‰€ä»¥ GELU å¯ä»¥çœ‹ä½œã€Œå¸¦æ¦‚ç‡çš„é—¨æ§ã€ã€‚
3. **æ›´é€‚åˆ NLP**ï¼šBERTã€GPT ç­‰æ¨¡å‹é»˜è®¤éƒ½ç”¨ GELUï¼Œè€Œä¸æ˜¯ ReLU/LeakyReLUã€‚å®è·µä¸­å‘ç°è®­ç»ƒæ›´ç¨³å®šï¼Œæ•ˆæœæ›´å¥½ã€‚

---

## 5. PyTorch å®ç°

åœ¨ PyTorch é‡Œç›´æ¥ç”¨ï¼š

```python
import torch
import torch.nn.functional as F

x = torch.linspace(-3, 3, 100)
y = F.gelu(x)  # è°ƒç”¨å†…ç½® GELU
```

---

### æ€»ç»“ä¸€å¥

**GELU = x Ã— Î¦(x)**ï¼Œå®ƒæ˜¯ä¸€ç§å¹³æ»‘ç‰ˆçš„ ReLUï¼Œå…¼é¡¾äº†ç¨€ç–æ€§å’Œè¿ç»­æ€§ï¼Œåœ¨ Transformer ç³»åˆ—æ¨¡å‹ä¸­éå¸¸å¸¸è§ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **å¯¹æ¯”å›¾ (ReLU vs Sigmoid vs GELU)**ï¼Œè®©ä½ ç›´è§‚çœ‹å‡ºå®ƒä»¬çš„å·®åˆ«ï¼Ÿ

https://docs.pytorch.org/docs/stable/generated/torch.nn.GELU.html

# ä¸ºä»€ä¹ˆåœ¨Self-Attentionå‰é¢åŠ ä¸ŠCausal

å¥½é—®é¢˜ ğŸ‘Œ
æˆ‘ä»¬è¯´çš„ **Causal Self-Attention**ï¼Œå…¶å®å°±æ˜¯åœ¨æ™®é€šçš„ Self-Attention åŸºç¡€ä¸Šï¼ŒåŠ äº†ä¸€ä¸ª **å› æœçº¦æŸ (causality constraint)**ã€‚

---

## 1. æ™®é€š Self-Attention

åœ¨æ ‡å‡† Transformerï¼ˆæ¯”å¦‚æœºå™¨ç¿»è¯‘çš„ Encoderï¼‰é‡Œï¼Œ**æ¯ä¸ª token å¯ä»¥å’Œåºåˆ—ä¸­æ‰€æœ‰ token äº¤äº’**ã€‚

* ç¬¬ 5 ä¸ªè¯å¯ä»¥â€œçœ‹åˆ°â€ç¬¬ 1\~10 ä¸ªè¯ï¼›
* ç¬¬ 3 ä¸ªè¯ä¹Ÿèƒ½çœ‹åˆ°ç¬¬ 4ã€5ã€6 â€¦ åé¢çš„è¯ã€‚

è¿™æ ·æ²¡é—®é¢˜ï¼Œå› ä¸ºåœ¨ç¿»è¯‘ä»»åŠ¡é‡Œï¼Œæ•´å¥è¯æ˜¯å·²çŸ¥çš„ã€‚

---

## 2. ä¸ºä»€ä¹ˆè¦åŠ  Causalï¼Ÿ

åœ¨ **è¯­è¨€æ¨¡å‹ (LM, GPT)** é‡Œï¼Œæˆ‘ä»¬æ˜¯åš **è‡ªå›å½’é¢„æµ‹ (autoregressive prediction)**ï¼š

$$
P(x_t \mid x_1, x_2, ..., x_{t-1})
$$

ä¹Ÿå°±æ˜¯è¯´ï¼Œç¬¬ $t$ ä¸ª token **åªèƒ½ä¾èµ–ä¹‹å‰çš„ token**ï¼Œä¸èƒ½å·çœ‹æœªæ¥ã€‚

* ä¸¾ä¾‹ï¼šé¢„æµ‹å¥å­ â€œæˆ‘ä»Šå¤©å» \_\_\_â€ï¼Œæ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ—¶ï¼Œåªèƒ½ç”¨â€œæˆ‘ ä»Šå¤© å»â€ï¼Œä¸èƒ½æå‰çœ‹åˆ°â€œåŒ—äº¬â€ã€‚

å¦‚æœå…è®¸çœ‹æœªæ¥ â†’ å°±ä¼šâ€œä½œå¼Šâ€ï¼Œè®­ç»ƒå’Œæ¨ç†ä¸åŒ¹é…ï¼Œæ¨¡å‹å¤±æ•ˆã€‚

---

## 3. æ€ä¹ˆå®ç° Causalï¼Ÿ

å°±æ˜¯åŠ ä¸€ä¸ª **ä¸‹ä¸‰è§’ mask**ï¼š

$$
\text{Mask}[i, j] =
\begin{cases} 
0 & j \leq i \quad (\text{å¯ä»¥çœ‹è‡ªå·±å’Œä¹‹å‰çš„}) \\
-\infty & j > i \quad (\text{æœªæ¥çš„ token è¢«å±è”½})
\end{cases}
$$

è¿™æ · softmax åï¼Œæœªæ¥ token çš„æ³¨æ„åŠ›æ¦‚ç‡å°±æ˜¯ 0ã€‚

ä»£ç é‡Œå°±æ˜¯ï¼š

```python
self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                 .view(1, 1, config.block_size, config.block_size))
...
att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
```

---

## 4. æ€»ç»“

* **Self-Attention**ï¼šæ¯ä¸ªä½ç½®å¯ä»¥æ³¨æ„åˆ°æ‰€æœ‰ tokenã€‚
* **Causal Self-Attention**ï¼šæ¯ä¸ªä½ç½® **åªèƒ½æ³¨æ„åˆ°è‡ªå·±å’Œä¹‹å‰çš„ token**ï¼Œä¿è¯å› æœæ€§ï¼Œé€‚åˆè‡ªå›å½’è¯­è¨€å»ºæ¨¡ï¼ˆGPT ç³»åˆ—ï¼‰ã€‚

---

ğŸ‘‰ æ¢å¥è¯è¯´ï¼š

* Encoder é‡Œçš„ Self-Attention æ˜¯ **åŒå‘**çš„ï¼ˆèƒ½çœ‹å…¨å±€ï¼‰ã€‚
* Decoder é‡Œçš„ Self-Attention æ˜¯ **å•å‘çš„ï¼ˆå› æœçš„ï¼‰**ï¼Œä¿è¯é¢„æµ‹æ—¶ä¸ä½œå¼Šã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **æ™®é€š Self-Attention vs Causal Self-Attention çš„å¯¹æ¯”å›¾**ï¼ˆå‰è€…èƒ½çœ‹åˆ°æœªæ¥ï¼Œåè€…åªèƒ½çœ‹è¿‡å»ï¼‰ï¼Œæ›´ç›´è§‚ï¼Ÿ

```
class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)
        # output projection
        self.c_proj = nn.Linear(config.n_embd, config.n_embd)
        # regularization
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though
        self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                     .view(1, 1, config.block_size, config.block_size))

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        # calculate query, key, values for all heads in batch and move head forward to be the batch dim
        # nh is "number of heads", hs is "head size", and C (number of channels) = nh * hs
        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer
        qkv = self.c_attn(x)
        q, k, v = qkv.split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        # attention (materializes the large (T,T) matrix for all the queries and keys)
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side
        # output projection
        y = self.c_proj(y)
        return y
```

å¥½é—®é¢˜ ğŸ‘Œ
æˆ‘ä»¬è¯´çš„ **Causal Self-Attention**ï¼Œå…¶å®å°±æ˜¯åœ¨æ™®é€šçš„ Self-Attention åŸºç¡€ä¸Šï¼ŒåŠ äº†ä¸€ä¸ª **å› æœçº¦æŸ (causality constraint)**ã€‚

---

## 1. æ™®é€š Self-Attention

åœ¨æ ‡å‡† Transformerï¼ˆæ¯”å¦‚æœºå™¨ç¿»è¯‘çš„ Encoderï¼‰é‡Œï¼Œ**æ¯ä¸ª token å¯ä»¥å’Œåºåˆ—ä¸­æ‰€æœ‰ token äº¤äº’**ã€‚

* ç¬¬ 5 ä¸ªè¯å¯ä»¥â€œçœ‹åˆ°â€ç¬¬ 1\~10 ä¸ªè¯ï¼›
* ç¬¬ 3 ä¸ªè¯ä¹Ÿèƒ½çœ‹åˆ°ç¬¬ 4ã€5ã€6 â€¦ åé¢çš„è¯ã€‚

è¿™æ ·æ²¡é—®é¢˜ï¼Œå› ä¸ºåœ¨ç¿»è¯‘ä»»åŠ¡é‡Œï¼Œæ•´å¥è¯æ˜¯å·²çŸ¥çš„ã€‚

---

## 2. ä¸ºä»€ä¹ˆè¦åŠ  Causalï¼Ÿ

åœ¨ **è¯­è¨€æ¨¡å‹ (LM, GPT)** é‡Œï¼Œæˆ‘ä»¬æ˜¯åš **è‡ªå›å½’é¢„æµ‹ (autoregressive prediction)**ï¼š

$$
P(x_t \mid x_1, x_2, ..., x_{t-1})
$$

ä¹Ÿå°±æ˜¯è¯´ï¼Œç¬¬ $t$ ä¸ª token **åªèƒ½ä¾èµ–ä¹‹å‰çš„ token**ï¼Œä¸èƒ½å·çœ‹æœªæ¥ã€‚

* ä¸¾ä¾‹ï¼šé¢„æµ‹å¥å­ â€œæˆ‘ä»Šå¤©å» \_\_\_â€ï¼Œæ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ—¶ï¼Œåªèƒ½ç”¨â€œæˆ‘ ä»Šå¤© å»â€ï¼Œä¸èƒ½æå‰çœ‹åˆ°â€œåŒ—äº¬â€ã€‚

å¦‚æœå…è®¸çœ‹æœªæ¥ â†’ å°±ä¼šâ€œä½œå¼Šâ€ï¼Œè®­ç»ƒå’Œæ¨ç†ä¸åŒ¹é…ï¼Œæ¨¡å‹å¤±æ•ˆã€‚

---

## 3. æ€ä¹ˆå®ç° Causalï¼Ÿ

å°±æ˜¯åŠ ä¸€ä¸ª **ä¸‹ä¸‰è§’ mask**ï¼š

$$
\text{Mask}[i, j] =
\begin{cases} 
0 & j \leq i \quad (\text{å¯ä»¥çœ‹è‡ªå·±å’Œä¹‹å‰çš„}) \\
-\infty & j > i \quad (\text{æœªæ¥çš„ token è¢«å±è”½})
\end{cases}
$$

è¿™æ · softmax åï¼Œæœªæ¥ token çš„æ³¨æ„åŠ›æ¦‚ç‡å°±æ˜¯ 0ã€‚

ä»£ç é‡Œå°±æ˜¯ï¼š

```python
self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                 .view(1, 1, config.block_size, config.block_size))
...
att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
```

---

## 4. æ€»ç»“

* **Self-Attention**ï¼šæ¯ä¸ªä½ç½®å¯ä»¥æ³¨æ„åˆ°æ‰€æœ‰ tokenã€‚
* **Causal Self-Attention**ï¼šæ¯ä¸ªä½ç½® **åªèƒ½æ³¨æ„åˆ°è‡ªå·±å’Œä¹‹å‰çš„ token**ï¼Œä¿è¯å› æœæ€§ï¼Œé€‚åˆè‡ªå›å½’è¯­è¨€å»ºæ¨¡ï¼ˆGPT ç³»åˆ—ï¼‰ã€‚

---

ğŸ‘‰ æ¢å¥è¯è¯´ï¼š

* Encoder é‡Œçš„ Self-Attention æ˜¯ **åŒå‘**çš„ï¼ˆèƒ½çœ‹å…¨å±€ï¼‰ã€‚
* Decoder é‡Œçš„ Self-Attention æ˜¯ **å•å‘çš„ï¼ˆå› æœçš„ï¼‰**ï¼Œä¿è¯é¢„æµ‹æ—¶ä¸ä½œå¼Šã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **æ™®é€š Self-Attention vs Causal Self-Attention çš„å¯¹æ¯”å›¾**ï¼ˆå‰è€…èƒ½çœ‹åˆ°æœªæ¥ï¼Œåè€…åªèƒ½çœ‹è¿‡å»ï¼‰ï¼Œæ›´ç›´è§‚ï¼Ÿ

# ä»‹ç»ä¸€ä¸‹ contiguous()

å¥½é—®é¢˜ ğŸ‘Œ ä½ æåˆ°çš„è¿™ä¸€è¡Œï¼š

```python
y = y.transpose(1, 2).contiguous().view(B, T, C)
```

å…³é”®åœ¨äº **`contiguous()`**ã€‚

---

## 1. PyTorch çš„å¼ é‡å†…å­˜å¸ƒå±€

åœ¨ PyTorch é‡Œï¼ŒTensor åœ¨å†…å­˜ä¸­æ˜¯æŒ‰è¡Œä¼˜å…ˆï¼ˆrow-majorï¼‰çš„æ–¹å¼å­˜å‚¨çš„ã€‚

* å¤§å¤šæ•°æ—¶å€™ï¼ŒTensor åœ¨å†…å­˜é‡Œæ˜¯è¿ç»­å­˜æ”¾çš„ï¼ˆcontiguousï¼‰ã€‚
* ä½†æ˜¯ä¸€äº›æ“ä½œï¼ˆæ¯”å¦‚ `transpose`, `permute`, `narrow`ï¼‰ä¸ä¼šçœŸæ­£ç§»åŠ¨æ•°æ®ï¼Œè€Œæ˜¯é€šè¿‡ä¿®æ”¹ **stride**ï¼ˆæ­¥é•¿ï¼‰æ¥â€œé‡æ–°è§£é‡Šâ€å¼ é‡ã€‚

ğŸ‘‰ è¿™æ ·åšæ›´é«˜æ•ˆï¼Œä½†å¯¼è‡´å†…å­˜ä¸æ˜¯è¿ç»­çš„ã€‚

---

## 2. ä»€ä¹ˆæ˜¯ `contiguous()`

å½“ä¸€ä¸ª Tensor åœ¨å†…å­˜ä¸­ä¸æ˜¯è¿ç»­çš„æ—¶ï¼Œè°ƒç”¨ `.contiguous()` ä¼šï¼š

* **æ‹·è´æ•°æ®**ï¼Œè®©å®ƒåœ¨å†…å­˜ä¸­å˜æˆä¸€ä¸ªçœŸæ­£è¿ç»­å­˜å‚¨çš„å‰¯æœ¬ï¼›
* è¿”å›è¿™ä¸ªæ–°çš„ Tensorã€‚

---

## 3. ä¸ºä»€ä¹ˆè¿™é‡Œè¦ç”¨ `contiguous()`

```python
y = y.transpose(1, 2)   # æ”¹å˜äº†ç»´åº¦é¡ºåºï¼Œä½†å†…å­˜ä¸è¿ç»­
y = y.contiguous()      # è®©å†…å­˜å˜æˆè¿ç»­å­˜æ”¾
y = y.view(B, T, C)     # reshape æˆç›®æ ‡å½¢çŠ¶
```

* `transpose(1, 2)` åçš„å¼ é‡åªæ˜¯ stride æ”¹å˜äº†ï¼Œå¹¶æ²¡æœ‰å®é™…è°ƒæ¢å†…å­˜é‡Œçš„æ•°æ®ã€‚
* å¦‚æœç›´æ¥ `.view(B, T, C)`ï¼ŒPyTorch å¯èƒ½ä¼šæŠ¥é”™ï¼ˆå› ä¸º `view` è¦æ±‚åº•å±‚å†…å­˜æ˜¯è¿ç»­çš„ï¼‰ã€‚
* `contiguous()` å°±æ˜¯ä¸ºäº†ä¿è¯æ¥ä¸‹æ¥çš„ `view` å¯ä»¥å®‰å…¨æ‰§è¡Œã€‚

---

## 4. å°ä¾‹å­

```python
import torch

x = torch.arange(12).view(3, 4)
print(x.is_contiguous())   # True

y = x.transpose(0, 1)
print(y.is_contiguous())   # False (å› ä¸ºåªæ˜¯æ¢äº†stride)

z = y.contiguous()
print(z.is_contiguous())   # True
```

---

## âœ… æ€»ç»“

* `.contiguous()` ç”¨äºæŠŠä¸€ä¸ª **éè¿ç»­çš„å¼ é‡ï¼ˆé€šå¸¸ç”± transpose/permute å¾—åˆ°ï¼‰** æ‹·è´æˆ **å†…å­˜è¿ç»­çš„å¼ é‡**ã€‚
* åœ¨ä»£ç é‡Œï¼Œå®ƒçš„ä½œç”¨æ˜¯ï¼š**ä¿è¯ `view` èƒ½æ­£ç¡® reshape**ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **å¼ é‡å†…å­˜å¸ƒå±€ï¼ˆcontiguous vs non-contiguousï¼‰** çš„å›¾ï¼Œç›´è§‚å±•ç¤ºä¸ºä»€ä¹ˆéœ€è¦ `contiguous()`ï¼Ÿ

# loading the huggingface/GPT-2 parameters

so here I've also changed the GPT config so that the numbers here the H parameters agree with the gpt2 124 M
model so the maximum sequence length which I call block size here is 124 the
number of tokens is 50250 257 which if you watch my tokenizer video know that
this is 50,000 m merges BP merges 256 bite tokens the leaves of the BP tree
and one special end of text token that delimits different documents and can start generation as well and there are
12 layers there are 12 heads in the attention and the dimension of the Transformers was
768 so here's how we can now load the parameters from hugging face to uh our
code here and initialize the GPT class with those parameters so let me just copy paste a bunch of code
here and I'm not going to go through this code too slow too quickly too slowly because um honestly it's not that
interesting it's not that exciting we're just loading the weights so it's kind of dry but as I mentioned there are four
models in this miniseries of gpt2 this is some of the Jupiter code um code that
we had here on the right I'm just pting it over these are the hyper parameters of the gpt2 models uh we're creating the
config object and creating our own model and then what's Happening Here is we're creating the state dict both for our
model and for the hugging face model um and then what we're doing here
is we're going over the hugging face model keys and we're copying over those
tensors and in the process we are kind of ignoring a few of the buffers they're not parameters they're buffers so for
example attention dobias uh that's just used for the autoaggressive mask and so we are ignoring some of those masks and
uh that's it and then then one additional kind of annoyance is that this comes from the tensorflow repo and
I'm not sure how this is a little bit annoying but some of the weights are transposed from what pytorch would want
and so manually I hardcoded the weights that should be transposed and then we transpose them if that is so and then we
return this model so the from pre-trained is a Constructor or class method in Python
that Returns the GPT object if we just give it the model type which in our case
is gpt2 the smallest model that we're interested in so this is the code and this is how you would use it and um we
can pop open the terminal here in vs code and we can python train gbt2 pi and
fingers crossed okay so we didn't crash and so
we can load the weights and the biases and everything else into our Ann module
but now let's also get additional confidence that this is working and let's try to actually generate from this model okay now before we can actually

å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## åŠ è½½ Hugging Face GPT-2 çš„å‚æ•°

åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä¿®æ”¹äº† GPT-2 çš„é…ç½®ï¼Œä½¿å¾—å®ƒçš„è¶…å‚æ•°ï¼ˆH å‚æ•°ï¼‰ä¸ GPT-2 124M æ¨¡å‹ç›¸åŒ¹é…ã€‚å…·ä½“æ¥è¯´ï¼š

* **æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆblock sizeï¼‰** è®¾ç½®ä¸º 124ï¼ˆä¹Ÿå°±æ˜¯ GPT-2 èƒ½å¤„ç†çš„æœ€å¤§ token æ•°é‡ï¼‰ï¼›
* **è¯è¡¨å¤§å°ï¼ˆvocab sizeï¼‰** ä¸º 50257ï¼Œè¿™ä¸ªæ•°å­—å¯¹åº” GPT-2 çš„è¯æ±‡è¡¨ï¼Œå…¶ä¸­åŒ…æ‹¬äº† 50,000 ä¸ªé€šè¿‡ Byte Pair Encodingï¼ˆBPï¼‰åˆå¹¶çš„è¯æ±‡ã€256 å­—èŠ‚çš„ token ä»¥åŠä¸€ä¸ªç‰¹æ®Šçš„ **end-of-text** tokenï¼Œç”¨æ¥æ ‡è¯†æ–‡æ¡£çš„ç»“æŸå¹¶å¯ä»¥ä½œä¸ºç”Ÿæˆçš„èµ·å§‹æ ‡è®°ï¼›
* æ¨¡å‹æœ‰ **12 å±‚** Transformerï¼Œæ¯å±‚æœ‰ **12 ä¸ª Attention heads**ï¼Œæ¯ä¸ªå¤´çš„ç»´åº¦æ˜¯ **768**ã€‚

---

### å¦‚ä½•åŠ è½½ Hugging Face æ¨¡å‹å‚æ•°

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ä»£ç åŠ è½½ Hugging Face ä¸­é¢„è®­ç»ƒçš„ GPT-2 å‚æ•°ï¼Œå¹¶å°†å®ƒä»¬åˆå§‹åŒ–åˆ°æˆ‘ä»¬è‡ªå·±çš„ GPT æ¨¡å‹ç±»ä¸­ã€‚è™½ç„¶è¿™éƒ¨åˆ†çš„ä»£ç æ¯”è¾ƒç®€å•ï¼ˆä¸»è¦æ˜¯åŠ è½½æƒé‡ï¼‰ï¼Œä½†ä¸ºäº†æ¸…æ¥šèµ·è§ï¼Œæˆ‘ä»¬è¿˜æ˜¯çœ‹ä¸€ä¸‹æ•´ä½“è¿‡ç¨‹ï¼š

1. æˆ‘ä»¬å®šä¹‰äº†ä¸€äº›è¶…å‚æ•°ï¼ˆå¦‚è¯æ±‡å¤§å°ã€å±‚æ•°ç­‰ï¼‰ï¼›
2. åˆ›å»ºäº†æ¨¡å‹çš„ **config å¯¹è±¡** å’Œ **GPT æ¨¡å‹**ï¼›
3. åˆ›å»ºäº†æ¨¡å‹çš„ **state dict**ï¼Œè¿™å®é™…ä¸Šæ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰æƒé‡ï¼ˆå’Œä¸€äº›åç½®ï¼‰çš„å­—å…¸ï¼›
4. ç„¶åï¼Œæˆ‘ä»¬ä» Hugging Face çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­è¯»å–æƒé‡ï¼Œé€ä¸ªå°†å…¶å¤åˆ¶åˆ°æˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹ä¸­ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼š

* æœ‰ä¸€äº› **buffer**ï¼ˆç¼“å†²åŒºï¼‰æˆ‘ä»¬ä¸éœ€è¦åŠ è½½ï¼Œå› ä¸ºå®ƒä»¬ä¸æ˜¯æ¨¡å‹çš„å‚æ•°ï¼Œæ¯”å¦‚åœ¨è‡ªå›å½’ï¼ˆautoregressiveï¼‰æ©ç ä¸­ä½¿ç”¨çš„ `attention_dobias`ã€‚
* æœ‰äº›æƒé‡éœ€è¦è½¬ç½®ï¼Œå› ä¸º Hugging Face çš„æ¨¡å‹æ˜¯ç”¨ TensorFlow å®ç°çš„ï¼Œè€Œæˆ‘ä»¬çš„ä»£ç ç”¨çš„æ˜¯ PyTorchã€‚è™½ç„¶å¤§éƒ¨åˆ†æƒé‡å¯ä»¥ç›´æ¥åŠ è½½ï¼Œä½†æœ‰äº›éœ€è¦æ‰‹åŠ¨è°ƒæ•´å…¶ç»´åº¦é¡ºåºï¼Œå› æ­¤æˆ‘ä»¬æ‰‹åŠ¨å¯¹è¿™äº›éœ€è¦è½¬ç½®çš„æƒé‡è¿›è¡Œäº†å¤„ç†ã€‚

---

### åŠ è½½æ¨¡å‹å¹¶ç”Ÿæˆæ–‡æœ¬

é€šè¿‡ä¸Šè¿°è¿‡ç¨‹ï¼Œæˆ‘ä»¬æˆåŠŸåœ°å°† Hugging Face çš„é¢„è®­ç»ƒæƒé‡åŠ è½½åˆ°æˆ‘ä»¬å®šä¹‰çš„æ¨¡å‹ä¸­ï¼Œå¹¶åˆå§‹åŒ–äº†æ¨¡å‹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥éªŒè¯æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

* æˆ‘ä»¬å¯ä»¥è¿è¡Œä¸€ä¸ª Python è„šæœ¬ï¼ˆå¦‚ `train_gpt2.py`ï¼‰ï¼Œæ£€æŸ¥æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿæ­£ç¡®åŠ è½½æƒé‡å¹¶é¡ºåˆ©å¯åŠ¨ï¼›
* å¦‚æœæ¨¡å‹èƒ½å¤ŸåŠ è½½è€Œä¸å´©æºƒï¼Œå°±è¯´æ˜æˆ‘ä»¬çš„æ¨¡å‹å’Œæƒé‡åŠ è½½éƒ¨åˆ†æˆåŠŸã€‚

ç„¶åï¼Œä¸ºäº†è¿›ä¸€æ­¥ç¡®è®¤æ¨¡å‹å·¥ä½œæ­£å¸¸ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æµ‹è¯• **æ–‡æœ¬ç”Ÿæˆ**ã€‚æˆ‘ä»¬å¯ä»¥è¾“å…¥ä¸€ä¸ªæç¤ºï¼ˆå¦‚ "Hello, I'm a language model,"ï¼‰å¹¶è§‚å¯Ÿæ¨¡å‹æ˜¯å¦èƒ½å¤Ÿç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ã€‚

---

æ€»ç»“æ¥è¯´ï¼Œè¿™éƒ¨åˆ†çš„å·¥ä½œä¸»è¦æ˜¯åŠ è½½å¹¶åˆå§‹åŒ–æ¨¡å‹æƒé‡ï¼Œå¹¶è¿›è¡Œä¸€äº›å¿…è¦çš„è°ƒæ•´ï¼Œç¡®ä¿ PyTorch å’Œ TensorFlow ä¹‹é—´çš„å…¼å®¹æ€§ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°è¯•ä½¿ç”¨è¿™ä¸ªåŠ è½½å¥½çš„æ¨¡å‹æ¥ç”Ÿæˆæ–‡æœ¬ã€‚

---

å¦‚æœä½ éœ€è¦æ›´è¯¦ç»†çš„ä»£ç è§£è¯»æˆ–ç”Ÿæˆæµ‹è¯•çš„å…·ä½“å®ç°æ­¥éª¤ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ã€‚

Â·
```
    @classmethod
    def from_pretrained(cls, model_type):
        """Loads pretrained GPT-2 model weights from huggingface"""
        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}
        from transformers import GPT2LMHeadModel
        print("loading weights from pretrained gpt: %s" % model_type)

        # n_layer, n_head and n_embd are determined from model_type
        config_args = {
            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params
            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params
            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params
            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params
        }[model_type]
        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints
        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints
        # create a from-scratch initialized minGPT model
        config = GPTConfig(**config_args)
        model = GPT(config)
        sd = model.state_dict()
        sd_keys = sd.keys()
        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param

        # init a huggingface/transformers model
        model_hf = GPT2LMHeadModel.from_pretrained(model_type)
        sd_hf = model_hf.state_dict()

        # copy while ensuring all of the parameters are aligned and match in names and shapes
        sd_keys_hf = sd_hf.keys()
        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer
        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)
        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']
        # basically the openai checkpoints use a "Conv1D" module, but we only want to use a vanilla Linear
        # this means that we have to transpose these weights when we import them
        assert len(sd_keys_hf) == len(sd_keys), f"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}"
        for k in sd_keys_hf:
            if any(k.endswith(w) for w in transposed):
                # special treatment for the Conv1D weights we need to transpose
                assert sd_hf[k].shape[::-1] == sd[k].shape
                with torch.no_grad():
                    sd[k].copy_(sd_hf[k].t())
            else:
                # vanilla copy over the other parameters
                assert sd_hf[k].shape == sd[k].shape
                with torch.no_grad():
                    sd[k].copy_(sd_hf[k])

        return model
```

è¿™æ®µä»£ç å®ç°çš„æ˜¯ï¼š**æŠŠ HuggingFace çš„é¢„è®­ç»ƒ GPT-2 æƒé‡åŠ è½½åˆ°è‡ªå·±å®ç°çš„ GPT ç±»ä¸­**ã€‚
æˆ‘ä»¬é€å—æ‹†å¼€æ¥çœ‹ï¼š

---

## 1. æ–¹æ³•å®šä¹‰

```python
@classmethod
def from_pretrained(cls, model_type):
    """Loads pretrained GPT-2 model weights from huggingface"""
```

* å®šä¹‰äº†ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºæ„é€  GPT å¯¹è±¡ï¼Œå¹¶ç›´æ¥åŠ è½½æŒ‡å®šå¤§å°çš„ GPT-2 é¢„è®­ç»ƒæƒé‡ã€‚
* `model_type` å¿…é¡»æ˜¯ HuggingFace æ”¯æŒçš„ GPT-2 æ¨¡å‹ä¹‹ä¸€ï¼š`gpt2, gpt2-medium, gpt2-large, gpt2-xl`ã€‚

---

## 2. æ¨¡å‹é…ç½®

```python
config_args = {
    'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params
    'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params
    'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params
    'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params
}[model_type]
config_args['vocab_size'] = 50257 # GPT-2 å›ºå®šè¯è¡¨å¤§å°
config_args['block_size'] = 1024  # GPT-2 å›ºå®šä¸Šä¸‹æ–‡é•¿åº¦
```

* æ ¹æ®ä¸åŒçš„ `model_type`ï¼Œè®¾ç½® Transformer çš„å±‚æ•°ã€å¤´æ•°å’Œ embedding ç»´åº¦ã€‚
* è¯è¡¨å¤§å°å’Œæœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦åœ¨ GPT-2 é‡Œæ˜¯å›ºå®šçš„ã€‚

---

## 3. åˆå§‹åŒ–ä¸€ä¸ªè‡ªå·±å†™çš„ GPT æ¨¡å‹

```python
config = GPTConfig(**config_args)
model = GPT(config)
sd = model.state_dict()
sd_keys = sd.keys()
sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]
```

* ç”¨è¿™äº›å‚æ•°åˆ›å»ºä¸€ä¸ªè‡ªå·±å®ç°çš„ `GPT` æ¨¡å‹ï¼ˆä»å¤´éšæœºåˆå§‹åŒ–ï¼‰ã€‚
* `state_dict()` è¿”å›æ¨¡å‹å‚æ•°å­—å…¸ã€‚
* å»æ‰ `attn.bias` è¿™ç§ bufferï¼ˆåªæ˜¯ maskï¼Œä¸æ˜¯éœ€è¦è®­ç»ƒçš„å‚æ•°ï¼‰ã€‚

---

## 4. åˆå§‹åŒ– HuggingFace çš„ GPT-2 æ¨¡å‹

```python
from transformers import GPT2LMHeadModel
model_hf = GPT2LMHeadModel.from_pretrained(model_type)
sd_hf = model_hf.state_dict()
```

* ä» HuggingFace åŠ è½½å¯¹åº”çš„ GPT-2 æƒé‡ã€‚
* `state_dict()` å¾—åˆ° HuggingFace æ¨¡å‹çš„å‚æ•°å­—å…¸ã€‚

---

## 5. å¤„ç† key å¯¹é½

```python
sd_keys_hf = sd_hf.keys()
sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]
sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]
```

* HuggingFace çš„ state\_dict é‡Œæœ‰ä¸€äº› bufferï¼ˆmaskï¼‰ï¼Œè¿™é‡Œä¹Ÿè¿‡æ»¤æ‰ï¼Œä¿è¯å‚æ•°æ•°é‡ä¸€è‡´ã€‚

---

## 6. å¤„ç† Conv1D ä¸ Linear çš„åŒºåˆ«

```python
transposed = ['attn.c_attn.weight', 'attn.c_proj.weight',
              'mlp.c_fc.weight', 'mlp.c_proj.weight']
```

* OpenAI åŸç‰ˆ GPT-2 ç”¨çš„æ˜¯ `Conv1D` å®ç°ï¼ˆå…¶å®å°±æ˜¯å¸¦è½¬ç½®çš„çº¿æ€§å±‚ï¼‰ï¼Œè€Œæˆ‘ä»¬å®ç°çš„ GPT ç”¨ `nn.Linear`ã€‚
* æ‰€ä»¥æœ‰äº›æƒé‡éœ€è¦è½¬ç½®ä¸€ä¸‹æ‰èƒ½å¯¹é½ã€‚

---

## 7. æ‹·è´å‚æ•°

```python
for k in sd_keys_hf:
    if any(k.endswith(w) for w in transposed):
        assert sd_hf[k].shape[::-1] == sd[k].shape
        with torch.no_grad():
            sd[k].copy_(sd_hf[k].t())  # è½¬ç½®åæ‹·è´
    else:
        assert sd_hf[k].shape == sd[k].shape
        with torch.no_grad():
            sd[k].copy_(sd_hf[k])      # ç›´æ¥æ‹·è´
```

* éå† HuggingFace çš„æƒé‡ã€‚
* å¦‚æœæ˜¯éœ€è¦è½¬ç½®çš„æƒé‡ï¼Œå°± `.t()` è½¬ç½®åå†æ‹·è´ï¼›å¦åˆ™ç›´æ¥æ‹·è´ã€‚
* ç”¨ `torch.no_grad()` é¿å…å½±å“æ¢¯åº¦è®¡ç®—ã€‚

---

## 8. è¿”å›æ¨¡å‹

```python
return model
```

* æœ€ç»ˆè¿”å›ä¸€ä¸ªåŠ è½½äº† HuggingFace é¢„è®­ç»ƒæƒé‡çš„è‡ªå®šä¹‰ GPT æ¨¡å‹ã€‚

---

## âœ… æ€»ç»“

è¿™ä¸ªæ–¹æ³•åšçš„äº‹æ˜¯ï¼š

1. æ ¹æ® `model_type` åˆ›å»ºä¸€ä¸ªè‡ªå·±å†™çš„ GPT æ¨¡å‹ï¼ˆç»“æ„ä¸€è‡´ï¼Œä½†éšæœºåˆå§‹åŒ–ï¼‰ã€‚
2. ä» HuggingFace ä¸‹è½½é¢„è®­ç»ƒå¥½çš„ GPT-2 æƒé‡ã€‚
3. å¯¹é½å¹¶æ‹·è´å‚æ•°ï¼ˆéƒ¨åˆ†æƒé‡éœ€è¦è½¬ç½®ï¼‰ã€‚
4. è¿”å›åŠ è½½å¥½é¢„è®­ç»ƒå‚æ•°çš„ GPT æ¨¡å‹ã€‚

---

ğŸ‘‰ ç®€å•è¯´ï¼Œå®ƒæ˜¯ä¸€ä¸ª **æ¡¥æ¥å™¨**ï¼Œè®©ä½ å†™çš„ `GPT` ç±»å¯ä»¥ç›´æ¥ç”¨ HuggingFace çš„é¢„è®­ç»ƒæƒé‡ï¼Œè€Œä¸ç”¨ä»é›¶è®­ç»ƒã€‚

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **æµç¨‹å›¾ï¼ˆHuggingFace GPT-2 â†’ æƒé‡å¯¹é½ â†’ è‡ªå·±çš„ GPT ç±»ï¼‰**ï¼Œè®©æ•´ä½“è¿‡ç¨‹æ›´ç›´è§‚ï¼Ÿ


# config = GPTConfig(**config_args) è§£é‡Šä¸€ä¸‹**

å¥½ ğŸ‘Œ æˆ‘æ¥æ‹†è§£è¿™ä¸€å¥ï¼š

```python
config = GPTConfig(**config_args)
```

---

## 1. `**` åœ¨ Python é‡Œçš„å«ä¹‰

* `**dict` è¯­æ³•è¡¨ç¤º **å­—å…¸è§£åŒ… (dictionary unpacking)**ã€‚
* ä¼šæŠŠå­—å…¸é‡Œçš„ **key-value å¯¹å±•å¼€**ï¼Œä½œä¸ºå…³é”®å­—å‚æ•°ä¼ ç»™å‡½æ•°/ç±»ã€‚

ä¸¾ä¸ªä¾‹å­ï¼š

```python
def foo(a, b, c):
    print(a, b, c)

args = {"a": 1, "b": 2, "c": 3}
foo(**args)   # ç›¸å½“äº foo(a=1, b=2, c=3)
```

æ‰€ä»¥ï¼š

```python
GPTConfig(**config_args)
```

ç­‰ä»·äºï¼š

```python
GPTConfig(n_layer=..., n_head=..., n_embd=..., vocab_size=..., block_size=...)
```

---

## 2. åœ¨ä½ çš„ä»£ç é‡Œ

`config_args` æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæ¯”å¦‚ï¼š

```python
config_args = {
    'n_layer': 12,
    'n_head': 12,
    'n_embd': 768,
    'vocab_size': 50257,
    'block_size': 1024
}
```

é‚£ä¹ˆï¼š

```python
config = GPTConfig(**config_args)
```

å°±æ˜¯åœ¨è°ƒç”¨ `GPTConfig` æ„é€ å‡½æ•°æ—¶ï¼Œä¼ å…¥è¿™äº›é…ç½®å‚æ•°ã€‚

---

## 3. ä¸ºä»€ä¹ˆè¦ç”¨ `**`

* **æ›´ç®€æ´**ï¼šä¸ç”¨ä¸€è¡Œè¡Œå†™ `GPTConfig(n_layer=config_args['n_layer'], ...)`ã€‚
* **æ›´çµæ´»**ï¼šå¦‚æœå­—å…¸é‡Œå‚æ•°æ›´å¤šæˆ–æœ‰å˜åŒ–ï¼Œä¸éœ€è¦æ”¹è°ƒç”¨ä»£ç ã€‚
* **é€šç”¨æ€§**ï¼šå¯ä»¥å¾ˆæ–¹ä¾¿åœ°ä» JSON / YAML / dict é‡ŒåŠ è½½å‚æ•°ï¼Œå†ç”¨ `**` ä¼ ç»™ç±»ã€‚

---

âœ… **æ€»ç»“**
`**config_args` çš„ä½œç”¨æ˜¯æŠŠä¸€ä¸ªå­—å…¸é‡Œçš„é”®å€¼å¯¹å±•å¼€ï¼Œä½œä¸ºå…³é”®å­—å‚æ•°ä¼ ç»™ `GPTConfig`ã€‚
è¿™è¡Œä»£ç çš„æ„æ€æ˜¯ï¼š**æ ¹æ® `config_args` é‡Œçš„å‚æ•°åˆ›å»ºä¸€ä¸ª GPTConfig é…ç½®å¯¹è±¡**ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠ `*args` å’Œ `**kwargs` çš„åŒºåˆ«ä¹Ÿè®²ä¸€ä¸‹ï¼Ÿè¿™æ ·ä½ èƒ½æŠŠè¿™ä¸ªè¯­æ³•æŒæ¡é€å½»ã€‚


# implementing the forward pass to get logits

generate from this model we have to be able to forward it we didn't actually write that code yet so here's the
forward function so the input to the forward is going to be our indices our tokens uh
token indices and they are always of shape B BYT and so we have batch
dimension of B and then we have the time dimension of up to T and the T can't be
more than the block size the block size is is the maximum sequence length so B BYT indices arranged is sort of like a
two-dimensional layout and remember that basically every single row of this is of size up to uh block size and this is T
tokens that are in a sequence and then we have B independent sequences stacked up in a batch so that this is
efficient now here we are forwarding the position embeddings and the token embeddings and this code should be very
recognizable from the previous lecture so um we basically use uh a range which
is kind of like a version of range but for pytorch uh and we're iterating from Z to T and creating this uh positions uh
sort of uh indices um and then we are making sure that
they're in the same device as idx because we're not going to be training on only CPU that's going to be too inefficient we want to be training on
GPU and that's going to come in in a bit uh then we have the position embeddings and the token embeddings and
the addition operation of those two now notice that the position embed are going to be identical for every single row of
uh of input and so there's broadcasting hidden inside this plus where we have to
create an additional Dimension here and then these two add up because the same position embeddings apply at every
single row of our example stacked up in a batch then we forward the Transformer blocks and finally the last layer norm
and the LM head so what comes out after forward is the logits and if the input
was B BYT indices then at every single B by T we will calculate the uh logits for
what token comes next in the sequence so what is the token B t+1 the one on the
right of this token and B app size here is the number of possible tokens and so
therefore this is the tensor that we're going to obtain and these low jits are just a softmax away from becoming
probabilities so this is the forward pass of the network and now we can get load and so we're going to be able to
generate from the model imminently okay so now we're going to try to set up the identical thing on the

å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## å®ç°å‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰ä»¥è·å¾— Logits

ä¸ºäº†ä»æ¨¡å‹ä¸­ç”Ÿæˆæ–‡æœ¬ï¼Œæˆ‘ä»¬å¿…é¡»å®ç°å‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°å°†è®¡ç®— **logits**ï¼ˆå³é¢„æµ‹ä¸‹ä¸€ä¸ª token çš„æœªå½’ä¸€åŒ–æ¦‚ç‡ï¼‰ã€‚

### è¾“å…¥æ•°æ®æ ¼å¼

* **è¾“å…¥**ï¼šæ¨¡å‹çš„è¾“å…¥æ˜¯ token çš„ **ç´¢å¼•**ï¼Œä¹Ÿå°±æ˜¯ä¸€ç»„æ•°å­—è¡¨ç¤ºçš„ token åºåˆ—ï¼Œå½¢çŠ¶æ˜¯ `[B, T]`ï¼Œå…¶ä¸­ï¼š

  * `B` æ˜¯ **æ‰¹æ¬¡å¤§å°ï¼ˆbatch sizeï¼‰**ï¼Œè¡¨ç¤ºæ¯æ¬¡è®­ç»ƒä¸­å¤„ç†çš„ç‹¬ç«‹åºåˆ—æ•°ï¼›
  * `T` æ˜¯ **æ—¶é—´ç»´åº¦ï¼ˆtime dimensionï¼‰**ï¼Œå³æ¯ä¸ªåºåˆ—çš„é•¿åº¦ï¼Œæœ€å¤šä¸º `block_size`ï¼ˆæ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼‰ã€‚

  è¿™æ ·ï¼Œè¾“å…¥çš„å½¢çŠ¶å°±å˜æˆäº†ä¸€ä¸ªäºŒç»´å¼ é‡ï¼Œå¤§å°ä¸º `[B, T]`ï¼Œå…¶ä¸­æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªåºåˆ—ï¼Œæ¯ä¸€åˆ—å¯¹åº”ä¸€ä¸ª token çš„ç´¢å¼•ã€‚

### å¤„ç†ä½ç½®å’Œ token åµŒå…¥ï¼ˆEmbeddingï¼‰

* æˆ‘ä»¬å°† **token åµŒå…¥ï¼ˆtoken embeddingsï¼‰** å’Œ **ä½ç½®åµŒå…¥ï¼ˆposition embeddingsï¼‰** ç›¸åŠ ï¼Œå¾—åˆ°æ¯ä¸ª token çš„æœ€ç»ˆè¡¨ç¤ºã€‚
* å¯¹äº **ä½ç½®åµŒå…¥**ï¼Œæ¯ä¸ªä½ç½®çš„åµŒå…¥æ˜¯ç›¸åŒçš„ï¼Œè¿™æ„å‘³ç€å¯¹äºæ¯ä¸€è¡Œï¼ˆå³æ¯ä¸ªåºåˆ—ï¼‰ï¼Œ**ä½ç½®åµŒå…¥æ˜¯å¹¿æ’­çš„**ï¼Œè¿™ä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æ·»åŠ ä¸€ä¸ªé¢å¤–çš„ç»´åº¦æ¥å®ç°å¹¿æ’­ã€‚
* é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¯ä¸ª token éƒ½ä¼šæœ‰ä¸€ä¸ª **ä½ç½®ç¼–ç ** å’Œ **è¯åµŒå…¥**ï¼Œå®ƒä»¬ç›¸åŠ å½¢æˆæœ€ç»ˆçš„è¾“å…¥è¡¨ç¤ºã€‚

### å‰å‘ä¼ æ’­è¿‡ç¨‹

1. **å‰å‘ä¼ æ’­ä½ç½®åµŒå…¥å’Œ token åµŒå…¥**ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å°† token ç´¢å¼•è½¬åŒ–ä¸º token åµŒå…¥ï¼Œå¹¶åŠ ä¸Šä½ç½®åµŒå…¥ã€‚
2. **é€šè¿‡ Transformer å—**ï¼šç„¶åï¼Œè¿™äº›åŠ å’Œåçš„åµŒå…¥ä¼šä¼ é€’åˆ° Transformer çš„å„ä¸ª **Block** ä¸­è¿›è¡Œå¤„ç†ã€‚æ¯ä¸ª Block åŒ…å«å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰å’Œå‰é¦ˆç½‘ç»œï¼ˆFeed-forward Networkï¼‰ã€‚
3. **æœ€åçš„ LayerNorm å’Œ LM Head**ï¼šç»è¿‡ Transformer å¤„ç†åï¼Œæœ€åä¼šç»è¿‡ä¸€ä¸ª **LayerNorm** å’Œä¸€ä¸ª **è¯­è¨€æ¨¡å‹å¤´ï¼ˆLM Headï¼‰**ï¼Œå³ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå°† Transformer è¾“å‡ºçš„ç»´åº¦æ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°ï¼Œç”Ÿæˆ logitsã€‚

### è¾“å‡ºç»“æœ

* **è¾“å‡º**ï¼šæœ€ç»ˆçš„è¾“å‡ºæ˜¯ **logits**ï¼Œå®ƒçš„å½¢çŠ¶æ˜¯ `[B, T, vocab_size]`ï¼Œå³æ¯ä¸ªåºåˆ—çš„æ¯ä¸ª token éƒ½æœ‰ä¸€ä¸ªä¸è¯æ±‡è¡¨å¤§å°ç›¸ç­‰çš„å‘é‡ï¼Œè¡¨ç¤ºé¢„æµ‹ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒã€‚
* **logits å˜ä¸ºæ¦‚ç‡**ï¼šé€šè¿‡å¯¹è¿™äº› logits è¿›è¡Œ **softmax** æ“ä½œï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»è€Œè¿›è¡Œé‡‡æ ·ç”Ÿæˆæ–‡æœ¬ã€‚

### æ€»ç»“

å‰å‘ä¼ æ’­çš„è¿‡ç¨‹éå¸¸ç®€å•ï¼š

1. è¾“å…¥æ˜¯ä¸€ä¸ª `[B, T]` çš„ token ç´¢å¼•ï¼›
2. é€šè¿‡ä½ç½®åµŒå…¥å’Œ token åµŒå…¥ç›¸åŠ ï¼Œå¾—åˆ°è¾“å…¥è¡¨ç¤ºï¼›
3. é€šè¿‡ Transformer æ¨¡å‹è¿›è¡Œå¤„ç†ï¼Œå¾—åˆ° logitsï¼›
4. logits å¯ä»¥é€šè¿‡ softmax è½¬åŒ–ä¸ºæ¦‚ç‡ï¼Œç”¨äºç”Ÿæˆä¸‹ä¸€ä¸ª tokenã€‚

é€šè¿‡è¿™ä¸ªå‰å‘ä¼ æ’­å‡½æ•°ï¼Œæˆ‘ä»¬å°±å¯ä»¥è®¡ç®—å‡ºæ¯ä¸ªä½ç½®çš„é¢„æµ‹ï¼Œæœ€ç»ˆå®ç°æ–‡æœ¬ç”Ÿæˆã€‚

---

å¦‚æœä½ æœ‰è¿›ä¸€æ­¥çš„ç–‘é—®æˆ–æƒ³è¦äº†è§£å¦‚ä½•å®ç°ç”Ÿæˆéƒ¨åˆ†çš„ä»£ç ï¼Œå¯ä»¥éšæ—¶å‘Šè¯‰æˆ‘ã€‚

```
    def forward(self, idx):
        # idx is of shape (B, T)
        B, T = idx.size()
        assert T <= self.config.block_size, f"Cannot forward sequence of length {T}, block size is only {self.config.block_size}"
        # forward the token and posisition embeddings
        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)
        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)
        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)
        x = tok_emb + pos_emb
        # forward the blocks of the transformer
        for block in self.transformer.h:
            x = block(x)
        # forward the final layernorm and the classifier
        x = self.transformer.ln_f(x)
        logits = self.lm_head(x) # (B, T, vocab_size)
        return logits
```


å¥½é—®é¢˜ ğŸ‘
è¿™æ®µä»£ç æ˜¯ **GPT æ¨¡å‹çš„å‰å‘ä¼ æ’­ (forward pass)**ï¼Œå®ƒä»è¾“å…¥çš„ token id è®¡ç®—å‡ºé¢„æµ‹çš„ä¸‹ä¸€ä¸ª token æ¦‚ç‡ã€‚é€è¡Œè§£æå¦‚ä¸‹ï¼š

---

## 1. è¾“å…¥ä¸æ£€æŸ¥

```python
def forward(self, idx):
    # idx is of shape (B, T)
    B, T = idx.size()
    assert T <= self.config.block_size, f"Cannot forward sequence of length {T}, block size is only {self.config.block_size}"
```

* è¾“å…¥ `idx` æ˜¯ä¸€ä¸ªæ•´å‹å¼ é‡ï¼Œå½¢çŠ¶ `(B, T)`ï¼š

  * `B` = batch size
  * `T` = åºåˆ—é•¿åº¦
* ä¾‹å¦‚ï¼Œ`idx` å¯èƒ½æ˜¯ï¼š

  ```
  [[ 502,  123,  345, ...],
   [  76,   11, 9873, ...]]
  ```
* `assert` ç”¨æ¥ä¿è¯åºåˆ—é•¿åº¦ `T` ä¸è¶…è¿‡æ¨¡å‹æ”¯æŒçš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ `block_size`ï¼ˆGPT-2 å›ºå®š 1024ï¼‰ã€‚

---

## 2. ä½ç½®ç´¢å¼•ä¸ embedding

```python
pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)
pos_emb = self.transformer.wpe(pos) # (T, n_embd)
tok_emb = self.transformer.wte(idx) # (B, T, n_embd)
x = tok_emb + pos_emb
```

* `pos`: ç”Ÿæˆ `[0, 1, 2, ..., T-1]`ï¼Œè¡¨ç¤ºåºåˆ—ä¸­æ¯ä¸ª token çš„ä½ç½®ã€‚
* `wpe`: ä½ç½® embedding (position embedding)ï¼Œè¾“å‡º `(T, n_embd)`ã€‚
* `wte`: è¯ embedding (word token embedding)ï¼ŒæŠŠ `idx` çš„ token id æ˜ å°„ä¸ºå‘é‡ï¼Œè¾“å‡º `(B, T, n_embd)`ã€‚
* `x = tok_emb + pos_emb`:

  * æŠŠè¯å‘é‡å’Œä½ç½®å‘é‡ç›¸åŠ ï¼Œå¾—åˆ°å¸¦æœ‰ä½ç½®ä¿¡æ¯çš„ token è¡¨ç¤ºã€‚
  * PyTorch ä¼šå¹¿æ’­ `(T, n_embd)` â†’ `(B, T, n_embd)`ã€‚

---

## 3. Transformer blocks

```python
for block in self.transformer.h:
    x = block(x)
```

* `self.transformer.h` æ˜¯ä¸€ä¸ª `nn.ModuleList`ï¼ŒåŒ…å«å¤šå±‚ Transformer Blockã€‚
* æ¯ä¸ª `block` å†…éƒ¨å°±æ˜¯ï¼š

  * å¤šå¤´è‡ªæ³¨æ„åŠ› (Causal Self-Attention)
  * å‰é¦ˆç½‘ç»œ (Feed Forward, MLP)
  * æ®‹å·®è¿æ¥ + LayerNorm
* æ¯ä¸€å±‚éƒ½ä¼šæ›´æ–° token è¡¨ç¤ºï¼Œä½¿å®ƒèƒ½èåˆä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

---

## 4. æœ€åçš„ LayerNorm

```python
x = self.transformer.ln_f(x)
```

* åœ¨ Transformer çš„æœ€ååŠ  LayerNormï¼Œç¨³å®šè®­ç»ƒã€‚
* è¾“å‡ºå½¢çŠ¶ä»æ˜¯ `(B, T, n_embd)`ã€‚

---

## 5. è¯­è¨€æ¨¡å‹å¤´ (lm\_head)

```python
logits = self.lm_head(x) # (B, T, vocab_size)
return logits
```

* `lm_head` æ˜¯ä¸€ä¸ª `nn.Linear(n_embd, vocab_size)`ã€‚
* è¾“å…¥ï¼š`(B, T, n_embd)`
* è¾“å‡ºï¼š`(B, T, vocab_size)`
* æ¯ä¸ªä½ç½® `t` éƒ½ä¼šé¢„æµ‹ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆsoftmax ä¹‹åï¼‰ã€‚

---

## âœ… æ€»ç»“

æ•´ä½“æµç¨‹æ˜¯ï¼š

```
idx (B,T) â†’ token embedding (B,T,n_embd)
          + position embedding (T,n_embd)
          â†“
å¤šå±‚ Transformer block (contextualized representation)
          â†“
LayerNorm
          â†“
lm_head Linear â†’ (B,T,vocab_size)
```

**è¾“å‡º logits**ï¼šæ˜¯æ¨¡å‹å¯¹æ¯ä¸ªæ—¶é—´æ­¥ã€æ¯ä¸ªè¯çš„é¢„æµ‹åˆ†æ•°ï¼Œsoftmax åå°±æ˜¯æ¦‚ç‡ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **æ•°æ®æµ shape å˜åŒ–å›¾**ï¼ˆä» idx â†’ embedding â†’ transformer â†’ lm\_head â†’ logitsï¼‰ï¼Ÿè¿™æ ·ä½ ä¼šæ›´ç›´è§‚åœ°çœ‹åˆ°æ¯ä¸€æ­¥çš„ç»´åº¦å˜åŒ–ã€‚


# idx.shapeå’Œidx.size()çš„åŒºåˆ«

å¥½é—®é¢˜ ğŸ‘
åœ¨ PyTorch é‡Œï¼Œ**`tensor.shape` å’Œ `tensor.size()` æœ¬è´¨ä¸Šå‡ ä¹ä¸€æ ·**ï¼Œä½†æœ‰ä¸€äº›å°åŒºåˆ«ï¼š

---

## 1. ç›¸åŒç‚¹

* éƒ½èƒ½è¿”å›å¼ é‡çš„ç»´åº¦ä¿¡æ¯ã€‚
* å¯¹äºä¸€ä¸ªå½¢çŠ¶ `(B, T)` çš„å¼ é‡ï¼š

  ```python
  idx.shape   # è¿”å› torch.Size([B, T])
  idx.size()  # è¿”å› torch.Size([B, T])
  ```

  ä¸¤è€…ç»“æœæ˜¯ä¸€æ ·çš„ã€‚

---

## 2. åŒºåˆ«

* **`tensor.shape`**

  * æ›´â€œPythonicâ€ï¼Œå†™æ³•ç®€æ´ã€‚
  * ç›´æ¥æ˜¯ `torch.Size` å¯¹è±¡ï¼ˆå…¶å®å°±æ˜¯ä¸€ä¸ª tuple çš„å­ç±»ï¼‰ã€‚
  * ä¸€èˆ¬åœ¨è°ƒè¯•æˆ–å¿«é€Ÿå–ç»´åº¦æ—¶ç”¨ï¼Œæ¯”å¦‚ `x.shape[0]`ã€‚

* **`tensor.size()`**

  * æ˜¯ä¸€ä¸ªæ–¹æ³•ï¼ˆè°ƒç”¨å‡½æ•°ï¼‰ã€‚
  * å’Œ `shape` è¿”å›ä¸€æ ·çš„ä¸œè¥¿ã€‚
  * è¿˜æ”¯æŒ `tensor.size(dim)`ï¼Œç›´æ¥å–æŸä¸€ç»´ï¼š

    ```python
    idx.size(0)  # B
    idx.size(1)  # T
    ```

---

## 3. å°ä¾‹å­

```python
import torch

x = torch.zeros(3, 5)

print(x.shape)      # torch.Size([3, 5])
print(x.size())     # torch.Size([3, 5])
print(x.size(0))    # 3
print(x.size(1))    # 5
```

---

## âœ… æ€»ç»“

* `tensor.shape` å’Œ `tensor.size()` ç»“æœä¸€æ ·ï¼Œè¿”å› **torch.Size**ï¼ˆå¯ä»¥å½“ tuple ç”¨ï¼‰ã€‚
* **`size(dim)`** æ˜¯ `size()` ç‹¬æœ‰çš„åŠŸèƒ½ï¼Œå¯ä»¥ç›´æ¥å–æŸä¸€ç»´ã€‚
* åœ¨ç°ä»£ PyTorch ä»£ç é‡Œï¼Œå¤§å®¶å¸¸ç”¨ `.shape`ï¼Œå› ä¸ºæ›´ç®€æ´ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ å¯¹æ¯”ä¸€ä¸‹ **`.shape`ã€`.size()`ã€`len(tensor)`** åœ¨å–ç»´åº¦æ—¶çš„å·®å¼‚ï¼Ÿ

# è§£é‡Šä¸€ä¸‹ idx.device

```
pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)
```

å¾ˆå¥½ ğŸ‘Œ ä½ æåˆ°çš„è¿™ä¸€è¡Œï¼š

```python
pos = torch.arange(0, T, dtype=torch.long, device=idx.device)
```

å…³é”®åœ¨äº **`idx.device`**ã€‚

---

## 1. ä»€ä¹ˆæ˜¯ `.device`

* åœ¨ PyTorch ä¸­ï¼Œ**æ¯ä¸ª Tensor éƒ½æœ‰ä¸€ä¸ªè®¾å¤‡å±æ€§**ï¼Œè¡¨ç¤ºå®ƒå½“å‰å­˜æ”¾åœ¨ **CPU** è¿˜æ˜¯ **GPU**ï¼ˆå“ªå— GPUï¼‰ã€‚
* ä½ å¯ä»¥é€šè¿‡ `.device` æŸ¥çœ‹ï¼š

  ```python
  x = torch.tensor([1,2,3])
  print(x.device)   # cpu

  x = torch.tensor([1,2,3], device="cuda:0")
  print(x.device)   # cuda:0  (è¡¨ç¤ºåœ¨ç¬¬0å—GPU)
  ```

---

## 2. ä¸ºä»€ä¹ˆè¦å†™ `device=idx.device`

* è¿™è¡Œä»£ç åˆ›å»ºäº†ä¸€ä¸ªä½ç½®åºåˆ— `pos = [0, 1, 2, ..., T-1]`ã€‚
* å¦‚æœ `idx` åœ¨ GPU ä¸Šï¼Œè€Œ `pos` é»˜è®¤åœ¨ CPU ä¸Šï¼Œå°±ä¼šå‡ºç° **è·¨è®¾å¤‡è®¡ç®—æŠ¥é”™**ï¼š

  ```
  RuntimeError: Expected all tensors to be on the same device
  ```
* å†™æˆ `device=idx.device`ï¼Œèƒ½ä¿è¯ `pos` å’Œ `idx` åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Šã€‚

  * å¦‚æœ `idx` åœ¨ CPUï¼Œ`pos` ä¹Ÿåœ¨ CPUã€‚
  * å¦‚æœ `idx` åœ¨ GPU:0ï¼Œ`pos` ä¹Ÿåœ¨ GPU:0ã€‚

è¿™æ ·å°±é¿å…äº†ä¸å¿…è¦çš„æ•°æ®ä¼ è¾“å’ŒæŠ¥é”™ã€‚

---

## 3. å°ä¾‹å­

```python
idx = torch.randint(0, 100, (2, 5), device="cuda:0")  # idx åœ¨ GPU
print(idx.device)  # cuda:0

pos = torch.arange(0, idx.size(1), device=idx.device)
print(pos.device)  # cuda:0
```

---

âœ… **æ€»ç»“**

* `idx.device` è¡¨ç¤ºå¼ é‡ `idx` å½“å‰æ‰€åœ¨çš„è®¾å¤‡ï¼ˆCPU æˆ–æŸå— GPUï¼‰ã€‚
* åœ¨ `torch.arange(..., device=idx.device)` é‡Œï¼Œç”¨å®ƒæ¥ç¡®ä¿ç”Ÿæˆçš„å¼ é‡ `pos` å’Œ `idx` åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Šï¼Œé¿å…è·¨è®¾å¤‡è®¡ç®—é”™è¯¯ã€‚

---

è¦ä¸è¦æˆ‘é¡ºä¾¿å¸®ä½ æ•´ç†ä¸€ä¸‹ PyTorch é‡Œ **Tensor è·¨è®¾å¤‡ç§»åŠ¨çš„æ–¹æ³•**ï¼ˆæ¯”å¦‚ `.to()`, `.cuda()`, `.cpu()`ï¼‰ï¼Ÿ


# sampling init, prefix tokens, tokenization

left here that matches hug and face on the right so here we've sampled from the pipeline and we sampled five times up to
30 tokens with the prefix of hello I'm a language model and these are the completions that we achieved so we're
going to try to replicate that on the left here so number turn sequences is five max length is 30 so the first thing
we do of course is we initialize our model then we put it into evaluation mode now this is a good practice to put
the model into eval when you're not going to be training it you're just going to be using it and I don't
actually know if this is doing anything right now for the following reason our model up above here contains no modules
or layers that actually have a different uh Behavior at training or evaluation time so for example Dropout batch norm
and a bunch of other layers have this kind of behavior but all of these layers that we've used here should be identical
in both training and evaluation time um so so potentially model that eval does
nothing but then I'm not actually sure if this is the case and maybe pytorch internals uh do some clever things
depending on the evaluation mode uh inside here the next thing we're doing here is we are moving the entire model
to Cuda so we're moving this all of the tensors to GPU so I'm sshed here to a
cloud box and I have a bunch of gpus on this box and here I'm moving the entire
model and all of its members and all of its tensors and everything like that everything gets shipped off to basically
a whole separate computer that is sitting on the GPU and the GPU is connected to the uh CPU and they can
communicate but it's basically a whole separate computer with its own computer architecture and it's really well catered to parallel processing tasks
like those of running neural networks so I'm doing this so that the model lives on the GPU a whole separate computer and
it's just going to make our code a lot more efficient because all of this stuff runs a lot more efficiently on the
gpus so that's the model itself now uh the next thing we want to
do is we want to start with this as the prefix when we do the generation so
let's actually create those prefix tokens so here's the code that I've written we're going to import the tich
token library from open Ai and we're going to get the gpt2 encoding so that's the tokenizer for gpt2 and then we're
going to encode this string and get a list of integers which are the tokens uh
now these integers here should actually be fairly straightforward because we can just copy paste this string and we can
sort of inspect what it is in tick tokenizer so just pasting that in these are the tokens that are going to come
out so this list of integers is what we expect tokens to become and as you
recall if you saw my video of course all the tokens they're just little string chunks right so these are this is the
chunc of this string into gpt2 tokens so once we have those tokens it's
a list of integers we can create a torch tensor out of it in this case it's eight tokens and then we're going to replicate
these eight tokens for five times to get five rows of eight tokens and that is
our initial um input X as I call it here and it lives on the GPU as well so X now
is this idx that we can put into forward to get our logits so that we know what
comes as the sixth token uh sorry as the ninth token in every one
of these five rows okay and we are now ready to generate so let me paste in one more code block

å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## é‡‡æ ·åˆå§‹åŒ–ã€å‰ç¼€ tokens å’Œåˆ†è¯ï¼ˆTokenizationï¼‰

åœ¨è¿™éƒ¨åˆ†ï¼Œæˆ‘ä»¬è¦æ¨¡æ‹Ÿä» Hugging Face çš„ GPT-2 æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹ï¼Œå¹¶å°½é‡å¤åˆ¶å‡ºç±»ä¼¼çš„æ•ˆæœã€‚æˆ‘ä»¬ä»ä¸€ä¸ªç»™å®šçš„å‰ç¼€ï¼ˆå¦‚â€œHello, I'm a language modelâ€ï¼‰å¼€å§‹ï¼Œç„¶åç”Ÿæˆæœ€å¤š 30 ä¸ª tokensã€‚

### 1. åˆå§‹åŒ–æ¨¡å‹å’Œè®¾ç½®è¯„ä¼°æ¨¡å¼

é¦–å…ˆï¼Œæˆ‘ä»¬ **åˆå§‹åŒ–æ¨¡å‹**ï¼Œç„¶åå°†å…¶åˆ‡æ¢åˆ° **è¯„ä¼°æ¨¡å¼ï¼ˆevalï¼‰**ï¼š

* è¿™æ˜¯ä¸€ä¸ªå¥½çš„å®è·µï¼Œå½“ä½ ä¸å†è®­ç»ƒæ¨¡å‹ï¼Œåªæ˜¯ç”¨å®ƒè¿›è¡Œæ¨ç†æ—¶ï¼Œå°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ã€‚
* ä½†æ˜¯ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼šæˆ‘ä»¬çš„æ¨¡å‹ä¸­æ²¡æœ‰ä½¿ç”¨ä»»ä½•åœ¨è®­ç»ƒå’Œè¯„ä¼°æ—¶è¡¨ç°ä¸åŒçš„å±‚ï¼ˆä¾‹å¦‚ Dropout æˆ– BatchNormï¼‰ã€‚è¿™äº›å±‚åœ¨è®­ç»ƒå’Œè¯„ä¼°æ—¶çš„è¡Œä¸ºæ˜¯ä¸åŒçš„ï¼Œä½†æˆ‘ä»¬è¿™é‡Œä½¿ç”¨çš„å±‚åœ¨ä¸¤ç§æ¨¡å¼ä¸‹éƒ½æ˜¯ç›¸åŒçš„ã€‚å› æ­¤ï¼Œå°†æ¨¡å‹è®¾ç½®ä¸º `eval()` å¯èƒ½ä¸ä¼šåšä»»ä½•æ”¹å˜ï¼Œä½†ä¸ºäº†ç¡®ä¿æœ€ä½³å®è·µï¼Œè¿˜æ˜¯è°ƒç”¨äº†è¿™ä¸ªå‡½æ•°ã€‚

### 2. å°†æ¨¡å‹è¿ç§»åˆ° GPU

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ¨¡å‹å’Œæ‰€æœ‰å‚æ•°è½¬ç§»åˆ° **GPU** ä¸Šï¼š

* æˆ‘ä»¬é€šè¿‡ SSH è¿æ¥åˆ°äº‘ç«¯æœåŠ¡å™¨ï¼Œå¹¶å°†æ¨¡å‹è¿ç§»åˆ° GPU ä¸Šã€‚
* GPU æ˜¯ä¸“é—¨è®¾è®¡æ¥å¤„ç†ç¥ç»ç½‘ç»œè¿™ç§å¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—ä»»åŠ¡çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†æ•´ä¸ªæ¨¡å‹ç§»åŠ¨åˆ° GPU ä¸Šèƒ½æ˜¾è‘—æé«˜æ•ˆç‡ã€‚
* è¿™æ ·ï¼Œæ‰€æœ‰çš„è®¡ç®—å°±ä¼šåœ¨ GPU ä¸Šå®Œæˆï¼Œèƒ½åŠ é€Ÿè®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚

### 3. åˆ›å»ºå‰ç¼€ tokens

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¦ç”¨ **GPT-2 çš„åˆ†è¯å™¨** æ¥å°†å‰ç¼€æ–‡æœ¬ï¼ˆå¦‚â€œHello, I'm a language modelâ€ï¼‰è½¬åŒ–ä¸º tokenï¼š

* æˆ‘ä»¬ä½¿ç”¨ OpenAI æä¾›çš„ `tiktoken` åº“æ¥è·å– GPT-2 çš„åˆ†è¯å™¨ï¼Œå¹¶å°†æ–‡æœ¬å­—ç¬¦ä¸²ç¼–ç æˆå¯¹åº”çš„ tokenã€‚

* é€šè¿‡ç¼–ç ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ç»„æ•´æ•°ï¼Œæ¯ä¸ªæ•´æ•°ä»£è¡¨ä¸€ä¸ª tokenã€‚ä½ å¯ä»¥æŠŠè¿™äº›æ•´æ•°ç†è§£ä¸ºå°†å­—ç¬¦ä¸²æ‹†åˆ†æˆçš„å°å—ï¼ˆtokenï¼‰ã€‚

  ä¾‹å¦‚ï¼Œå‰ç¼€â€œHello, I'm a language modelâ€ ä¼šè¢«æ‹†åˆ†æˆä¸€ç³»åˆ—çš„ tokenï¼Œè¿™äº› token ä¼šè¢«è½¬æ¢æˆæ•´æ•°è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼š

  * `"Hello"` â†’ `token_id_1`
  * `","` â†’ `token_id_2`
  * `"I'm"` â†’ `token_id_3`
  * ä¾æ­¤ç±»æ¨...

* ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›æ•´æ•°è½¬æ¢ä¸º PyTorch å¼ é‡ï¼ˆtensorï¼‰ï¼Œå¹¶æŠŠå®ƒä»¬æ”¾åˆ° GPU ä¸Šã€‚

### 4. åˆå§‹åŒ–è¾“å…¥å¹¶å‡†å¤‡ç”Ÿæˆ

* æˆ‘ä»¬å°†ç”Ÿæˆçš„ token åˆ—è¡¨ï¼ˆä¾‹å¦‚ 8 ä¸ª tokenï¼‰**å¤åˆ¶ 5 æ¬¡**ï¼Œè¿™æ ·å°±å¾—åˆ°äº† 5 è¡Œï¼Œæ¯è¡ŒåŒ…å«ç›¸åŒçš„å‰ç¼€ tokensã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±åˆ›å»ºäº†ä¸€ä¸ªå¤§å°ä¸º `[5, 8]` çš„å¼ é‡ï¼Œä»£è¡¨ 5 ä¸ªç‹¬ç«‹çš„è¾“å…¥åºåˆ—ï¼Œæ¯ä¸ªåºåˆ—åŒ…å« 8 ä¸ª tokenã€‚
* è¿™ä¸ªå¼ é‡å°±æ˜¯æˆ‘ä»¬çš„ **åˆå§‹è¾“å…¥**ï¼Œå¹¶ä¸”å®ƒä¹Ÿå­˜å‚¨åœ¨ GPU ä¸Šã€‚

### 5. å‰å‘ä¼ æ’­ç”Ÿæˆ logits

* æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿™äº› token ç´¢å¼•ä½œä¸ºè¾“å…¥ä¼ é€’ç»™æ¨¡å‹ï¼Œå¹¶é€šè¿‡å‰å‘ä¼ æ’­ï¼ˆ`forward()`ï¼‰è·å¾— **logits**ã€‚
* logits æ˜¯æ¨¡å‹å¯¹æ¯ä¸ª token ä½ç½®ä¸Šä¸‹ä¸€ä¸ª token çš„é¢„æµ‹ï¼Œè¡¨ç¤ºçš„æ˜¯â€œä¸‹ä¸€ä¸ª token æ˜¯ä»€ä¹ˆâ€çš„æœªå½’ä¸€åŒ–æ¦‚ç‡ã€‚

é€šè¿‡è¿™äº› logitsï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»è€Œè¿›è¡Œé‡‡æ ·ã€‚

---

æ€»ä¹‹ï¼Œæ•´ä¸ªè¿‡ç¨‹çš„æ ¸å¿ƒæ˜¯ï¼š

1. å°†è¾“å…¥æ–‡æœ¬è½¬åŒ–ä¸º token å¹¶ç”Ÿæˆè¾“å…¥å¼ é‡ï¼›
2. å°†æ¨¡å‹å’Œæ•°æ®è¿ç§»åˆ° GPU ä¸Šï¼›
3. ä½¿ç”¨å‰ç¼€ tokens åˆå§‹åŒ–ç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶è¿›è¡Œå‰å‘ä¼ æ’­è®¡ç®— logitsã€‚

é€šè¿‡è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬å°±èƒ½ä»æ¨¡å‹ä¸­ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ã€‚

---

å¦‚æœä½ éœ€è¦è¿›ä¸€æ­¥äº†è§£å¦‚ä½•å®ç°åç»­çš„æ–‡æœ¬ç”Ÿæˆï¼Œæˆ–è€…å¦‚ä½•å¤„ç† logits è½¬åŒ–ä¸ºæ¦‚ç‡è¿›è¡Œé‡‡æ ·ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼

```
model = GPT.from_pretrained('gpt2')
model.eval()
model.to('cuda')

# prefix tokens
import tiktoken
enc = tiktoken.get_encoding('gpt2')
tokens = enc.encode("Hello, I'm a language model,")
tokens = torch.tensor(tokens, dtype=torch.long) # (8,)
tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5, 8)
x = tokens.to('cuda')

# generate! right now x is (B, T) where B = 5, T = 8
# set the seed to 42
torch.manual_seed(42)
torch.cuda.manual_seed(42)
while x.size(1) < max_length:
    # forward the model to get the logits
    with torch.no_grad():
        logits = model(x) # (B, T, vocab_size)
        # take the logits at the last position
        logits = logits[:, -1, :] # (B, vocab_size)
        # get the probabilities
        probs = F.softmax(logits, dim=-1)
        # do top-k sampling of 50 (huggingface pipeline default)
        # topk_probs here becomes (5, 50), topk_indices is (5, 50)
        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)
        # select a token from the top-k probabilities
        # note: multinomial does not demand the input to sum to 1
        ix = torch.multinomial(topk_probs, 1) # (B, 1)
        # gather the corresponding indices
        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)
        # append to the sequence
        x = torch.cat((x, xcol), dim=1)

# print the generated text
for i in range(num_return_sequences):
    tokens = x[i, :max_length].tolist()
    decoded = enc.decode(tokens)
    print(">", decoded)
```

éå¸¸æ£’çš„é—®é¢˜ ğŸ‘ ä½ è´´çš„è¿™æ®µä»£ç å…¶å®å°±æ˜¯åœ¨ç”¨ä½ è‡ªå·±å®ç°çš„ **GPT + HuggingFace é¢„è®­ç»ƒæƒé‡**åš **æ–‡æœ¬ç”Ÿæˆ (Text Generation)**ï¼Œè€Œä¸”è¿˜åŠ äº† **Top-k é‡‡æ ·**ã€‚æˆ‘æ¥é€æ­¥è§£é‡Šï¼š

---

## 1. åŠ è½½æ¨¡å‹

```python
model = GPT.from_pretrained('gpt2')
model.eval()
model.to('cuda')
```

* `GPT.from_pretrained('gpt2')`ï¼šç”¨æˆ‘ä»¬è‡ªå·±å®ç°çš„ `from_pretrained` æ–¹æ³•ï¼ŒåŠ è½½ HuggingFace çš„ GPT-2 é¢„è®­ç»ƒå‚æ•°ã€‚
* `model.eval()`ï¼šè®¾ç½®æˆæ¨ç†æ¨¡å¼ï¼ˆå…³é—­ dropout ç­‰è®­ç»ƒç›¸å…³æ“ä½œï¼‰ã€‚
* `model.to('cuda')`ï¼šæŠŠæ¨¡å‹æ”¾åˆ° GPU ä¸Šã€‚

---

## 2. å‡†å¤‡è¾“å…¥ tokens

```python
import tiktoken
enc = tiktoken.get_encoding('gpt2')
tokens = enc.encode("Hello, I'm a language model,") # ç¼–ç æˆ token id
tokens = torch.tensor(tokens, dtype=torch.long) # shape (8,)
tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5, 8)
x = tokens.to('cuda')
```

* ç”¨ `tiktoken` å¯¹å­—ç¬¦ä¸²ç¼–ç ï¼Œå¾—åˆ° GPT-2 çš„ token åºåˆ—ï¼ˆ8 ä¸ª tokenï¼‰ã€‚
* `unsqueeze(0)` â†’ ä» `(8,)` å˜æˆ `(1, 8)`ã€‚
* `.repeat(num_return_sequences, 1)` â†’ å¤åˆ¶ 5 ä»½ï¼Œå¾—åˆ° `(5, 8)`ã€‚

  * `B=5` è¡¨ç¤ºç”Ÿæˆ 5 ä¸ªä¸åŒæ ·æœ¬ã€‚
  * `T=8` è¡¨ç¤ºå‰ç¼€åºåˆ—é•¿åº¦ä¸º 8ã€‚
* `x.to('cuda')` â†’ æŠŠè¾“å…¥æ”¾åˆ° GPU ä¸Šã€‚

---

## 3. ç”Ÿæˆå¾ªç¯

```python
torch.manual_seed(42)
torch.cuda.manual_seed(42)
while x.size(1) < max_length:
```

* è®¾ç½®éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°ã€‚
* å¾ªç¯ç›´åˆ°ç”Ÿæˆé•¿åº¦è¾¾åˆ° `max_length`ã€‚

---

## 4. å‰å‘æ¨ç†ï¼Œå–æœ€åä½ç½® logits

```python
with torch.no_grad():
    logits = model(x) # (B, T, vocab_size)
    logits = logits[:, -1, :] # (B, vocab_size)
```

* `model(x)`ï¼šè¾“å‡ºé¢„æµ‹åˆ†å¸ƒ `(B, T, vocab_size)`ã€‚
* `logits[:, -1, :]`ï¼šåªå–åºåˆ—æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹ï¼Œshape = `(B, vocab_size)`ã€‚

  * å› ä¸ºæˆ‘ä»¬è¦é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚

---

## 5. Softmax + Top-k é‡‡æ ·

```python
probs = F.softmax(logits, dim=-1) # (B, vocab_size)
topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # (B, 50)
```

* `softmax` â†’ æŠŠ logits è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚
* `topk(50)` â†’ åªä¿ç•™æ¦‚ç‡æœ€é«˜çš„ 50 ä¸ª tokenï¼ˆTop-k ç­–ç•¥ï¼Œé¿å…ä½æ¦‚ç‡è¯ç ´åç»“æœï¼‰ã€‚

```python
ix = torch.multinomial(topk_probs, 1) # (B, 1)
xcol = torch.gather(topk_indices, -1, ix) # (B, 1)
```

* `torch.multinomial(topk_probs, 1)` â†’ æŒ‰ç…§æ¦‚ç‡éšæœºé‡‡æ · 1 ä¸ª token ç´¢å¼•ï¼ˆåœ¨ Top-50 é‡Œï¼‰ã€‚
* `torch.gather(topk_indices, -1, ix)` â†’ æŠŠé‡‡æ ·åˆ°çš„ç´¢å¼•è¿˜åŸæˆçœŸæ­£çš„ token idã€‚

---

## 6. æ‹¼æ¥åˆ°è¾“å…¥åºåˆ—

```python
x = torch.cat((x, xcol), dim=1)  # (B, T+1)
```

* æŠŠæ–°ç”Ÿæˆçš„ token æ‹¼åˆ°åºåˆ—æœ«å°¾ï¼Œè¿›å…¥ä¸‹ä¸€è½®å¾ªç¯ã€‚

---

## 7. è¾“å‡ºç»“æœ

```python
for i in range(num_return_sequences):
    tokens = x[i, :max_length].tolist()
    decoded = enc.decode(tokens)
    print(">", decoded)
```

* å¯¹ 5 ä¸ªç”Ÿæˆç»“æœé€ä¸€è§£ç ï¼Œè¾“å‡ºæœ€ç»ˆæ–‡æœ¬ã€‚

---

## âœ… æ€»ç»“

è¿™æ®µä»£ç å®Œæ•´å®ç°äº† **è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ**ï¼š

1. **å‡†å¤‡è¾“å…¥**ï¼šæŠŠå‰ç¼€ `"Hello, I'm a language model,"` ç¼–ç æˆ tokensã€‚
2. **è¿­ä»£ç”Ÿæˆ**ï¼šæ¯æ¬¡å–‚å…¥åºåˆ— â†’ å¾—åˆ°æœ€å token çš„é¢„æµ‹åˆ†å¸ƒ â†’ Top-k ç­–ç•¥ â†’ éšæœºé‡‡æ ·ä¸€ä¸ª token â†’ æ‹¼æ¥åˆ°åºåˆ—ã€‚
3. **å¾ªç¯**ï¼šç›´åˆ°è¾¾åˆ° `max_length`ã€‚
4. **è§£ç è¾“å‡º**ï¼šæŠŠ token id è½¬å›å­—ç¬¦ä¸²ã€‚

---

ğŸ‘‰ ç›´ç™½ä¸€ç‚¹ï¼š
è¿™å°±æ˜¯ HuggingFace `pipeline("text-generation")` çš„æ‰‹å·¥å®ç°ç‰ˆæœ¬ï¼Œåªä¸è¿‡ä½ è‡ªå·±æ§åˆ¶äº† logitsã€softmaxã€top-kã€é‡‡æ ·è¿™äº›ç»†èŠ‚ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **ç”Ÿæˆå¾ªç¯çš„æµç¨‹å›¾**ï¼ˆä»è¾“å…¥ â†’ logits â†’ softmax â†’ top-k â†’ é‡‡æ · â†’ æ‹¼æ¥ â†’ å¾ªç¯ â†’ è¾“å‡ºæ–‡æœ¬ï¼‰ï¼Œè®©æµç¨‹æ›´ç›´è§‚ï¼Ÿ

```
model = GPT.from_pretrained('gpt2')
model.eval()
model.to('cuda')

# prefix tokens
import tiktoken
enc = tiktoken.get_encoding('gpt2')
tokens = enc.encode("Hello, I'm a language model,")
tokens = torch.tensor(tokens, dtype=torch.long) # (8,)
tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5, 8)
x = tokens.to('cuda')

# generate! right now x is (B, T) where B = 5, T = 8
# set the seed to 42
torch.manual_seed(42)
torch.cuda.manual_seed(42)
while x.size(1) < max_length:
    # forward the model to get the logits
    with torch.no_grad():
        logits = model(x) # (B, T, vocab_size)
        # take the logits at the last position
        logits = logits[:, -1, :] # (B, vocab_size)
        # get the probabilities
        probs = F.softmax(logits, dim=-1)
        # do top-k sampling of 50 (huggingface pipeline default)
        # topk_probs here becomes (5, 50), topk_indices is (5, 50)
        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)
        # select a token from the top-k probabilities
        # note: multinomial does not demand the input to sum to 1
        ix = torch.multinomial(topk_probs, 1) # (B, 1)
        # gather the corresponding indices
        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)
        # append to the sequence
        x = torch.cat((x, xcol), dim=1)

# print the generated text
for i in range(num_return_sequences):
    tokens = x[i, :max_length].tolist()
    decoded = enc.decode(tokens)
    print(">", decoded)
```

éå¸¸æ£’çš„é—®é¢˜ ğŸ‘ ä½ è´´çš„è¿™æ®µä»£ç å…¶å®å°±æ˜¯åœ¨ç”¨ä½ è‡ªå·±å®ç°çš„ **GPT + HuggingFace é¢„è®­ç»ƒæƒé‡**åš **æ–‡æœ¬ç”Ÿæˆ (Text Generation)**ï¼Œè€Œä¸”è¿˜åŠ äº† **Top-k é‡‡æ ·**ã€‚æˆ‘æ¥é€æ­¥è§£é‡Šï¼š

---

## 1. åŠ è½½æ¨¡å‹

```python
model = GPT.from_pretrained('gpt2')
model.eval()
model.to('cuda')
```

* `GPT.from_pretrained('gpt2')`ï¼šç”¨æˆ‘ä»¬è‡ªå·±å®ç°çš„ `from_pretrained` æ–¹æ³•ï¼ŒåŠ è½½ HuggingFace çš„ GPT-2 é¢„è®­ç»ƒå‚æ•°ã€‚
* `model.eval()`ï¼šè®¾ç½®æˆæ¨ç†æ¨¡å¼ï¼ˆå…³é—­ dropout ç­‰è®­ç»ƒç›¸å…³æ“ä½œï¼‰ã€‚
* `model.to('cuda')`ï¼šæŠŠæ¨¡å‹æ”¾åˆ° GPU ä¸Šã€‚

---

## 2. å‡†å¤‡è¾“å…¥ tokens

```python
import tiktoken
enc = tiktoken.get_encoding('gpt2')
tokens = enc.encode("Hello, I'm a language model,") # ç¼–ç æˆ token id
tokens = torch.tensor(tokens, dtype=torch.long) # shape (8,)
tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5, 8)
x = tokens.to('cuda')
```

* ç”¨ `tiktoken` å¯¹å­—ç¬¦ä¸²ç¼–ç ï¼Œå¾—åˆ° GPT-2 çš„ token åºåˆ—ï¼ˆ8 ä¸ª tokenï¼‰ã€‚
* `unsqueeze(0)` â†’ ä» `(8,)` å˜æˆ `(1, 8)`ã€‚
* `.repeat(num_return_sequences, 1)` â†’ å¤åˆ¶ 5 ä»½ï¼Œå¾—åˆ° `(5, 8)`ã€‚

  * `B=5` è¡¨ç¤ºç”Ÿæˆ 5 ä¸ªä¸åŒæ ·æœ¬ã€‚
  * `T=8` è¡¨ç¤ºå‰ç¼€åºåˆ—é•¿åº¦ä¸º 8ã€‚
* `x.to('cuda')` â†’ æŠŠè¾“å…¥æ”¾åˆ° GPU ä¸Šã€‚

---

## 3. ç”Ÿæˆå¾ªç¯

```python
torch.manual_seed(42)
torch.cuda.manual_seed(42)
while x.size(1) < max_length:
```

* è®¾ç½®éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°ã€‚
* å¾ªç¯ç›´åˆ°ç”Ÿæˆé•¿åº¦è¾¾åˆ° `max_length`ã€‚

---

## 4. å‰å‘æ¨ç†ï¼Œå–æœ€åä½ç½® logits

```python
with torch.no_grad():
    logits = model(x) # (B, T, vocab_size)
    logits = logits[:, -1, :] # (B, vocab_size)
```

* `model(x)`ï¼šè¾“å‡ºé¢„æµ‹åˆ†å¸ƒ `(B, T, vocab_size)`ã€‚
* `logits[:, -1, :]`ï¼šåªå–åºåˆ—æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹ï¼Œshape = `(B, vocab_size)`ã€‚

  * å› ä¸ºæˆ‘ä»¬è¦é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚

---

## 5. Softmax + Top-k é‡‡æ ·

```python
probs = F.softmax(logits, dim=-1) # (B, vocab_size)
topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # (B, 50)
```

* `softmax` â†’ æŠŠ logits è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚
* `topk(50)` â†’ åªä¿ç•™æ¦‚ç‡æœ€é«˜çš„ 50 ä¸ª tokenï¼ˆTop-k ç­–ç•¥ï¼Œé¿å…ä½æ¦‚ç‡è¯ç ´åç»“æœï¼‰ã€‚

```python
ix = torch.multinomial(topk_probs, 1) # (B, 1)
xcol = torch.gather(topk_indices, -1, ix) # (B, 1)
```

* `torch.multinomial(topk_probs, 1)` â†’ æŒ‰ç…§æ¦‚ç‡éšæœºé‡‡æ · 1 ä¸ª token ç´¢å¼•ï¼ˆåœ¨ Top-50 é‡Œï¼‰ã€‚
* `torch.gather(topk_indices, -1, ix)` â†’ æŠŠé‡‡æ ·åˆ°çš„ç´¢å¼•è¿˜åŸæˆçœŸæ­£çš„ token idã€‚

---

## 6. æ‹¼æ¥åˆ°è¾“å…¥åºåˆ—

```python
x = torch.cat((x, xcol), dim=1)  # (B, T+1)
```

* æŠŠæ–°ç”Ÿæˆçš„ token æ‹¼åˆ°åºåˆ—æœ«å°¾ï¼Œè¿›å…¥ä¸‹ä¸€è½®å¾ªç¯ã€‚

---

## 7. è¾“å‡ºç»“æœ

```python
for i in range(num_return_sequences):
    tokens = x[i, :max_length].tolist()
    decoded = enc.decode(tokens)
    print(">", decoded)
```

* å¯¹ 5 ä¸ªç”Ÿæˆç»“æœé€ä¸€è§£ç ï¼Œè¾“å‡ºæœ€ç»ˆæ–‡æœ¬ã€‚

---

## âœ… æ€»ç»“

è¿™æ®µä»£ç å®Œæ•´å®ç°äº† **è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ**ï¼š

1. **å‡†å¤‡è¾“å…¥**ï¼šæŠŠå‰ç¼€ `"Hello, I'm a language model,"` ç¼–ç æˆ tokensã€‚
2. **è¿­ä»£ç”Ÿæˆ**ï¼šæ¯æ¬¡å–‚å…¥åºåˆ— â†’ å¾—åˆ°æœ€å token çš„é¢„æµ‹åˆ†å¸ƒ â†’ Top-k ç­–ç•¥ â†’ éšæœºé‡‡æ ·ä¸€ä¸ª token â†’ æ‹¼æ¥åˆ°åºåˆ—ã€‚
3. **å¾ªç¯**ï¼šç›´åˆ°è¾¾åˆ° `max_length`ã€‚
4. **è§£ç è¾“å‡º**ï¼šæŠŠ token id è½¬å›å­—ç¬¦ä¸²ã€‚

---

ğŸ‘‰ ç›´ç™½ä¸€ç‚¹ï¼š
è¿™å°±æ˜¯ HuggingFace `pipeline("text-generation")` çš„æ‰‹å·¥å®ç°ç‰ˆæœ¬ï¼Œåªä¸è¿‡ä½ è‡ªå·±æ§åˆ¶äº† logitsã€softmaxã€top-kã€é‡‡æ ·è¿™äº›ç»†èŠ‚ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **ç”Ÿæˆå¾ªç¯çš„æµç¨‹å›¾**ï¼ˆä»è¾“å…¥ â†’ logits â†’ softmax â†’ top-k â†’ é‡‡æ · â†’ æ‹¼æ¥ â†’ å¾ªç¯ â†’ è¾“å‡ºæ–‡æœ¬ï¼‰ï¼Œè®©æµç¨‹æ›´ç›´è§‚ï¼Ÿ


```
tokens = torch.tensor(tokens, dtype=torch.long)  # shape (8,)
```

å¥½çš„ ğŸ‘ æˆ‘æ¥è§£é‡Šè¿™è¡Œä»£ç ï¼š

```python
tokens = torch.tensor(tokens, dtype=torch.long)  # shape (8,)
```

---

## 1. ä¸Šä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆ

```python
tokens = enc.encode("Hello, I'm a language model,")
```

* `enc.encode(...)` ä¼šæŠŠæ–‡æœ¬è½¬æˆ **token id åˆ—è¡¨**ï¼Œæ¯”å¦‚ï¼š

  ```python
  [15496, 11, 314, 257, 5766, 1158, 11]
  ```
* è¿™æ˜¯ä¸€ä¸ª Python **list**ï¼Œé•¿åº¦æ˜¯ 8ï¼ˆæ‰€ä»¥ shape = `(8,)`ï¼‰ã€‚

---

## 2. è½¬æˆ PyTorch Tensor

```python
torch.tensor(tokens, dtype=torch.long)
```

* `torch.tensor(list, dtype=...)` ä¼šæŠŠ Python åˆ—è¡¨è½¬æ¢æˆ **PyTorch å¼ é‡**ã€‚
* `dtype=torch.long` è¡¨ç¤ºç”¨ **64 ä½æ•´æ•°**ï¼ˆ`int64`ï¼‰ï¼Œè¿™æ˜¯ PyTorch é‡Œ embedding å±‚å¸¸ç”¨çš„ç´¢å¼•ç±»å‹ï¼ˆtoken id å¿…é¡»æ˜¯ long ç±»å‹ï¼‰ã€‚
* è½¬æ¢åå°±æ˜¯ï¼š

  ```python
  tensor([15496,    11,   314,   257,  5766,  1158,    11, ...], dtype=torch.int64)
  ```
* shape æ˜¯ `(8,)`ï¼Œå³ä¸€ç»´å¼ é‡ï¼Œé•¿åº¦ä¸º 8ã€‚

---

## 3. ä¸ºä»€ä¹ˆå¿…é¡»ç”¨ `long` ç±»å‹ï¼Ÿ

* åœ¨ PyTorch é‡Œï¼Œ`nn.Embedding` çš„è¾“å…¥å¿…é¡»æ˜¯ `torch.long` æˆ– `torch.int64` ç±»å‹ã€‚
* å¦‚æœæ˜¯ `float32` æˆ– `int32` ä¼šæŠ¥é”™ã€‚
* æ‰€ä»¥è¿™é‡Œæ˜¾å¼æŒ‡å®š `dtype=torch.long`ï¼Œä¿è¯ token å¯ä»¥æ­£ç¡®ä¼ å…¥ embedding å±‚ã€‚

---

âœ… **æ€»ç»“**
è¿™è¡Œä»£ç åšäº†ä¸¤ä»¶äº‹ï¼š

1. æŠŠ HuggingFace çš„ token id åˆ—è¡¨ï¼ˆPython listï¼‰è½¬æ¢æˆ PyTorch å¼ é‡ã€‚
2. æŒ‡å®šä¸º `torch.long` ç±»å‹ï¼ˆint64ï¼‰ï¼Œå› ä¸º embedding éœ€è¦æ•´æ•°ç´¢å¼•ã€‚
3. æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªå½¢çŠ¶ `(8,)` çš„å¼ é‡ï¼Œè¡¨ç¤ºä¸€å¥è¯çš„ 8 ä¸ª tokenã€‚

---

è¦ä¸è¦æˆ‘é¡ºä¾¿ç»™ä½ è§£é‡Šä¸€ä¸‹æ¥ä¸‹æ¥é‚£è¡Œï¼š

```python
tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)  # (5, 8)
```

ä¸ºä»€ä¹ˆè¦è¿™æ ·æ‰©å±•ç»´åº¦ã€å¤åˆ¶æˆ `(5, 8)`ï¼Ÿ


```
tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)  # (5, 8)
```

å¥½ ğŸ‘ æˆ‘æ¥è§£é‡Šè¿™ä¸€è¡Œï¼š

```python
tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)  # (5, 8)
```

---

## 1. å‰ä¸€æ­¥

æ­¤æ—¶ `tokens` æ˜¯ï¼š

```python
tensor([15496,    11,   314,   257,  5766,  1158,    11, ...], dtype=torch.int64)
```

* shape = `(8,)`ï¼Œè¡¨ç¤ºä¸€å¥è¯çš„ 8 ä¸ª token idã€‚
* ä½†æ¨¡å‹çš„è¾“å…¥ä¸€èˆ¬æ˜¯ **äºŒç»´å¼ é‡** `(B, T)`ï¼š

  * `B` = batch size
  * `T` = åºåˆ—é•¿åº¦

---

## 2. `unsqueeze(0)`

```python
tokens = tokens.unsqueeze(0)
```

* åœ¨ **ç¬¬ 0 ç»´**å¢åŠ ä¸€ä¸ªç»´åº¦ï¼š

  * åŸæ¥ `(8,)` â†’ å˜æˆ `(1, 8)`
* ç›¸å½“äºå‘Šè¯‰æ¨¡å‹ï¼šæˆ‘ä»¬ç°åœ¨æœ‰ **1 ä¸ªæ ·æœ¬ï¼Œé•¿åº¦ä¸º 8**ã€‚

---

## 3. `.repeat(num_return_sequences, 1)`

```python
tokens = tokens.repeat(num_return_sequences, 1)
```

* æŠŠ `(1, 8)` çš„å¼ é‡æ²¿ç€ç¬¬ 0 ç»´ï¼ˆbatch ç»´ï¼‰å¤åˆ¶ `num_return_sequences` æ¬¡ã€‚
* å¦‚æœ `num_return_sequences = 5`ï¼š

  * `(1, 8)` â†’ `(5, 8)`
  * ç›¸å½“äºå¤åˆ¶äº† 5 ä»½ç›¸åŒçš„å‰ç¼€åºåˆ—ã€‚

---

## 4. ä¸ºä»€ä¹ˆè¦è¿™æ ·åšï¼Ÿ

å› ä¸ºæˆ‘ä»¬å¸Œæœ›ç”¨ **åŒä¸€ä¸ªå‰ç¼€** ç”Ÿæˆ **å¤šä¸ªä¸åŒçš„åºåˆ—**ï¼ˆé€šè¿‡éšæœºé‡‡æ ·ä¿è¯å¤šæ ·æ€§ï¼‰ã€‚

* å¦‚æœåªç”¨ `(1, 8)`ï¼Œåªèƒ½ç”Ÿæˆ 1 ä¸ªç»“æœã€‚
* ç”¨ `(5, 8)`ï¼Œæ¨¡å‹å°±èƒ½å¹¶è¡Œç”Ÿæˆ 5 ä¸ªç»“æœï¼Œæ¯ä¸ªç»“æœåœ¨åç»­çš„ top-k é‡‡æ ·é‡Œå¯èƒ½ä¼šä¸åŒã€‚

---

## 5. ä¸¾ä¾‹

```python
tokens = torch.tensor([10, 20, 30])
print(tokens.shape)   # (3,)

tokens = tokens.unsqueeze(0)
print(tokens.shape)   # (1, 3)

tokens = tokens.repeat(5, 1)
print(tokens.shape)   # (5, 3)
print(tokens)
# tensor([[10, 20, 30],
#         [10, 20, 30],
#         [10, 20, 30],
#         [10, 20, 30],
#         [10, 20, 30]])
```

---

âœ… **æ€»ç»“**
è¿™è¡Œä»£ç çš„ä½œç”¨æ˜¯ï¼š

1. `unsqueeze(0)` â†’ æŠŠä¸€ç»´ token åºåˆ— `(8,)` å˜æˆ `(1, 8)`ï¼ŒåŠ ä¸Š batch ç»´ã€‚
2. `repeat(num_return_sequences, 1)` â†’ å¤åˆ¶å¤šä»½ï¼Œå¾—åˆ° `(5, 8)`ï¼Œå¯ä»¥ä¸€æ¬¡ç”Ÿæˆ 5 ä¸ªä¸åŒç»“æœã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠ `(8,) â†’ (1,8) â†’ (5,8)` çš„ç»´åº¦å˜åŒ–ç”»æˆä¸€ä¸ªå°ç¤ºæ„å›¾ï¼Ÿ

```
logits = logits[:, -1, :]  # (B, vocab_size)
```

å¥½ ğŸ‘Œ æˆ‘æ¥è§£é‡Šè¿™ä¸€è¡Œï¼š

```python
logits = logits[:, -1, :]  # (B, vocab_size)
```

---

## 1. ä¸Šä¸€æ­¥è¾“å‡ºæ˜¯ä»€ä¹ˆï¼Ÿ

å‰é¢ `model(x)` çš„è¾“å‡ºæ˜¯ï¼š

```python
logits = model(x)   # (B, T, vocab_size)
```

* **å½¢çŠ¶ (B, T, vocab\_size)**

  * `B` = batch size
  * `T` = å½“å‰åºåˆ—é•¿åº¦
  * `vocab_size` = è¯è¡¨å¤§å° (50257 for GPT-2)
* å«ä¹‰ï¼šæ¨¡å‹å¯¹åºåˆ—ä¸­ **æ¯ä¸ªä½ç½®çš„ token** éƒ½ç»™å‡ºäº†ä¸€ä¸ªé¢„æµ‹åˆ†å¸ƒã€‚

---

## 2. `logits[:, -1, :]`

* `:` â†’ ä¿ç•™æ‰€æœ‰ batchã€‚
* `-1` â†’ å–åºåˆ—çš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼ˆä¸‹æ ‡ -1 è¡¨ç¤ºæœ€åä¸€ä¸ªä½ç½®ï¼‰ã€‚
* `:` â†’ ä¿ç•™ vocab ç»´åº¦ã€‚

æ‰€ä»¥å½¢çŠ¶ä» `(B, T, vocab_size)` å˜æˆï¼š

```
(B, vocab_size)
```

---

## 3. ä¸ºä»€ä¹ˆåªå–æœ€åä¸€ä¸ªä½ç½®ï¼Ÿ

å› ä¸º **è‡ªå›å½’è¯­è¨€æ¨¡å‹** æ˜¯é€æ­¥ç”Ÿæˆçš„ï¼š

* ç»™å®šåºåˆ—å‰ `T` ä¸ª tokenï¼Œè¦é¢„æµ‹ç¬¬ `T+1` ä¸ª tokenã€‚
* æ‰€ä»¥æˆ‘ä»¬åªå…³å¿ƒ **æœ€åä¸€ä¸ª token çš„é¢„æµ‹åˆ†å¸ƒ**ã€‚
* å‰é¢çš„é¢„æµ‹ï¼ˆå¯¹ä½ç½® 1â€¦T-1 çš„ logitsï¼‰åœ¨è¿™ä¸€è½®ç”Ÿæˆé‡Œç”¨ä¸åˆ°ã€‚

---

## 4. ä¸¾ä¸ªä¾‹å­

å‡è®¾ï¼š

* batch size B=2
* å½“å‰åºåˆ—é•¿åº¦ T=3
* è¯è¡¨å¤§å° vocab\_size=5

é‚£ä¹ˆ `model(x)` è¾“å‡ºï¼š

```
logits.shape = (2, 3, 5)
```

æ„æ€æ˜¯ï¼š

* å¯¹ batch=2 ä¸ªæ ·æœ¬
* æ¯ä¸ªåºåˆ—æœ‰ 3 ä¸ªä½ç½®
* æ¯ä¸ªä½ç½®æœ‰ 5 ä¸ªè¯çš„é¢„æµ‹åˆ†å¸ƒ

å–æœ€åä¸€ä¸ªä½ç½®ï¼š

```
logits = logits[:, -1, :]  
logits.shape = (2, 5)
```

ç°åœ¨åªå‰©ä¸‹æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹åˆ†å¸ƒï¼ˆæ¯ä¸ªæ ·æœ¬ä¸€ä¸ªé•¿åº¦=5 çš„å‘é‡ï¼‰ã€‚

---

âœ… **æ€»ç»“**
`logits[:, -1, :]` çš„ä½œç”¨æ˜¯ï¼š

* ä»æ¨¡å‹çš„è¾“å‡º `(B, T, vocab_size)` ä¸­
* åªå–å‡º **æœ€åä¸€ä¸ª token çš„é¢„æµ‹åˆ†å¸ƒ**
* å¾—åˆ° `(B, vocab_size)`ï¼Œæ–¹ä¾¿åç»­åš softmax â†’ é‡‡æ ·ä¸‹ä¸€ä¸ª tokenã€‚

---

è¦ä¸è¦æˆ‘å†å¸®ä½ è§£é‡Šä¸€ä¸‹ **å¦‚æœä¸å– `-1`ï¼Œè€Œç›´æ¥ç”¨ `(B,T,vocab_size)` çš„ logits ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ**


å¥½ ğŸ‘ æˆ‘æ¥ç”¨ä¸€ä¸ªå°ä¾‹å­è§£é‡Šæ¸…æ¥š **`torch.multinomial` + `torch.gather`** è¿™ä¸¤æ­¥æ˜¯æ€ä¹ˆæŠŠæ¦‚ç‡é‡‡æ ·å˜æˆçœŸæ­£çš„ token id çš„ã€‚

---

## ä»£ç ç‰‡æ®µ

```python
ix = torch.multinomial(topk_probs, 1)      # (B, 1)
xcol = torch.gather(topk_indices, -1, ix)  # (B, 1)
```

---

## 1. åœºæ™¯è®¾å®š

å‡è®¾ batch size $B=2$ï¼Œè¯è¡¨å¤§å° 6ï¼Œæˆ‘ä»¬åšäº† top-k (k=3)ã€‚

æ­¤æ—¶ï¼š

```python
topk_probs = tensor([
    [0.5, 0.3, 0.2],   # batch 1 çš„ top-3 æ¦‚ç‡
    [0.6, 0.25, 0.15]  # batch 2 çš„ top-3 æ¦‚ç‡
])  # shape (2, 3)

topk_indices = tensor([
    [10, 20, 30],   # batch 1 çš„ top-3 è¯è¡¨ç´¢å¼•
    [40, 50, 60]    # batch 2 çš„ top-3 è¯è¡¨ç´¢å¼•
])  # shape (2, 3)
```

è¿™é‡Œ `topk_indices` é‡Œçš„æ•°å­—æ˜¯ **çœŸå®è¯è¡¨ä¸­çš„ token id**ã€‚

---

## 2. `torch.multinomial`

```python
ix = torch.multinomial(topk_probs, 1)
```

* æŒ‰æ¦‚ç‡é‡‡æ ·ä¸€ä¸ªç´¢å¼•ï¼ˆèŒƒå›´æ˜¯ 0\~2ï¼Œå¯¹åº” top-k é‡Œçš„ç¬¬å‡ ä¸ªï¼‰ã€‚
* å‡è®¾ç»“æœæ˜¯ï¼š

```python
ix = tensor([
    [0],   # batch 1 é‡‡æ ·åˆ° top-1ï¼ˆid=10ï¼‰
    [2]    # batch 2 é‡‡æ ·åˆ° top-3ï¼ˆid=60ï¼‰
])  # shape (2,1)
```

---

## 3. `torch.gather`

```python
xcol = torch.gather(topk_indices, -1, ix)
```

* åœ¨ `topk_indices` é‡Œï¼Œæ ¹æ® `ix` æŠŠçœŸæ­£çš„ token id å–å‡ºæ¥ã€‚
* ç»“æœï¼š

```python
xcol = tensor([
    [10],   # ä» [10,20,30] é‡Œå–ä¸‹æ ‡ 0 â†’ 10
    [60]    # ä» [40,50,60] é‡Œå–ä¸‹æ ‡ 2 â†’ 60
])  # shape (2,1)
```

---

## 4. æ€»ç»“æµç¨‹

* `multinomial` â†’ åœ¨ top-k çš„æ¦‚ç‡åˆ†å¸ƒé‡Œï¼Œé‡‡æ ·ä¸€ä¸ªâ€œä½ç½®ç´¢å¼•â€ã€‚
* `gather` â†’ ç”¨è¿™ä¸ªä½ç½®ç´¢å¼•ï¼Œåˆ° `topk_indices` é‡Œæ‰¾åˆ°çœŸæ­£çš„ token idã€‚

æœ€ç»ˆæˆ‘ä»¬å¾—åˆ°äº†è¦ç”Ÿæˆçš„æ–° token id `(B,1)`ï¼Œå¯ä»¥æ‹¼æ¥åˆ°åºåˆ—åé¢ã€‚

---

âœ… **ä¸€å¥è¯æ€»ç»“**
`multinomial` å†³å®šâ€œé€‰ç¬¬å‡ ä¸ªå€™é€‰è¯â€ï¼Œ`gather` å†³å®šâ€œè¿™ä¸ªå€™é€‰è¯åœ¨è¯è¡¨é‡Œçš„çœŸå® idâ€ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠè¿™ä¸ªè¿‡ç¨‹ç”»æˆä¸€ä¸ª **é‡‡æ ·ç¤ºæ„è¡¨**ï¼ˆtopk\_probs â†’ multinomial â†’ topk\_indices â†’ gatherï¼‰ï¼Ÿ

å¥½ ğŸ‘ æˆ‘æ¥è§£é‡Šè¿™ä¸€è¡Œï¼š

```python
tokens = x[i, :max_length].tolist()
```

---

## 1. èƒŒæ™¯

æ­¤æ—¶ `x` æ˜¯ç”Ÿæˆå¥½çš„ token åºåˆ—ï¼Œshape å¤§æ¦‚æ˜¯ï¼š

```
x.shape = (B, T_generated)
```

* `B` = batch sizeï¼ˆæ¯”å¦‚ 5ï¼Œè¡¨ç¤ºç”Ÿæˆ 5 ä¸ªä¸åŒåºåˆ—ï¼‰
* `T_generated` = å®é™…ç”Ÿæˆçš„é•¿åº¦ï¼ˆå¯èƒ½å¤§äºç­‰äº `max_length`ï¼‰

---

## 2. `x[i, :max_length]`

* `x[i]` â†’ å–ç¬¬ `i` ä¸ªæ ·æœ¬çš„åºåˆ—ï¼Œshape = `(T_generated,)`
* `:max_length` â†’ åªä¿ç•™å‰ `max_length` ä¸ª tokenï¼Œé¿å…è¶…é•¿
* ç»“æœæ˜¯ä¸€ä¸ª **ä¸€ç»´å¼ é‡**ï¼Œé•¿åº¦ = `max_length`

ä¾‹å¦‚ï¼š

```python
x[i, :max_length] = tensor([15496, 11, 314, 257, 5766, 1158, 11, ...])
```

---

## 3. `.tolist()`

* æŠŠ PyTorch å¼ é‡è½¬æ¢æˆ **Python list**ã€‚
* å› ä¸ºåé¢è¦ç”¨ `enc.decode(tokens)`ï¼Œè€Œ `decode` æ¥æ”¶çš„æ˜¯ Python listï¼ˆtoken id åˆ—è¡¨ï¼‰ï¼Œä¸æ˜¯ PyTorch å¼ é‡ã€‚

---

## 4. ä¸¾ä¸ªä¾‹å­

å‡è®¾ï¼š

```python
x = torch.tensor([
    [10, 20, 30, 40, 50],
    [60, 70, 80, 90, 100]
])  # shape (2, 5)

max_length = 3
i = 1

tokens = x[i, :max_length].tolist()
print(tokens)  # [60, 70, 80]
```

---

## âœ… æ€»ç»“

è¿™ä¸€è¡Œçš„ä½œç”¨æ˜¯ï¼š

1. ä»ç”Ÿæˆå¥½çš„ batch ç»“æœ `x` é‡Œå–å‡ºç¬¬ `i` ä¸ªæ ·æœ¬ï¼›
2. æˆªæ–­åˆ° `max_length` ä¸ª tokenï¼›
3. è½¬æˆ Python listï¼Œæ–¹ä¾¿åç»­è§£ç æˆå­—ç¬¦ä¸²ã€‚

---

è¦ä¸è¦æˆ‘å†å¸®ä½ è§£é‡Šä¸€ä¸‹ **ä¸ºä»€ä¹ˆæœ€åä¸€æ­¥éœ€è¦ç”¨ `enc.decode(tokens)` æ‰èƒ½å˜å›æ–‡å­—**ï¼Ÿ



# sampling loop

here um so what's happening here in this code block is we have this x which is of
size B BYT right so batch by time and we're going to be in every iteration of
this loop we're going to be adding a column of new indices into each one of these rows right and so these are the
new indices and we're appending them to the the sequence as we're sampling so with each Loop iteration we get one more
column into X and all of the operations happen in the context manager of torch. nograd this is just telling pytorch that
we're not going to be calling that backward on any of this so it doesn't have to cach all the intermediate tensors it's not going to have to
prepare in any way for a potential backward later and this saves a lot of space and also possibly uh some time so
we get our low jits we get the loow jits at only the last location we throw away all the other low jits uh we don't need
them we only care about the last columns low jits so this is being wasteful uh
but uh this is just kind of like an inefficient implementation of sampling um so it's correct but
inefficient so we get the last column of loow jits pass it through soft Max to get our probabilities then here I'm
doing top case sampling of 50 and I'm doing that because this is the hugging face default so just looking at the
hugging face docks here of a pipeline um there's a bunch of
quarks that go into hugging face and I mean it's it's kind of a lot honestly
but I guess the important one that I noticed is that they're using top K by default which is 50 and what that does
is that uh so that's being used here as well and what that does is basically we want to take our probabilities and we
only want to keep the top 50 probabilities and anything that is lower than the 50th probability uh we just
clamp to zero and renormalize and so that way we are never sampling very rare
tokens uh the tokens we're going to be sampling are always in the top 50 of most likely tokens and this helps keep
the model kind of on track and it doesn't blabber on and it doesn't get lost and doesn't go off the rails as easily uh and it kind of like um sticks
in the vicinity of likely tokens a lot better so this is the way to do it in pytorch and you can step through it if
you like I don't think it's super insightful so I'll speed through it but roughly speaking we get this new column
of of tokens we append them on x and basically The Columns of X grow until
this y Loop gets tripped up and then finally we have an entire X of size um 5
by 30 in this case in this example and we can just basically print all those
individual rows so I'm getting all the rows I'm getting all the tokens that were sampled and I'm using the decode
function from Tik tokenizer to get back the string which we can print and so
terminal new terminal and let me python train
gpt2 okay so these are the generations that we're getting hello I'm a language model not a
program um new line new line Etc hello I'm a language model and one of the main
things that bothers me when they create languages is how easy it becomes to create something that I me so this will
just like blabber on right in all these cases now one thing you will notice is that these Generations are not the
generations of hugging face here and I can't find the discrepancy to be honest
and I didn't fully go through all these options but probably there's something else hiding in on addition to the top P
so I'm not able to match it up but just for correctness um down here Below in the juper notebook and using the hugging
face model so this is the hugging face model here I was I replicated the code
and if I do this and I run that then I am getting the same results so basically
the model internals are not wrong it's just I'm not 100% sure what the pipeline does in hugging face and that's why
we're not able to match them up but otherwise the code is correct and we've loaded all the um tensors correctly so
we're initializing the model correctly and everything here works so long story short uh We've Port it all the weights
we initialize the gpt2 this is the exact opening gpt2 and it can generate
sequences and they look sensible and now here of course we're initializing with gbt2 model weights but now we want to
initialize from scratch from random numbers and we want to actually train a model that will give us sequences as
good as or better than these ones in quality and so that's what we turn to
next so it turns out that using the random model is actually fairly straightforward because pytorch already

å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## é‡‡æ ·å¾ªç¯ï¼ˆSampling Loopï¼‰

è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯è¿›è¡Œæ–‡æœ¬ç”Ÿæˆçš„é‡‡æ ·ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå¾ªç¯ä¸æ–­ç”Ÿæˆä¸‹ä¸€ä¸ª tokenï¼Œå¹¶å°†å…¶åŠ å…¥åˆ°å½“å‰åºåˆ—ä¸­ï¼Œç›´åˆ°è¾¾åˆ°æŒ‡å®šçš„ç”Ÿæˆé•¿åº¦ã€‚

### 1. å¤„ç†è¾“å…¥å’Œç”Ÿæˆæ–° token

* **è¾“å…¥**ï¼šæˆ‘ä»¬ä»åˆå§‹è¾“å…¥ `X` å¼€å§‹ï¼Œ`X` çš„å½¢çŠ¶æ˜¯ `[B, T]`ï¼Œå³æ¯ä¸ªæ‰¹æ¬¡ `B` æœ‰ä¸€ä¸ªæœ€å¤§é•¿åº¦ä¸º `T` çš„åºåˆ—ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬ä¼šå‘æ¯è¡Œåºåˆ—çš„æœ«å°¾æ·»åŠ ä¸€ä¸ªæ–° tokenï¼Œè¿™ä¸ª token æ˜¯æ¨¡å‹é¢„æµ‹çš„ä¸‹ä¸€ä¸ª tokenã€‚
* **æ“ä½œ**ï¼šåœ¨æ¯ä¸€è½®å¾ªç¯ä¸­ï¼Œæˆ‘ä»¬åªå…³æ³¨å½“å‰é¢„æµ‹çš„æœ€åä¸€ä¸ª token çš„ logitsï¼ˆæ¦‚ç‡å€¼ï¼‰ã€‚é€šè¿‡è¿™äº› logitsï¼Œæˆ‘ä»¬èƒ½å¤Ÿé¢„æµ‹ä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„ tokenã€‚
* **æ³¨æ„**ï¼šè™½ç„¶æ¯æ¬¡æˆ‘ä»¬éƒ½ä¼šè®¡ç®—å‡ºæ•´ä¸ªåºåˆ—çš„ logitsï¼Œä½†è¿™é‡Œåªå…³å¿ƒæœ€åä¸€åˆ—çš„ logitsï¼Œæ‰€ä»¥å…¶ä½™çš„ logits ä¼šè¢«ä¸¢å¼ƒæ‰ã€‚è¿™ç§åšæ³•æ˜¯ä½æ•ˆçš„ï¼Œä½†ä¸ºäº†ä»£ç ç®€æ´å’ŒéªŒè¯åŠŸèƒ½ï¼Œå®ƒæ˜¯æ­£ç¡®çš„ã€‚

### 2. è·å– logits å¹¶è¿›è¡Œé‡‡æ ·

* æˆ‘ä»¬ä½¿ç”¨ **softmax** å‡½æ•°å°† logits è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚
* ç„¶åï¼Œä½¿ç”¨ **top-k é‡‡æ ·** æ¥é€‰æ‹©æœ€æœ‰å¯èƒ½çš„ k ä¸ª tokenã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ`k=50`ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬åªä¿ç•™æ¦‚ç‡æ’åå‰ 50 çš„ tokenï¼Œå…¶ä»–çš„æ¦‚ç‡è®¾ç½®ä¸º 0ï¼Œå†é‡æ–°å½’ä¸€åŒ–æ¦‚ç‡ã€‚è¿™ç§åšæ³•æœ‰åŠ©äºé¿å…é‡‡æ ·åˆ°æ¦‚ç‡æä½çš„ tokenï¼Œä½¿ç”Ÿæˆçš„æ–‡æœ¬æ›´åŠ è¿è´¯å’Œåˆç†ã€‚

### 3. å¾ªç¯ç”Ÿæˆæ–‡æœ¬

* æ¯æ¬¡ç”Ÿæˆä¸€ä¸ªæ–°çš„ tokenï¼Œå¹¶å°†å®ƒè¿½åŠ åˆ°å½“å‰çš„åºåˆ— `X` ä¸­ï¼Œæ›´æ–°è¾“å…¥ï¼Œç»§ç»­é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚
* è¿™æ ·ï¼Œéšç€æ¯æ¬¡å¾ªç¯çš„è¿›è¡Œï¼Œ`X` çš„åˆ—æ•°ä¼šå¢åŠ ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šçš„ç”Ÿæˆé•¿åº¦ï¼ˆä¾‹å¦‚ 30ï¼‰ã€‚

### 4. æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬

* æœ€åï¼Œæˆ‘ä»¬æ‰“å°å‡ºæ‰€æœ‰ç”Ÿæˆçš„æ–‡æœ¬åºåˆ—ã€‚é€šè¿‡è°ƒç”¨ `Tik tokenizer` çš„è§£ç åŠŸèƒ½ï¼Œå°†ç”Ÿæˆçš„ token åˆ—è¡¨è½¬æ¢å›äººç±»å¯è¯»çš„æ–‡æœ¬ã€‚
* ä½ ä¼šçœ‹åˆ°ä¸€äº›ç”Ÿæˆçš„å¥å­ï¼ˆä¾‹å¦‚ï¼š**"Hello, I'm a language model, not a program"**ï¼‰ï¼Œå®ƒä»¬æ˜¯åŸºäºç»™å®šçš„å‰ç¼€ï¼ˆ"Hello, I'm a language model"ï¼‰ç”Ÿæˆçš„ã€‚

### 5. é—®é¢˜å’Œç»“æœä¸ä¸€è‡´

* éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç”Ÿæˆçš„æ–‡æœ¬ä¸ Hugging Face é»˜è®¤æ¨¡å‹çš„ç”Ÿæˆç»“æœç•¥æœ‰ä¸åŒã€‚åŸå› å¯èƒ½åœ¨äº Hugging Face æ¨¡å‹çš„ **pipeline** ä¸­ä½¿ç”¨äº†ä¸€äº›é¢å¤–çš„å‚æ•°ï¼ˆå¦‚ top-p é‡‡æ ·ç­‰ï¼‰ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ¨¡å‹å†…éƒ¨çš„è¿ä½œæ˜¯æ­£ç¡®çš„ï¼Œç”Ÿæˆçš„æ–‡æœ¬ç»“æ„å’Œé€»è¾‘æ˜¯åˆç†çš„ã€‚
* ä¸ºäº†ç¡®ä¿æˆ‘ä»¬çš„æ–¹æ³•æ˜¯æ­£ç¡®çš„ï¼Œæˆ‘ä¹Ÿåœ¨ Jupyter Notebook ä¸­éªŒè¯äº† Hugging Face çš„ä»£ç ï¼Œç»“æœæ˜¯ç›¸åŒçš„ã€‚

### 6. ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹

* å°½ç®¡æˆ‘ä»¬å·²ç»æˆåŠŸåŠ è½½äº† GPT-2 çš„é¢„è®­ç»ƒæƒé‡å¹¶èƒ½ç”Ÿæˆåˆç†çš„æ–‡æœ¬ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬æƒ³è¦åšçš„æ˜¯ï¼š**ä»éšæœºåˆå§‹åŒ–çš„æ¨¡å‹å¼€å§‹è®­ç»ƒ**ï¼Œé€šè¿‡è®­ç»ƒç”Ÿæˆä¸ Hugging Face ç›¸ä¼¼æˆ–æ›´ä¼˜è´¨çš„æ–‡æœ¬ã€‚
* è¿™ç§ä»éšæœºåˆå§‹åŒ–è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•å…¶å®éå¸¸ç®€å•ï¼Œå› ä¸º **PyTorch å·²ç»ä¸ºæˆ‘ä»¬æä¾›äº†å¾ˆå¤šå·¥å…·**ï¼Œå¯ä»¥è½»æ¾å®ç°è¿™ä¸€è¿‡ç¨‹ã€‚

---

æ€»ç»“æ¥è¯´ï¼Œè¿™æ®µä»£ç å®ç°äº†é€šè¿‡ **top-k é‡‡æ ·** ç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹ï¼Œé€šè¿‡æ¯è½®å¾ªç¯æ·»åŠ ä¸€ä¸ªæ–° tokenï¼Œå¹¶åˆ©ç”¨ softmax å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒï¼Œæœ€ç»ˆç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ã€‚å°½ç®¡å’Œ Hugging Face çš„ç”Ÿæˆç»“æœå­˜åœ¨å°å·®å¼‚ï¼Œä½†æ•´ä½“ä¸Šï¼Œæˆ‘ä»¬æˆåŠŸåœ°ä» GPT-2 æƒé‡åˆå§‹åŒ–æ¨¡å‹ï¼Œå¹¶èƒ½ç”Ÿæˆåˆç†çš„æ–‡æœ¬åºåˆ—ã€‚

---

å¦‚æœä½ æƒ³äº†è§£å¦‚ä½•è°ƒæ•´é‡‡æ ·å‚æ•°ï¼Œæˆ–è€…å¦‚ä½•ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼

# model.to('cuda')
model.to('cpu')

# x = tokens.to('cuda')
x = tokens.to('cpu')

torch.manual_seed(42)
# torch.cuda.manual_seed(42)

# sample, auto-detect the device

initializes our model randomly and by default so when we create the GPT model
and the Constructor this is all um all of these layers and modules have random
initializers that are there by default so when these linear layers get created and so on there's default Constructors
for example using the Javier initialization that we saw in the past uh to construct the weights of these
layers and so creating a random model instead of a gpt2 model is actually
fairly straightforward and we would just come here and instead we would create model equals GPT and then we want to use
the default config GPT config and the default config uses the 124 M parameters
so this is the random model initialization and we can run
it and we should be able to get uh results now the results here of course
are total garbage carbal and that's because this is random model and so we're just getting all these random token string pieces chunked up totally
at random so that's what we have right now uh now one more thing I wanted to point out by the way is in case you do
not have Cuda available because you don't have a GPU you can still follow along with uh with what we're doing here
uh to some extent uh and probably not to the very end because by the end we're going to be using multiple gpus and
actually doing a serious training run uh but for now you can actually follow along decently okay uh so one thing that
I like to do in pytorch is I like to autod detect the device that is available to you so in particular you
could do that like this so here we are trying to detect a device
to run on that has the highest compute capability you can think about it that way so by default we start with CPU
which of course is available everywhere because every single computer will have a CPU but then we can try to detect do
you have a GPU you so use a Cuda and then if you don't have a Cuda uh do you
at least have MPS MPS is the back end for Apple silicon so if you have a Macbook that is fairly new you probably
have apple silicon on the inside and then that has a GPU that is actually fairly capable uh depending on which
MacBook you have and so you can use MPS which will be potentially faster than CPU and so we can print the device here
now once we have the device we can actually use it in place of Puda so we
just swap it in and notice that here when we call model on X if this x here
is on CPU instead of GPU then it will work fine because here in the forward
which is where P to will come when we create a pose we were careful to use the
device of idx to create this tensor as well and so there won't be any mismatch
where one tensor is on CPU one is on GPU and uh that you can't combine those but here we are um carefully initializing on
the correct device as indicated by the input to this model so this will autod
detect device for me this will be of course GPU so using device
Cuda uh but uh you can also run with um as I mentioned another device and it's
not going to be too much slower so if I override device here oops if I override device equals
CPU then we'll still print Cuda of course but now we're actually using CPU one 2 3
4 5 6 okay about 6 seconds and actually
we're not using torch compile and stuff like that which will speed up everything a lot faster as well but you can follow
even on a CPU I think to a decent extent um so that's note on that okay so I do
want to loop around eventually into what it means to have different devices in pytorch and what it is exactly that
pytorch does in the background for you when you do something like module. 2 device or where you take a torch tensor
and do A2 device and what exactly happens and how that works but for now I'd like to get to training and I'd like

å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## é‡‡æ ·ä¸è‡ªåŠ¨æ£€æµ‹è®¾å¤‡

### 1. éšæœºåˆå§‹åŒ–æ¨¡å‹

å½“æˆ‘ä»¬åˆå§‹åŒ–æ¨¡å‹æ—¶ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰çš„å±‚å’Œæ¨¡å—éƒ½ä½¿ç”¨éšæœºåˆå§‹åŒ–ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œå½“åˆ›å»ºçº¿æ€§å±‚æ—¶ï¼Œä¼šä½¿ç”¨ **Xavier åˆå§‹åŒ–**ï¼ˆä¸€ç§å¸¸è§çš„æƒé‡åˆå§‹åŒ–æ–¹æ³•ï¼‰æ¥æ„é€ è¿™äº›å±‚çš„æƒé‡ã€‚å› æ­¤ï¼Œ**ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒæ¨¡å‹**æ˜¯éå¸¸ç®€å•çš„ï¼Œåªéœ€è¦è°ƒç”¨ï¼š

```python
model = GPT(config)
```

è¿™é‡Œçš„ `config` æ˜¯é»˜è®¤çš„é…ç½®ï¼Œå®ƒä¼šåˆ›å»ºä¸€ä¸ª 124M å‚æ•°çš„æ¨¡å‹ã€‚è¿™æ ·åˆå§‹åŒ–åçš„æ¨¡å‹å°†éšæœºç”Ÿæˆæƒé‡ï¼Œæ‰€ä»¥ç”Ÿæˆçš„ç»“æœå°†æ¯«æ— æ„ä¹‰ã€‚ä¾‹å¦‚ï¼Œç”Ÿæˆçš„æ–‡æœ¬ä¼šæ˜¯ä¸€äº›éšæœºçš„ token å­—ç¬¦ä¸²ï¼Œå®Œå…¨æ²¡æœ‰å®é™…çš„æ„ä¹‰ã€‚

### 2. è‡ªåŠ¨æ£€æµ‹è®¾å¤‡

æ¥ä¸‹æ¥ï¼Œä»£ç ä¼šè‡ªåŠ¨æ£€æµ‹ä½ è®¡ç®—æœºä¸Šå¯ç”¨çš„è®¾å¤‡ï¼Œå¹¶å°†æ¨¡å‹å’Œæ•°æ®è½¬ç§»åˆ°è¯¥è®¾å¤‡ä¸Šè¿›è¡Œè®¡ç®—ã€‚**PyTorch æä¾›äº†è®¾å¤‡è‡ªåŠ¨æ£€æµ‹åŠŸèƒ½**ï¼Œå®ƒä¼šæ ¹æ®è®¾å¤‡çš„è®¡ç®—èƒ½åŠ›æ¥é€‰æ‹©åˆé€‚çš„è®¡ç®—èµ„æºã€‚

å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š

1. é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¼šé¦–å…ˆå°è¯•åœ¨ **CPU** ä¸Šè¿è¡Œï¼Œå› ä¸ºæ¯å°è®¡ç®—æœºéƒ½ä¼šæœ‰ CPUã€‚
2. ç„¶åï¼Œä»£ç ä¼šæ£€æŸ¥æ˜¯å¦æœ‰ **CUDA** å¯ç”¨ï¼Œå¦‚æœæœ‰ GPUï¼Œåˆ™ä¼šå°†æ¨¡å‹è¿ç§»åˆ° GPU ä¸Šã€‚
3. å¦‚æœæ²¡æœ‰ CUDA å¯ç”¨ï¼Œä»£ç ä¼šæ£€æŸ¥æ˜¯å¦æœ‰ **MPS**ï¼ˆå³ Apple Silicon çš„åç«¯ï¼‰ã€‚å¯¹äºä½¿ç”¨ **Apple Silicon**ï¼ˆå¦‚ M1 æˆ– M2 èŠ¯ç‰‡çš„ MacBookï¼‰çš„ç”¨æˆ·ï¼ŒMPS å¯ä»¥ä½œä¸º GPU è¿›è¡Œè®¡ç®—ï¼Œé€šå¸¸ä¼šæ¯” CPU å¿«ã€‚

### 3. åˆ‡æ¢åˆ°è®¾å¤‡ä¸Šè¿è¡Œ

æ— è®ºæ˜¯ CPU è¿˜æ˜¯ GPUï¼Œä»£ç ä¼šç¡®ä¿è¾“å…¥æ•°æ®ï¼ˆå¦‚ `X`ï¼‰å’Œæ¨¡å‹åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Šï¼Œé¿å…å› ä¸ºè®¾å¤‡ä¸åŒ¹é…å¯¼è‡´çš„é”™è¯¯ã€‚ä¾‹å¦‚ï¼Œä¸‹é¢çš„ä»£ç å¯ä»¥è‡ªåŠ¨æ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©è®¾å¤‡ï¼š

```python
device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")
model.to(device)
```

è¿™è¡Œä»£ç ä¼šæ ¹æ®è®¾å¤‡çš„å¯ç”¨æ€§ï¼Œè‡ªåŠ¨é€‰æ‹© **CUDA**ã€**MPS** æˆ– **CPU**ï¼Œç„¶åå°†æ¨¡å‹è½¬ç§»åˆ°è¯¥è®¾å¤‡ä¸Šã€‚

### 4. åœ¨ä¸åŒè®¾å¤‡ä¸Šè¿è¡Œ

* å¦‚æœè®¾å¤‡æ˜¯ **GPU**ï¼ˆCUDAï¼‰ï¼Œæ¨¡å‹ä¼šåˆ©ç”¨ GPU è¿›è¡Œè®¡ç®—ï¼Œè¿™æ ·èƒ½å¤§å¤§åŠ é€Ÿè®­ç»ƒã€‚
* å¦‚æœæ²¡æœ‰ GPU å¯ç”¨ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä½¿ç”¨ **CPU**ï¼Œè™½ç„¶é€Ÿåº¦ä¼šæ…¢ä¸€äº›ï¼Œä½†ä»ç„¶å¯ä»¥ç»§ç»­è¿è¡Œã€‚

ä¾‹å¦‚ï¼Œå¦‚æœä½ å¼ºåˆ¶å°†è®¾å¤‡è®¾ç½®ä¸º **CPU**ï¼Œé‚£ä¹ˆæ¨¡å‹ä»ç„¶å¯ä»¥åœ¨ CPU ä¸Šè¿è¡Œï¼Œåªæ˜¯é€Ÿåº¦ä¼šå˜æ…¢ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå³ä½¿æ²¡æœ‰ GPUï¼Œä»ç„¶å¯ä»¥è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œç”Ÿæˆã€‚

### 5. è®¾å¤‡è½¬æ¢ç»†èŠ‚

PyTorch ä¼šè‡ªåŠ¨ç®¡ç†è®¾å¤‡ä¹‹é—´çš„æ•°æ®è½¬æ¢ã€‚å½“ä½ å°†æ¨¡å‹æˆ–å¼ é‡ä»ä¸€ä¸ªè®¾å¤‡ï¼ˆå¦‚ CPUï¼‰è½¬ç§»åˆ°å¦ä¸€ä¸ªè®¾å¤‡ï¼ˆå¦‚ GPUï¼‰æ—¶ï¼ŒPyTorch ä¼šåœ¨åå°å¤„ç†è¿™äº›è½¬æ¢ï¼Œç¡®ä¿ä¸ä¼šå‘ç”Ÿè®¾å¤‡ä¸åŒ¹é…çš„é”™è¯¯ã€‚

---

æ€»çš„æ¥è¯´ï¼Œè¿™æ®µä»£ç çš„ç›®çš„æ˜¯ç¡®ä¿æ— è®ºä½ ä½¿ç”¨ CPU è¿˜æ˜¯ GPUï¼Œæ¨¡å‹å’Œæ•°æ®éƒ½èƒ½å¤Ÿæ­£ç¡®åœ°æ”¾ç½®åœ¨é€‚å½“çš„è®¡ç®—è®¾å¤‡ä¸Šï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡å¹¶ä¿è¯ä»£ç çš„å…¼å®¹æ€§ã€‚

å¦‚æœä½ æœ‰æ›´å¤šå…³äºè®¾å¤‡ç®¡ç†æˆ–è®­ç»ƒç»†èŠ‚çš„é—®é¢˜ï¼Œå¯ä»¥éšæ—¶å‘Šè¯‰æˆ‘ï¼


# letâ€™s train: data batches (B,T) â†’ logits (B,T,C)

to start training the model and for now let's just say the device makes code go fast um and let's go into how we can
actually train the model so to train the model we're going to need some data set and for me the best debugging simplest
data set that I like to use is the tiny Shakespeare data set um and it's available at this URL so you can W get
it or you can just search tiny Shakespeare data set and so um I have in my file system
as just LS input.txt so I already downloaded it and here I'm
reading the data set getting the first 1,000 characters and printing the first 100
now remember that gpt2 has uh roughly a compression ratio the tokenizer has a
compression ratio of rly 3 to1 so th000 characters is roughly 300 tokens here uh
that will come out of this in the slice that we're currently getting so this is the first few uh
characters and uh if you want to get a few more statistics on this we can do work count on input.txt
so we can see that this is uh 40,000 lines about 200,000 words in this data
set and about 1 million bytes in this file and knowing that this file is only
asky characters there's no crazy unic code here as far as I know and so every asky character is encoded with one bite
and so this is uh the same number roughly a million characters inside this data set so that's the data set size uh
by default very small and minimal data set for debugging to get us off the ground in order to tokenize this data
set we're going to get Tik token encoding for gbt2 encode the data uh the
first um 1,000 characters and then I'm only going to print the first 24 tokens
so these are the tokens as a list of integers and if you can read gpt2 tokens
you will see that 198 here you'll recognize that as the slashing character so that is a new line and then here for
example we have two new lines so that's 198 twice here uh so this is just a tokenization of the first 24 tokens so
what we want to do now is we want to actually process these token sequences and feed them into a Transformer and in
particular we want them we want to rearrange these tokens into this idx
variable that we're going to be feeding into the Transformer so we don't want a single very long onedimensional sequence
we want an entire batch where each sequence is up to uh is basically T
tokens and T cannot be larger than the maximum sequence length and then we have these t uh tlong uh sequences of tokens
and we have B independent examples of sequences so how can we create a b BYT
tensor that we can feed into the forward out of these onedimensional sequences so here's my favorite way to
to achieve this uh so if we take torch and then we create a tensor object out of this list of integers and just the
first 24 tokens my favorite way to do this is basically you do a do view of um
of uh for example 4x6 which multiply to 24 and so it's just a two-dimensional
rearrangement of these tokens and you'll is that when you view this onedimensional sequence as two-dimensional 4x6 here the first six
uh tokens uh up to here end up being the first row the next six tokens here end
up being the second row and so on and so basically it's just going to stack up this the um every six tokens in this
case as independent rows and it creates a batch of tokens in this case and so for example if we are token 25 in the
Transformer when we feed this in and this becomes the idx this token is going to see these three tokens and it's going
to try to predict that 198 comes next so in this way we are able to
create this two-dimensional batch that's that's quite nice now in terms of the label that we're going to need for the
Target to calculate the loss function how do we get that well we could write some code inside the forward pass
because we know that the next uh token in a sequence which is the label is just to the right of us but you'll notice
that actually we for this token at the very end 13 we don't actually have the next correct token because we didn't
load it so uh we actually didn't get enough information here so I'll show you
my favorite way of basically getting these batches and I like to personally have not just the input to the
Transformer which I like to call X but I also like to create the labels uh tensor
which is of the exact same size as X but contains the targets at every single position
and so here's the way that I like to do that I like to make sure that I fetch plus one uh token because we need the
ground Truth for the very last token uh for 13 and then when we're creating the
input we take everything up to the last token not including and view it as 4x6
and when we're creating targets we do the buffer but starting at index one not
index zero so we're skipping the first element and we view it in the exact same size and then when I print this
here's what happens where we see that basically as an example for this token 25 its Target was 198 and that's now
just stored at the exact same position in the Target tensor which is 198 and also this last token 13 now has its
label which is 198 and that's just because we loaded this plus one here so
basically this is the way I like to do it you take long sequences you uh view them in two- dimensional terms so that
you get batch of time and then we make sure to load one additional token so we
basically load a buffer of tokens of B * t+ one and then we sort of offset things
and view them and then we have two tensors one of them is the input to the Transformer and the other exactly is the
labels and so let's now reorganize this code and um create a very simple data
loader object that tries to basically load these tokens and um feed them to the Transformer and calculate the loss
okay so I reshuffled the code here uh accordingly so as you can see here I'm temporarily overwriting U to run a CPU
and importing TI token and all of this should look familiar we're loading a th000 characters I'm setting BT to just
be 4 and 32 right now just because we're debugging we just want to have a single batch that's very small and all of this
should now look familiar and follows what we did on the right and then here we get the we create the model and get
the lits and so so here as you see I already ran this only runs in a few
seconds but because we have a batch of uh 4X 32 our lits are now of size 4X 32x
50257 so those are the lit for what comes next at every position and now we
have the labels which are stored in y so now is the time to calculate the loss and then do the backward pass and then
the optimization so let's first calculate the loss okay so to calculate the loss we're

å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## è®­ç»ƒæ¨¡å‹ï¼šæ•°æ®æ‰¹æ¬¡ï¼ˆB,Tï¼‰ â†’ logitsï¼ˆB,T,Cï¼‰

### 1. å‡†å¤‡è®­ç»ƒæ•°æ®

ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸€ä¸ªæ•°æ®é›†ã€‚è¿™é‡Œé€‰æ‹©äº†ä¸€ä¸ªéå¸¸ç®€å•çš„å°æ•°æ®é›†â€”â€”**Tiny Shakespeare æ•°æ®é›†**ï¼Œå®ƒåŒ…å«äº†èå£«æ¯”äºšçš„ä½œå“ï¼Œéå¸¸é€‚åˆç”¨æ¥è¿›è¡Œè°ƒè¯•å’Œæµ‹è¯•ã€‚ä½ å¯ä»¥ä»ä»¥ä¸‹é“¾æ¥è·å–è¿™ä¸ªæ•°æ®é›†ï¼Œæˆ–è€…åœ¨æœç´¢å¼•æ“ä¸­æŸ¥æ‰¾â€œTiny Shakespeare æ•°æ®é›†â€ã€‚

* æ•°æ®é›†å¤§å°ï¼šçº¦ 40,000 è¡Œï¼Œçº¦ 200,000 ä¸ªå•è¯ï¼Œæ–‡ä»¶å¤§çº¦ 1MBã€‚
* æ•°æ®é›†ä¸­çš„æ¯ä¸ªå­—ç¬¦éƒ½æ˜¯ ASCII å­—ç¬¦ï¼ˆå ç”¨ 1 ä¸ªå­—èŠ‚ï¼‰ã€‚

æˆ‘ä»¬å°†æ–‡ä»¶åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œå¹¶ä»ä¸­è¯»å–å‰ 1,000 ä¸ªå­—ç¬¦ã€‚æ ¹æ® GPT-2 çš„ **tokenizerï¼ˆåˆ†è¯å™¨ï¼‰**ï¼Œè¿™äº›å­—ç¬¦ä¼šè¢«è½¬åŒ–ä¸º tokenï¼Œå¤§çº¦ä¼šå¾—åˆ° 300 ä¸ª tokenã€‚

### 2. åˆ†è¯å’Œç¼–ç 

æˆ‘ä»¬ä½¿ç”¨ **tiktoken** åº“æ¥å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œå°†æ–‡æœ¬è½¬åŒ–ä¸º GPT-2 å¯ä»¥å¤„ç†çš„ tokenã€‚æ¯ä¸ª token å¯¹åº”ä¸€ä¸ªæ•´æ•°ï¼Œä¸‹é¢çš„ä»£ç å±•ç¤ºäº†å¦‚ä½•å°†å‰ 1,000 ä¸ªå­—ç¬¦ç¼–ç æˆ tokenã€‚

```python
tokens = tokenizer.encode(input_data[:1000])
print(tokens[:24])
```

è¾“å‡ºçš„ **tokens** æ˜¯ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼Œæ¯ä¸ªæ•´æ•°å¯¹åº”ä¸€ä¸ª tokenã€‚ä¾‹å¦‚ï¼Œ`198` å¯¹åº”çš„æ˜¯æ¢è¡Œç¬¦ `\n`ï¼Œè€Œè¿ç»­ä¸¤ä¸ª `198` å°±è¡¨ç¤ºä¸¤ä¸ªæ¢è¡Œç¬¦ã€‚

### 3. åˆ›å»ºæ‰¹æ¬¡æ•°æ®ï¼ˆB,Tï¼‰

ä¸ºäº†å°†æ•°æ®è¾“å…¥åˆ° Transformer ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™äº› token è½¬æ¢ä¸ºæ‰¹æ¬¡æ•°æ®ã€‚æ¯ä¸ªæ‰¹æ¬¡ï¼ˆbatchï¼‰åŒ…å«å¤šä¸ªåºåˆ—ï¼Œæ¯ä¸ªåºåˆ—çš„é•¿åº¦æœ€å¤šä¸º `T`ã€‚ä¸ºäº†åˆ›å»ºè¿™ä¸ªæ‰¹æ¬¡æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™äº›é•¿çš„ token åºåˆ—é‡æ’ä¸ºä¸€ä¸ªäºŒç»´çš„å¼ é‡ã€‚

ä¸¾ä¸ªä¾‹å­ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ª 1 ç»´çš„ token åˆ—è¡¨ï¼ˆä¾‹å¦‚é•¿åº¦ä¸º 24ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶é‡æ’ä¸ºä¸€ä¸ªå½¢çŠ¶ä¸º `[4, 6]` çš„äºŒç»´å¼ é‡ï¼ˆ4 è¡Œ 6 åˆ—ï¼‰ã€‚è¿™æ ·å°±å¾—åˆ°äº†ä¸€æ‰¹æ¬¡åŒ…å« 4 ä¸ªåºåˆ—ï¼Œæ¯ä¸ªåºåˆ—é•¿åº¦ä¸º 6 çš„æ•°æ®ã€‚

```python
batch = torch.tensor(tokens[:24]).view(4, 6)
```

è¿™æ ·ï¼Œ`batch` çš„å†…å®¹å°±å˜æˆäº†ä¸€ä¸ªäºŒç»´å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„åºåˆ—ï¼Œé•¿åº¦ä¸º 6ã€‚

### 4. åˆ›å»ºæ ‡ç­¾ï¼ˆLabelsï¼‰

æ¨¡å‹è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®— **æŸå¤±å‡½æ•°ï¼ˆlossï¼‰**ï¼Œè€ŒæŸå¤±å‡½æ•°çš„è®¡ç®—éœ€è¦æœ‰æ ‡ç­¾ã€‚æ ‡ç­¾æ˜¯æ¯ä¸ª token åé¢ç´§è·Ÿçš„é‚£ä¸ª tokenï¼Œä¹Ÿå°±æ˜¯é¢„æµ‹ç›®æ ‡ã€‚

ä¾‹å¦‚ï¼Œåœ¨è¾“å…¥åºåˆ— `X = [1, 2, 3]` ä¸­ï¼Œç›®æ ‡ï¼ˆæ ‡ç­¾ï¼‰åº”è¯¥æ˜¯ `[2, 3, 4]`ã€‚ä½†æ˜¯å¯¹äºåºåˆ—çš„æœ€åä¸€ä¸ª tokenï¼Œæˆ‘ä»¬æ²¡æœ‰åç»­çš„ tokenï¼Œæ‰€ä»¥éœ€è¦ç¡®ä¿ä¸ºæœ€åä¸€ä¸ª token åˆ›å»ºä¸€ä¸ªæ­£ç¡®çš„æ ‡ç­¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†æ ‡ç­¾åºåˆ—å‘å³åç§»ä¸€ä¸ªä½ç½®ï¼Œå¹¶å¡«å……æœ€åä¸€ä¸ªä½ç½®çš„æ ‡ç­¾ã€‚

```python
X = batch
y = batch.roll(-1, dims=1)
y[:, -1] = 198  # æœ€åä¸€ä¸ªä½ç½®çš„æ ‡ç­¾ï¼ˆå‡è®¾ä¸ºæ¢è¡Œç¬¦ï¼‰
```

### 5. è®¡ç®—æŸå¤±å¹¶è¿›è¡Œåå‘ä¼ æ’­

ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº† **è¾“å…¥æ•°æ®ï¼ˆXï¼‰** å’Œ **æ ‡ç­¾ï¼ˆyï¼‰**ï¼Œæ¥ä¸‹æ¥å¯ä»¥è®¡ç®— **logits**ï¼ˆé¢„æµ‹çš„æœªå½’ä¸€åŒ–æ¦‚ç‡ï¼‰ï¼Œå¹¶æ ¹æ® **logits å’Œæ ‡ç­¾** è®¡ç®— **æŸå¤±**ã€‚

æŸå¤±è®¡ç®—å®Œæˆåï¼Œå°±å¯ä»¥è¿›è¡Œ **åå‘ä¼ æ’­ï¼ˆbackward passï¼‰** å’Œ **ä¼˜åŒ–ï¼ˆoptimizationï¼‰**ï¼Œæ›´æ–°æ¨¡å‹çš„æƒé‡ã€‚

---

### æ€»ç»“

* æˆ‘ä»¬é¦–å…ˆåŠ è½½å¹¶åˆ†è¯æ•°æ®ï¼Œç„¶åå°†å…¶è½¬åŒ–ä¸ºæ‰¹æ¬¡æ•°æ®ã€‚
* æ¯ä¸ªæ‰¹æ¬¡çš„æ•°æ®å½¢çŠ¶æ˜¯ `[B, T]`ï¼Œå…¶ä¸­ `B` æ˜¯æ‰¹æ¬¡å¤§å°ï¼Œ`T` æ˜¯åºåˆ—çš„é•¿åº¦ã€‚
* å¯¹äºæ¯ä¸ª tokenï¼Œæˆ‘ä»¬ç”Ÿæˆå¯¹åº”çš„æ ‡ç­¾ï¼Œå¹¶æ ¹æ®æ¨¡å‹çš„é¢„æµ‹è®¡ç®—æŸå¤±ã€‚
* æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ **åå‘ä¼ æ’­** æ›´æ–°æ¨¡å‹çš„æƒé‡ã€‚

è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ã€‚

---

å¦‚æœä½ æƒ³äº†è§£å¦‚ä½•å®ç°å…·ä½“çš„æŸå¤±è®¡ç®—ã€ä¼˜åŒ–æ­¥éª¤ï¼Œæˆ–è€…å¦‚ä½•è¿›ä¸€æ­¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼

```
# get a data batch
import tiktoken
enc = tiktoken.get_encoding('gpt2')
with open('input.txt', 'r') as f:
    text = f.read()
text = text[:1000]
tokens = enc.encode(text)
B, T = 4, 32
buf = torch.tensor(tokens[:B*T + 1])
x = buf[:-1].view(B, T)
y = buf[1:].view(B, T)
```

å•Šï¼Œæˆ‘æ˜ç™½å•¦ ğŸ™‚ ä½ æ˜¯é—®è¿™é‡Œçš„ **å†’å· `:`**ï¼š

```python
buf = torch.tensor(tokens[:B*T + 1])
```

---

## 1. Python åˆ‡ç‰‡è¯­æ³•

åœ¨ Python é‡Œï¼Œå†’å· `:` æ˜¯ **åˆ‡ç‰‡è¿ç®—ç¬¦ (slice)**ã€‚
è¯­æ³•ä¸€èˆ¬æ˜¯ï¼š

```python
list[start:end:step]
```

* `start`ï¼šå¼€å§‹ä½ç½®ï¼ˆåŒ…å«ï¼‰ï¼Œé»˜è®¤æ˜¯ 0
* `end`ï¼šç»“æŸä½ç½®ï¼ˆä¸åŒ…å«ï¼‰
* `step`ï¼šæ­¥é•¿ï¼Œé»˜è®¤æ˜¯ 1

---

## 2. åœ¨ä½ è¿™è¡Œä»£ç é‡Œ

```python
tokens[:B*T + 1]
```

æ„æ€æ˜¯ï¼š

* ä»å¤´å¼€å§‹ (`start` çœç•¥ = 0)
* å–åˆ° **ç¬¬ `B*T+1` ä¸ªå…ƒç´ ä¹‹å‰**ï¼ˆä¸åŒ…å«ï¼‰
* æ­¥é•¿é»˜è®¤ä¸º 1

æ‰€ä»¥å®ƒç›¸å½“äºï¼š

```python
tokens[0 : B*T+1 : 1]
```

---

## 3. ä¸¾ä¸ªä¾‹å­

```python
tokens = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

B, T = 2, 4
tokens[:B*T + 1]  # = tokens[:9]
```

ç»“æœæ˜¯ï¼š

```python
[0, 1, 2, 3, 4, 5, 6, 7, 8]
```

å–äº†å‰ 9 ä¸ªå…ƒç´ ã€‚

---

## âœ… æ€»ç»“

* `:` æ˜¯ **åˆ‡ç‰‡ç¬¦å·**ã€‚
* `tokens[:B*T + 1]` è¡¨ç¤º **ä»å¤´å¼€å§‹ï¼Œå–å‰ `B*T+1` ä¸ªå…ƒç´ **ã€‚

---

è¦ä¸è¦æˆ‘é¡ºä¾¿æŠŠ Python åˆ‡ç‰‡é‡Œä¸€äº›å¸¸è§ç”¨æ³•ï¼ˆæ¯”å¦‚ `[::-1]`ã€`[2:]`ã€`[:-2]`ï¼‰ä¹Ÿç»™ä½ æ€»ç»“ä¸€ä¸‹ï¼Ÿ



å•Šï¼Œæˆ‘æ˜ç™½å•¦ ğŸ™‚ ä½ æ˜¯é—®è¿™é‡Œçš„ **å†’å· `:`**ï¼š

```python
buf = torch.tensor(tokens[:B*T + 1])
```

---

## 1. Python åˆ‡ç‰‡è¯­æ³•

åœ¨ Python é‡Œï¼Œå†’å· `:` æ˜¯ **åˆ‡ç‰‡è¿ç®—ç¬¦ (slice)**ã€‚
è¯­æ³•ä¸€èˆ¬æ˜¯ï¼š

```python
list[start:end:step]
```

* `start`ï¼šå¼€å§‹ä½ç½®ï¼ˆåŒ…å«ï¼‰ï¼Œé»˜è®¤æ˜¯ 0
* `end`ï¼šç»“æŸä½ç½®ï¼ˆä¸åŒ…å«ï¼‰
* `step`ï¼šæ­¥é•¿ï¼Œé»˜è®¤æ˜¯ 1

---

## 2. åœ¨ä½ è¿™è¡Œä»£ç é‡Œ

```python
tokens[:B*T + 1]
```

æ„æ€æ˜¯ï¼š

* ä»å¤´å¼€å§‹ (`start` çœç•¥ = 0)
* å–åˆ° **ç¬¬ `B*T+1` ä¸ªå…ƒç´ ä¹‹å‰**ï¼ˆä¸åŒ…å«ï¼‰
* æ­¥é•¿é»˜è®¤ä¸º 1

æ‰€ä»¥å®ƒç›¸å½“äºï¼š

```python
tokens[0 : B*T+1 : 1]
```

---

## 3. ä¸¾ä¸ªä¾‹å­

```python
tokens = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

B, T = 2, 4
tokens[:B*T + 1]  # = tokens[:9]
```

ç»“æœæ˜¯ï¼š

```python
[0, 1, 2, 3, 4, 5, 6, 7, 8]
```

å–äº†å‰ 9 ä¸ªå…ƒç´ ã€‚

---

## âœ… æ€»ç»“

* `:` æ˜¯ **åˆ‡ç‰‡ç¬¦å·**ã€‚
* `tokens[:B*T + 1]` è¡¨ç¤º **ä»å¤´å¼€å§‹ï¼Œå–å‰ `B*T+1` ä¸ªå…ƒç´ **ã€‚

---

è¦ä¸è¦æˆ‘é¡ºä¾¿æŠŠ Python åˆ‡ç‰‡é‡Œä¸€äº›å¸¸è§ç”¨æ³•ï¼ˆæ¯”å¦‚ `[::-1]`ã€`[2:]`ã€`[:-2]`ï¼‰ä¹Ÿç»™ä½ æ€»ç»“ä¸€ä¸‹ï¼Ÿ


# cross entropy loss

going to adjust the forward function of this NN module in the model and in particular we're not just going to be
returning logits but also we're going to return the loss uh and we're going to not just pass in the input in thees but
also the targets uh in y and now we will
print not Lo just. shape anymore we're actually going to print the loss function and then c. exit of zero so
that we skip some of the sampling logic so now let's swing up to the forward function which gets called there because
now we also have these optional targets and when we get the targets we
can also calculate uh the loss and remember that we want to basically return uh log just loss and loss by
default is none but um let's put this here if uh targets is
not none then we want to calculate loss and co-pilot is already getting excited
here and calculating the what looks to be correct loss it is using the cross entropy loss as is documented here uh so
this is a function in pytorch under the functional now what is actually
happening here because it looks a little bit scary uh basically uh the F that cross entropy does not like
multi-dimensional inputs it can't take a b BYT by vocap size so what's happening here is that we are flattening out this
three-dimensional tensor into just two Dimensions the First Dimension is going to be calculated automatically and it's
going to be B * T and then the last Dimension is vocap size so basically
this is uh flattening out this three-dimensional tensor of logits to just be two- dimensional B * T all
individual examples and vocap size on uh in terms of the length of each row and
then it's also flattening out the targets which are also two- dimensional at this stage but we're going to just
flatten them out so they're just a single tensor of B * T and this can then pass into cross entropy to calculate a
loss which we return so this should basically at this point run because this
is not too complicated so let's run it and let's see if we
should be printing the
loss and here we see that we printed 11 uh roughly and so
um and notice that this is the tensor of a single element which is this number 11
now we also want to be able to calculate a reasonable uh kind of starting point for a random rationalized Network so we
covered this in previous videos but our vocabulary size is 50257 at initialization of the network
you would hope that um every vocab element is getting roughly a uniform
probability uh so that we're not favoring at initialization any token way too much we're not confidently wrong at
initialization so what we're hoping is that the probability of any arbitrary token is roughly 1 over 50,2 57 and now
we can sanity check the loss because remember that the cross entropy loss is just basically the negative um log
likelihood so if we now take this probability and we take it through the natural logarithm and then we do the
negative that is the loss we expect at initialization and we covered this in previous videos so I would expect
something around 10.82 and we're seeing something around 11 so it's not way off this is roughly the probability I expect
at initialization so that tells me that the at initialization or probability distribtion is roughly diffused it's a
good starting point and we can now uh perform the optimization and tell the network which elements you know should
follow correctly in what order so at this point we can do a l step backward calculate the gradients and do an
optimization so let's get to that okay so let's do the optimization now um so

å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## äº¤å‰ç†µæŸå¤±ï¼ˆCross Entropy Lossï¼‰

### 1. ä¿®æ”¹å‰å‘ä¼ æ’­å‡½æ•°

ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æŸå¤±ã€‚åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹çš„ **å‰å‘ä¼ æ’­ï¼ˆforwardï¼‰** å‡½æ•°è¿›è¡Œäº†è°ƒæ•´ã€‚é™¤äº†è¿”å› logits ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®¡ç®—å¹¶è¿”å› **æŸå¤±**ï¼Œå¹¶ä¸”è¾“å…¥ä¸ä»…åŒ…å«æ•°æ®ï¼Œè¿˜åŒ…æ‹¬ **ç›®æ ‡ï¼ˆtargetsï¼‰**ã€‚

ä¿®æ”¹åçš„ `forward` å‡½æ•°å¦‚ä¸‹ï¼š

* å¦‚æœä¼ å…¥çš„ **ç›®æ ‡**ï¼ˆtargetsï¼‰ä¸ä¸ºç©ºï¼Œæˆ‘ä»¬å°†è®¡ç®—æŸå¤±ã€‚
* è®¡ç®—æŸå¤±æ—¶ï¼Œä½¿ç”¨ **PyTorch** ä¸­çš„ **äº¤å‰ç†µæŸå¤±ï¼ˆcross entropy lossï¼‰** å‡½æ•°ã€‚

### 2. äº¤å‰ç†µæŸå¤±çš„è®¡ç®—

åœ¨è®¡ç®—äº¤å‰ç†µæŸå¤±æ—¶ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ç†è§£ PyTorch çš„ `cross_entropy` å‡½æ•°çš„ä¸€äº›è¦æ±‚ï¼š

* äº¤å‰ç†µæŸå¤±å‡½æ•°ä¸èƒ½å¤„ç†å¤šç»´è¾“å…¥ï¼Œå®ƒéœ€è¦è¾“å…¥ä¸ºäºŒç»´å¼ é‡ï¼Œå…¶ä¸­ï¼š

  * ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯ **B\*T**ï¼Œå³æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦çš„ä¹˜ç§¯ï¼›
  * ç¬¬äºŒä¸ªç»´åº¦æ˜¯ **vocab\_size**ï¼Œå³è¯æ±‡è¡¨çš„å¤§å°ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°† **logits** å’Œ **targets** è¿™ä¸¤ä¸ªä¸‰ç»´å¼ é‡å±•å¹³ï¼ˆflattenï¼‰ä¸ºäºŒç»´å¼ é‡ï¼š

* `logits` çš„å½¢çŠ¶ä¼šè¢«å±•å¹³ä¸º `[B*T, vocab_size]`ï¼Œæ¯è¡Œä»£è¡¨ä¸€ä¸ªæ ·æœ¬çš„é¢„æµ‹ï¼›
* `targets` ä¹Ÿä¼šè¢«å±•å¹³ä¸º `[B*T]`ï¼Œå³æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„çœŸå®æ ‡ç­¾ã€‚

è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä»¬ä¼ å…¥ `cross_entropy` å‡½æ•°æ¥è®¡ç®—æŸå¤±ã€‚

### 3. è®¡ç®—æŸå¤±

é€šè¿‡ `cross_entropy` å‡½æ•°è®¡ç®—æŸå¤±åï¼Œæˆ‘ä»¬å¾—åˆ°çš„æŸå¤±å€¼åº”è¯¥æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚ç„¶åï¼Œæ‰“å°å‡ºæŸå¤±å€¼ï¼Œç»“æœæ¥è¿‘ **11**ã€‚è¿™ä¸ªæŸå¤±å€¼è¯´æ˜äº†æ¨¡å‹å½“å‰çš„è¡¨ç°ã€‚

### 4. æ£€æŸ¥åˆå§‹åŒ–æ—¶çš„æŸå¤±

æˆ‘ä»¬å¸Œæœ›åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶ï¼Œ**æ¯ä¸ª token çš„é¢„æµ‹æ¦‚ç‡æ˜¯å‡åŒ€çš„**ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨åˆå§‹åŒ–æ—¶ï¼Œæ¨¡å‹å¯¹æ¯ä¸ª token çš„æ¦‚ç‡å¤§çº¦æ˜¯ `1 / vocab_size`ï¼ˆå³çº¦ 1 / 50257ï¼‰ã€‚è¿™æ˜¯å› ä¸ºåœ¨åˆå§‹åŒ–æ—¶ï¼Œæ¨¡å‹ä¸åº”è¯¥åå‘ä»»ä½•ç‰¹å®šçš„ tokenï¼Œè€Œæ˜¯åº”è¯¥æœ‰ä¸€ä¸ªåˆç†çš„åˆ†å¸ƒï¼Œé¿å…è¿‡åº¦è‡ªä¿¡çš„é”™è¯¯ã€‚

### 5. æŸå¤±çš„æœŸæœ›å€¼

äº¤å‰ç†µæŸå¤±æœ¬è´¨ä¸Šæ˜¯è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆnegative log-likelihoodï¼‰ã€‚å› æ­¤ï¼Œåœ¨åˆå§‹åŒ–æ—¶ï¼Œå¦‚æœæ¦‚ç‡å‡åŒ€åˆ†å¸ƒï¼Œæˆ‘ä»¬çš„æœŸæœ›æŸå¤±å€¼å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¡ç®—ï¼š

* å– `1 / vocab_size`ï¼Œç„¶åå¯¹å…¶å–è‡ªç„¶å¯¹æ•°ï¼ˆ`log`ï¼‰ï¼Œå†å–è´Ÿå€¼ã€‚
* è¿™ä¸ªè®¡ç®—ç»“æœåº”è¯¥æ¥è¿‘ **10.82**ï¼Œè€Œå®é™…è¾“å‡ºæ˜¯ **11**ï¼Œè¿™ä¸ªç»“æœæ˜¯åˆç†çš„ï¼Œè¡¨æ˜åˆå§‹åŒ–æ—¶çš„æ¦‚ç‡åˆ†å¸ƒæ˜¯å‡åŒ€çš„ã€‚

### 6. å¯åŠ¨ä¼˜åŒ–è¿‡ç¨‹

ç°åœ¨ï¼Œæˆ‘ä»¬çŸ¥é“æ¨¡å‹çš„åˆå§‹åŒ–æ˜¯åˆç†çš„ï¼Œæ¦‚ç‡åˆ†å¸ƒå¤§è‡´å‡åŒ€ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹ **ä¼˜åŒ–ï¼ˆoptimizationï¼‰**ï¼š

* æˆ‘ä»¬è¿›è¡Œ **åå‘ä¼ æ’­ï¼ˆbackwardï¼‰**ï¼Œè®¡ç®—æ¢¯åº¦ï¼›
* ç„¶åï¼Œæˆ‘ä»¬æ‰§è¡Œ **ä¼˜åŒ–æ­¥éª¤**ï¼Œæ›´æ–°æ¨¡å‹çš„å‚æ•°ï¼Œä½¿å…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸æé«˜æ€§èƒ½ã€‚

---

### æ€»ç»“

* æˆ‘ä»¬ä¿®æ”¹äº† `forward` å‡½æ•°ï¼Œä½¿å…¶ä¸ä»…è¿”å› logitsï¼Œè¿˜èƒ½è¿”å›æŸå¤±ï¼ˆcross-entropy lossï¼‰ã€‚
* é€šè¿‡äº¤å‰ç†µæŸå¤±ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¨¡å‹çš„é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®è·ã€‚
* åœ¨åˆå§‹åŒ–æ—¶ï¼ŒæŸå¤±å€¼å¤§è‡´ä¸º **11**ï¼Œè¿™ä¸æˆ‘ä»¬æœŸæœ›çš„å€¼ **10.82** éå¸¸æ¥è¿‘ï¼Œè¯´æ˜æ¨¡å‹çš„åˆå§‹åŒ–æ˜¯åˆç†çš„ã€‚
* ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹è¿›è¡Œä¼˜åŒ–æ­¥éª¤ï¼Œé€šè¿‡åå‘ä¼ æ’­å’Œæ¢¯åº¦æ›´æ–°ï¼Œè®­ç»ƒæ¨¡å‹ã€‚

---

å¦‚æœä½ éœ€è¦æ›´è¯¦ç»†çš„ä¼˜åŒ–æ­¥éª¤æˆ–æŸå¤±å‡½æ•°çš„ç†è§£ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼

```
targets.view(-1)
```

å¥½ ğŸ‘Œ æˆ‘æ¥è§£é‡Š **`targets.view(-1)`**ã€‚

---

## 1. èƒŒæ™¯

åœ¨è¯­è¨€æ¨¡å‹è®­ç»ƒé‡Œï¼Œæˆ‘ä»¬æœ‰ï¼š

* **logits** â†’ é¢„æµ‹ç»“æœï¼Œå½¢çŠ¶æ˜¯ `(B, T, vocab_size)`
* **targets** â†’ çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶æ˜¯ `(B, T)`

äº¤å‰ç†µ `F.cross_entropy` éœ€è¦çš„è¾“å…¥æ˜¯ï¼š

* é¢„æµ‹ï¼š`(N, C)`ï¼Œå…¶ä¸­ `N` = æ ·æœ¬æ•°ï¼Œ`C` = ç±»åˆ«æ•°
* æ ‡ç­¾ï¼š`(N,)`ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ç±»åˆ« id

æ‰€ä»¥è¦æŠŠ `(B, T)` çš„ `targets` æ‹‰å¹³æˆä¸€ç»´ `(B*T,)`ã€‚

---

## 2. `.view(-1)` çš„ä½œç”¨

```python
targets.view(-1)
```

* `view`ï¼šæ”¹å˜å¼ é‡çš„å½¢çŠ¶ï¼ˆä¸å¤åˆ¶å†…å­˜ï¼‰ã€‚
* `-1`ï¼šè®© PyTorch **è‡ªåŠ¨æ¨ç®—**è¿™ä¸€ç»´çš„é•¿åº¦ã€‚

å¦‚æœ `targets.shape = (B, T)`ï¼Œ
é‚£ä¹ˆ `targets.view(-1)` â†’ `(B*T,)`ã€‚

---

## 3. ä¸¾ä¸ªä¾‹å­

```python
import torch

targets = torch.tensor([
    [1, 2, 3],
    [4, 5, 6]
])  # shape (2,3)

print(targets.view(-1))  # -> shape (6,)
# tensor([1, 2, 3, 4, 5, 6])
```

æŠŠäºŒç»´ `(2,3)` å±•å¹³æˆä¸€ç»´ `(6,)`ã€‚

---

## 4. åœ¨ä½ çš„ä»£ç é‡Œçš„æ„ä¹‰

```python
loss = F.cross_entropy(
    logits.view(-1, logits.size(-1)),  # (B*T, vocab_size)
    targets.view(-1)                   # (B*T,)
)
```

è¿™æ · logits å’Œ targets å¯¹é½ï¼š

* logits çš„æ¯ä¸€è¡Œ â†’ é¢„æµ‹æŸä¸ªä½ç½®çš„è¯æ¦‚ç‡åˆ†å¸ƒ
* targets çš„æ¯ä¸ªå…ƒç´  â†’ è¯¥ä½ç½®çš„çœŸå®è¯ id

---

âœ… **æ€»ç»“**

* `targets.view(-1)` å°±æ˜¯æŠŠ `(B,T)` çš„æ ‡ç­¾æ‹‰å¹³æˆ `(B*T,)`ï¼Œæ–¹ä¾¿å’Œå±•å¹³åçš„ logits å¯¹é½è®¡ç®—äº¤å‰ç†µæŸå¤±ã€‚

---

è¦ä¸è¦æˆ‘é¡ºä¾¿å†å¸®ä½ è®²ä¸€ä¸‹ `reshape` å’Œ `view` çš„åŒºåˆ«ï¼Ÿ

```
# optimize!
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
for i in range(50):
    optimizer.zero_grad()
    logits, loss = model(x, y)
    loss.backward()
    optimizer.step()
    print(f"step {i}, loss: {loss.item()}")
```

å¥½ ğŸ‘ è¿™ä¸€æ®µå°±æ˜¯ä¸€ä¸ªæœ€å°åŒ–è¯­è¨€æ¨¡å‹çš„ **è®­ç»ƒå¾ªç¯**ã€‚æˆ‘é€è¡Œè§£é‡Šï¼š

---

## 1. å®šä¹‰ä¼˜åŒ–å™¨

```python
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
```

* ä½¿ç”¨ **AdamW ä¼˜åŒ–å™¨**ï¼ˆAdam + æƒé‡è¡°å‡ï¼ŒTransformer é‡Œå¸¸ç”¨ï¼‰ã€‚
* `model.parameters()` â†’ æŠŠæ¨¡å‹çš„æ‰€æœ‰å¯è®­ç»ƒå‚æ•°äº¤ç»™ä¼˜åŒ–å™¨ç®¡ç†ã€‚
* `lr=3e-4` â†’ å­¦ä¹ ç‡ 0.0003ã€‚

---

## 2. è®­ç»ƒå¾ªç¯

```python
for i in range(50):
```

* å¾ªç¯ 50 ä¸ªè®­ç»ƒ stepï¼ˆä¸æ˜¯å®Œæ•´ epochï¼Œåªæ˜¯æ¼”ç¤ºï¼‰ã€‚

---

## 3. æ¢¯åº¦æ¸…é›¶

```python
optimizer.zero_grad()
```

* PyTorch çš„æ¢¯åº¦æ˜¯ **ç´¯åŠ çš„**ï¼Œå¦‚æœä¸æ¸…é›¶ï¼Œæ¯æ¬¡ `loss.backward()` çš„æ¢¯åº¦ä¼šç´¯åŠ åˆ°ä¹‹å‰çš„ã€‚
* æ‰€ä»¥æ¯ä¸ª step è®­ç»ƒå‰è¦å…ˆæŠŠæ¢¯åº¦æ¸…é›¶ã€‚

---

## 4. å‰å‘ä¼ æ’­

```python
logits, loss = model(x, y)
```

* è¾“å…¥ `x`ï¼ˆè¾“å…¥åºåˆ—ï¼‰ã€`y`ï¼ˆç›®æ ‡åºåˆ—ï¼‰ã€‚
* `model` è¿”å›ï¼š

  * `logits` â†’ é¢„æµ‹å€¼ `(B, T, vocab_size)`
  * `loss` â†’ æ ¹æ® `x` å’Œ `y` ç®—å‡ºæ¥çš„äº¤å‰ç†µæŸå¤±ã€‚

---

## 5. åå‘ä¼ æ’­

```python
loss.backward()
```

* PyTorch ä¼šæ ¹æ® `loss` è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼Œå­˜åˆ° `param.grad`ã€‚
* è¿™æ˜¯è‡ªåŠ¨å¾®åˆ† (autograd) çš„æ ¸å¿ƒã€‚

---

## 6. æ›´æ–°å‚æ•°

```python
optimizer.step()
```

* ç”¨ä¼˜åŒ–å™¨ï¼ˆAdamWï¼‰æ›´æ–°æ¨¡å‹å‚æ•°ï¼š

  $$
  \theta \leftarrow \theta - \eta \cdot \text{AdamW}(\nabla_\theta L)
  $$
* ä¹Ÿå°±æ˜¯æ‰§è¡Œä¸€æ¬¡æ¢¯åº¦ä¸‹é™ã€‚

---

## 7. æ‰“å°è®­ç»ƒè¿‡ç¨‹

```python
print(f"step {i}, loss: {loss.item()}")
```

* `.item()` æŠŠ PyTorch å¼ é‡ï¼ˆæ ‡é‡ï¼‰è½¬æˆ Python floatã€‚
* æ‰“å°å½“å‰ step çš„ lossï¼Œæ–¹ä¾¿è§‚å¯Ÿæ”¶æ•›æƒ…å†µã€‚

---

## âœ… æ€»ç»“

è¿™ä¸€æ®µä»£ç æ˜¯ **è®­ç»ƒä¸€ä¸ª GPT è¯­è¨€æ¨¡å‹çš„æœ€å° demo**ï¼š

1. å®šä¹‰ä¼˜åŒ–å™¨ `AdamW`ã€‚
2. å¾ªç¯è®­ç»ƒï¼š

   * æ¸…é›¶æ¢¯åº¦
   * å‰å‘ä¼ æ’­ç®— loss
   * åå‘ä¼ æ’­ç®—æ¢¯åº¦
   * å‚æ•°æ›´æ–°
3. æ‰“å° loss è¿½è¸ªè®­ç»ƒæ•ˆæœã€‚

---

ğŸ‘‰ å¦‚æœç»§ç»­æ‰©å±•ï¼Œå®ƒå°±èƒ½å˜æˆå®Œæ•´çš„ **miniGPT è®­ç»ƒå™¨**ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **è®­ç»ƒå¾ªç¯æµç¨‹å›¾**ï¼ˆæ•°æ® â†’ å‰å‘ä¼ æ’­ â†’ loss â†’ backward â†’ optimizer â†’ æ›´æ–°å‚æ•° â†’ ä¸‹ä¸€æ­¥ï¼‰ï¼Ÿ

```
import tiktoken

class DataLoaderLite:
    def __init__(self, B, T):
        self.B = B
        self.T = T

        # at init load tokens from disk and store them in memory
        with open('input.txt', 'r') as f:
            text = f.read()
        enc = tiktoken.get_encoding('gpt2')
        tokens = enc.encode(text)
        self.tokens = torch.tensor(tokens)
        print(f"loaded {len(self.tokens)} tokens")
        print(f"1 epoch = {len(self.tokens) // (B * T)} batches")

        # state
        self.current_position = 0

    def next_batch(self):
        B, T = self.B, self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance the position in the tensor
        self.current_position += B * T
        # if loading the next batch would be out of bounds, reset
        if self.current_position + (B * T + 1) > len(self.tokens):
            self.current_position = 0
        return x, y
```

å¥½ ğŸ‘Œ è¿™æ®µä»£ç å®ç°äº†ä¸€ä¸ªéå¸¸ç®€åŒ–ç‰ˆçš„ **æ•°æ®åŠ è½½å™¨ (DataLoaderLite)**ï¼Œç”¨æ¥ç»™ GPT è®­ç»ƒæä¾›å°æ‰¹æ¬¡ `(x, y)` æ•°æ®ã€‚æˆ‘ä»¬é€å—çœ‹ï¼š

---

## 1. ç±»å®šä¹‰ä¸åˆå§‹åŒ–

```python
class DataLoaderLite:
    def __init__(self, B, T):
        self.B = B
        self.T = T
```

* `B` = batch size
* `T` = åºåˆ—é•¿åº¦
* åˆå§‹åŒ–æ—¶æŠŠå®ƒä»¬ä¿å­˜ä¸‹æ¥ï¼Œåé¢ç”Ÿæˆ batch æ—¶ç”¨ã€‚

---

## 2. åŠ è½½æ–‡æœ¬å¹¶ç¼–ç æˆ tokens

```python
with open('input.txt', 'r') as f:
    text = f.read()
enc = tiktoken.get_encoding('gpt2')
tokens = enc.encode(text)
self.tokens = torch.tensor(tokens)
```

* ä»ç£ç›˜è¯»å– `input.txt` çš„å†…å®¹ã€‚
* ç”¨ GPT-2 çš„åˆ†è¯å™¨ (`tiktoken`) æŠŠæ–‡æœ¬è½¬æˆ **token id åˆ—è¡¨**ã€‚
* è½¬æˆ PyTorch `tensor`ï¼Œæ–¹ä¾¿åç»­åˆ‡ç‰‡æ“ä½œã€‚

```python
print(f"loaded {len(self.tokens)} tokens")
print(f"1 epoch = {len(self.tokens) // (B * T)} batches")
```

* æ‰“å°æ€» token æ•°ã€‚
* ä¸€ä¸ª epoch èƒ½ç”Ÿæˆå¤šå°‘ä¸ª batchï¼ˆå–æ•´é™¤ï¼‰ã€‚

---

## 3. çŠ¶æ€æŒ‡é’ˆ

```python
self.current_position = 0
```

* ç»´æŠ¤ä¸€ä¸ª **å½“å‰ä½ç½®æŒ‡é’ˆ**ï¼Œè¡¨ç¤ºä¸‹ä¸€æ¬¡å– batch æ—¶ä»å“ªé‡Œå¼€å§‹ã€‚

---

## 4. è·å–ä¸€ä¸ª batch

```python
def next_batch(self):
    B, T = self.B, self.T
    buf = self.tokens[self.current_position : self.current_position+B*T+1]
```

* ä»å½“å‰æŒ‡é’ˆä½ç½®åˆ‡ `B*T+1` ä¸ª token å‡ºæ¥ã€‚

  * ä¸ºä»€ä¹ˆ `+1`ï¼Ÿå› ä¸ºè¦æ„é€  `(x, y)`ï¼Œéœ€è¦é”™å¼€ä¸€ä½ã€‚

---

## 5. æ„é€ è¾“å…¥å’Œç›®æ ‡

```python
x = (buf[:-1]).view(B, T) # inputs
y = (buf[1:]).view(B, T)  # targets
```

* `buf[:-1]` â†’ å‰ `B*T` ä¸ª tokenï¼Œreshape æˆ `(B, T)`ï¼Œä½œä¸ºè¾“å…¥åºåˆ—ã€‚
* `buf[1:]` â†’ å `B*T` ä¸ª tokenï¼Œreshape æˆ `(B, T)`ï¼Œä½œä¸ºç›®æ ‡åºåˆ—ã€‚
* æ•ˆæœï¼šæ¨¡å‹å­¦ä¹  **é¢„æµ‹ä¸‹ä¸€ä¸ª token**ã€‚

ä¾‹å¦‚ï¼š

```
buf = [5, 6, 7, 8, 9]
x   = [5, 6, 7, 8]
y   = [6, 7, 8, 9]
```

---

## 6. æ›´æ–°ä½ç½®

```python
self.current_position += B * T
if self.current_position + (B * T + 1) > len(self.tokens):
    self.current_position = 0
```

* æ¯æ¬¡å–å®Œä¸€ä¸ª batchï¼Œå°±æŠŠæŒ‡é’ˆå¾€å‰ç§»åŠ¨ `B*T` ä¸ª tokenã€‚
* å¦‚æœå‰©ä¸‹çš„ token ä¸å¤Ÿä¸€ä¸ª batchï¼Œå°±é‡ç½®åˆ° 0ï¼Œä»å¤´å¼€å§‹ã€‚
* è¿™ç›¸å½“äºä¸€è½® epochã€‚

---

## 7. è¿”å›ç»“æœ

```python
return x, y
```

* æ¯æ¬¡è°ƒç”¨ `next_batch()`ï¼Œè¿”å›ä¸€å¯¹ `(x, y)`ï¼Œå½¢çŠ¶æ˜¯ `(B, T)`ã€‚
* è®­ç»ƒå¾ªç¯å°±èƒ½ä¸æ–­è°ƒç”¨ `dataloader.next_batch()` æ¥è·å–æ–°æ•°æ®ã€‚

---

## âœ… æ€»ç»“

è¿™ä¸ª `DataLoaderLite` åšçš„äº‹æƒ…å°±æ˜¯ï¼š

1. **ä¸€æ¬¡æ€§è¯»å–æ–‡æœ¬** â†’ ç¼–ç æˆ token å¼ é‡ã€‚
2. **ç»´æŠ¤ä¸€ä¸ªæŒ‡é’ˆ** â†’ ä» token åºåˆ—é‡ŒæŒ‰é¡ºåºåˆ‡åˆ†å‡ºå° batchã€‚
3. **æ„é€  (x, y)** â†’

   * `x` æ˜¯è¾“å…¥ tokensã€‚
   * `y` æ˜¯ç›®æ ‡ tokensï¼ˆæ¯” `x` å‘å³é”™ä¸€ä½ï¼‰ã€‚
4. **è‡ªåŠ¨å¾ªç¯** â†’ æ•°æ®è¯»åˆ°ç»“å°¾æ—¶è‡ªåŠ¨å›ç»•åˆ°å¼€å¤´ã€‚

---

ğŸ‘‰ æ¢å¥è¯è¯´ï¼Œè¿™å°±æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ **è®­ç»ƒæ•°æ®è¿­ä»£å™¨**ï¼Œç”¨æ¥ä¸æ–­æä¾› `(x, y)` ç»™ GPT æ¨¡å‹ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **buf â†’ x,y çš„é”™ä½ç¤ºæ„å›¾**ï¼Œç›´è§‚å±•ç¤ºè¾“å…¥å’Œç›®æ ‡æ˜¯æ€ä¹ˆå¯¹åº”çš„ï¼Ÿ

# ä¸€ä¸ª epoch ä»€ä¹ˆæ„æ€

å¥½é—®é¢˜ ğŸ‘

åœ¨æœºå™¨å­¦ä¹  / æ·±åº¦å­¦ä¹ è®­ç»ƒé‡Œï¼š

---

## 1. **epoch çš„å®šä¹‰**

* **ä¸€ä¸ª epoch** = æ¨¡å‹ **å®Œæ•´çœ‹è¿‡ä¸€éè®­ç»ƒé›†**ã€‚
* ä¹Ÿå°±æ˜¯è¯´ï¼ŒæŠŠè®­ç»ƒæ•°æ®å…¨éƒ¨é€è¿›æ¨¡å‹ä¸€è½®ï¼Œè¿™å°±å« **1 ä¸ª epoch**ã€‚

---

## 2. å…³ç³»ï¼šepochã€batchã€iteration

* **batchï¼ˆå°æ‰¹é‡ï¼‰**ï¼šä¸€æ¬¡é€è¿›æ¨¡å‹çš„ä¸€å°éƒ¨åˆ†æ ·æœ¬ã€‚
* **iterationï¼ˆè¿­ä»£ï¼‰**ï¼š**ä¸€æ¬¡å‚æ•°æ›´æ–°** = å‰å‘ä¼ æ’­ + åå‘ä¼ æ’­ + ä¼˜åŒ–å™¨ stepã€‚
* **epoch**ï¼šè®­ç»ƒé›†å…¨éƒ¨æ•°æ®éƒ½è¢«è¿­ä»£è¿‡ä¸€éã€‚

å…¬å¼ï¼š

$$
\text{#iterations per epoch} = \frac{\text{dataset size}}{\text{batch size}}
$$

---

## 3. å›åˆ°ä½ ä»£ç é‡Œçš„æ‰“å°

```python
print(f"1 epoch = {len(self.tokens) // (B * T)} batches")
```

* `len(self.tokens)` = è®­ç»ƒæ•°æ®æ€» token æ•°ã€‚
* `B * T` = æ¯ä¸ª batch æ¶ˆè€—çš„ token æ•°ã€‚
* `len(self.tokens) // (B*T)` = è®­ç»ƒé›†å¯ä»¥åˆ‡å‡ºå¤šå°‘ä¸ª batchã€‚
* ä¹Ÿå°±æ˜¯ï¼š**1 ä¸ª epoch é‡Œä¸€å…±èƒ½è¿­ä»£å¤šå°‘ä¸ª batch**ã€‚

---

## 4. ä¸¾ä¾‹

å‡è®¾ï¼š

* æ•°æ®é›†æ€» token æ•° = 10,000
* `B=4, T=50` â†’ æ¯ä¸ª batch ç”¨åˆ° 200 ä¸ª token

é‚£ä¹ˆï¼š

```
1 epoch = 10000 // 200 = 50 batches
```

ä¹Ÿå°±æ˜¯è¯´ï¼š

* è¿­ä»£ 50 ä¸ª batchï¼Œåˆšå¥½æŠŠæ•°æ®é›†å®Œæ•´è¿‡ä¸€é = 1 epochã€‚
* å¦‚æœè®­ç»ƒ 10 ä¸ª epochï¼Œå°±ç›¸å½“äºæŠŠæ•´ä¸ªæ•°æ®é›†çœ‹äº† 10 éã€‚

---

âœ… **æ€»ç»“**

* **epoch** = æ¨¡å‹å®Œæ•´éå†ä¸€æ¬¡è®­ç»ƒæ•°æ®ã€‚
* åœ¨ä½ ä»£ç é‡Œï¼Œå®ƒè¡¨ç¤º **ç”¨æ‰€æœ‰ tokens æ­£å¥½åˆ‡å‡ºå¤šå°‘ä¸ª batch**ã€‚

---

è¦ä¸è¦æˆ‘å†å¸®ä½ ç”»ä¸€ä¸ªå°å›¾ï¼ŒæŠŠ **dataset â†’ batch â†’ iteration â†’ epoch** çš„å…³ç³»ç›´è§‚å¯¹æ¯”å‡ºæ¥ï¼Ÿ

```
        # weight sharing scheme
        self.transformer.wte.weight = self.lm_head.weight
```

è¿™ä¸¤è¡Œæ˜¯åœ¨åš **æƒé‡å…±äº«ï¼ˆweight tyingï¼‰**ï¼š

```python
# weight sharing scheme
self.transformer.wte.weight = self.lm_head.weight
```

## å®ƒåˆ°åº•åšäº†ä»€ä¹ˆï¼Ÿ

* `wte` æ˜¯ **è¯åµŒå…¥çŸ©é˜µ**ï¼ˆ`nn.Embedding`ï¼‰ï¼Œå½¢çŠ¶æ˜¯ `(vocab_size, n_embd)`ã€‚
* `lm_head` æ˜¯ **è¾“å‡ºåˆ†ç±»å±‚**ï¼ˆ`nn.Linear(n_embd, vocab_size, bias=False)`ï¼‰ï¼Œå…¶ `weight` å½¢çŠ¶æ˜¯ `(vocab_size, n_embd)`ï¼ˆPyTorch çš„ Linear æƒé‡æ˜¯ `(out_features, in_features)`ï¼‰ã€‚

ä¸¤è€…å½¢çŠ¶å®Œå…¨ä¸€è‡´ï¼Œæ‰€ä»¥æŠŠå®ƒä»¬**æŒ‡å‘åŒä¸€ä¸ªå‚æ•°å¯¹è±¡**ï¼š

> ä¸æ˜¯æ‹·è´æ•°æ®ï¼Œè€Œæ˜¯è®©ä¸¤ä¸ªæ¨¡å—çš„ `.weight` **å¼•ç”¨åŒä¸€å—å‚æ•°**ï¼ˆåŒä¸€ä¸ª `nn.Parameter`ï¼‰ã€‚

å› æ­¤ï¼š

* è®­ç»ƒæ—¶ï¼Œè¿™ä¸ªå‚æ•°çš„ **æ¢¯åº¦æ¥è‡ªä¸¤è¾¹ï¼ˆåµŒå…¥ + è¾“å‡ºå¤´ï¼‰**ï¼Œä¼šè¢«ä¸€èµ·ç´¯åŠ ï¼Œç„¶åæ›´æ–°ä¸€æ¬¡ã€‚
* æ›´æ–°ä¸€æ¬¡å°±åŒæ—¶å½±å“åµŒå…¥å’Œè¾“å‡ºå¤´ï¼Œå› ä¸ºæœ¬è´¨ä¸Šå°±æ˜¯**åŒä¸€ä¸ªæƒé‡**ã€‚

> è‹¥ä½ ç”¨çš„æ˜¯ `copy_`ï¼ˆå¦‚ `wte.weight.data.copy_(lm_head.weight.data)`ï¼‰ï¼Œé‚£åªæ˜¯â€œå€¼ç›¸åŒâ€ï¼Œ**ä¸ä¼šå…±äº«**ï¼Œä¹‹åä¼šå„è‡ªç‹¬ç«‹æ›´æ–°ã€‚è¿™é‡Œçš„**èµ‹å€¼**æ˜¯â€œåŒä¸€å¯¹è±¡â€çš„å…±äº«ã€‚

---

## ä¸ºä»€ä¹ˆè¦è¿™æ ·åšï¼Ÿï¼ˆå¥½å¤„ï¼‰

1. **çœå‚æ•°**ï¼š
   æŠŠåŸæœ¬ä¸¤ä»½å¤§å°ä¸º `vocab_size Ã— n_embd` çš„çŸ©é˜µåˆå¹¶ä¸ºä¸€ä»½ã€‚
   ä»¥ GPT-2 å°æ¨¡å‹ä¸ºä¾‹ï¼š`50257 Ã— 768 â‰ˆ 3,86e7` ä¸ªå‚æ•°ï¼ŒFP32 å¤§çº¦ **154MB**ï¼Œå…±äº«åèƒ½çœä¸‹è¿™ä¸€ä»½ï¼ˆFP16 ä¹Ÿèƒ½çœ \~77MBï¼‰ã€‚

2. **æ­£åˆ™åŒ–æ•ˆæœ**ï¼ˆæ›´ç¨³ã€æ›´å‡†ï¼‰ï¼š
   æ–‡çŒ®ï¼ˆPress & Wolf 2017ï¼›Inan et al. 2017ï¼‰æŒ‡å‡ºæƒé‡ tying èƒ½é™ä½å›°æƒ‘åº¦ï¼ˆperplexityï¼‰ï¼Œå¸¸ä½œä¸ºè¯­è¨€æ¨¡å‹çš„é»˜è®¤é…ç½®ã€‚

3. **è¯­ä¹‰ä¸€è‡´æ€§**ï¼š
   è¾“å‡º logits çš„ç¬¬ *i* è¡Œç”¨çš„å°±æ˜¯è¯ *i* çš„åµŒå…¥å‘é‡ `e_i` ä¸éšè—æ€ `h_t` çš„ç‚¹ç§¯ï¼š

   $$
   \text{logits}_t = h_t \, E^\top \quad (\text{å…¶ä¸­ } E=\text{wte.weight}=\text{lm\_head.weight})
   $$

   ä¹Ÿå°±æ˜¯â€œ**ç”¨åŒä¸€ç»„è¯å‘é‡åšè¾“å…¥å’Œè¾“å‡º**â€ã€‚

---

## ä¸å®ç°ç»†èŠ‚ç›¸å…³çš„æ³¨æ„ç‚¹

* éœ€è¦ `lm_head` **æ— åç½®**ï¼ˆ`bias=False`ï¼‰ï¼Œå¦åˆ™å…±äº«çŸ©é˜µå¤–å†åŠ ä¸€ç»„åç½®ä¼šç ´åå¯¹ç§°æ€§ï¼Œä¹Ÿä¸å†æ˜¯çº¯ç²¹çš„ tyingã€‚
* è¿™æ˜¯ **å‚æ•°åˆ«å**ï¼ˆåŒä¸€ `nn.Parameter`ï¼‰ï¼šä¼˜åŒ–å™¨ä¸ä¼šâ€œé‡å¤æ›´æ–°â€ã€‚PyTorch åœ¨æ”¶é›†å‚æ•°æ—¶ä¼šåŸºäºå¯¹è±¡ id å»é‡ã€‚
* ä»»ä½•å¯¹å…¶ä¸­ä¸€æ–¹ `.weight` çš„ **åŸåœ°ä¿®æ”¹**éƒ½ä¼šå½±å“å¦ä¸€æ–¹ï¼ˆå› ä¸ºåŒä¸€å¯¹è±¡ï¼‰ã€‚
* å¦‚æœåæ¥åˆç»™ä»»æ„ä¸€æ–¹é‡æ–°èµ‹ä¸€ä¸ªæ–°çš„ `nn.Parameter`ï¼Œå°±**è§£ç»‘**äº†ï¼ˆä¸å†å…±äº«ï¼‰ã€‚

---

## å°æµ‹è¯•ï¼ˆéªŒè¯æ˜¯å¦å…±äº«ï¼‰

```python
# åŒä¸€å¯¹è±¡ï¼ˆid ä¸€æ ·ï¼‰
id(model.transformer.wte.weight) == id(model.lm_head.weight)  # True

# åŒä¸€å­˜å‚¨ï¼ˆåº•å±‚åœ°å€ä¸€æ ·ï¼‰
model.transformer.wte.weight.data_ptr() == model.lm_head.weight.data_ptr()  # True
```

**ä¸€å¥è¯æ€»ç»“**ï¼š
è¿™è¡Œä»£ç è®© **è¾“å…¥åµŒå…¥çŸ©é˜µ** å’Œ **è¾“å‡ºåˆ†ç±»æƒé‡** å…±ç”¨åŒä¸€å‚æ•°ï¼Œæ—¢çœå†…å­˜åˆå¸¸å¸¸æ›´å‡†ï¼Œæ˜¯ç°ä»£è¯­è¨€æ¨¡å‹çš„å¸¸è§åšæ³•ã€‚


```
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            std = 0.02
            if hasattr(module, 'NANOGPT_SCALE_INIT'):
                std *= (2 * self.config.n_layer) ** -0.5
            torch.nn.init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
```

å¥½ ğŸ‘Œ è¿™æ®µ `_init_weights` æ–¹æ³•æ˜¯ **è‡ªå®šä¹‰å‚æ•°åˆå§‹åŒ–å‡½æ•°**ï¼Œåœ¨æ„é€ æ¨¡å‹æ—¶ä¼šå¯¹ `Linear` å’Œ `Embedding` å±‚è¿›è¡Œæƒé‡åˆå§‹åŒ–ã€‚é€è¡Œæ¥çœ‹ï¼š

---

## 1. å‡½æ•°å…¥å£

```python
def _init_weights(self, module):
```

* è¿™ä¸ªå‡½æ•°é€šå¸¸ä¼šè¢« `model.apply(self._init_weights)` è°ƒç”¨ï¼Œéå†æ¨¡å‹é‡Œçš„æ‰€æœ‰å­æ¨¡å—ï¼Œå¯¹æ¯ä¸€å±‚åˆå§‹åŒ–ã€‚

---

## 2. å¤„ç† `nn.Linear` å±‚

```python
if isinstance(module, nn.Linear):
    std = 0.02
    if hasattr(module, 'NANOGPT_SCALE_INIT'):
        std *= (2 * self.config.n_layer) ** -0.5
    torch.nn.init.normal_(module.weight, mean=0.0, std=std)
    if module.bias is not None:
        torch.nn.init.zeros_(module.bias)
```

### (1) åˆ¤æ–­ç±»å‹

* å¦‚æœå­æ¨¡å—æ˜¯ `nn.Linear`ï¼ˆå…¨è¿æ¥å±‚ï¼‰ï¼Œæ‰§è¡Œè¿™ä¸ªåˆ†æ”¯ã€‚

### (2) è®¾ç½®æ ‡å‡†å·®

* é»˜è®¤ `std = 0.02`ï¼ˆå’Œ GPT-2ã€BERT çš„åˆå§‹åŒ–ä¸€è‡´ï¼‰ã€‚
* å¦‚æœè¿™ä¸ª `Linear` å±‚å¸¦æœ‰å±æ€§ `NANOGPT_SCALE_INIT`ï¼Œåˆ™è¿›ä¸€æ­¥ç¼©æ”¾æ ‡å‡†å·®ï¼š

  $$
  std = 0.02 \times (2 \cdot n\_layer)^{-0.5}
  $$

  â†’ è¿™ä¸ªæŠ€å·§åœ¨ **æ®‹å·®è¿æ¥æ·±ç½‘ç»œ**é‡Œæœ‰æ—¶èƒ½æ”¹å–„ç¨³å®šæ€§ã€‚
  â†’ ç±»ä¼¼äº Transformer é‡Œ "scaled initialization" çš„æ€æƒ³ã€‚

### (3) æƒé‡åˆå§‹åŒ–

```python
torch.nn.init.normal_(module.weight, mean=0.0, std=std)
```

* ç”¨ **å‡å€¼ 0ï¼Œæ ‡å‡†å·® std çš„é«˜æ–¯åˆ†å¸ƒ**æ¥åˆå§‹åŒ–æƒé‡ã€‚

### (4) åç½®åˆå§‹åŒ–

```python
if module.bias is not None:
    torch.nn.init.zeros_(module.bias)
```

* å¦‚æœæœ‰åç½®ï¼Œå°±å…¨è®¾ä¸º 0ã€‚

---

## 3. å¤„ç† `nn.Embedding` å±‚

```python
elif isinstance(module, nn.Embedding):
    torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
```

* è¯åµŒå…¥çŸ©é˜µä¹Ÿç”¨å‡å€¼ 0ã€æ ‡å‡†å·® 0.02 çš„é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–ã€‚
* è¿™æ˜¯ GPT-2 è®ºæ–‡é‡Œçš„åšæ³•ï¼šembedding å±‚å’Œ Linear å±‚ä¿æŒä¸€è‡´ã€‚

---

## 4. ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåšï¼Ÿ

1. **ä¿è¯è®­ç»ƒç¨³å®šæ€§**

   * æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–èƒ½é¿å…æ¢¯åº¦è¿‡å¤§/è¿‡å°ã€‚
   * æ ‡å‡†å·® 0.02 æ˜¯ç»éªŒå€¼ï¼ŒGPT-2/BERT ç­‰æ¨¡å‹é‡‡ç”¨çš„ã€‚

2. **æ®‹å·®ç½‘ç»œçš„ç‰¹æ®Šç¼©æ”¾**

   * æ·±å±‚ Transformer ä¸­ï¼Œå¦‚æœä¸ç¼©æ”¾ï¼Œæ®‹å·®ä¼šè®©æ–¹å·®å åŠ ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚
   * `(2*n_layer)^(-0.5)` è¿™ä¸ªç¼©æ”¾æ˜¯ä¸ª trickï¼Œå¯ä»¥è®©è¾“å‡ºæ–¹å·®å¤§è‡´ä¿æŒæ’å®šã€‚

3. **Embedding ä¸ Linear å¯¹é½**

   * è¾“å…¥åµŒå…¥ã€è¾“å‡ºå¤´ã€MLPã€æ³¨æ„åŠ›é‡Œçš„ Linear å±‚ï¼Œéƒ½ä¿æŒä¸€è‡´çš„åˆå§‹åŒ–åˆ†å¸ƒã€‚

---

## âœ… æ€»ç»“

`_init_weights` çš„ä½œç”¨æ˜¯ï¼š

* **Linear å±‚**ï¼šæƒé‡ç”¨ `N(0, std^2)` åˆå§‹åŒ–ï¼Œåç½®è®¾ä¸º 0ï¼›å¦‚æœæ ‡è®°äº† `NANOGPT_SCALE_INIT`ï¼Œå°±é¢å¤–ç¼©æ”¾ stdã€‚
* **Embedding å±‚**ï¼šæƒé‡ç”¨ `N(0, 0.02^2)` åˆå§‹åŒ–ã€‚

è¿™æ ·ä¿è¯äº†æ¨¡å‹ä¸€å¼€å§‹çš„å‚æ•°åˆ†å¸ƒåˆç†ï¼Œæœ‰åˆ©äºç¨³å®šè®­ç»ƒã€‚

---

è¦ä¸è¦æˆ‘é¡ºä¾¿å¸®ä½ è§£é‡Šä¸€ä¸‹ **ä¸ºä»€ä¹ˆ GPT/BERT éƒ½é€‰æ‹© `std=0.02`**ï¼Œè€Œä¸æ˜¯ Xavier/He åˆå§‹åŒ–ï¼Ÿ

# Xavier initialization è§£é‡Šä¸€ä¸‹

å¥½é—®é¢˜ ğŸ‘ ä½ é—®çš„ **Xavier åˆå§‹åŒ–**ï¼ˆåˆå« Glorot åˆå§‹åŒ–ï¼ŒGlorot & Bengio 2010ï¼‰æ˜¯æ·±åº¦å­¦ä¹ é‡Œæœ€ç»å…¸çš„æƒé‡åˆå§‹åŒ–æ–¹æ³•ä¹‹ä¸€ã€‚

---

## 1. ä¸ºä»€ä¹ˆéœ€è¦ç‰¹æ®Šåˆå§‹åŒ–ï¼Ÿ

* å¦‚æœæƒé‡åˆå§‹åŒ–å¾—å¤ªå¤§ï¼šå‰å‘ä¼ æ’­æ—¶æ¿€æ´»ä¼šçˆ†ç‚¸ï¼Œåå‘ä¼ æ’­æ—¶æ¢¯åº¦ä¹Ÿä¼šçˆ†ç‚¸ã€‚
* å¦‚æœæƒé‡åˆå§‹åŒ–å¾—å¤ªå°ï¼šæ¿€æ´»å’Œæ¢¯åº¦ä¼šé€å±‚ç¼©å°ï¼Œæœ€åæ¶ˆå¤±ã€‚
* æ‰€ä»¥éœ€è¦ä¸€ç§åˆå§‹åŒ–æ–¹æ³•ï¼Œè®©**è¾“å…¥å’Œè¾“å‡ºçš„æ–¹å·®å°½é‡ä¿æŒä¸€è‡´**ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚

---

## 2. æ€æƒ³

Xavier åˆå§‹åŒ–çš„ç›®æ ‡æ˜¯ï¼š

$$
Var(Wx) \approx Var(x), \quad Var(\nabla W) \approx Var(\nabla x)
$$

ä¹Ÿå°±æ˜¯è¯´ï¼šä¿æŒå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸­ï¼Œä¿¡å·çš„æ–¹å·®å°½é‡æ’å®šã€‚

---

## 3. å…¬å¼

å‡è®¾ä¸€å±‚ç½‘ç»œæœ‰ï¼š

* è¾“å…¥ç»´åº¦ = $fan\_in$
* è¾“å‡ºç»´åº¦ = $fan\_out$

é‚£ä¹ˆæƒé‡ $W$ åˆå§‹åŒ–æ–¹å¼ä¸ºï¼š

* **å‡åŒ€åˆ†å¸ƒ**ï¼ˆæœ€å¸¸è§ï¼‰ï¼š

$$
W \sim U\left[-\sqrt{\frac{6}{fan\_in + fan\_out}}, \; \sqrt{\frac{6}{fan\_in + fan\_out}} \right]
$$

* **æ­£æ€åˆ†å¸ƒ**ï¼ˆä¹Ÿå¸¸ç”¨ï¼‰ï¼š

$$
W \sim \mathcal{N}\left(0, \; \frac{2}{fan\_in + fan\_out}\right)
$$

---

## 4. PyTorch å®ç°

```python
import torch
import torch.nn as nn

linear = nn.Linear(128, 64)
nn.init.xavier_uniform_(linear.weight)   # å‡åŒ€åˆ†å¸ƒç‰ˆæœ¬
nn.init.xavier_normal_(linear.weight)    # æ­£æ€åˆ†å¸ƒç‰ˆæœ¬
```

---

## 5. ä½¿ç”¨åœºæ™¯

* å¸¸ç”¨äº **tanh / sigmoid æ¿€æ´»å‡½æ•°** çš„ç½‘ç»œã€‚
* åœ¨ ReLU ç½‘ç»œé‡Œï¼Œåæ¥æ›´æµè¡Œ **He åˆå§‹åŒ– (Kaiming Init)**ï¼Œå®ƒæ›´é€‚åˆéå¯¹ç§°çš„ ReLU æ¿€æ´»ã€‚

---

## 6. å’Œ GPT/BERT åˆå§‹åŒ–çš„åŒºåˆ«

* GPT/BERT ç”¨çš„æ˜¯ **é«˜æ–¯åˆ†å¸ƒ N(0, 0.02Â²)** å›ºå®š stdï¼Œè€Œä¸æ˜¯ Xavierã€‚
* è¿™æ˜¯ç»éªŒé€‰æ‹©ï¼Œå’Œ Transformer çš„æ®‹å·®ç»“æ„ã€LayerNorm ç»“åˆåï¼Œèƒ½ä¿æŒè®­ç»ƒç¨³å®šã€‚
* å¦‚æœä¸ç”¨ LayerNormï¼Œé‚£ Xavier/He åˆå§‹åŒ–æ›´å¸¸è§ã€‚

---

âœ… **ä¸€å¥è¯æ€»ç»“**
Xavier åˆå§‹åŒ–é€šè¿‡è€ƒè™‘è¾“å…¥/è¾“å‡ºç»´åº¦ï¼Œè‡ªåŠ¨é€‰æ‹©æƒé‡èŒƒå›´ï¼Œè®©ä¿¡å·åœ¨å‰å‘å’Œåå‘ä¼ æ’­æ—¶ä¿æŒæ–¹å·®ç¨³å®šï¼Œä»è€Œé¿å…æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±ã€‚

---

è¦ä¸è¦æˆ‘å†å¸®ä½ å¯¹æ¯”ä¸€ä¸‹ **Xavier åˆå§‹åŒ– vs He åˆå§‹åŒ–**ï¼Œçœ‹çœ‹å®ƒä»¬å„è‡ªé€‚åˆå“ªäº›æ¿€æ´»å‡½æ•°ï¼Ÿ

# ä¸ºä»€ä¹ˆè¦ä¹˜ä»¥2

å•Šï½ä½ è¯´çš„æ˜¯è¿™æ®µåˆå§‹åŒ–é‡Œçš„ï¼š

```python
if hasattr(module, 'NANOGPT_SCALE_INIT'):
    std *= (2 * self.config.n_layer) ** -0.5
```

å…³é”®å°±æ˜¯è¿™ä¸ª `2 * self.config.n_layer`ï¼Œä¸ºä»€ä¹ˆè¦æœ‰ä¸ª **2**ã€‚

---

## 1. èƒŒæ™¯ï¼šæ·±å±‚ Transformer çš„æ®‹å·®

åœ¨ GPT è¿™ç§æ¨¡å‹é‡Œï¼Œå †å äº†å¾ˆå¤šå±‚ï¼Œæ¯å±‚éƒ½æœ‰ **æ®‹å·®è¿æ¥ (residual connection)**ï¼š

$$
x_{l+1} = x_l + f(x_l)
$$

å…¶ä¸­ $f(x_l)$ æ˜¯æ³¨æ„åŠ›æˆ– MLPã€‚

* æ¯å±‚éƒ½ä¼šå¾€æ®‹å·®é‡ŒåŠ ä¸€ä¸ªâ€œå¢é‡â€ã€‚
* å¦‚æœä¸ç¼©æ”¾ï¼Œå¢é‡çš„æ–¹å·®ä¼šéšå±‚æ•°ç´¯ç§¯ï¼Œè¶Šåˆ°æ·±å±‚è¶Šå¤§ï¼Œè®­ç»ƒå¯èƒ½ä¸ç¨³å®šã€‚

---

## 2. ä¸ºä»€ä¹ˆæ˜¯ `(2 * n_layer)^(-0.5)`ï¼Ÿ

è¿™ä¸ªç¼©æ”¾æ€æƒ³æ¥è‡ª **DeepNet/Transformer Scaling Trick**ï¼ˆè§ GPT-NeoX å’Œä¸€äº›è®­ç»ƒæŠ€å·§æ€»ç»“ï¼‰ï¼š

* **n\_layer éƒ¨åˆ†**

  * æ¯å±‚çš„å¢é‡å¤§æ¦‚æœ‰ `n_layer` æ¬¡ç´¯åŠ ã€‚
  * ä¸ºäº†è®©æ€»æ–¹å·®ä¿æŒå¸¸æ•°ï¼Œéœ€è¦è®©æ¯å±‚åˆå§‹åŒ–æ—¶çš„æ–¹å·®ç¼©å°å¤§çº¦ $1/\sqrt{n\_layer}$ã€‚

* **ä¹˜ä»¥ 2**

  * ä¸€ä¸ªå®Œæ•´çš„ Transformer block é‡Œæœ‰ **ä¸¤ä¸ªæ®‹å·®è·¯å¾„**ï¼š

    1. Multi-Head Self-Attention (æ³¨æ„åŠ›åˆ†æ”¯)
    2. MLP (å‰é¦ˆç½‘ç»œåˆ†æ”¯)
  * ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨æ¯ä¸€å±‚é‡Œï¼Œæ®‹å·®é‡Œä¼šåŠ ä¸¤æ¬¡å¢é‡ã€‚
  * æ‰€ä»¥æ€»çš„ç´¯åŠ æ¬¡æ•° â‰ˆ $2 \times n\_layer$ã€‚
  * ä¸ºäº†æŠµæ¶ˆè¿™ç‚¹ï¼Œå°±è¦ç”¨ $(2 \times n\_layer)^{-0.5}$ æ¥ç¼©æ”¾æ ‡å‡†å·®ã€‚

---

## 3. ç›´è§‚ç†è§£

* å¦‚æœåªæœ‰ä¸€ä¸ªæ®‹å·®åˆ†æ”¯ï¼Œæ¯å±‚çš„ç¼©æ”¾å› å­æ˜¯ $(n\_layer)^{-0.5}$ã€‚
* ä½† Transformer block æœ‰ä¸¤ä¸ªæ®‹å·®åˆ†æ”¯ â†’ æ–¹å·®ç´¯ç§¯é€Ÿåº¦ç¿»å€ã€‚
* æ‰€ä»¥è¦å†™æˆ $(2 \times n\_layer)^{-0.5}$ã€‚

---

## âœ… æ€»ç»“

è¿™é‡Œçš„ **2** æ¥è‡ª Transformer block é‡Œæœ‰ **ä¸¤æ¡æ®‹å·®è·¯å¾„ï¼ˆAttention + MLPï¼‰**ã€‚
æ‰€ä»¥ä¸ºäº†è®©ä¿¡å·æ–¹å·®åœ¨å±‚æ•°å¢åŠ æ—¶ä¿æŒç¨³å®šï¼Œåˆå§‹åŒ–æ—¶è¦æŠŠæ ‡å‡†å·®é¢å¤–ç¼©å°ä¸€å€ï¼š

$$
std = 0.02 \times (2 \cdot n\_layer)^{-0.5}
$$

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **æ®‹å·®ç´¯ç§¯ vs ç¼©æ”¾å› å­** çš„å°ç¤ºæ„å›¾ï¼Œè®©ä½ æ›´ç›´è§‚ç†è§£ä¸ºä»€ä¹ˆæ˜¯ `2*n_layer` è€Œä¸æ˜¯ `n_layer`ï¼Ÿ



# optimization loop: overfit a single batch

here we have the loss is this is how we get the loss but now basically we want a load
for Loop here so 4 I in range let's do 50 steps or something like that uh let's create an Optimizer object in
pytorch um and so here we are using the atom um Optimizer which is an
alternative to the stochastic radian descent Optimizer SGD that we were using so SGD is a lot simpler atom is a bit
more involved and I actually specifically like the atom W variation because in my opinion it kind of just
like fixes a bug um so adom w is a bug fix of atom is what I would say when we
go to the documentation for atom W oh my gosh we see um that it takes a bunch of
hyper parameters and it's a little bit more complicated than the SGD we were looking at before uh because in addition
to basically updating the parameters with the gradient uh scaled by the Learning rate it keeps these buffers
around and it keeps two buffers the m and the V which it calls the first and the second moment so something that
looks a bit like momentum and something that looks a bit like RMS prop if you're familiar with it but you don't have to
be it's just kind of a normalization that happens on each gradient element individually and speeds up the
optimization especially for language models but I'm not going to go into the detail right here we're going to treat
it as a bit of a black box and it just optimizes um the objective faster than
SGD which is what we've seen in the previous lectures so let's use it as a black box in our case uh create the
optimizer object and then go through the optimization
the first thing to always make sure the co-pilot did not forget to zero the gradients so um always remember that you
have to start with a zero gradient then when you get your loss and you do a DOT backward dot backward adds to gradients
so it deposits gradients it it always does a plus equals on whatever the gradients are which is why you must set
them to zero so this accumulates the gradient from this loss and then we call the step function on the optimizer to um
update the parameters and to um decrease the loss and then we print a step and the
loss do item is used here because loss is a tensor with a single element do item will actually uh convert that to a
single float and this float will live not will will live on the CPU so this
gets to some of the internals again of the devices but loss is a is a tensor with a single element and it lifts on
GPU for me because I'm using gpus when you call item P torch behind the scenes
will take that one-dimensional tensor ship it back to the CPU uh memory and convert it into a float that we can just
print so this is the optimization and this should probably just
work let's see what happens actually sorry let me instead of
using CPU override let me delete that so this is a bit faster for me and it runs on Cuda
oh expected all tensors to be on the same device but found at least two devices Cuda zero and CPU so Cuda zero
is the zeroth GPU because I actually have eight gpus on this box uh so the zeroth GPU in my box and CPU and model
we have moved to device but when I was writing this code I actually introduced a bug because buff we never moved to
device and you have to be careful because you can't just do buff dot two of
device um it's not stateful it doesn't convert it to be a device it instead uh
returns pointer to a new memory which is on the device so you see how we can just do model that two a device that does not
apply to tensors you have to do buff equals um b.2 device and then this should work
okay so what do we expect to see we expect to see a reasonable loss in the beginning and then we continue to
optimize just the single batch and so we want to see that we can overfit this single batch we can we can crush this
little batch and we can perfectly predict the indices on just this little batch and indeed that is roughly what
we're seeing here so um we started off at roughly 10.82 11
in this case and then as we continue optimizing on this single batch without loading new examples we are making sure
that we can overfit a single batch and we are getting to very very low loss so the Transformer is memorizing this
single individual batch and one more thing I didn't mention is uh the learning rate here is 3 E4 which is a
pretty good default for most uh optimizations that you want to run at a very early debugging stage so this is
our simple inter Loop and uh we are overfitting a single batch and this looks good so now what uh what comes
next is we don't just want to overfit a single batch we actually want to do an optimization so we actually need to iterate these XY batches and create a
little data loader uh that makes sure that we're always getting a fresh batch and that we're actually optimizing a
reasonable objective so let's do that next okay so this is what I came up with and I wrote a little data loader

å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## ä¼˜åŒ–å¾ªç¯ï¼šåœ¨å•ä¸ªæ‰¹æ¬¡ä¸Šè¿‡æ‹Ÿåˆï¼ˆOverfit a Single Batchï¼‰

### 1. åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆOptimizerï¼‰

ä¸ºäº†å¼€å§‹ä¼˜åŒ–æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ **AdamW** ä¼˜åŒ–å™¨ï¼Œè€Œä¸æ˜¯ä¹‹å‰è®²è§£çš„ **SGD**ï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰ã€‚AdamW æ˜¯ Adam ä¼˜åŒ–å™¨çš„ä¸€ä¸ªå˜ç§ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤„ç†å…·æœ‰æƒé‡è¡°å‡ï¼ˆweight decayï¼‰çš„ä»»åŠ¡ï¼Œæ¯”å¦‚è¯­è¨€æ¨¡å‹è®­ç»ƒã€‚

* **AdamW** ä¼˜åŒ–å™¨ä¸ä»…ä½¿ç”¨æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œè¿˜ç»´æŠ¤ä¸¤ä¸ªç¼“å†²åŒºï¼š**m** å’Œ **v**ï¼Œåˆ†åˆ«ä»£è¡¨ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªçŸ©ï¼ˆç±»ä¼¼äºåŠ¨é‡å’Œ RMSpropï¼‰ã€‚è¿™äº›ç¼“å†²åŒºå¸®åŠ©å¯¹æ¯ä¸ªæ¢¯åº¦å…ƒç´ è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä»è€ŒåŠ é€Ÿä¼˜åŒ–è¿‡ç¨‹ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒè¯­è¨€æ¨¡å‹æ—¶è¡¨ç°å¾—æ›´å¥½ã€‚

å°½ç®¡ AdamW æ¯” SGD æ›´å¤æ‚ï¼Œä½†å®ƒèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†å…¶å½“ä½œä¸€ä¸ªâ€œé»‘ç®±â€ï¼Œç›´æ¥ä½¿ç”¨å®ƒæ¥è¿›è¡Œä¼˜åŒ–ã€‚

### 2. ä¼˜åŒ–è¿‡ç¨‹

åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸€æ­¥éœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š

* **é›¶åŒ–æ¢¯åº¦**ï¼šæ¯æ¬¡å¼€å§‹è®¡ç®—ä¹‹å‰ï¼Œå¿…é¡»ä½¿ç”¨ `optimizer.zero_grad()` å°†æ¢¯åº¦æ¸…é›¶ï¼Œå› ä¸º PyTorch é»˜è®¤ä¼šåœ¨æ¯æ¬¡åå‘ä¼ æ’­æ—¶ç´¯åŠ æ¢¯åº¦ã€‚å¦‚æœä¸æ¸…é›¶ï¼Œæ¢¯åº¦ä¼šä¸æ–­ç´¯ç§¯ï¼Œä»è€Œå¯¼è‡´ä¸æ­£ç¡®çš„æ›´æ–°ã€‚

* **è®¡ç®—æ¢¯åº¦å¹¶åå‘ä¼ æ’­**ï¼šä½¿ç”¨ `loss.backward()` è®¡ç®—æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Œå¹¶å°†æ¢¯åº¦å­˜å‚¨åœ¨æ¨¡å‹çš„å‚æ•°ä¸­ã€‚

* **æ›´æ–°å‚æ•°**ï¼šé€šè¿‡ `optimizer.step()` æ›´æ–°æ¨¡å‹çš„å‚æ•°ï¼Œä»¥æœ€å°åŒ–æŸå¤±ã€‚

* **æ‰“å°æŸå¤±**ï¼šæˆ‘ä»¬å¯ä»¥æ‰“å°å‡ºæŸå¤±å€¼ä»¥è·Ÿè¸ªè®­ç»ƒè¿›åº¦ã€‚å› ä¸ºæŸå¤±æ˜¯ä¸€ä¸ªåŒ…å«å•ä¸€å…ƒç´ çš„å¼ é‡ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ `.item()` æ–¹æ³•å°†å…¶è½¬æ¢ä¸ºæ™®é€šçš„æµ®åŠ¨æ•°å€¼ï¼Œç„¶åæ‰“å°å‡ºæ¥ã€‚

```python
loss.item()
```

### 3. è°ƒè¯•æ¨¡å¼ä¸‹çš„å­¦ä¹ ç‡

åœ¨ä¼˜åŒ–åˆæœŸï¼Œå­¦ä¹ ç‡ï¼ˆlearning rateï¼‰é€šå¸¸è®¾å®šä¸ºè¾ƒå°çš„å€¼ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œå­¦ä¹ ç‡è®¾å®šä¸º `3e-4`ï¼Œè¿™æ˜¯å¤§å¤šæ•°ä¼˜åŒ–ä»»åŠ¡åœ¨è°ƒè¯•é˜¶æ®µçš„ä¸€ä¸ªè‰¯å¥½é»˜è®¤å€¼ã€‚è¿™ä¸ªå­¦ä¹ ç‡èƒ½å¤Ÿç¡®ä¿è®­ç»ƒç¨³å®šï¼Œé˜²æ­¢è¿‡å¿«è·³è¿‡æœ€ä¼˜è§£ã€‚

### 4. åœ¨å•ä¸ªæ‰¹æ¬¡ä¸Šè¿‡æ‹Ÿåˆ

åœ¨è¿™ä¸ªä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯**åœ¨å•ä¸ªæ‰¹æ¬¡ä¸Šè¿‡æ‹Ÿåˆ**ï¼Œä¹Ÿå°±æ˜¯è®©æ¨¡å‹èƒ½å¤Ÿå®Œç¾åœ°é¢„æµ‹è¯¥æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰ tokenã€‚åœ¨è®­ç»ƒåˆæœŸï¼Œæˆ‘ä»¬çœ‹åˆ°æŸå¤±ä»åˆå§‹çš„ **10.82**ï¼ˆç†è®ºå€¼ï¼‰é€æ¸é™ä½ï¼Œæ¥è¿‘ **0**ï¼Œè¿™è¯´æ˜æ¨¡å‹å·²ç»èƒ½å¤Ÿâ€œè®°ä½â€è¿™ä¸ªå°æ‰¹æ¬¡çš„æ•°æ®ï¼Œè¾¾åˆ°äº†å®Œç¾çš„æ‹Ÿåˆã€‚

### 5. è®¾å¤‡ç®¡ç†ï¼ˆDevice Handlingï¼‰

åœ¨è¿™æ®µä»£ç ä¸­ï¼Œä¹Ÿæœ‰å…³äºè®¾å¤‡ï¼ˆCPU å’Œ GPUï¼‰çš„ç®¡ç†ï¼š

* æ¨¡å‹å’Œæ•°æ®éœ€è¦ç¡®ä¿ä½äºåŒä¸€è®¾å¤‡ä¸Šï¼Œä¸èƒ½åŒæ—¶å­˜åœ¨äºä¸åŒè®¾å¤‡ï¼ˆå¦‚ CPU å’Œ GPUï¼‰ä¸Šã€‚
* å½“ä½¿ç”¨å¤š GPU æ—¶ï¼Œéœ€è¦ç‰¹åˆ«å°å¿ƒï¼Œå› ä¸ºæŸäº›ç¼“å­˜ï¼ˆå¦‚ `buff`ï¼‰æœªè‡ªåŠ¨è¿ç§»åˆ°è®¾å¤‡ä¸Šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ `buff = buff.to(device)` æ¥ç¡®ä¿æ•°æ®å’Œæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Šã€‚

### 6. ä¸‹ä¸€æ­¥ï¼šä¼˜åŒ–å¤šä¸ªæ‰¹æ¬¡

è™½ç„¶æˆ‘ä»¬åœ¨å•ä¸ªæ‰¹æ¬¡ä¸ŠæˆåŠŸè¿›è¡Œäº†è¿‡æ‹Ÿåˆï¼Œä½†æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹åœ¨å¤šä¸ªæ‰¹æ¬¡ä¸Šè¿›è¡Œä¼˜åŒ–ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ï¼š

* åˆ›å»ºä¸€ä¸ª **æ•°æ®åŠ è½½å™¨**ï¼ˆDataLoaderï¼‰ï¼Œç¡®ä¿æ¯æ¬¡éƒ½èƒ½è·å–åˆ°ä¸€ä¸ªæ–°çš„æ‰¹æ¬¡ï¼›
* è¿›è¡Œå¤šæ¬¡è®­ç»ƒï¼Œä¼˜åŒ–ä¸€ä¸ªåˆç†çš„ç›®æ ‡ï¼Œè€Œä¸ä»…ä»…æ˜¯è®°ä½ä¸€ä¸ªå•ä¸€æ‰¹æ¬¡ã€‚

---

### æ€»ç»“

* æˆ‘ä»¬ä½¿ç”¨ **AdamW ä¼˜åŒ–å™¨** æ¥ä¼˜åŒ–æ¨¡å‹ï¼Œç¡®ä¿èƒ½å¤Ÿæœ‰æ•ˆåœ°è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚
* åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œå¿…é¡»æ¸…é™¤æ¢¯åº¦ã€è®¡ç®—æŸå¤±å¹¶è¿›è¡Œåå‘ä¼ æ’­ã€‚
* æˆ‘ä»¬é€šè¿‡åœ¨å•ä¸ªæ‰¹æ¬¡ä¸Šè¿‡æ‹Ÿåˆï¼Œç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹æ­£å¸¸ï¼Œå¹¶æˆåŠŸé™ä½äº†æŸå¤±å€¼ã€‚
* ä¸‹ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†å¼•å…¥ **æ•°æ®åŠ è½½å™¨**ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªæ‰¹æ¬¡ä¸Šè¿›è¡Œè®­ç»ƒã€‚

---

å¦‚æœä½ éœ€è¦æ›´è¯¦ç»†çš„ä»£ç å®ç°æˆ–æœ‰å…¶ä»–é—®é¢˜ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼


# data loader lite

light um so what this data loader does is we're importing the token up here
we're reading the entire text file from this single input.txt tokenizing it and then we're just
printing the number of tokens in total and the number of batches in a single Epoch of iterating over this data set so
how many unique batches do we output before we loop back around the beginning of the document and start reading it
again so we start off at position zero and then we simply walk the document in
batches of B * T so we take chunks of B * T and then always Advance by B * T and
um it's important to note that we're always advancing our position by exactly B * T but when we're fetching the tokens
we're actually fetching from current position to B * t + 1 and we need that
plus one because remember uh we need the target token um for the last token in the current
batch and so that way we can do um the XY exactly as we did it before and if we
are to um run out of data we'll just loop back around to zero so this is one
way to write a very very simple data loader um that simply just goes through the file in chunks and is good enough
for us uh for current purposes and we're going to complexify it later and now
we'd like to come back around here and we'd like to actually use our data loader so the import Tik token has moved
up and actually all of this is now useless so instead we just want a train
loader for the training data and we want to use the same hyper parameters for four so B size was four and time was
32 and then here we need to get the XY for the current batch so let's see if
copal gets it because this is simple enough uh so we call the next batch and then we um make sure that we have to
move our tensors from CPU to the device
so here when I converted the tokens notice that I didn't actually move these tokens to the GPU I left them on CPU
which is the default um and that's just because I'm trying not to waste too much memory on the GPU in this case this is a
tiny data set and it would fit uh but it's fine to just uh ship it to GPU right now for for our purposes right now
so we get the next batch we keep the data loader simple CPU class and then here we actually ship it to the GPU and
do all the computation and uh let's see if this runs so python train gbt2 pi and
what do we expect to see before this actually happens what we expect to see is now we're actually getting the next batch so we expect to not overfit a
single batch and so I expect our loss to come down but not too much and that's
because I still expect it to come down because in the 50257 tokens many of those tokens never
occur in our data set so there are some very easy gains to be made here in the optimization by for example taking the
biases of all the loits that never occur and driving them to negative infinity and that would basically just it's just
that all of these crazy unic codes or different languages those tokens never occur so their probability should be
very low and so the gains that we should be seeing are along the lines of basically deleting the usage of tokens
that never occur that's probably most of the loss gain that we're going to see at this scale right now uh but we shouldn't
come to a zero uh because um we are only doing 50 iterations and I don't think
that's enough to do an eoch right now so let's see what we got we um we have 338,000
tokens which makes sense with our 3:1 compression ratio because there are 1 million uh characters so one Epoch with
the current setting of B and T will take 2, 600 batches and we're only doing 50
batches of optimization in here so we start off in a familiar territory as expected and then we seem
to come down to about 6.6 so basically things seem to be working okay right now
with respect to our expectations so that's good okay next I want to actually fix a bug that we have in our code um

å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## æ•°æ®åŠ è½½å™¨ç®€åŒ–ç‰ˆï¼ˆData Loader Liteï¼‰

### 1. æ•°æ®åŠ è½½å™¨çš„ä½œç”¨

è¿™ä¸ª **æ•°æ®åŠ è½½å™¨** çš„ä½œç”¨æ˜¯ï¼š

* æˆ‘ä»¬é¦–å…ˆå¯¼å…¥ **tiktoken** åº“ã€‚
* ç„¶åè¯»å–æ•´ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼ˆ`input.txt`ï¼‰ï¼Œå¯¹å…¶è¿›è¡Œåˆ†è¯ã€‚
* æ¥ç€ï¼Œæ‰“å°å‡ºæ€»å…±çš„ **token æ•°é‡** ä»¥åŠä¸€ä¸ª **epoch** ä¸­çš„æ‰¹æ¬¡æ•°é‡ï¼Œè¡¨ç¤ºåœ¨éå†æ•´ä¸ªæ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆå¤šå°‘ä¸ªæ‰¹æ¬¡ã€‚

### 2. å¤„ç†æ•°æ®çš„æ–¹å¼

æˆ‘ä»¬ä»æ–‡ä»¶çš„å¼€å¤´å¼€å§‹ï¼ŒæŒ‰ **æ‰¹æ¬¡å¤§å° B \* T**ï¼ˆæ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦çš„ä¹˜ç§¯ï¼‰æ¥åˆ†æ‰¹å¤„ç†æ•°æ®ã€‚æ¯æ¬¡è¯»å–ä¸€ä¸ªæ‰¹æ¬¡çš„ tokenï¼Œå¹¶å°†å½“å‰ä½ç½®æ¨è¿› **B \* T**ã€‚è¿™é‡Œçš„å…³é”®ç‚¹æ˜¯ï¼š

* æ¯æ¬¡è·å–çš„æ•°æ®æ˜¯ä»å½“å‰çš„ä½ç½®å¼€å§‹ï¼Œåˆ° **B \* T + 1**ï¼ˆé¢å¤–å¤šå–ä¸€ä¸ª tokenï¼‰ï¼Œè¿™æ ·åšæ˜¯å› ä¸ºæˆ‘ä»¬éœ€è¦è·å–ç›®æ ‡ tokenï¼ˆå³å½“å‰æ‰¹æ¬¡ä¸­æœ€åä¸€ä¸ª token çš„ä¸‹ä¸€ä¸ª tokenï¼‰ï¼Œä»¥ä¾¿è®¡ç®—æŸå¤±ã€‚

* å¦‚æœæ•°æ®é›†çš„å†…å®¹è¯»å–å®Œäº†ï¼Œæˆ‘ä»¬å°±ä¼šä»å¤´å¼€å§‹é‡æ–°è¯»å–æ•°æ®ï¼Œè¿™æ ·ä¿è¯æ•°æ®èƒ½å¾ªç¯åˆ©ç”¨ï¼Œé€‚ç”¨äºè®­ç»ƒä¸­ã€‚

### 3. ç®€å•çš„æ•°æ®åŠ è½½å™¨å®ç°

æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªéå¸¸ç®€åŒ–çš„ **æ•°æ®åŠ è½½å™¨**ï¼Œå®ƒé€šè¿‡ä¸æ–­ä»æ–‡ä»¶ä¸­æŒ‰æ‰¹æ¬¡è¯»å–æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œè¶³å¤Ÿæ»¡è¶³å½“å‰çš„è°ƒè¯•éœ€æ±‚ã€‚ä¹‹åï¼Œæˆ‘ä»¬ä¼šåœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥å¤æ‚åŒ–ã€‚

### 4. è®­ç»ƒæ•°æ®åŠ è½½ä¸å¤„ç†

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¸Œæœ›åˆ©ç”¨è¿™ä¸ªæ•°æ®åŠ è½½å™¨æ¥è·å–è®­ç»ƒæ•°æ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç§»åŠ¨äº† `Tik token` çš„å¯¼å…¥éƒ¨åˆ†ï¼Œå¹¶ç®€åŒ–äº†ä¸€äº›ä¸å†éœ€è¦çš„éƒ¨åˆ†ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª **è®­ç»ƒæ•°æ®åŠ è½½å™¨**ï¼ˆ`train_loader`ï¼‰ï¼Œå¹¶è®¾ç½®äº†è®­ç»ƒæ—¶ä½¿ç”¨çš„è¶…å‚æ•°ï¼š

* æ‰¹æ¬¡å¤§å°ï¼ˆB sizeï¼‰ï¼š4
* åºåˆ—é•¿åº¦ï¼ˆTï¼‰ï¼š32

é€šè¿‡ `train_loader` è·å–å½“å‰æ‰¹æ¬¡çš„æ•°æ®åï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿ **æ•°æ®å¼ é‡** ä» **CPU** è½¬ç§»åˆ° **GPU**ã€‚è™½ç„¶æ•°æ®é›†è¾ƒå°ï¼Œç†è®ºä¸Šå¯ä»¥ç›´æ¥ä½¿ç”¨ **CPU** å¤„ç†ï¼Œä½†ä¸ºäº†åŠ é€Ÿè®¡ç®—ï¼Œæˆ‘ä»¬å°†æ•°æ®è½¬ç§»åˆ° **GPU** ä¸Šè¿›è¡Œå¤„ç†ã€‚

### 5. æœŸæœ›çš„ä¼˜åŒ–æ•ˆæœ

æˆ‘ä»¬ç°åœ¨å¼€å§‹è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¹¶è§‚å¯ŸæŸå¤±å‡½æ•°çš„å˜åŒ–ã€‚ç”±äºæ•°æ®é›†å¾ˆå°ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸æŒ‡æœ›æ¨¡å‹ä¼šåœ¨ä¸€ä¸ªæ‰¹æ¬¡ä¸Šå®Œç¾è¿‡æ‹Ÿåˆã€‚æˆ‘ä»¬æœŸæœ›æŸå¤±ä¼šä¸‹é™ï¼Œä½†ä¸ä¼šé™å¾—å¤ªå¿«ï¼Œå› ä¸ºæ•°æ®é›†ä¸­çš„è®¸å¤š token åœ¨è®­ç»ƒä¸­æ˜¯ä»æœªå‡ºç°è¿‡çš„ã€‚è¿™äº› **ä¸å‡ºç°çš„ token** çš„æ¦‚ç‡åº”è¯¥éå¸¸ä½ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ **è°ƒæ•´æŸå¤±** æ¥å¤„ç†è¿™äº› tokenï¼Œä½¿å®ƒä»¬çš„é¢„æµ‹å€¼è¶‹è¿‘äºé›¶ã€‚

* åœ¨è®­ç»ƒåˆæœŸï¼Œæ¨¡å‹ä¼šå¿«é€Ÿå¯¹è¿™äº›æœªå‡ºç°çš„ token è¿›è¡Œä¼˜åŒ–ï¼Œè°ƒæ•´å®ƒä»¬çš„æƒé‡ã€‚
* é¢„è®¡éšç€è®­ç»ƒçš„è¿›è¡Œï¼ŒæŸå¤±å€¼ä¼šé€æ­¥ä¸‹é™åˆ°åˆç†èŒƒå›´ï¼Œä½†ä¸ä¼šé™åˆ°é›¶ï¼Œå› ä¸ºæˆ‘ä»¬åªæ˜¯åšäº† **50 æ¬¡ä¼˜åŒ–æ­¥éª¤**ï¼ˆè¿˜ä¸å¤Ÿä¸€ä¸ªå®Œæ•´çš„ Epochï¼‰ã€‚

### 6. æ‰¹æ¬¡ä¸æ•°æ®é›†å¤§å°

åœ¨å½“å‰è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬æœ‰ **338,000 ä¸ª token**ï¼Œæ ¹æ® **3:1 çš„å‹ç¼©æ¯”**ï¼Œæ„å‘³ç€åŸå§‹æ–‡æœ¬æ–‡ä»¶çº¦æœ‰ **1 ç™¾ä¸‡å­—ç¬¦**ã€‚ä»¥å½“å‰çš„æ‰¹æ¬¡å¤§å°ï¼ˆB=4ï¼ŒT=32ï¼‰è¿›è¡Œè®­ç»ƒï¼Œæ¯ä¸ª Epoch å°†åŒ…å« **2600 ä¸ªæ‰¹æ¬¡**ï¼Œä½†æˆ‘ä»¬åªè¿›è¡Œ **50 æ¬¡ä¼˜åŒ–æ­¥éª¤**ã€‚å› æ­¤ï¼Œæ¨¡å‹çš„æŸå¤±å€¼å°†é€æ­¥ä¸‹é™ï¼Œä½†æˆ‘ä»¬è¿˜ä¸èƒ½æœŸå¾…åœ¨è¿™ä¹ˆå°‘çš„æ­¥éª¤ä¸­å°±è¾¾åˆ°å®Œç¾çš„æ•ˆæœã€‚

é€šè¿‡è®­ç»ƒåï¼Œæˆ‘ä»¬çš„æŸå¤±å€¼ä»å¤§çº¦ **10.82** å¼€å§‹ï¼Œé€æ¸ä¸‹é™åˆ° **6.6**ï¼Œè¿™æ˜¯ç¬¦åˆé¢„æœŸçš„ã€‚æ¨¡å‹å¼€å§‹æ­£å¸¸å·¥ä½œï¼ŒæŸå¤±å€¼é€æ­¥é™ä½ã€‚

### 7. ä¿®å¤ä»£ç ä¸­çš„ bug

æœ€åï¼Œä½œè€…æåˆ°ä»£ç ä¸­å­˜åœ¨ä¸€ä¸ª bugï¼Œéœ€è¦ä¿®å¤ã€‚è¿™ä¸ª bug å¯èƒ½å½±å“åˆ°æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡ï¼Œåç»­ä¼šå¯¹å…¶è¿›è¡Œä¿®å¤ã€‚

---

### æ€»ç»“

* **æ•°æ®åŠ è½½å™¨**ï¼šé€šè¿‡ç®€å•çš„ä»£ç ä»æ–‡ä»¶ä¸­æ‰¹æ¬¡åŠ è½½æ•°æ®ï¼Œå¹¶æŒ‰éœ€é‡æ–°å¼€å§‹ã€‚
* **è®­ç»ƒæ•°æ®å¤„ç†**ï¼šä½¿ç”¨åˆ†æ‰¹çš„æ–¹å¼å¤„ç†æ•°æ®ï¼Œä¿è¯æ¯ä¸ªæ‰¹æ¬¡çš„æ•°æ®èƒ½åœ¨ **GPU** ä¸Šè®¡ç®—ã€‚
* **æŸå¤±è®¡ç®—ä¸ä¼˜åŒ–**ï¼šé€šè¿‡äº¤å‰ç†µæŸå¤±å‡½æ•°è®¡ç®—æ¨¡å‹çš„æŸå¤±ï¼Œå¹¶ä½¿ç”¨ä¼˜åŒ–å™¨ï¼ˆAdamWï¼‰è¿›è¡Œä¼˜åŒ–ã€‚
* **æœŸæœ›æ•ˆæœ**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒæŸå¤±é€æ¸ä¸‹é™ï¼Œä½†ç”±äºè®­ç»ƒæ­¥éª¤è¾ƒå°‘ï¼ŒæŸå¤±ä¸ä¼šé™åˆ°é›¶ã€‚

---

å¦‚æœä½ æœ‰æ›´å¤šå…³äºæ•°æ®åŠ è½½ã€è®­ç»ƒè¿‡ç¨‹æˆ– bug ä¿®å¤çš„é—®é¢˜ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼


# parameter sharing wte and lm_head

it's not a major bug but it is a bug with respect to how gpt2 training uh should
happen um so the buck is the following we were not being careful enough when we were
loading the weights from hugging face and we actually missed a little detail so if we come
here notice that um the shape of these two tensors is the same so this one here
is the token embedding at the bottom of the Transformer right so and this one here
is the language modeling head at the top of the Transformer and both of these are
basically two-dimensional tensors and they shape is identical so here the
first one is the output embedding the token embedding and the second one is this linear layer at the very top the
classifier layer both of them are of shape 50257 X
768 um this one here is giving us our token embeddings at the bottom and this
one here is taking the 768 channels of the Transformer and trying to upscale
that to 50, 257 to get the Lis for the next token so they're both the same
shape but more than that actually if you look at um comparing their elements um
in pytorch this is an element wise equality so then we use do all and we see that every single element is
identical and more than that we see that if we actually look at the data pointer
uh this is what this is a way in pytorch to get the actual pointer to the uh data and the storage we see that actually the
pointer is identical so not only are these two separate tensors that happen to have the same shape and elements
they're actually pointing to the identical tensor so what's happening here is that this is a common weight
tying scheme uh that actually comes from the original um from the original attention is all
you need paper and actually even the reference before it so if we come
here um eddings and softmax in the attention
is all you need paper they mentioned that in our model we shared the same weight Matrix between the two embedding
layers and the pre softmax linear transformation similar to 30 um so this
is an awkward way to phrase that these two are shared and they're tied and they're the same Matrix and the 30
reference is this paper um so this came out in 2017 and you can read the full paper but
basically it argues for this weight tying scheme and I think intuitively the
idea for why you might want to do this comes from from this paragraph here and basically you you can observe
that um you actually want these two matrices to behave similar in the
following sense if two tokens are very similar semantically like maybe one of
them is all lowercase and the other one is all uppercase or it's the same token in a different language or something like that if you have similarity between
two tokens presumably you would expect that they are uh nearby in the token embedding space but in the exact same
way you'd expect that if you have two tokens that are similar semantically you'd expect them to get the same
probabilities at the output of a transformer because they are semantically similar and so both
positions in the Transformer at the very bottom and at the top have this property that similar tokens should have similar
embeddings or similar weights and so this is what motivates their exploration
here and they they kind of you know I don't want to go through the entire paper and and uh you can go through it
but this is what they observe they also observe that if you look at the output embeddings they also behave like word
embeddings um if you um if you just kind of try to use those weights as word
embeddings um so they kind of observe this similarity they try to tie them and they observe that they can get much
better performance in that way and so this was adopted and the attention is all need paper and then it was used
again in gpt2 as well so I couldn't find it in the
Transformers implementation I'm not sure where they tie those embeddings but I can find it in the original gpt2 code U
introduced by open aai so this is um openai gpt2 Source model and here where
they are forwarding this model and this is in tensorflow but uh that's okay we see that they get the wte token
embeddings and then here is the incoder of the token embeddings and the
position and then here at the bottom they Ed the WT again to do the lits so
when they get the loits it's a math Mo of uh this output from the Transformer and the wte tensor is
reused um and so the wte tensor basically is used twice on the bottom of the Transformer and on the top of the
Transformer and in the backward pass we'll get gradients contributions from both branches right and these gradients
will add up um on the wte tensor um so we'll get a contribution from the
classifier list and then at the very end of the Transformer we'll get a contribution at the at the bottom of it float floating
again into the wte uh tensor so we want to we are currently not sharing WT and
our code but we want to do that um
so weight sharing scheme um and one way to do this let's see if goil gets it oh
it does okay uh so this is one way to do it
uh basically relatively straightforward what we're doing here is we're taking the wte do weight and we're simply uh
redirecting it to point to the LM head so um this basically copies the data
pointer right it copies the reference and now the wte weight becomes orphaned
uh the old value of it and uh pytorch will clean it up python will clean it up
and so we are only left with a single tensor and it's going to be used twice
in the forward pass and uh this is to my knowledge all that's required so we
should be able to use this and this should probably train uh we're just going to basically be using this exact
same sensor twice and um we weren't being careful with
tracking the likelihoods but uh according to the paper and according to the results you'd actually expect slightly better results doing this and
in addition to that one other reason that this is very very nice for us is that this is a ton of parameters right
uh what is the size here it's 768 * 50257 so This Is 40 million parameters
and this is a 124 million parameter model so 40 divide 124 so this is like
30% of the parameters are being saved using this weight time scheme and so
this might be one of the reasons that this is working slightly better if you're not training the model long enough because of the weight tying uh
you don't have to train as many parameters and so you become more efficient um in terms of the training
process uh because you have fewer parameters and you're putting in this inductive bias that these two embeddings
should share similarities between tokens so this is the way time scheme and we've
saved a ton of parameters and we expect our model to work slightly better because of the scheme okay next I would

å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## æƒé‡å…±äº«ï¼š`wte` å’Œ `lm_head`

### 1. æƒé‡å…±äº«é—®é¢˜

åœ¨è®­ç»ƒ GPT-2 æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬é‡åˆ°äº†ä¸€ä¸ªå°é—®é¢˜ï¼Œä¸»è¦æ˜¯åœ¨åŠ è½½ Hugging Face çš„é¢„è®­ç»ƒæƒé‡æ—¶ï¼Œæ¼æ‰äº†ä¸€ä¸ªç»†èŠ‚ã€‚å…·ä½“æ¥è¯´ï¼Œé—®é¢˜å‡ºåœ¨ **`wte`ï¼ˆtoken embeddingï¼‰** å’Œ **`lm_head`ï¼ˆè¯­è¨€æ¨¡å‹å¤´ï¼‰** çš„æƒé‡æ²¡æœ‰å…±äº«ã€‚

* **`wte`** æ˜¯ Transformer ä¸­åº•éƒ¨çš„ token åµŒå…¥ï¼ˆtoken embeddingï¼‰ï¼Œå®ƒçš„ä½œç”¨æ˜¯å°† token ç´¢å¼•æ˜ å°„ä¸ºåµŒå…¥å‘é‡ã€‚
* **`lm_head`** æ˜¯ Transformer é¡¶éƒ¨çš„çº¿æ€§å±‚ï¼Œå®ƒçš„ä½œç”¨æ˜¯å°† Transformer çš„è¾“å‡ºæ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°ï¼Œç”¨äºç”Ÿæˆä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒã€‚

è¿™ä¸¤ä¸ªéƒ¨åˆ†çš„æƒé‡å½¢çŠ¶æ˜¯ç›¸åŒçš„ï¼Œéƒ½æ˜¯ `[50257, 768]`ï¼ˆ50257 æ˜¯è¯æ±‡è¡¨å¤§å°ï¼Œ768 æ˜¯åµŒå…¥ç»´åº¦ï¼‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å†…å®¹å®é™…ä¸Šæ˜¯ç›¸åŒçš„ï¼Œ**å®ƒä»¬æŒ‡å‘ç›¸åŒçš„å†…å­˜åœ°å€**ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ**è¿™ä¸¤ä¸ªå¼ é‡ä½¿ç”¨çš„æ˜¯ç›¸åŒçš„æƒé‡**ï¼Œè¿™å°±æ˜¯ **æƒé‡å…±äº«**ï¼ˆweight tyingï¼‰ã€‚

### 2. æƒé‡å…±äº«çš„åŠ¨æœº

æƒé‡å…±äº«çš„åŠ¨æœºæ¥è‡ªäºåŸå§‹çš„ Transformer è®ºæ–‡ã€ŠAttention is All You Needã€‹ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œä½œè€…æåˆ°åœ¨æ¨¡å‹çš„ **token åµŒå…¥å±‚** å’Œ **é¢„ softmax çº¿æ€§å±‚** ä¹‹é—´å…±äº«æƒé‡ã€‚è¿™æ˜¯å› ä¸ºè¿™äº›å±‚çš„ä½œç”¨æ˜¯ç›¸ä¼¼çš„ï¼Œéƒ½æ˜¯å¤„ç†è¯æ±‡ä¿¡æ¯ã€‚

* åœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œè¯­ä¹‰ç›¸ä¼¼çš„ tokenï¼ˆä¾‹å¦‚ç›¸åŒå•è¯çš„å°å†™å’Œå¤§å†™å½¢å¼ï¼‰åº”è¯¥æœ‰ç›¸ä¼¼çš„åµŒå…¥å’Œè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒã€‚
* å› æ­¤ï¼Œå¦‚æœä¸¤ä¸ª token åœ¨è¯­ä¹‰ä¸Šç›¸ä¼¼ï¼Œå®ƒä»¬åœ¨åµŒå…¥ç©ºé—´ä¸­çš„ä½ç½®åº”è¯¥æ¥è¿‘ï¼Œå¹¶ä¸”åœ¨ç”Ÿæˆä¸‹ä¸€ä¸ª token æ—¶ï¼Œæ¨¡å‹ä¹Ÿåº”è¯¥ä¸ºå®ƒä»¬åˆ†é…ç›¸ä¼¼çš„æ¦‚ç‡ã€‚

è¿™ç§ **æƒé‡å…±äº«** çš„åšæ³•å¯ä»¥å¸®åŠ©æ¨¡å‹æé«˜æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘å‚æ•°æ•°é‡ã€‚ç”±äºä¸¤ä¸ªçŸ©é˜µå…±äº«åŒä¸€ä¸ªæƒé‡ï¼Œæ¨¡å‹çš„è®­ç»ƒæ•ˆç‡æé«˜äº†ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒè¾ƒå¤§çš„è¯­è¨€æ¨¡å‹æ—¶ï¼Œè¿™ç§åšæ³•èƒ½èŠ‚çœå¤§é‡å†…å­˜å’Œè®¡ç®—èµ„æºã€‚

### 3. æƒé‡å…±äº«çš„å®ç°

åœ¨å®é™…å®ç°ä¸­ï¼Œæˆ‘ä»¬å‘ç° Hugging Face çš„ä»£ç æ²¡æœ‰æ˜ç¡®åœ°è¿›è¡Œæƒé‡å…±äº«ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å¤„ç†è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•æ¥å®ç°æƒé‡å…±äº«ï¼š

```python
# å°† wte æƒé‡å…±äº«ç»™ lm_head
model.lm_head.weight = model.wte.weight
```

è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯å°† **`wte`** æƒé‡çš„å¼•ç”¨èµ‹å€¼ç»™ **`lm_head`**ï¼Œä»è€Œå®ç° **æƒé‡å…±äº«**ã€‚è¿™æ ·ï¼Œ`wte` å’Œ `lm_head` å°±æŒ‡å‘åŒä¸€å—å†…å­˜ç©ºé—´ï¼Œç¡®ä¿å®ƒä»¬ä½¿ç”¨ç›¸åŒçš„æƒé‡ã€‚

### 4. æƒé‡å…±äº«çš„å¥½å¤„

* **èŠ‚çœå‚æ•°**ï¼šç”±äº **`wte`** å’Œ **`lm_head`** ä½¿ç”¨ç›¸åŒçš„æƒé‡ï¼Œè¿™å°±å‡å°‘äº† 40 ç™¾ä¸‡ï¼ˆ768 \* 50257ï¼‰çš„å‚æ•°ï¼Œè¿™å¯¹äº 124 ç™¾ä¸‡å‚æ•°çš„æ¨¡å‹æ¥è¯´ï¼Œç›¸å½“äºèŠ‚çœäº†å¤§çº¦ 30% çš„å‚æ•°ã€‚
* **æé«˜è®­ç»ƒæ•ˆç‡**ï¼šå‡å°‘å‚æ•°é‡ä½¿å¾—è®­ç»ƒå˜å¾—æ›´åŠ é«˜æ•ˆï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹è®­ç»ƒæ—¶é—´è¾ƒçŸ­æ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¿å…è¿‡å¤šçš„å‚æ•°é€ æˆçš„è®­ç»ƒä¸ç¨³å®šã€‚
* **æé«˜æ¨¡å‹æ€§èƒ½**ï¼šé€šè¿‡å…±äº«æƒé‡ï¼Œæ¨¡å‹åœ¨å¤„ç†è¯­ä¹‰ç›¸ä¼¼çš„ token æ—¶èƒ½å¤Ÿè¡¨ç°å¾—æ›´å¥½ï¼Œå› ä¸ºå®ƒå°†è¿™ä¸¤éƒ¨åˆ†çš„æƒé‡çº¦æŸä¸ºç›¸åŒï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰åˆ°è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚

### 5. æƒé‡å…±äº«çš„åŸç†

è¿™ç§æƒé‡å…±äº«çš„æ–¹å¼ä½“ç°äº†ä¸€ç§ **å½’çº³åç½®**ï¼ˆinductive biasï¼‰ï¼Œå³ï¼š**token åµŒå…¥å’Œè¾“å‡ºæ¦‚ç‡ä¹‹é—´åº”è¯¥æœ‰ç›¸ä¼¼æ€§**ã€‚è¿™ç§çº¦æŸä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´åŠ é«˜æ•ˆï¼Œå‡å°‘äº†è¿‡æ‹Ÿåˆçš„é£é™©ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†è¯­ä¹‰ç›¸ä¼¼çš„ tokenã€‚

### æ€»ç»“

* **æƒé‡å…±äº«** æ˜¯ä¸€ä¸ªå¸¸è§çš„æŠ€æœ¯ï¼Œå°¤å…¶åœ¨è¯­è¨€æ¨¡å‹ä¸­éå¸¸æœ‰æ•ˆã€‚é€šè¿‡å…±äº« **`wte`** å’Œ **`lm_head`** çš„æƒé‡ï¼Œæˆ‘ä»¬å‡å°‘äº†æ¨¡å‹çš„å‚æ•°ï¼Œå¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚
* è¿™ç§åšæ³•æ¥æºäºåŸå§‹ Transformer è®ºæ–‡ï¼Œå¹¶è¢« GPT-2 ç­‰æ¨¡å‹å¹¿æ³›ä½¿ç”¨ã€‚
* å®ç°æ–¹æ³•ç›¸å¯¹ç®€å•ï¼Œåªéœ€è¦å°† **`wte`** çš„æƒé‡å¼•ç”¨èµ‹ç»™ **`lm_head`** å³å¯ã€‚

---

å¦‚æœä½ éœ€è¦è¿›ä¸€æ­¥äº†è§£å¦‚ä½•å®ç°æˆ–è€…ä¼˜åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼


# model initialization: std 0.02, residual init

like us to be a bit more careful with the initialization and to try to follow the way gpt2 initialized their model now
unfortunately the gpt2 paper and the gpt3 paper are not very explicit about initialization so we kind of have to
read between the lines uh and instead of going to the paper which is quite vague um there's a bit of information in the
code that open I released so when we go to the model.py we see that when they initialize their weights they are using
the standard deviation of 0.02 and that's how they they so this is
a normal distribution for the weights and the standard deviation is 0.02 for the bias they initialize that
with zero and then when we scroll down here why is this not scrolling
um the token embeddings are initialized at 0.02 and position embeddings at 0.01 for
some reason so those are the initializations and we'd like to mirror that in gpt2 uh in our module here so here's a
snippet of code that I sort of came up with very quickly so what's happening here is at
the end of our initializer for the GPT module we're calling the apply function of NN module and that iterates all the
sub modules of this module and uh applies in it weights function on them
and so what's happening here is that we're in we're iterating all the modules here and if they are an nn. linear
module then we're going to make sure to initialize the weight using a normal with the standard deviation of
0.02 if there's a bias in this layer we will make sure to initialize that to zero note that zero initialization for
the bias is not actually the pyto default um by default the bias here is
initialized with a uniform so uh that's interesting so we make sure to use zero
and for the embedding we're just going to use 0.02 and um keep it the same um so we're not going to change it to 0.01
for positional because it's about the same and then if you look through our model the only other layer that requires
initialization and that has parameters is the layer norm and the fighter defer initialization sets the scale in the
layer Norm to be one and the offset in the layer Norm to be zero so that's exactly what we want and so we're just
going to uh keep it that way and so this is the default initialization if we are
following the um where is it the uh gpt2
uh source code that they released I would like to point out by the way that um typically the standard deviation here
on this initialization if you follow the Javier initialization would be one of over the square root of the number of
features that are incoming into this layer but if you'll notice actually 0.02 is basically consistent with that
because the the model sizes inside these Transformers for gpt2 are roughly 768 1600 Etc so 1 over the square root of
for example 768 gives us 0.03 if we plug in 600 1,600 we get
0.02 if we plug in three times that 0.014 Etc so basically 0.02 is roughly
in the vicinity of reasonable values for the for um for these initializations
anyway so so it's not uh completely crazy to be hard coding 0.02 here uh but
you'd like typically uh some something that grows with the model size instead
but we will keep this because that is the gpt2 initialization per their source code but we are not fully done yet on
initialization because there's one more caveat here so here a mod initialization which accounts
for the accumulation on the residual path with model depth is used we scale the weight of residual layers of
initialization by factor of one over squ of n where n is the number of residual layers so this is what gbt2 paper says
so we have not implemented that yet and uh we can do so now now I'd like to actually kind of like motivate a little
bit what they mean here I think um so here's roughly what they
mean if you start out with zeros in your residual stream remember that each
residual stream is a is of this form where we continue adding to it X is X
plus something some kind of contribution so every single block of the residual uh
Network contributes some uh amount and it gets added and so what ends up
happening is that the variance of the activations in the residual stream grows
so here's a small example if we start at zero and then we for 100 times uh we
have sort of this residual stream of of 768 uh zeros and then 100 times we add
um random which is a normal distribution zero mean one standard deviation if we
add to it then by the end the residual stream has grown to have standard deviation of 10 and that's just because
um we're always adding um these numbers
and so this scaling factor that they use here exactly compensates for that growth
so if we take n and we basically um scale down every one of these
contributions into the residual stream by one over theare Ro of n so 1 over theun of n is n to the 0.5
right because n the5 is the square root and then one over the square root is n.5
if we scale it in this way then we see that we actually get um
one so this is a way to control the growth of of activations inside the residual
stream in the forward pass and so we'd like to initialize in the same way where these weights that are at the end of
each block so this C uh layer uh the gbt paper proposes to scale down those
weights by one over the square root of the number of residual layers so one crude way to implement
this is the following I don't know if this is uh pyro sanctioned but it works for me is we'll do in the
initialization see that s that do special nanog GPT uh scale in it is one so we're
setting um kind of like a flag for this module there must be a better way in py torch right but I don't
know okay so we're basically attaching this flag and trying to make sure that it doesn't conflict with anything
previously and then when we come down here this STD should be 0.02 by default
but then if haat um module of this thing
then STD * equals
um copal is not guessing correctly uh so we want one over the square root of the number of layers so
um the number of residual layers here is twice times Salt out config layers and then
this times .5 so we want to scale down that standard deviation and this should
be um correct and Implement that I should clarify by the way that the two times number of layers comes from the
fact that every single one of our layers in the Transformer actually has two blocks that add to the ridal pathway
right we have the attention and then the MLP so that's where the two times comes from and the other thing to mention is
that uh what's slightly awkward but we're not going to fix it is that um
because we are weight sharing the wte and the LM head in this iteration of our
old subm modules we're going to actually come around to that tensor twice so we're going to first initialize it as an
embedding with 0.02 and then we're going to come back around it again in a linear and initialize it again using 0.02 and
it's going to be 0.02 because the LM head is of course not not scaled so it's not going to come here it's just it's
going to be basically initialized twice using the identical same initialization but that's okay and then scrolling over
here I added uh some code here so that we have reproducibility um to set the seeds and
now we should be able to python train gpt2 pi and let this running and as far
as I know this is the gpt2 initialization uh in the way we've implemented it right now so this
looks uh reasonable to me okay so at this point we have the gpt2 model we

å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## æ¨¡å‹åˆå§‹åŒ–ï¼šæ ‡å‡†å·® 0.02 å’Œæ®‹å·®åˆå§‹åŒ–

### 1. åˆå§‹åŒ–æƒé‡

æˆ‘ä»¬éœ€è¦æ›´å°å¿ƒåœ°åˆå§‹åŒ–æ¨¡å‹ï¼Œä»¥ä¾¿ä¸ GPT-2 çš„åˆå§‹åŒ–æ–¹æ³•ç›¸ä¸€è‡´ã€‚ä¸å¹¸çš„æ˜¯ï¼Œ**GPT-2 è®ºæ–‡** å’Œ **GPT-3 è®ºæ–‡** å¯¹åˆå§‹åŒ–çš„æè¿°å¹¶ä¸è¯¦ç»†ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä»ä»£ç ä¸­æ¨æµ‹å…·ä½“ç»†èŠ‚ã€‚é€šè¿‡æŸ¥çœ‹ OpenAI å‘å¸ƒçš„ä»£ç ï¼Œæˆ‘ä»¬å‘ç°ä»–ä»¬ä½¿ç”¨äº†ä»¥ä¸‹åˆå§‹åŒ–æ–¹æ³•ï¼š

* **æƒé‡åˆå§‹åŒ–**ï¼šæƒé‡ä½¿ç”¨ **æ ‡å‡†å·® 0.02** çš„æ­£æ€åˆ†å¸ƒè¿›è¡Œåˆå§‹åŒ–ã€‚
* **åç½®åˆå§‹åŒ–**ï¼šåç½®è¢«åˆå§‹åŒ–ä¸º **0**ã€‚
* **Token åµŒå…¥ï¼ˆWTEï¼‰**ï¼šToken åµŒå…¥ä½¿ç”¨ **æ ‡å‡†å·® 0.02** åˆå§‹åŒ–ã€‚
* **ä½ç½®åµŒå…¥**ï¼šä½ç½®åµŒå…¥åˆå§‹åŒ–ä¸º **æ ‡å‡†å·® 0.01**ï¼ˆè¿™ä¸ªå€¼ç•¥æœ‰ä¸åŒï¼Œä½†å·®å¼‚ä¸å¤§ï¼‰ã€‚

æˆ‘ä»¬å¸Œæœ›åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­æ¨¡æ‹Ÿè¿™äº›åˆå§‹åŒ–è®¾ç½®ï¼Œä»¥ç¡®ä¿ä¸ GPT-2 ä¸€è‡´ã€‚

### 2. å¦‚ä½•åˆå§‹åŒ–æ¨¡å‹

åœ¨ GPT æ¨¡å—çš„åˆå§‹åŒ–è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ `apply` å‡½æ•°ï¼Œå®ƒä¼šéå†æ‰€æœ‰å­æ¨¡å—å¹¶å¯¹å®ƒä»¬è¿›è¡Œåˆå§‹åŒ–ï¼š

* **çº¿æ€§å±‚ï¼ˆnn.Linearï¼‰**ï¼šå¯¹äºæ¯ä¸ªçº¿æ€§å±‚ï¼Œæˆ‘ä»¬ä½¿ç”¨ **æ ‡å‡†å·® 0.02** åˆå§‹åŒ–æƒé‡ã€‚å¦‚æœè¯¥å±‚æœ‰åç½®ï¼Œåˆ™åˆå§‹åŒ–ä¸º 0ã€‚
* **åµŒå…¥å±‚ï¼ˆEmbeddingï¼‰**ï¼šToken åµŒå…¥å±‚åˆå§‹åŒ–ä¸º **æ ‡å‡†å·® 0.02**ï¼Œä½ç½®åµŒå…¥å±‚åˆå§‹åŒ–ä¸º **æ ‡å‡†å·® 0.01**ã€‚
* **å±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰**ï¼šå±‚å½’ä¸€åŒ–çš„ **scale** è¢«åˆå§‹åŒ–ä¸º **1**ï¼Œ**offset** è¢«åˆå§‹åŒ–ä¸º **0**ã€‚

è¿™ç§åˆå§‹åŒ–æ–¹æ³•ç¬¦åˆ GPT-2 çš„å®ç°ã€‚

### 3. æ ‡å‡†å·®çš„é€‰æ‹©

é€šå¸¸ï¼Œæƒé‡åˆå§‹åŒ–ä½¿ç”¨ Xavier åˆå§‹åŒ–ï¼Œå³æƒé‡çš„æ ‡å‡†å·®åº”è¯¥æ˜¯è¾“å…¥ç‰¹å¾æ•°çš„å€’æ•°çš„å¹³æ–¹æ ¹ã€‚æ ¹æ® GPT-2 çš„æ¨¡å‹è®¾ç½®ï¼Œæ ‡å‡†å·® **0.02** è¿‘ä¼¼ç¬¦åˆè¿™ä¸€è¦æ±‚ã€‚ä¾‹å¦‚ï¼š

* å¯¹äºè¾“å…¥ç‰¹å¾æ•°ä¸º **768** çš„æƒ…å†µï¼Œæ ‡å‡†å·®åº”ä¸º **1 / sqrt(768) â‰ˆ 0.036**ï¼Œè€Œ 0.02 å¾ˆæ¥è¿‘è¿™ä¸ªå€¼ã€‚
* å¯¹äºæ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚ **1600**ï¼‰ï¼Œæ ‡å‡†å·® **0.02** ä¹Ÿç¬¦åˆé¢„æœŸã€‚

å› æ­¤ï¼Œæ ‡å‡†å·® 0.02 æ˜¯ä¸€ä¸ªåˆç†çš„åˆå§‹åŒ–å€¼ã€‚

### 4. æ®‹å·®è·¯å¾„çš„åˆå§‹åŒ–

GPT-2 è¿˜å¯¹æ®‹å·®è·¯å¾„çš„åˆå§‹åŒ–åšäº†å¤„ç†ã€‚åœ¨æ¯ä¸€å±‚çš„æ®‹å·®è·¯å¾„ä¸­ï¼Œéšç€ç½‘ç»œæ·±åº¦çš„å¢åŠ ï¼Œæ¿€æ´»å€¼çš„æ–¹å·®ä¼šé€æ¸å¢å¤§ã€‚ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼ŒGPT-2 åœ¨åˆå§‹åŒ–æ—¶ä½¿ç”¨äº†ä¸€ä¸ª **ç¼©æ”¾å› å­**ï¼Œé€šè¿‡ç¼©æ”¾æ¯ä¸ªæ®‹å·®å±‚çš„æƒé‡æ¥æ§åˆ¶æ¿€æ´»å€¼çš„å¢é•¿ã€‚

* **ç¼©æ”¾å› å­**ï¼šè¯¥å› å­ä¸º **1 / sqrt(n)**ï¼Œå…¶ä¸­ **n** æ˜¯æ®‹å·®å±‚çš„æ•°é‡ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿éšç€å±‚æ•°å¢åŠ ï¼Œæ¿€æ´»å€¼çš„å¢é•¿ä¸ä¼šå¤±æ§ã€‚

åœ¨å®ç°ä¸­ï¼Œæˆ‘ä»¬å°†æ¯ä¸€å±‚çš„æƒé‡æ ¹æ®å±‚æ•°è¿›è¡Œç¼©æ”¾ï¼Œç¡®ä¿æ¯ä¸ªæ®‹å·®è·¯å¾„çš„è´¡çŒ®ä¸ä¼šè¿‡å¤§ã€‚

### 5. æƒé‡å…±äº«

æˆ‘ä»¬è¿˜æ³¨æ„åˆ°ï¼Œ**`wte`ï¼ˆtoken embeddingï¼‰** å’Œ **`lm_head`ï¼ˆè¯­è¨€æ¨¡å‹å¤´ï¼‰** ä¹‹é—´çš„æƒé‡æ˜¯å…±äº«çš„ã€‚å³ **`wte`** ç”¨äºç”Ÿæˆ token åµŒå…¥ï¼Œè€Œ **`lm_head`** ç”¨äºå°† Transformer è¾“å‡ºæ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°ã€‚åœ¨ GPT-2 ä¸­ï¼Œè¿™ä¸¤ä¸ªéƒ¨åˆ†å…±äº«ç›¸åŒçš„æƒé‡ï¼Œè¿™æ ·å¯ä»¥å‡å°‘æ¨¡å‹çš„å‚æ•°æ•°é‡ï¼Œå¹¶ä¸”å¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒæ—¶æ›´æœ‰æ•ˆåœ°æ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚

* ç”±äºæˆ‘ä»¬ä½¿ç”¨äº† **æƒé‡å…±äº«**ï¼Œ`wte` å’Œ `lm_head` ä¼šæŒ‡å‘åŒä¸€å—å†…å­˜ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ç¡®ä¿åœ¨åˆå§‹åŒ–æ—¶æ­£ç¡®åœ°å¤„ç†è¿™ä¸¤ä¸ªå±‚çš„æƒé‡ã€‚

### 6. é‡å¤åˆå§‹åŒ–ä¸æ¨¡å‹å‚æ•°

ç”±äºæˆ‘ä»¬åœ¨è®­ç»ƒæ—¶å¯¹ `wte` å’Œ `lm_head` ä½¿ç”¨äº†ç›¸åŒçš„æƒé‡ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦ç¡®ä¿å®ƒä»¬åœ¨æ¨¡å‹çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­èƒ½æ­£ç¡®æ›´æ–°ã€‚åœ¨åˆå§‹åŒ–æ—¶ï¼Œæˆ‘ä»¬é¦–å…ˆåˆå§‹åŒ– `wte`ï¼Œç„¶åå°†å…¶æŒ‡å‘ `lm_head`ï¼Œç¡®ä¿è¿™ä¸¤ä¸ªå±‚çš„æƒé‡å…±äº«ã€‚

### 7. å‚æ•°èŠ‚çœä¸è®­ç»ƒæ•ˆç‡

ä½¿ç”¨æƒé‡å…±äº«çš„å¥½å¤„ä¹‹ä¸€æ˜¯èƒ½å¤ŸèŠ‚çœå¤§é‡å‚æ•°ã€‚`wte` å’Œ `lm_head` çš„æƒé‡å½¢çŠ¶æ˜¯ **50257 x 768**ï¼Œè¿™å ç”¨äº†çº¦ 40 ç™¾ä¸‡ä¸ªå‚æ•°ã€‚é€šè¿‡å…±äº«è¿™ä¸¤ä¸ªéƒ¨åˆ†çš„æƒé‡ï¼Œæˆ‘ä»¬å¯ä»¥èŠ‚çœå¤§é‡çš„å†…å­˜å’Œè®¡ç®—èµ„æºï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸éœ€è¦é‡å¤å­˜å‚¨è¿™éƒ¨åˆ†æƒé‡ã€‚

æ€»çš„æ¥è¯´ï¼Œè¿™ç§æƒé‡å…±äº«çš„æ–¹å¼ä¸ä»…å‡å°‘äº†æ¨¡å‹çš„å‚æ•°æ•°é‡ï¼Œè¿˜å¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æœ‰æ•ˆåœ°æ•æ‰è¯­ä¹‰ä¿¡æ¯ï¼Œå‡å°‘äº†è¿‡æ‹Ÿåˆçš„é£é™©ã€‚

### 8. å¯å¤ç°æ€§ä¸è°ƒè¯•

ä¸ºäº†ç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„å¯å¤ç°æ€§ï¼Œæˆ‘ä»¬åœ¨åˆå§‹åŒ–æ—¶è¿˜è®¾ç½®äº† **éšæœºç§å­**ï¼Œä¿è¯æ¯æ¬¡è®­ç»ƒçš„ç»“æœæ˜¯ä¸€è‡´çš„ã€‚è¿™æ ·æœ‰åŠ©äºæˆ‘ä»¬è°ƒè¯•æ¨¡å‹å¹¶è¿›è¡Œå¤šæ¬¡å®éªŒã€‚

---

### æ€»ç»“

* æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ **æ ‡å‡†å·® 0.02** åˆå§‹åŒ–æƒé‡ï¼Œå¹¶æŒ‰ç…§ GPT-2 çš„å®ç°æ–¹å¼åˆå§‹åŒ–æ¨¡å‹ï¼Œç¡®ä¿äº†æ¨¡å‹çš„åˆå§‹åŒ–ä¸ GPT-2 ä¸€è‡´ã€‚
* é€šè¿‡ **æƒé‡å…±äº«**ï¼ˆ`wte` å’Œ `lm_head`ï¼‰ï¼Œæˆ‘ä»¬èŠ‚çœäº†å¤§é‡çš„å‚æ•°ï¼Œå¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚
* é€šè¿‡ **æ®‹å·®è·¯å¾„çš„åˆå§‹åŒ–**ï¼Œæˆ‘ä»¬æ§åˆ¶äº†æ¿€æ´»å€¼çš„å¢é•¿ï¼Œé¿å…äº†æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸çš„é—®é¢˜ã€‚

è¿™ç§åˆå§‹åŒ–æ–¹æ³•ç¬¦åˆ GPT-2 çš„æºä»£ç ï¼Œå¸®åŠ©æˆ‘ä»¬æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

---

å¦‚æœä½ æœ‰æ›´å¤šå…³äºæ¨¡å‹åˆå§‹åŒ–æˆ–å…¶ä»–è®­ç»ƒç»†èŠ‚çš„é—®é¢˜ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼


# SECTION 2: Letâ€™s make it fast. GPUs, mixed precision, 1000ms

have some confidence that it's correctly implemented we've initialized it properly and we have a data loader that's iterating through data batches
and we can train so now comes the fun part I'd like us to speed up the training by a lot so we're getting our
money's worth with respect to the hardware that we are uh using here and uh we're going to speed up the training
by quite a bit uh now you always want to start with what Hardware do you have what does it offer and are you fully
utilizing it so in my case if we go to Nvidia SMI we can see
that I have eight gpus and each one of those gpus is an a100 sxm 80 gb so this
is the GPU that I have available to me in this box now when I look when I use
um to spin up these kinds of Boxes by the way my favorite place to go to is Lambda Labs um they do sponsor my
development and that of my projects uh but I this is my favorite place to go
and this is where you can spin up one of these machines and you pay per hour and it's very very simple so I like to spin them up and then
connect vsod to it and that's how I develop now when we look at the A1 100s that are available here a100 80 GB sxm
is the um GPU that I have here and we have a bunch of numbers here for um how
many calculations you can expect out of this GPU so when I come over here and I break in right after here so
python trity so I'm breaking in right after we calculate the loit and
laws and the interesting thing I'd like you to note is when I do lit. dtype this
prints a torch. FL 32 so by default iny torch when you create tensors um and
this is the case for all the activations and for the parameters of the network and so on by default everything is in float 32 that means that every single
number activation or weight and so on is using a float representation that has 32
bits and uh that's actually quite a bit of memory and it turns out empirically that for deep learning as a
computational workload this is way too much and deep learning and the training of these networks can tolerate
significantly lower precisions um not all computational workflows can tolerate small Precision so for example um if we
go back to to the data sheet you'll see that actually these gpus support up to fp64 and this is quite useful I
understand for a lot of um scientific Computing applications and there really need this uh but we don't need that much
Precision for deep learning training So currently we are here fp32 and with this code as it is right
now we expect to get at at most 19.5 Tera flops of performance that means
we're doing 19.5 trillion operations floating Point operations so this is floating Point multiply add most um most
likely and so these are the floating Point operations
uh now notice that if we are willing to go down in Precision so tf32 is a lower
Precision format we're going to see in a second you can actually get an 8X Improvement here and if you're willing
to go down to float 16 or B float 16 you can actually get time 16x performance
all the way to 312 Tera flops you see here that Nvidia likes to site numbers
that have an asterisk here this asterisk uh says with sparsity uh but we are not going to be using sparsity in R code and
I don't know that this is very widely used in the industry right now so most people look at this number here uh
without sparcity and you'll notice that we could have got even more here but this is int 8 and int 8 is used for
inference not for training uh because int 8 has a um it basically has um
uniform spacing um and uh we actually require a float so that we get a better match to
the uh normal distributions that occur during training of neural networks where
both activations and weights are distributed as a normal distribution and so uh floating points are really
important to to match that uh representation so we're not typically using int 8 uh for training but we are
using it for inference and if we bring down the Precision we can get a lot more Terra flops out of the tensor course
available in the gpus we'll talk about that in a second but in addition to that if all of these numbers have fewer bits
of representation it's going to be much easier to move them around and that's where we start to get into the memory
bandwidth and the memory of the model so not only do we have a finite capacity of the number of bits that our GPU can
store but in addition to that there's a speed with which you can access this memory um and you have a certain memory
bandwidth it's a very precious resource and in fact many of the deep learning uh
work workloads for training are memory bound and what that means is actually that the tensor cores that do all these
extremely fast multiplications most of the time they're waiting around they're idle um because we can't feed them with
data fast enough we can't load the data fast enough from memory so typical utilizations of your Hardware if you're
getting 60% uh utilization you're actually doing extremely well um so half
of the time in a well-tuned application your tensor cores are not doing multiplies because the data is not
available so the memory bandwidth here is extremely important as well and if we come down in the Precision for all the
floats all the numbers weights and activations suddenly require less memory so we can store more and we can access
it faster so everything speeds up and it's amazing and now let's reap the benefits of it um and let's first look
at the tensor float 32 format okay so first of all what are tensor cores well tensor course tensor

å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## è®©è®­ç»ƒæ›´å¿«ï¼šGPUã€æ··åˆç²¾åº¦å’Œ 1000ms

### 1. ç¡®ä¿è®­ç»ƒæ­£ç¡®å¹¶å‡†å¤‡åŠ é€Ÿ

åœ¨ç¡®ä¿æ¨¡å‹æ­£ç¡®å®ç°å’Œæ•°æ®åŠ è½½å™¨æ­£å¸¸å·¥ä½œçš„åŸºç¡€ä¸Šï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬è¦åšçš„å°±æ˜¯å¤§å¹…åº¦åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œä»¥ä¾¿å……åˆ†åˆ©ç”¨ç¡¬ä»¶çš„æ€§èƒ½ï¼Œç¡®ä¿è®­ç»ƒæ•ˆç‡ã€‚

é¦–å…ˆï¼Œæ£€æŸ¥ç¡¬ä»¶é…ç½®æ˜¯éå¸¸é‡è¦çš„ã€‚æˆ‘çš„ç¡¬ä»¶é…ç½®å¦‚ä¸‹ï¼š

* æˆ‘æœ‰ **8 ä¸ª NVIDIA A100 GPU**ï¼Œæ¯ä¸ª GPU æ‹¥æœ‰ **80GB å†…å­˜**ã€‚è¿™æ˜¯éå¸¸å¼ºå¤§çš„ç¡¬ä»¶ï¼Œå¯ä»¥æä¾›æé«˜çš„è®¡ç®—æ€§èƒ½ã€‚

### 2. ä½¿ç”¨ Lambda Labs

æˆ‘å–œæ¬¢ä½¿ç”¨ **Lambda Labs** æä¾›çš„äº‘è®¡ç®—æœåŠ¡ï¼Œå®ƒå…è®¸æˆ‘æŒ‰å°æ—¶ç§Ÿç”¨å¸¦æœ‰ A100 GPU çš„æœºå™¨ï¼Œéå¸¸é€‚åˆæ·±åº¦å­¦ä¹ å¼€å‘ã€‚é€šè¿‡ Lambda Labsï¼Œæˆ‘èƒ½å¤Ÿè½»æ¾è¿æ¥åˆ°è¿™äº›æœºå™¨ï¼Œå¹¶ä½¿ç”¨ VSCode å¼€å‘ç¯å¢ƒè¿›è¡Œå·¥ä½œã€‚

### 3. é»˜è®¤çš„ç²¾åº¦å’Œæµ®ç‚¹è®¡ç®—

å½“æˆ‘è¿è¡Œè®­ç»ƒæ—¶ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰çš„å¼ é‡ï¼ˆå¦‚æ¿€æ´»å€¼å’Œæƒé‡ï¼‰éƒ½ä½¿ç”¨ **FP32ï¼ˆ32 ä½æµ®ç‚¹ï¼‰** ç²¾åº¦ã€‚è¿™æ„å‘³ç€æ¯ä¸ªæ•°å­—éƒ½ä½¿ç”¨ 32 ä½è¡¨ç¤ºï¼Œè¿™åœ¨æ·±åº¦å­¦ä¹ ä¸­éå¸¸å¸¸è§ã€‚å°½ç®¡è¿™ç§ç²¾åº¦å¯ä»¥ä¿è¯è®¡ç®—çš„å‡†ç¡®æ€§ï¼Œä½†å®ƒä¼šæ¶ˆè€—å¤§é‡å†…å­˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§å‹æ¨¡å‹æ—¶ã€‚

### 4. é™ä½ç²¾åº¦æé«˜æ€§èƒ½

å¯¹äºæ·±åº¦å­¦ä¹ æ¥è¯´ï¼Œå¹¶ä¸éœ€è¦éå¸¸é«˜çš„ç²¾åº¦æ¥è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡é™ä½ç²¾åº¦ï¼Œå¯ä»¥æ˜¾è‘—æå‡è®¡ç®—æ€§èƒ½ï¼Œå¹¶å‡å°‘å†…å­˜æ¶ˆè€—ã€‚NVIDIA çš„ A100 GPU æ”¯æŒæ›´ä½ç²¾åº¦çš„è®¡ç®—ï¼Œå¦‚ **TF32** å’Œ **FP16**ï¼Œå¯ä»¥å¸¦æ¥å·¨å¤§çš„æ€§èƒ½æå‡ï¼š

* **TF32**ï¼šåœ¨ç²¾åº¦é™ä½çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½å¯ä»¥æå‡ **8 å€**ï¼Œè¾¾åˆ° 160TeraFlopsã€‚
* **FP16ï¼ˆåŠç²¾åº¦æµ®ç‚¹ï¼‰** æˆ– **Bfloat16**ï¼šå°†æ€§èƒ½æå‡ **16 å€**ï¼Œè¾¾åˆ° 312TeraFlopsã€‚

é€šè¿‡é™ä½ç²¾åº¦ï¼Œæˆ‘ä»¬ä¸ä»…èƒ½åŠ é€Ÿè®¡ç®—ï¼Œè¿˜èƒ½æœ‰æ•ˆèŠ‚çœå†…å­˜å¸¦å®½ï¼ˆmemory bandwidthï¼‰å’Œå­˜å‚¨ç©ºé—´ã€‚æ·±åº¦å­¦ä¹ ä¸­çš„è®¡ç®—ä»»åŠ¡å¾€å¾€æ˜¯ **å†…å­˜å¸¦å®½é™åˆ¶** çš„ï¼Œå³å¤§éƒ¨åˆ†æ—¶é—´ï¼Œè®¡ç®—æ ¸å¿ƒï¼ˆtensor coresï¼‰å¹¶æ²¡æœ‰è¢«å……åˆ†åˆ©ç”¨ï¼Œå› ä¸ºæ•°æ®åŠ è½½çš„é€Ÿåº¦è·Ÿä¸ä¸Šè®¡ç®—é€Ÿåº¦ã€‚

### 5. ç²¾åº¦ä¸å†…å­˜å¸¦å®½

å†…å­˜å¸¦å®½æ˜¯éå¸¸å®è´µçš„èµ„æºï¼Œå¾ˆå¤šæ·±åº¦å­¦ä¹ å·¥ä½œè´Ÿè½½çš„è®­ç»ƒè¿‡ç¨‹å—åˆ°å†…å­˜å¸¦å®½çš„é™åˆ¶ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬éœ€è¦å‡å°‘æ¯ä¸ªæ•°æ®å…ƒç´ çš„å­˜å‚¨ç©ºé—´å’Œè®¡ç®—é‡ï¼Œè¿™æ ·æ‰èƒ½æ›´å¿«åœ°ä»å†…å­˜ä¸­åŠ è½½æ•°æ®å¹¶è¿›è¡Œè®¡ç®—ã€‚é™ä½ç²¾åº¦èƒ½å¤Ÿæ˜¾è‘—å‡å°‘æ¯ä¸ªå¼ é‡å ç”¨çš„å†…å­˜ç©ºé—´ï¼Œè¿›è€Œæé«˜å†…å­˜å¸¦å®½çš„åˆ©ç”¨ç‡ã€‚

### 6. Tensor Cores

**Tensor Cores** æ˜¯ä¸“é—¨ç”¨äºé«˜æ•ˆçŸ©é˜µè¿ç®—çš„ç¡¬ä»¶å•å…ƒï¼Œå°¤å…¶åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼ŒçŸ©é˜µä¹˜æ³•æ˜¯æœ€å¸¸è§çš„æ“ä½œã€‚Tensor Cores åœ¨å¤„ç†ä½ç²¾åº¦æµ®ç‚¹è®¡ç®—æ—¶éå¸¸é«˜æ•ˆï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ä½ç²¾åº¦æ ¼å¼ï¼ˆå¦‚ **FP16** æˆ– **TF32**ï¼‰æ¥åŠ é€Ÿè®¡ç®—ã€‚

### æ€»ç»“

* æˆ‘ä»¬é€šè¿‡ **é™ä½ç²¾åº¦**ï¼ˆå¦‚ä½¿ç”¨ **FP16** æˆ– **TF32**ï¼‰ï¼Œæ˜¾è‘—æé«˜äº† **GPU** çš„è®¡ç®—æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†å†…å­˜çš„ä½¿ç”¨ã€‚
* **Tensor Cores** çš„é«˜æ•ˆæ€§ä½¿å¾—åœ¨ä½ç²¾åº¦ä¸‹è¿›è¡Œå¤§è§„æ¨¡è®¡ç®—å˜å¾—æ›´åŠ é«˜æ•ˆã€‚
* é™ä½ç²¾åº¦ä¸ä»…èƒ½æå‡è®¡ç®—é€Ÿåº¦ï¼Œè¿˜èƒ½æ”¹å–„å†…å­˜å¸¦å®½çš„ä½¿ç”¨ï¼Œé¿å…å†…å­˜ç“¶é¢ˆã€‚

é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥æ›´å……åˆ†åœ°åˆ©ç”¨ç¡¬ä»¶èµ„æºï¼Œæ˜¾è‘—åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚

---

å¦‚æœä½ éœ€è¦æ›´å¤šå…³äºæ··åˆç²¾åº¦è®­ç»ƒã€Tensor Cores æˆ–å…¶ä»–ç¡¬ä»¶ä¼˜åŒ–çš„ç»†èŠ‚ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼

# ä»‹ç»ä¸€ä¸‹ FP32 TF32 FP16

å¥½é—®é¢˜ ğŸ‘ è¿™ä¸‰ä¸ªéƒ½æ˜¯ **æ•°å€¼è¡¨ç¤ºæ ¼å¼**ï¼ˆæµ®ç‚¹æ•°ç²¾åº¦æ ¼å¼ï¼‰ï¼Œä¸»è¦ç”¨åœ¨ **GPU åŠ é€Ÿæ·±åº¦å­¦ä¹ **é‡Œã€‚
æˆ‘ç»™ä½ é€ä¸ªä»‹ç»ï¼Œå¹¶å¯¹æ¯”å®ƒä»¬çš„å·®å¼‚ï¼š

---

## 1. FP32 ï¼ˆå•ç²¾åº¦æµ®ç‚¹æ•°ï¼‰

* **å…¨ç§°**ï¼šIEEE 754 **single precision float**
* **ä½å®½**ï¼š32 ä½

  * ç¬¦å·ä½ï¼š1 bit
  * é˜¶ç  (exponent)ï¼š8 bit
  * å°¾æ•° (mantissa/significand)ï¼š23 bit
* **æ•°å€¼èŒƒå›´**ï¼šå¤§çº¦ $10^{\pm 38}$
* **æœ‰æ•ˆç²¾åº¦**ï¼šçº¦ 7 ä½åè¿›åˆ¶æ•°å­—

ğŸ‘‰ è¿™æ˜¯æ·±åº¦å­¦ä¹ é‡Œä¼ ç»Ÿçš„é»˜è®¤ç²¾åº¦ï¼ˆPyTorchã€TensorFlow æ—©æœŸè®­ç»ƒé»˜è®¤ FP32ï¼‰ã€‚

---

## 2. FP16 ï¼ˆåŠç²¾åº¦æµ®ç‚¹æ•°ï¼‰

* **å…¨ç§°**ï¼šIEEE 754 **half precision float**
* **ä½å®½**ï¼š16 ä½

  * ç¬¦å·ä½ï¼š1 bit
  * é˜¶ç ï¼š5 bit
  * å°¾æ•°ï¼š10 bit
* **æ•°å€¼èŒƒå›´**ï¼šå¤§çº¦ $10^{\pm 5}$
* **æœ‰æ•ˆç²¾åº¦**ï¼šçº¦ 3 ä½åè¿›åˆ¶æ•°å­—

ğŸ‘‰ **ä¼˜ç‚¹**

* æ˜¾å­˜å ç”¨å‡åŠï¼ˆåŒæ ·å¤§å°çš„ batch æ›´å¤§ï¼‰
* è®¡ç®—ååé‡æ›´é«˜ï¼ˆç‰¹åˆ«æ˜¯åœ¨æ”¯æŒ Tensor Cores çš„ GPU ä¸Šï¼Œé€Ÿåº¦æ¯” FP32 å¿«å¾ˆå¤šï¼‰

ğŸ‘‰ **ç¼ºç‚¹**

* è¡¨ç¤ºèŒƒå›´å°ï¼Œå®¹æ˜“ **æº¢å‡º / ä¸‹æº¢**
* ç²¾åº¦ä½ï¼Œå¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®šï¼ˆloss NaNï¼‰
* é€šå¸¸ä¼šç»“åˆ **æ··åˆç²¾åº¦è®­ç»ƒ (AMP, Automatic Mixed Precision)** æ¥ç¼“è§£

---

## 3. TF32 ï¼ˆTensorFloat-32ï¼‰

* **ç”± NVIDIA Ampere æ¶æ„ (A100, RTX30xx) å¼•å…¥**
* **ä½å®½**ï¼šè¿˜æ˜¯ 32 ä½ï¼ˆæ‰€ä»¥å­˜å‚¨å¤§å°å’Œ FP32 ä¸€æ ·ï¼‰ï¼Œä½†æ˜¯è®¡ç®—æ—¶â€œæ··åˆäº† FP32 å’Œ FP16 çš„ç‰¹ç‚¹â€ï¼š

  * ç¬¦å·ä½ï¼š1 bit
  * é˜¶ç ï¼š8 bitï¼ˆå’Œ FP32 ä¸€æ · â†’ ä¿æŒå¤§èŒƒå›´è¡¨ç¤ºèƒ½åŠ›ï¼‰
  * å°¾æ•°ï¼š10 bitï¼ˆå’Œ FP16 ä¸€æ · â†’ ç²¾åº¦è¾ƒä½ï¼‰
* **æœ‰æ•ˆç²¾åº¦**ï¼šçº¦ 3 ä½åè¿›åˆ¶æ•°å­—ï¼ˆå’Œ FP16 ç±»ä¼¼ï¼‰ï¼Œä½†èŒƒå›´è·Ÿ FP32 ä¸€æ ·å¤§

ğŸ‘‰ **è®¾è®¡ç›®æ ‡**

* é’ˆå¯¹æ·±åº¦å­¦ä¹ çŸ©é˜µä¹˜æ³•ä¼˜åŒ–ï¼š

  * è¾“å…¥ â†’ ç”¨ TF32 è¿›è¡Œè®¡ç®—ï¼ˆå¿«ã€å¤Ÿç”¨ï¼‰
  * ç´¯åŠ  â†’ ä¿æŒ FP32 ç²¾åº¦ï¼ˆé¿å…ä¸¥é‡è¯¯å·®ï¼‰

ğŸ‘‰ **æ•ˆæœ**

* å‡ ä¹ä¸éœ€è¦æ”¹ä»£ç ï¼Œå°±èƒ½åœ¨ **A100ã€3090** ç­‰ GPU ä¸Šå¾—åˆ° **æ¯” FP32 å¿« 2\~8 å€çš„è®­ç»ƒé€Ÿåº¦**
* æ•°å€¼ç¨³å®šæ€§å’Œ FP32 æ¥è¿‘ï¼Œå¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¨¡å‹æ”¶æ•›ç»“æœå‡ ä¹ä¸€è‡´

---

## 4. å¯¹æ¯”æ€»ç»“è¡¨

| æ ¼å¼       | ä½å®½  | ç²¾åº¦(æœ‰æ•ˆå°æ•°ä½) | æ•°å€¼èŒƒå›´  | å†…å­˜å ç”¨      | é€Ÿåº¦ (Tensor Core) | ç”¨é€”                     |
| -------- | --- | --------- | ----- | --------- | ---------------- | ---------------------- |
| **FP32** | 32ä½ | \~7 ä½     | 1eÂ±38 | é«˜         | æ…¢                | ä¼ ç»Ÿæ·±åº¦å­¦ä¹ é»˜è®¤               |
| **FP16** | 16ä½ | \~3 ä½     | 1eÂ±5  | ä½ (çœä¸€åŠæ˜¾å­˜) | å¾ˆå¿«               | æ··åˆç²¾åº¦è®­ç»ƒï¼Œæ¨ç†              |
| **TF32** | 32ä½ | \~3 ä½     | 1eÂ±38 | é«˜         | æ¥è¿‘ FP16          | NVIDIA A100/RTX30 è®­ç»ƒé»˜è®¤ |

---

## âœ… æ€»ç»“

* **FP32**ï¼šæœ€ç¨³ï¼Œæœ€æ ‡å‡†ï¼Œä½†é€Ÿåº¦å’Œæ˜¾å­˜å¼€é”€å¤§ã€‚
* **FP16**ï¼šå¿«ã€çœæ˜¾å­˜ï¼Œä½†æ•°å€¼èŒƒå›´å°ï¼Œéœ€è¦ AMP æŠ€æœ¯ã€‚
* **TF32**ï¼šå…¼é¡¾ FP32 çš„èŒƒå›´å’Œ FP16 çš„é€Ÿåº¦ï¼Œåœ¨æ–° GPU ä¸Šå‡ ä¹æ›¿ä»£ FP32 åšè®­ç»ƒã€‚

---

è¦ä¸è¦æˆ‘å†å¸®ä½ è®²ä¸€ä¸‹ **AMPï¼ˆè‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼‰é‡Œ FP16 + FP32 æ˜¯æ€ä¹ˆé…åˆå·¥ä½œçš„**ï¼Ÿ

```
torch.set_float32_matmul_precision('high')
```

å¥½é—®é¢˜ ğŸ‘ ä½ é—®çš„è¿™å¥ï¼š

```python
torch.set_float32_matmul_precision('high')
```

æ˜¯ **PyTorch 1.12+** æ–°å¢çš„æ¥å£ï¼Œç”¨æ¥æ§åˆ¶ **çŸ©é˜µä¹˜æ³• (matmul, GEMM)** æ—¶ï¼Œ
åœ¨ **æ”¯æŒ TensorFloat-32 (TF32)** çš„ GPUï¼ˆNVIDIA Ampere æ¶æ„ A100 / RTX30xx åŠæ›´æ–°ï¼‰ä¸Šï¼Œ
PyTorch åˆ°åº•ç”¨ **FP32 è®¡ç®—** è¿˜æ˜¯ **TF32 è®¡ç®—**ã€‚

---

## 1. èƒŒæ™¯ï¼šFP32 vs TF32

* **FP32**ï¼šç²¾åº¦é«˜ï¼ˆ7 ä½æœ‰æ•ˆæ•°å­—ï¼‰ï¼Œä½†æ˜¯é€Ÿåº¦æ…¢ã€‚
* **TF32**ï¼šæŒ‡æ•°èŒƒå›´å’Œ FP32 ä¸€æ ·ï¼Œä½†å°¾æ•°ç²¾åº¦åªæœ‰ 10bitï¼ˆâ‰ˆ FP16ï¼‰ï¼Œæ‰€ä»¥æœ‰æ•ˆæ•°å­— \~3 ä½ï¼›
  â†’ **é€Ÿåº¦å¿«å¾ˆå¤šï¼ˆåˆ©ç”¨ Tensor Coresï¼‰**ï¼Œç²¾åº¦å¤Ÿå¤§å¤šæ•°æ·±åº¦å­¦ä¹ ä»»åŠ¡ç”¨ã€‚

åœ¨ Ampere GPU ä¸Šï¼Œé»˜è®¤ PyTorch ä¼š **è‡ªåŠ¨æŠŠ FP32 çŸ©é˜µä¹˜æ³•ç”¨ TF32 æ‰§è¡Œ**ï¼ˆé™¤éå…³é—­ï¼‰ã€‚

---

## 2. è¿™ä¸ªå‡½æ•°çš„ä½œç”¨

```python
torch.set_float32_matmul_precision(precision)
```

å…¶ä¸­ `precision` å¯é€‰å€¼æœ‰ï¼š

* `'high'`  â†’ **å¼ºåˆ¶ FP32 ç²¾åº¦**ï¼ˆä¸å¼€ TF32ï¼ŒåŠ é€Ÿæ•ˆæœå·®ï¼Œä½†ç»“æœå’Œè€ç‰ˆæœ¬å®Œå…¨ä¸€è‡´ï¼‰
* `'medium'` â†’ **é»˜è®¤**ï¼ˆç”¨ TF32 åŠ é€Ÿï¼Œä½†ä¿æŒç´¯åŠ åœ¨ FP32ï¼Œä¸å®¹æ˜“å´©ç²¾åº¦ï¼‰
* `'highest'` â†’ å¼ºåˆ¶æœ€ç²¾ç¡®å®ç°ï¼ˆå‡ ä¹ä¸ç”¨ TF32/å…¶ä»–è¿‘ä¼¼ï¼Œæœ€æ…¢ï¼Œä¸»è¦ç”¨åœ¨éœ€è¦ä¸¥æ ¼æ•°å€¼ä¸€è‡´æ€§çš„åœºæ™¯ï¼Œæ¯”å¦‚ç§‘å­¦è®¡ç®—ï¼‰

---

## 3. ç¤ºä¾‹

```python
import torch

# é»˜è®¤ medium
torch.set_float32_matmul_precision('medium')

# æ”¹æˆ highï¼Œå¼ºåˆ¶ FP32 ç²¾åº¦
torch.set_float32_matmul_precision('high')

# æ”¹æˆæœ€é«˜ç²¾åº¦ï¼ˆæœ€æ…¢ï¼‰
torch.set_float32_matmul_precision('highest')
```

---

## 4. å®é™…æ•ˆæœ

* å¦‚æœä½ ç”¨çš„æ˜¯ A100 / RTX 3090 / RTX 4090 ä¹‹ç±»çš„æ˜¾å¡ï¼š

  * `'medium'`ï¼šè®­ç»ƒé€Ÿåº¦æœ€å¿«ï¼ˆç”¨ TF32ï¼‰ï¼Œç²¾åº¦å¯¹æ·±åº¦å­¦ä¹ è¶³å¤Ÿã€‚
  * `'high'`ï¼šè®­ç»ƒé€Ÿåº¦ä¸‹é™ï¼ˆFP32ï¼‰ï¼Œç»“æœå’Œä¼ ç»Ÿ FP32 å®Œå…¨ä¸€è‡´ã€‚
  * `'highest'`ï¼šæœ€æ…¢ï¼Œä¸€èˆ¬ä¸ç”¨ï¼Œé™¤éåšç§‘å­¦ä»¿çœŸã€‚
* å¦‚æœä½ ç”¨çš„æ˜¯æ—§ GPUï¼ˆä¸æ”¯æŒ TF32ï¼‰ï¼Œè¿™ä¸ªè®¾ç½®æ²¡å•¥å½±å“ã€‚

---

## âœ… æ€»ç»“

```python
torch.set_float32_matmul_precision('high')
```

çš„æ„æ€æ˜¯ï¼š**åœ¨ FP32 çŸ©é˜µä¹˜æ³•æ—¶ï¼Œç¦ç”¨ TF32 åŠ é€Ÿï¼Œå¼ºåˆ¶ç”¨çœŸæ­£çš„ FP32 ç²¾åº¦è®¡ç®—**ã€‚

* `'high'`ï¼šæ›´ç²¾ç¡®ï¼ˆå…¼å®¹æ—§ç»“æœï¼‰ï¼Œé€Ÿåº¦è¾ƒæ…¢
* `'medium'`ï¼šé»˜è®¤å€¼ï¼Œå¼€å¯ TF32ï¼Œé€Ÿåº¦å¿«ï¼Œç²¾åº¦è¶³å¤Ÿ
* `'highest'`ï¼šæœ€ç²¾ç¡®ï¼ˆå‡ ä¹ä¸ç”¨è¿‘ä¼¼ï¼‰ï¼Œæœ€æ…¢

---

è¦ä¸è¦æˆ‘ç»™ä½ å¯¹æ¯”ä¸€ä¸‹ **è®­ç»ƒ GPT-2 ç”¨ TF32 vs FP32 çš„é€Ÿåº¦å·®å¼‚å’Œç²¾åº¦å½±å“**ï¼Ÿ

```
import code; code.interact(local=locals())
```

å¥½é—®é¢˜ ğŸ‘
è¿™ä¸€å¥ï¼š

```python
import code; code.interact(local=locals())
```

å…¶å®å°±æ˜¯åœ¨ Python ç¨‹åºé‡Œ **å¯åŠ¨ä¸€ä¸ªäº¤äº’å¼ REPLï¼ˆç±»ä¼¼ `python` å‘½ä»¤è¡Œç¯å¢ƒï¼‰**ï¼Œå¹¶ä¸”æŠŠå½“å‰çš„å±€éƒ¨å˜é‡ç¯å¢ƒä¼ è¿›å»ã€‚

---

## 1. `code.interact` æ˜¯ä»€ä¹ˆï¼Ÿ

* Python æ ‡å‡†åº“é‡Œæœ‰ä¸ª [`code`](https://docs.python.org/3/library/code.html) æ¨¡å—ï¼Œæä¾›äº†ä¸€ä¸ªç®€å•çš„äº¤äº’å¼è§£é‡Šå™¨æ¥å£ã€‚
* `code.interact()` ä¼šæ‰“å¼€ä¸€ä¸ªäº¤äº’å¼ shellï¼Œä½ å¯ä»¥åœ¨é‡Œé¢è¾“å…¥ Python è¯­å¥ï¼Œå°±åƒåœ¨ç»ˆç«¯è¿è¡Œ `python` ä¸€æ ·ã€‚

---

## 2. `local=locals()` çš„ä½œç”¨

* `locals()` ä¼šè¿”å›å½“å‰ä½œç”¨åŸŸä¸‹çš„å±€éƒ¨å˜é‡å­—å…¸ã€‚
* æŠŠå®ƒä¼ ç»™ `code.interact(local=locals())`ï¼Œå°±èƒ½åœ¨äº¤äº’ç¯å¢ƒé‡Œè®¿é—®åˆ°ä½ ç¨‹åºå½“å‰çš„å˜é‡ã€‚

æ¯”å¦‚ï¼š

```python
a = 10
b = 20

import code
code.interact(local=locals())
```

è¿è¡Œåä¼šè¿›å…¥äº¤äº’æ¨¡å¼ï¼Œä½ å¯ä»¥ç›´æ¥æ“ä½œå˜é‡ `a` å’Œ `b`ï¼š

```
Python 3.11.9 (main, May  5 2024, 00:00:00) 
[GCC 11.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
(InteractiveConsole)
>>> a
10
>>> b
20
>>> a + b
30
```

ç­‰äºæ˜¯æŠŠè¿è¡Œç¯å¢ƒâ€œå†»ç»“â€ï¼Œè®©ä½ åœ¨é‚£ä¸€åˆ»æ‰“å¼€ä¸€ä¸ª Python æ§åˆ¶å°è°ƒè¯•ã€‚

---

## 3. å…¸å‹ç”¨é€”

* **è°ƒè¯•**ï¼šåœ¨æŸä¸ªåœ°æ–¹æ’ä¸€ä¸ªäº¤äº’å¼ shellï¼Œæ£€æŸ¥å˜é‡å€¼ã€‚
* **æ¢ç´¢æ¨¡å‹/æ•°æ®**ï¼šæ¯”å¦‚åœ¨è®­ç»ƒå¾ªç¯ä¸­é—´æ’å…¥ `code.interact`ï¼Œéšæ—¶çœ‹æ¨¡å‹æƒé‡ã€æ¢¯åº¦ã€‚
* **ä¸´æ—¶æ›¿ä»£ `pdb.set_trace()`**ï¼Œä½†æä¾›çš„æ˜¯ REPL è€Œä¸æ˜¯å•æ­¥è°ƒè¯•å™¨ã€‚

---

## 4. å’Œ `pdb.set_trace()` çš„åŒºåˆ«

* `pdb.set_trace()` â†’ è¿›å…¥è°ƒè¯•æ¨¡å¼ï¼Œå¯ä»¥å•æ­¥æ‰§è¡Œã€æŸ¥çœ‹å †æ ˆã€‚
* `code.interact(local=locals())` â†’ è¿›å…¥äº¤äº’æ¨¡å¼ï¼Œåªèƒ½è¿è¡Œ Python è¯­å¥ï¼Œä¸èƒ½å•æ­¥èµ°ä»£ç ã€‚

---

âœ… **æ€»ç»“**
`import code; code.interact(local=locals())` ä¼šå¯åŠ¨ä¸€ä¸ª Python REPLï¼Œå¹¶å¸¦ä¸Šå½“å‰ä½œç”¨åŸŸé‡Œçš„å˜é‡ï¼Œæ–¹ä¾¿ä½ éšæ—¶æŸ¥çœ‹å’Œæ“ä½œã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ å†™ä¸€ä¸ª **åœ¨è®­ç»ƒ GPT æ—¶ç”¨ `code.interact` ä¸´æ—¶æ£€æŸ¥ logits/æ¢¯åº¦çš„ä¾‹å­**ï¼Ÿ

# ä»‹ç»ä¸€ä¸‹ TFLOPS

å¥½é—®é¢˜ ğŸ‘

---

# ä»€ä¹ˆæ˜¯ **TFLOPS**

**TFLOPS** å…¨ç§° **Tera Floating Point Operations Per Second**ï¼Œæ„æ€æ˜¯ï¼š
ğŸ‘‰ **æ¯ç§’é’Ÿèƒ½è¿›è¡Œä¸‡äº¿æ¬¡ï¼ˆ10Â¹Â²ï¼‰æµ®ç‚¹è¿ç®—** çš„èƒ½åŠ›ã€‚

* **FLOPS**ï¼šFloating Point Operations Per Secondï¼Œæµ®ç‚¹è¿ç®—æ¬¡æ•°/ç§’ã€‚
* **T**ï¼šTeraï¼Œä»£è¡¨ 10Â¹Â²ï¼ˆ1 ä¸‡äº¿ï¼‰ã€‚
* æ‰€ä»¥ **1 TFLOPS = 1 Ã— 10Â¹Â² æ¬¡æµ®ç‚¹è¿ç®—/ç§’**ã€‚

---

# 1. ä¸ºä»€ä¹ˆè¦çœ‹ FLOPSï¼Ÿ

FLOPS æ˜¯è¡¡é‡ **è®¡ç®—æœº/æ˜¾å¡/AI èŠ¯ç‰‡** å¤„ç†èƒ½åŠ›çš„å¸¸è§æŒ‡æ ‡ï¼Œå°¤å…¶åœ¨ **ç§‘å­¦è®¡ç®—**ã€**æ·±åº¦å­¦ä¹ è®­ç»ƒ**ã€**3D å›¾å½¢æ¸²æŸ“** ä¸­éå¸¸é‡è¦ï¼Œå› ä¸ºè¿™äº›ä»»åŠ¡å¤§é‡ä¾èµ–æµ®ç‚¹æ•°è®¡ç®—ã€‚

æ¯”å¦‚ï¼š

* AI è®­ç»ƒä¸­çš„çŸ©é˜µä¹˜æ³•ã€å·ç§¯è¿ç®—ã€‚
* ç§‘å­¦æ¨¡æ‹Ÿä¸­çš„ç‰©ç†/åŒ–å­¦æ–¹ç¨‹è®¡ç®—ã€‚
* å›¾å½¢æ¸²æŸ“ä¸­çš„å…‰ç…§ã€ç‰©ç†æ¨¡æ‹Ÿã€‚

---

# 2. å¦‚ä½•è®¡ç®—ç†è®º TFLOPSï¼ˆä»¥ GPU ä¸ºä¾‹ï¼‰

è®¡ç®—å…¬å¼å¸¸è§å†™æ³•ï¼š

$$
\text{TFLOPS} = \frac{\text{CUDA cores} \times \text{æ—¶é’Ÿé¢‘ç‡ (GHz)} \times \text{æ¯å‘¨æœŸFLOPæ•°}}{10^{12}}
$$

ä¸¾ä¸ªä¾‹å­ï¼š
å‡è®¾æŸ GPU æœ‰ **5000 ä¸ª CUDA Core**ï¼Œé¢‘ç‡ **1.5 GHz**ï¼Œæ¯ä¸ªæ ¸å¿ƒ **æ¯å‘¨æœŸæ‰§è¡Œ 2 æ¬¡æµ®ç‚¹è¿ç®— (FMA)**ï¼š

$$
\text{FLOPS} = 5000 \times 1.5 \times 10^9 \times 2 = 15 \times 10^{12} = 15 \, \text{TFLOPS}
$$

---

# 3. ç†è®º TFLOPS vs å®é™…æ€§èƒ½

* **ç†è®º TFLOPS**ï¼šé€šè¿‡ç¡¬ä»¶è§„æ ¼ç®—å‡ºæ¥çš„å³°å€¼ï¼ˆå‚å®¶å®£ä¼ çš„æ•°å­—ï¼‰ã€‚
* **å®é™… TFLOPS**ï¼šå–å†³äºä»»åŠ¡ã€å†…å­˜å¸¦å®½ã€æŒ‡ä»¤è°ƒåº¦ç­‰ï¼Œé€šå¸¸è¾¾ä¸åˆ°ç†è®ºå€¼ã€‚

  > æ¯”å¦‚ GPU å†…å­˜ä¸è¶³æˆ– I/O ç“¶é¢ˆï¼ŒTFLOPS å¯èƒ½åªè·‘åˆ° 30â€“60%ã€‚

---

# 4. å•ä½å¯¹æ¯”

* **GFLOPS**ï¼šGiga FLOPS = 10â¹ æ¬¡è¿ç®—/ç§’ï¼ˆåäº¿ï¼‰ã€‚
* **TFLOPS**ï¼šTera FLOPS = 10Â¹Â² æ¬¡è¿ç®—/ç§’ï¼ˆä¸‡äº¿ï¼‰ã€‚
* **PFLOPS**ï¼šPeta FLOPS = 10Â¹âµ æ¬¡è¿ç®—/ç§’ï¼ˆåƒä¸‡äº¿ï¼Œè¶…ç®—å¸¸ç”¨ï¼‰ã€‚
* **EFLOPS**ï¼šExa FLOPS = 10Â¹â¸ æ¬¡è¿ç®—/ç§’ï¼ˆç™¾äº¿äº¿ï¼Œç›®å‰é¡¶çº§è¶…ç®—ç›®æ ‡ï¼‰ã€‚

---

# 5. ä¸¾ä¾‹

* **NVIDIA RTX 3090**ï¼šç†è®º FP32 ç®—åŠ› â‰ˆ **35.6 TFLOPS**ã€‚
* **NVIDIA A100 (80GB)**ï¼šç†è®º FP32 â‰ˆ **19.5 TFLOPS**ï¼ŒTensor Core ç”¨ FP16/TF32 ç®—åŠ›èƒ½è¶…è¿‡ **300 TFLOPS**ã€‚
* **é¡¶çº§è¶…ç®—ï¼ˆFrontierï¼Œç¾å›½ï¼‰**ï¼šè¶…è¿‡ **1 EFLOPS**ï¼ˆç™¾äº¿äº¿æ¬¡ï¼‰ã€‚

---

âœ… **ä¸€å¥è¯æ€»ç»“**
TFLOPS æ˜¯è¡¡é‡å¤„ç†å™¨/æ˜¾å¡è®¡ç®—èƒ½åŠ›çš„æŒ‡æ ‡ï¼Œè¡¨ç¤º **æ¯ç§’é’Ÿèƒ½æ‰§è¡Œå¤šå°‘ä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®—**ã€‚å®ƒæ˜¯ AI è®­ç»ƒã€ç§‘å­¦è®¡ç®—ã€å›¾å½¢æ¸²æŸ“æ€§èƒ½å¯¹æ¯”çš„é‡è¦å‚æ•°ã€‚

---

è¦ä¸è¦æˆ‘ç»™ä½ åšä¸€å¼ è¡¨ï¼Œå¯¹æ¯”ä¸€ä¸‹ **å¸¸è§ GPU çš„ TFLOPSï¼ˆFP32/FP16/TF32ï¼‰**ï¼Ÿ

# Tensor Cores, timing the code, TF32 precision, 333ms

core is just an instruction in the a100 architecture right so so what it does is
it does basically a little 4x4 Matrix multiply so uh this is just matrix
multiplication here of 4x4 matrices and there are multiple configurations as to
what Precision any of these matrices are it in what Precision the internal accumulate happens and then what is the
output Precision input precisions Etc so there's a few switches but it's basically a 4x4 multiply and then
anytime we have any operations that require Magic multiplication uh they get broken up into these into this
instruction of little 4x4 multiply and so everything gets broken up into this instruction because it's the fastest way
to multiply matrices and it turns out that most of the computational work that we're doing up above uh all of it really
is matrix multiplication most of the work computationally happens in the linear layers um linear linear Etc
there's a few things sandwiched in between so there's some additions in residuals there's some G nonlinearities
there's some layer Norms Etc but if you just time them you'll see that these are nothing like basically the in
Transformer is just a bunch of Matrix multiplications really um and especially
at this small scale 124 million parameter model actually the biggest matrix multiplication by far is the
classifier layer at the top that is a massive Matrix multiply of going from 768 to
50257 and that Matrix multiply dominates anything else that happens in that Network roughly speaking so it's Matrix
multiplies that become a lot faster which are hidden inside our linear layers and they're accelerated through
tensor course now the best reference I would say for tensor course is basically just go to the um a 100 architecture
white paper and then it's pretty detailed and but I think people it's
like relatively readable mostly if you half understand what's happening um so
figure 9 tensor float 32 so this is the explanation basically for tf32 and what happens here and you
see that there's many configuration options here available so the input operands and what precisions are they in
the accumulator and um what um basically the um the internal representation
within the instruction when you do the accumulate of this matrix multiplication so the intermediate plus
equals um of the intermediate little vector multiplies here that all happens in
fp32 and then uh this is an aex improvement as I mentioned to the Ops that we get so tf32 specifically we're
looking at this row here and the way this works is
um normally fp32 has 32 bits tf32 is the exact same bits we have one
sign bit we have eight exponent bits except the mantisa bits get cropped in
the float and so basically um we end up with just 19 bits instead of 32 bits
because the last 133 bits get truncated they get dropped um and all this is
internal to the instruction so none of it is visible to anything in our pytorch uh none of our pytorch code will change
all of the numbers will look identical it's just that when you call the tensor core um instruction internally in the
hardware it will crop out these 13 bits and that allows it to uh calculate this
little Matrix multiply significantly faster 8X faster now of course this speed up comes at a cost and the cost is
that we are reducing the Precision our accumulate is still an fp32 our output is fp32 our inputs are fp32 but
internally things get truncated in the operand to perform the operation faster and so our results are starting to be a
bit more approximate but empirically when you actually train with this you basically can't tell the difference
so the reason I like tf32 is because if you can tolerate a little bit of a Precision fudge um then this is free
like none of your codes sees this it's fully internal to the operation and the operation to you just go 8X faster and
it's a bit more approximate and so it's a pretty sweet spot I would say in optimization and uh let's see what that
looks like first so I've set up our Cod to just time the uh iterations so import
time I changed the hyper parameters so that we have something a bit more that reflects uh kind of workload that we
want to run uh because we want to do a fairly large run at the end of this so let's use batch size 16 and let's now
use the actual gpt2 um maximum sequence length of 10,24 tokens uh so this is the
configuration and then for 50 iterations I'm just doing something very lazy here
I'm doing time. time to get the current time and then this is the optimization Loop and now I want to time how long
this takes now one issue with working with gpus is that as your
CPU um when your CPU runs it's just scheduling work on GPU it's ordering
some work right and so it send a request and then it continues running and so we
can actually it can happen sometimes that we sort of um speed through this and we queue up a lot of kernels to run
on the GPU and then the CPU sort of like gets here and takes time at time but actually the GPU is still running
because it takes it time to actually work through the work that was scheduled to run and so you're just building up a
queue for the GPU and so actually if you need to you want to wait toat data synchronize and this will wait for the
GPU to finish all the work that was scheduled to run up above here and then
we can actually take the time so basically we're waiting for the GPU to stop this iteration take time and then
we're going to just print it so so here I'm going to run the training Loop and here on the right I'm watching
Nvidia SMI so we start off at zero um we're not using the GPU and then by
default P will use gpu0 so we see that it gets filled up and we're using 35 GB
out of 80 gabt available and then here on the left we see that because we've cranked up the
batch size now it's only 20 batches to do a single Epoch on our tiny Shakespeare
and we see that we're seeing roughly a th000 milliseconds per iteration here right
so the first iteration sometimes is slower and that's because pytorch might be doing a lot of initializations here
on the very first iteration and so it's probably initializing all these uh tensors and buffers to hold all the
gradients and I'm not 100% sure all the work that happens here but uh this could be a slower iteration when you're timing
your logic you always want to be careful with that but basically we're seeing a th000 milliseconds per iteration
um and so this will run for roughly 50 seconds as we have it right now so
that's our Baseline in flo 32 one more thing I wanted to mention is that if
this doesn't fit into your GPU and you're getting out of memory errors then start decreasing your batch size until
things fit so instead of 16 try eight or four or whatever you need to fit um the
batch into your GPU and if you have a bigger GPU you can actually potentially get away with 32 and so on uh by default
you want to basically max out has Max Max out the batch size that fits on your GPU and you want to keep it nice numbers
so use numbers that have lots of powers of two in them so 16 is a good number 8
24 32 48 These are nice numbers but don't use something like 17 uh because
that will run very inefficiently on a GPU uh and we're going to see that a bit later as well so for now let's just
stick with 16124 and uh the one thing that I added also here and I ran it again is I'm
calculating a tokens per second throughput during training because we might end up changing the
backat size around over time but tokens per second is the objective measure that we actually really care about how many
tokens of data are we training on and what is the throughput of tokens that we're getting in our optimization so
right now we're processing and training on 163,000 tokens per second roughly and
that's a bit more objective metric okay so let's now enable tf32 now
luckily pytorch makes this fairly easy for us and uh to enable tf32 you just
need to do a single line and is this and when we go to the py documentation here
for this function basically this tells pych what kind of kernels to run and by
default I believe it is highest highest Precision for mat M and that means that
everything happens in float 32 just like it did before but if we set it to high as we do right now Matrix
multiplications will not use tensor flow 32 when it's available my GPU is a100 so it's an
ampere series and therefore tf32 is available if you have an older GPU this
might not be available for you but for my GPU it's available and so what I expect P to do is that every single
place where we see an nn. linear inside there there's a matrix multiplication and I expect that matrix multiplication
now to be um running on tensor course utilizing the TF 32%
so this is the single line of change that is I believe necessary and let's rerun this now we saw that um in terms
of the throughput that is promised to us we're supposed to be getting 8X roughly
so let's see what happens and that 8X came from here right
um 8X and it also came from looking at it um here 156 T flops instead of of
19.5 okay so what actually happened uh so we're seeing that our throughput
roughly 3x not aex so we are going we're
from 1,000 milliseconds we're going down to 300 milliseconds and our throughput is now about 50,000 tokens per second so
we have a roughly 3x instead of 8X so what happened and basically What's Happening Here is again a lot of these
workloads are memory bound and so even though the tf32 offers in principle a lot faster
throughput all of these numbers everywhere are still float 32s and it's float 32 numbers that are being shipped
all over the place through the memory system and is just costing us way too much time to shuttle around all this
data and so even though we've made the multiply itself much faster uh we are memory bound and we're not actually
seeing the full benefit uh that would come from uh this napkin math here uh
that said we are getting one a 3X faster throughput and this is free um single
line of code in P torch all your variables are still float 32 everywhere it just runs faster and it's slightly
more approximate but we're not going to notice it basically uh so that's
tf32 okay so let's now continue so we've exercised this row and um we saw that we

å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## åŠ é€Ÿè®­ç»ƒï¼ˆäºŒï¼‰ï¼šTensor Coresã€ä»£ç è®¡æ—¶ã€TF32 ç²¾åº¦ã€333 æ¯«ç§’æ¯æ­¥

### 1. ä»€ä¹ˆæ˜¯ Tensor Coresï¼Ÿ

åœ¨ A100 GPU æ¶æ„ä¸­ï¼Œ**Tensor Core æ˜¯ä¸€ç§ç‰¹æ®Šçš„ç¡¬ä»¶æŒ‡ä»¤**ï¼Œç”¨äºåŠ é€Ÿ **å°è§„æ¨¡çŸ©é˜µä¹˜æ³•**ï¼ˆå¦‚ 4x4 çŸ©é˜µçš„ä¹˜æ³•ï¼‰ã€‚è™½ç„¶å®ƒæ˜¯ä¸€ä¸ªâ€œæŒ‡ä»¤â€ï¼Œä½†æœ¬è´¨ä¸Šå¯ä»¥ç†è§£ä¸ºä¸“ç”¨ç¡¬ä»¶æ¨¡å—ï¼Œæ‰§è¡Œå¤§é‡é«˜æ•ˆçš„çŸ©é˜µä¹˜æ³•æ“ä½œã€‚

åœ¨ Transformer ä¸­ï¼Œå¤§éƒ¨åˆ†è®¡ç®—é‡éƒ½é›†ä¸­åœ¨ **çº¿æ€§å±‚ï¼ˆlinear layersï¼‰** ä¸­ï¼Œè¿™äº›æœ¬è´¨ä¸Šå°±æ˜¯çŸ©é˜µä¹˜æ³•ã€‚ä¾‹å¦‚ï¼š

* å¤šå¤´æ³¨æ„åŠ›çš„çº¿æ€§æ˜ å°„
* MLPï¼ˆå‰é¦ˆç½‘ç»œï¼‰
* æœ€é¡¶å±‚çš„è¾“å‡ºåˆ†ç±»å™¨ï¼ˆä»éšè—å±‚ç»´åº¦ 768 å˜æ¢åˆ°è¯è¡¨ç»´åº¦ 50257ï¼‰
  è¿™äº›éƒ½å¯ä»¥æ‹†è§£æˆè‹¥å¹²ä¸ªå°å‹çŸ©é˜µä¹˜æ³•ï¼Œå› æ­¤ Tensor Core å¯ä»¥å¤§å¤§åŠ é€Ÿè¿™äº›æ“ä½œã€‚

### 2. ä»€ä¹ˆæ˜¯ TF32ï¼Ÿ

**TF32ï¼ˆTensorFloat-32ï¼‰** æ˜¯ä¸€ç§ NVIDIA é’ˆå¯¹æ·±åº¦å­¦ä¹ ä¼˜åŒ–çš„æµ®ç‚¹æ ¼å¼ï¼Œåœ¨ A100 ç­‰æ–°ä¸€ä»£ GPU ä¸Šæ”¯æŒã€‚

å®ƒçš„ä¸»è¦ç‰¹ç‚¹æ˜¯ï¼š

* ä»ç„¶ä½¿ç”¨ 32 ä½æµ®ç‚¹è¡¨ç¤ºï¼Œä½† **ç²¾åº¦æ›´ä½**

  * æŒ‡æ•°ä½ä¸ FP32 ä¸€æ ·ï¼ˆ8 ä½ï¼‰
  * æœ‰æ•ˆä½ï¼ˆmantissaï¼‰è¢«è£å‰ªï¼Œåªä¿ç•™äº† 10 ä½ï¼ˆæ€»å…±å¤§çº¦ 19 ä½ä¿¡æ¯ï¼‰
* å¯¹ç”¨æˆ·ä»£ç æ˜¯å®Œå…¨é€æ˜çš„ï¼ŒPyTorch ä¸­å˜é‡ä»ç„¶æ˜¯ float32
* ä¹˜æ³•ä½¿ç”¨ TF32ï¼Œ**ç´¯åŠ å’Œè¾“å‡ºä»ç„¶æ˜¯ FP32**

> è¿™æ ·èƒ½åœ¨å‡ ä¹æ— æŸç²¾åº¦çš„æƒ…å†µä¸‹ï¼Œè·å¾— **é«˜è¾¾ 8 å€çš„é€Ÿåº¦æå‡**

### 3. å¦‚ä½•å¯ç”¨ TF32ï¼Ÿ

PyTorch ä¸­åªéœ€ä¸€å¥ä»£ç å³å¯å¯ç”¨ TF32ï¼š

```python
torch.backends.cuda.matmul.allow_tf32 = True
```

è¿™ä¼šè®© `torch.matmul()` æˆ– `nn.Linear()` èƒŒåçš„çŸ©é˜µä¹˜æ³•é»˜è®¤ä½¿ç”¨ TF32 æ¨¡å¼ï¼ˆå¦‚æœ GPU æ”¯æŒï¼Œä¾‹å¦‚ A100ï¼‰ã€‚

### 4. å®æµ‹æ•ˆæœï¼šä» 1000ms â†’ 333ms

ä½œè€…è®¾ç½®äº†å¦‚ä¸‹è¶…å‚æ•°ï¼š

* `batch_size = 16`
* `sequence_length = 1024`
* ä½¿ç”¨äº†æ—¶é—´æ¨¡å— `time.time()` å¯¹è®­ç»ƒå¾ªç¯è®¡æ—¶

ä¸ºäº†ç¡®ä¿ GPU çš„æ“ä½œå®Œæˆåæ‰è®¡æ—¶ï¼Œä»–ä½¿ç”¨äº†ï¼š

```python
torch.cuda.synchronize()
```

è¿™ä¸ªå‡½æ•°ç¡®ä¿ CPU ç­‰å¾… GPU ä¸Šçš„æ“ä½œçœŸæ­£æ‰§è¡Œå®Œå†ç»§ç»­ï¼Œä»¥é¿å…æ—¶é—´ç»Ÿè®¡è¢«â€œæå‰â€ã€‚

**åœ¨å¯ç”¨ TF32 ä¹‹å‰ï¼š**

* æ¯æ¬¡è®­ç»ƒè¿­ä»£æ—¶é—´å¤§çº¦æ˜¯ **1000ms**
* ååé‡çº¦ä¸º **16 ä¸‡ tokens / ç§’**

**å¯ç”¨ TF32 åï¼š**

* æ¯æ¬¡è®­ç»ƒè¿­ä»£æ—¶é—´é™åˆ° **å¤§çº¦ 333ms**
* ååé‡ä¸Šå‡åˆ° **50 ä¸‡ tokens / ç§’**
* å®é™…åŠ é€Ÿä¸º **çº¦ 3 å€**

è™½ç„¶ç†è®ºä¸Š TF32 æä¾› **8 å€** çš„è®¡ç®—åŠ é€Ÿï¼Œä½†æ²¡æœ‰å®Œå…¨ä½“ç°å‡ºæ¥ã€‚è¿™æ˜¯å› ä¸ºï¼š

> **è®­ç»ƒè¿‡ç¨‹ä¸æ˜¯è®¡ç®—ç“¶é¢ˆï¼Œè€Œæ˜¯**â€œ**å†…å­˜å¸¦å®½ç“¶é¢ˆ**â€ï¼š
>
> * å³ï¼šGPU çš„è®¡ç®—æ ¸å¿ƒï¼ˆTensor Coreï¼‰åœ¨ç­‰å¾…æ•°æ®ä»å†…å­˜åŠ è½½ï¼Œè€Œä¸æ˜¯åœ¨å¿™äºä¹˜æ³•è¿ç®—
> * æ•°æ®ä¾ç„¶æ˜¯ float32 ç±»å‹ï¼Œå ç”¨å¤§é‡å†…å­˜å’Œå¸¦å®½

### 5. å°è´´å£«

* å¦‚æœä½ é‡åˆ° **æ˜¾å­˜ä¸è¶³** é”™è¯¯ï¼ˆOut of Memoryï¼‰ï¼Œè¯·å‡å° `batch_size`
* é€‰æ‹© `batch_size` æ—¶å°½é‡ä½¿ç”¨ **2 çš„å€æ•°**ï¼ˆå¦‚ 8ã€16ã€32ï¼‰ï¼Œè¿™äº›åœ¨ GPU ä¸Šè¿è¡Œæ›´é«˜æ•ˆ
* ååé‡ï¼ˆtokens/secï¼‰æ˜¯æœ€å®¢è§‚çš„æ€§èƒ½æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡è®­ç»ƒæ˜¯å¦åŠ å¿«

---

### æ€»ç»“

* **Tensor Core + TF32** æ˜¯ç°ä»£ NVIDIA GPU æä¾›çš„å…è´¹åŠ é€Ÿç¥å™¨
* åœ¨ PyTorch ä¸­åªéœ€ 1 è¡Œä»£ç å°±èƒ½å¯ç”¨ï¼Œå¤§çº¦èƒ½è·å¾— **3 å€ä»¥ä¸Š** çš„é€Ÿåº¦æå‡
* å®Œå…¨å‘åå…¼å®¹ï¼Œè®­ç»ƒè¿‡ç¨‹æ— æ„ŸçŸ¥ï¼Œä»£ç ä¸éœ€è¦å¤§æ”¹
* å®é™…æ•ˆæœå—é™äº **å†…å­˜å¸¦å®½**ï¼Œä½†å·²æ˜¯éå¸¸å®ç”¨çš„ä¼˜åŒ–æ‰‹æ®µ

---

æ¥ä¸‹æ¥ï¼Œä½œè€…ä¼šç»§ç»­è®²è§£å¦‚ä½•ä½¿ç”¨ **FP16ï¼ˆæ··åˆç²¾åº¦ï¼‰** è¿›ä¸€æ­¥æé€Ÿã€‚å¦‚æœä½ æƒ³ç»§ç»­äº†è§£ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼

torch.set_float32_matmul_precision(precision)[source]
Sets the internal precision of float32 matrix multiplications.

Running float32 matrix multiplications in lower precision may significantly increase performance, and in some programs the loss of precision has a negligible impact.

Supports three settings:

â€œhighestâ€, float32 matrix multiplications use the float32 datatype (24 mantissa bits with 23 bits explicitly stored) for internal computations.

â€œhighâ€, float32 matrix multiplications either use the TensorFloat32 datatype (10 mantissa bits explicitly stored) or treat each float32 number as the sum of two bfloat16 numbers (approximately 16 mantissa bits with 14 bits explicitly stored), if the appropriate fast matrix multiplication algorithms are available. Otherwise float32 matrix multiplications are computed as if the precision is â€œhighestâ€. See below for more information on the bfloat16 approach.

â€œmediumâ€, float32 matrix multiplications use the bfloat16 datatype (8 mantissa bits with 7 bits explicitly stored) for internal computations, if a fast matrix multiplication algorithm using that datatype internally is available. Otherwise float32 matrix multiplications are computed as if the precision is â€œhighâ€.

When using â€œhighâ€ precision, float32 multiplications may use a bfloat16-based algorithm that is more complicated than simply truncating to some smaller number mantissa bits (e.g. 10 for TensorFloat32, 7 for bfloat16 explicitly stored). Refer to [Henry2019] for a complete description of this algorithm. To briefly explain here, the first step is to realize that we can perfectly encode a single float32 number as the sum of three bfloat16 numbers (because float32 has 23 mantissa bits while bfloat16 has 7 explicitly stored, and both have the same number of exponent bits). This means that the product of two float32 numbers can be exactly given by the sum of nine products of bfloat16 numbers. We can then trade accuracy for speed by dropping some of these products. The â€œhighâ€ precision algorithm specifically keeps only the three most significant products, which conveniently excludes all of the products involving the last 8 mantissa bits of either input. This means that we can represent our inputs as the sum of two bfloat16 numbers rather than three. Because bfloat16 fused-multiply-add (FMA) instructions are typically >10x faster than float32 ones, itâ€™s faster to do three multiplications and 2 additions with bfloat16 precision than it is to do a single multiplication with float32 precision.

è§£é‡Šä¸€ä¸‹

å¥½çš„ï¼Œæˆ‘æ¥å¸®ä½ é€æ¡è§£é‡Šä¸€ä¸‹ **`torch.set_float32_matmul_precision(precision)`** çš„ä½œç”¨å’ŒåŸç†ã€‚

---

## ğŸ”¹ è¿™ä¸ªå‡½æ•°åšä»€ä¹ˆï¼Ÿ

åœ¨ PyTorch ä¸­ï¼Œ`torch.set_float32_matmul_precision(precision)` ç”¨æ¥è®¾ç½® **float32 çŸ©é˜µä¹˜æ³•çš„å†…éƒ¨è®¡ç®—ç²¾åº¦**ã€‚

æœ‰æ—¶å€™æˆ‘ä»¬å¹¶ä¸éœ€è¦æ»¡ç²¾åº¦çš„ FP32ï¼ˆ23 ä½å°¾æ•°ä½ï¼‰ï¼Œä½¿ç”¨ç¨ä½ç²¾åº¦çš„æ•°æ®æ ¼å¼ï¼ˆå¦‚ TensorFloat32 æˆ– bfloat16ï¼‰å¯ä»¥ **æ˜¾è‘—åŠ é€ŸçŸ©é˜µä¹˜æ³•**ï¼Œè€ŒæŸå¤±çš„ç²¾åº¦åœ¨å¾ˆå¤šåœºæ™¯ä¸‹å½±å“å¹¶ä¸å¤§ï¼ˆä¾‹å¦‚æ·±åº¦å­¦ä¹ è®­ç»ƒ/æ¨ç†ï¼‰ã€‚

---

## ğŸ”¹ æ”¯æŒçš„ä¸‰ç§æ¨¡å¼

1. **"highest"**

   * å¼ºåˆ¶ä½¿ç”¨ **float32ï¼ˆæ ‡å‡† 23 ä½å°¾æ•°ä½ï¼‰** æ¥è®¡ç®—çŸ©é˜µä¹˜æ³•ã€‚
   * è¿™æ˜¯ä¼ ç»Ÿçš„æœ€é«˜ç²¾åº¦ FP32 ç®—æ³•ï¼Œé€Ÿåº¦è¾ƒæ…¢ä½†æ•°å€¼æœ€å‡†ç¡®ã€‚

2. **"high"**

   * å°è¯•ä½¿ç”¨ **TensorFloat32 (TF32)** æˆ– **bfloat16 åˆ†è§£ç®—æ³•** æ¥åŠ é€Ÿè®¡ç®—ã€‚
   * **TensorFloat32 (TF32)**ï¼šåªæœ‰ 10 ä½æœ‰æ•ˆå°¾æ•°ä½ï¼ˆæ¯” FP32 å°‘ï¼Œä½†æ¯” bfloat16 å¤šï¼‰ï¼Œæ˜¯ NVIDIA Ampere GPU çš„ç‰¹æ€§ã€‚
   * **bfloat16 åˆ†è§£ç®—æ³•**ï¼šæŠŠä¸€ä¸ª FP32 æ•°å­—è¿‘ä¼¼è¡¨ç¤ºä¸º **ä¸¤ä¸ª bfloat16 æ•°å­—çš„å’Œ**ï¼ˆè€Œä¸æ˜¯ç›´æ¥æˆªæ–­ï¼‰ï¼Œè¿™æ ·èƒ½ä¿ç•™å¤§çº¦ 16 ä½ç²¾åº¦ã€‚
   * å¦‚æœç¡¬ä»¶/ç®—æ³•ä¸æ”¯æŒ TF32 æˆ– bfloat16ï¼Œå°±ä¼šå›é€€åˆ° `"highest"` æ¨¡å¼ã€‚

3. **"medium"**

   * å°è¯•ç›´æ¥ä½¿ç”¨ **bfloat16ï¼ˆ7 ä½å°¾æ•°ä½ï¼‰** åšçŸ©é˜µä¹˜æ³•ã€‚
   * ç²¾åº¦æ›´ä½ï¼Œä½†é€Ÿåº¦æœ€å¿«ã€‚
   * å¦‚æœä¸æ”¯æŒï¼Œå°±ä¼šå›é€€åˆ° `"high"`ã€‚

---

## ğŸ”¹ ä¸ºä»€ä¹ˆ "high" æ¨¡å¼æ›´å·§å¦™ï¼Ÿ

è¿™é‡Œå¼•ç”¨äº† \[Henry2019] çš„æ–¹æ³•ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

* **float32 æœ‰ 23 ä½å°¾æ•°ä½**ï¼Œè€Œ **bfloat16 åªæœ‰ 7 ä½å°¾æ•°ä½**ã€‚
* ä½ å¯ä»¥ç”¨ **3 ä¸ª bfloat16 æ•°å­—çš„å’Œ** æ¥å®Œç¾è¡¨ç¤ºä¸€ä¸ª float32ã€‚

  * æ¯”å¦‚ï¼šFP32 = bf16\_1 + bf16\_2 + bf16\_3
* ä¸¤ä¸ª float32 ç›¸ä¹˜ = 9 ä¸ª bfloat16 ç›¸ä¹˜ï¼ˆå› ä¸ºæ¯ä¸€é¡¹å±•å¼€ï¼‰ã€‚

ä¸ºäº†åŠ é€Ÿï¼ŒPyTorch é€‰æ‹©åªä¿ç•™å…¶ä¸­æœ€é‡è¦çš„ 3 ä¸ªä¹˜ç§¯ï¼ˆèˆå¼ƒæ¶‰åŠä½ 8 ä½å°¾æ•°çš„éƒ¨åˆ†ï¼‰ã€‚è¿™æ ·å°±ç›¸å½“äºæŠŠ FP32 è¿‘ä¼¼ä¸º **ä¸¤ä¸ª bfloat16 çš„å’Œ**ï¼Œæ¯”å•çº¯æˆªæ–­æ›´ç²¾ç¡®ã€‚

â¡ï¸ è¿™æ ·åšçš„ç»“æœï¼š

* **ç²¾åº¦æŸå¤±ï¼š** å¿½ç•¥äº†æœ€å 8 ä½å°¾æ•°ã€‚
* **æ€§èƒ½æå‡ï¼š** bfloat16 ä¹˜åŠ ï¼ˆFMAï¼‰åœ¨ç¡¬ä»¶ä¸Šæ¯” FP32 ä¹˜æ³• **å¿« 10 å€ä»¥ä¸Š**ã€‚
* **æ•´ä½“æ•ˆæœï¼š** åªéœ€è¦ 3 æ¬¡ bfloat16 ä¹˜æ³• + 2 æ¬¡åŠ æ³•ï¼Œå°±èƒ½è¿‘ä¼¼ 1 æ¬¡ FP32 ä¹˜æ³•ï¼Œé€Ÿåº¦è¿œè¶… FP32ã€‚

---

## ğŸ”¹ ç›´è§‚æ€»ç»“

* `"highest"` â†’ 100% ç²¾åº¦ï¼Œæœ€æ…¢ã€‚
* `"high"` â†’ èˆå¼ƒéƒ¨åˆ†å°¾æ•°ä½ï¼ˆæœ€å ~~8 ä½ï¼‰ï¼Œç²¾åº¦å¤§çº¦ä¿ç•™åˆ° FP16~~FP32 ä¹‹é—´ï¼Œé€Ÿåº¦å¤§å¹…æå‡ã€‚
* `"medium"` â†’ ç›´æ¥ç”¨ bfloat16 ç®—ï¼Œç²¾åº¦æœ€ä½ï¼Œä½†é€Ÿåº¦æœ€å¿«ã€‚

åœ¨æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­ï¼Œä¸€èˆ¬ä½¿ç”¨ `"high"` æˆ– `"medium"` è¶³å¤Ÿï¼Œå› ä¸ºç¥ç»ç½‘ç»œå¯¹å°‘é‡æ•°å€¼å™ªå£°ä¸æ•æ„Ÿï¼Œè¿˜å¯èƒ½å› éšæœºæ€§å¸¦æ¥ç±»ä¼¼æ­£åˆ™åŒ–çš„æ•ˆæœã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **ç²¾åº¦ vs æ€§èƒ½å¯¹æ¯”è¡¨**ï¼Œç›´è§‚å±•ç¤º `"highest" / "high" / "medium"` ä¸‰ç§æ¨¡å¼çš„æ•°å€¼ç²¾åº¦å’Œè¿è¡Œé€Ÿåº¦å·®å¼‚ï¼Ÿ



# float16, gradient scalers, bfloat16, 300ms

can crop out some of the Precision inside the operation itself but we saw that we're still memory bound we're
still moving around all these floats right otherwise and we're paying that cost because of this so let's now
decrease the amount of stuff that we're going to be moving around and we're going to do that by dropping down to B
float 16 so we're only going to be maintaining 16 bits per float and we're
going to use the B flat 16 and I'll explain in a bit uh fp16 difference and uh we're going to be in this row so when
we go back to the documentation here for the a 100 um we see here the precisions that
are are available and this is the original fp32 the tf32 crops out the
Precision and then here in bf16 you see that it is very similar to
tf32 but it's even more aggressive in cropping off of the Precision the
mantisa of this float so the important thing with B float 16 is that the exponent bits and the sign bit of course
remain unchanged so if you're familiar with your float numbers and I think this should should probably be an entire
video by itself the exponent sets the range that you can represent of your numbers and the
Precision is how much Precision you have for your numbers and so the range of
numbers is identical but we can we have fewer possibilities within that range
because we are truncating the Mena so we have less Precision in that range what that means is that things are
actually fairly nice because we have the original range of numbers that are representable in float but we just have
less Precision for it and the difference with fp16 is that they actually touch
and change the range so fp16 cannot represent the full range of fp32 it has
a reduced range and that's where you start to actually run into issues because now you need uh these gradient
scalers and things like that and I'm not going to go into the detail of that in this video because that's a whole video
by itself but fb16 actually historically came first that was available in the Volta series before Amper and so fp16
came first and everyone started to train in fp16 but everyone had to use all these gradient scaling operations which
are kind of annoying and it's an additional source of state and complexity and the reason for that was
because the exponent range was reduced in fp16 so that's the i e fp16 spec and
then they came out with bf16 and the Ampere and they made it much simpler because we're just truncating manessa we
have the exact same range and we do not need gradient scalers so everything is much much simpler now when we do use
bf16 though we are impacting the numbers that we might be seeing in our pytorch
code these this change is not just local to the operation itself so let's see how
that works um there's some documentation here that
so I think this is probably the best best page to explain how to use mixed Precision in pytorch um because there
are many other tutorials and so on even within pitor documentation that are a lot more confusing and so I recommend
specifically this one because there's five other copies that I would not recommend and then when we come
here ignore everything about everything ignore everything about gradient scalers and only look at torch.
AutoCast and basically also this comes to a single line of code at the end so this is the context manager that we
want and we want to use that in our Network when you click into the torch.
AutoCast autocasting it has a few more uh a bit more guideline for you so it's
telling you do not call B flat 16 on any of your tensors just use AutoCast and
only surround the uh forward pass of the model and the loss calculation and that's the only two things that you
should be surrounding leave the backward and the optimizer step alone so that's the guidance that comes from the P team
so we're going to follow that guidance and for us because the L calculation is inside of the model forward pass for us
we are going to be doing this and then we don't want to be using torch Flo 16 because if we do that we
need to start using gradient scalers as well so we are going to be using B float 16 this is only possible to do an ampere
uh but this means that the changes are extremely minimal like basically just this one line of
code um let me first break in to here before we actually run this
so right after logits I'd like to show you that different from the tf32 that we
saw this is actually going to impact our tensors so this Lis tensor if we now look at
this and we look at the dtype we suddenly see that this is now B float 16 uh it's not float 32 anymore so our
activations have been changed the activations tensor is now B FL 16 but not everything has changed so model.
Transformer wte uh this is the weight uh token
embedding table it has a weight inside it and the dtype of this weight this
parameter is still torch float 32 so our parameters seem to still be in float 32
but our activations the loits are now in P 16 so clearly this is why we get the
mixed Precision some things pytorch is keeping inlow 32 some things pytorch is
converting to lower Precision um and what gets converted at what point is not
super clear I remember scrolling down is it
here okay I can't find it I I thought it was here okay there we
go so there are a few docks on when you're using this AutoCast what gets
converted to B FL 16 and and when so for example only these Matrix multiply like
operations get converted to float 16 but a lot of operations remain in float 32 so in particular a lot of normalizations
like layer norms and things like that not all of those layers might be converted um so only some layers
selectively would be running B flat 16 but things like softmax uh layer Norms
uh log um log soft Max so loss function calculations a lot of those things might
remain in float 32 because they are more susceptible to Precision changes major multiplies are fairly um
robust to Precision changes uh so some parts of the network are um impacted
more or less by the Precision change um so basically only some parts
of the of the model are running in reduced Precision let's take it for a spin and let's actually see what kind of
improvement we achieve
here okay so we used to be 333 milliseconds we're now 300
and we used to be somewhere around 50,000 tokens per second we're now at 55 so we're definitely running faster but
maybe not a lot faster and that's because there are still many many bottlenecks in our gbt2 we're just
getting started but we have dropped down the precision as far as we can with my current GPU which is a100 we're using
pytorch AutoCast unfortunately I don't actually exactly know what pytorch AutoCast do uh does I don't actually
know exactly what's in B flat 16 what's in float 32 we could go in and we could start to scrutinize it um but these are the kinds
of rules that pytorch has internally and unfortunately they don't documented very well uh so we're not going to go into
that into in too much detail but for now we are training in B flow 16 we do not need a gradient scaler and the reason
things are running faster is because um we are able to run tensor course in B FL
16 now that means we are in this row but uh we are also paying in Precision for
this uh so um we expect slightly less accurate results with respect to the original fp32 but empirically in many
cases this is a worth it uh kind of tradeoff because it allows you to run faster and you could for example train
longer and make up for the uh for that Precision decrease so um that's b46 for

å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯å…³äº **BFloat16ã€æ··åˆç²¾åº¦è®­ç»ƒã€ä»¥åŠ PyTorch è‡ªåŠ¨è½¬æ¢æœºåˆ¶ï¼ˆAutoCastï¼‰** çš„ä¸­æ–‡è§£é‡Šï¼š

---

## åŠ é€Ÿè®­ç»ƒï¼ˆä¸‰ï¼‰ï¼šä½¿ç”¨ Float16 / BFloat16ï¼Œæ··åˆç²¾åº¦ï¼Œ300 æ¯«ç§’æ¯æ­¥

### ğŸ§  èƒŒæ™¯ï¼šæˆ‘ä»¬è¦è§£å†³çš„é—®é¢˜

åœ¨ä¸Šä¸€èŠ‚æˆ‘ä»¬ä½¿ç”¨äº† **TF32**ï¼Œè™½ç„¶å¾—åˆ°äº†çº¦ 3 å€æé€Ÿï¼Œä½†ä»ç„¶ **å­˜åœ¨å†…å­˜å¸¦å®½ç“¶é¢ˆ** â€”â€” ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬è™½ç„¶è®©è®¡ç®—æ›´å¿«äº†ï¼Œä½†ä¼ è¾“è¿™äº› **float32 æ•°å€¼** ä»ç„¶å¾ˆæ…¢ï¼Œå æ®äº†å¤§é‡å†…å­˜å¸¦å®½ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬å¸Œæœ›è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æ˜¯ï¼š
â¡ï¸ **è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä¸­æ¯ä¸ªæ•°å€¼çš„å¤§å°** â€”â€” æŠŠ 32 ä½çš„ float æ¢æˆ 16 ä½ã€‚

---

## ğŸ¯ ä¸¤ç§ 16 ä½æµ®ç‚¹æ ¼å¼çš„åŒºåˆ«ï¼š`fp16` vs `bfloat16`

| ç‰¹æ€§           | `fp16` (float16) | `bfloat16` (Brain float 16) |
| ------------ | ---------------- | --------------------------- |
| ç²¾åº¦ï¼ˆmantissaï¼‰ | é«˜ï¼ˆ10 ä½ï¼‰          | ä½ï¼ˆ7 ä½ï¼‰                      |
| èŒƒå›´ï¼ˆexponentï¼‰ | å°ï¼ˆ5 ä½ï¼‰           | å¤§ï¼ˆ8 ä½ï¼Œå’Œ float32 ä¸€æ ·ï¼‰         |
| æ˜¯å¦éœ€è¦æ¢¯åº¦ç¼©æ”¾å™¨    | âœ… éœ€è¦             | âŒ ä¸éœ€è¦                       |
| æ˜¯å¦æ›´å®‰å…¨ç¨³å®š      | âŒ å®¹æ˜“çˆ†ç‚¸/æ¶ˆå¤±        | âœ… å’Œ float32 èŒƒå›´ä¸€è‡´            |
| æ¨èä½¿ç”¨åœºæ™¯       | æ—§ GPUï¼ˆVoltaï¼‰     | æ–° GPUï¼ˆAmpereï¼Œå¦‚ A100ï¼‰        |

> ğŸ“Œ æ€»ç»“ï¼š**`bfloat16` æ˜¯æ¯” `fp16` æ›´å®‰å…¨ã€å¯æ›¿ä»£çš„æ··åˆç²¾åº¦æ–¹æ¡ˆï¼Œå…¼å®¹æ€§å’Œç¨³å®šæ€§éƒ½æ›´å¥½ã€‚**

---

## âš™ï¸ åœ¨ PyTorch ä¸­å¯ç”¨ `bfloat16` çš„æ–¹æ³•ï¼šä½¿ç”¨ `autocast`

PyTorch æä¾›äº†ä¸€ä¸ªè‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒå·¥å…· â€”â€” `torch.autocast`
ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š

```python
with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
    logits, loss = model(x, y)
```

* è¿™æ®µä»£ç åªå½±å“ **æ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆforwardï¼‰å’Œ loss è®¡ç®—**
* **ä¸å½±å“åå‘ä¼ æ’­ï¼ˆbackwardï¼‰å’Œä¼˜åŒ–å™¨æ­¥éª¤ï¼ˆoptimizer.stepï¼‰**
* ä¸éœ€è¦ä½¿ç”¨ `gradient scaler`

### ä¸ºä»€ä¹ˆ `autocast` å¾ˆæ£’ï¼Ÿ

* PyTorch ä¼š **è‡ªåŠ¨å†³å®šå“ªäº›æ“ä½œé™ç²¾åº¦**ï¼ˆæ¯”å¦‚çŸ©é˜µä¹˜æ³•ï¼‰
* æ›´æ•æ„Ÿçš„æ“ä½œï¼ˆå¦‚ softmaxã€layernormã€logï¼‰ä¼šä¿ç•™åœ¨ float32 ç²¾åº¦ä¸‹
* åªéœ€ä¸€è¡Œä»£ç  âœ…
* æ— éœ€äººä¸ºå¹²é¢„ï¼Œè‡ªåŠ¨é€‰æ‹©å®‰å…¨é«˜æ•ˆçš„æ‰§è¡Œæ–¹å¼ âœ…

---

## ğŸ‘€ æŸ¥çœ‹å®é™…æ•ˆæœ

åœ¨ä½¿ç”¨ `autocast` + `bfloat16` åï¼š

* logits çš„ç²¾åº¦ä» float32 â†’ bfloat16 âœ…
* æ¨¡å‹çš„å‚æ•°ä»ç„¶ä¿æŒåœ¨ float32ï¼ˆè¿™æ˜¯ PyTorch é»˜è®¤è¡Œä¸ºï¼‰âœ…
* æ‰€ä»¥è¿™å°±æ˜¯ **â€œæ··åˆç²¾åº¦â€**ï¼šæ¿€æ´»å€¼ä½ç²¾åº¦ï¼Œå‚æ•°é«˜ç²¾åº¦

---

## ğŸ“Š æ€§èƒ½è¡¨ç°

* å¯ç”¨ `bfloat16` åï¼Œå•æ¬¡è®­ç»ƒè¿­ä»£ä» **333ms â†’ 300ms**
* ååé‡ä» **æ¯ç§’ 50,000 tokens â†’ 55,000 tokens**
* æå‡ä¸æ˜¯ç‰¹åˆ«å¤§ï¼Œå› ä¸ºï¼š

  * ç›®å‰æ¨¡å‹ä¾ç„¶å­˜åœ¨å†…å­˜å¸¦å®½ç“¶é¢ˆ
  * GPT2 æ¨¡å‹è¿˜æœªæ·±åº¦ä¼˜åŒ–

ä½†ï¼š

* è¿™æ˜¯åœ¨ä¸å¼•å…¥é¢å¤–å¤æ‚åº¦çš„å‰æä¸‹è·å¾—çš„ **å…è´¹æé€Ÿ**
* åŒæ—¶èŠ‚çœäº†å¤§é‡æ˜¾å­˜

---

## ğŸ“ æ€»ç»“

| é¡¹ç›®        | æ•ˆæœ                                                         |
| --------- | ---------------------------------------------------------- |
| ä½¿ç”¨æ ¼å¼      | `bfloat16`ï¼ˆæ¨èï¼Œç‰¹åˆ«é€‚ç”¨äº A100ï¼‰                                  |
| ä½¿ç”¨æ–¹æ³•      | `torch.autocast(device_type='cuda', dtype=torch.bfloat16)` |
| æ˜¯å¦éœ€è¦æ¢¯åº¦ç¼©æ”¾å™¨ | âŒ ä¸éœ€è¦                                                      |
| æ”¹åŠ¨å¤æ‚åº¦     | ä»… 1 è¡Œä»£ç                                                     |
| æ€§èƒ½æå‡      | é¢å¤–æé€Ÿ + æ˜¾å­˜èŠ‚çœ                                                |
| ç²¾åº¦å½±å“      | ç•¥ä½äº float32ï¼Œä½†å¯æ¥å—                                           |

---

å¦‚æœä½ å¸Œæœ›ç»§ç»­å­¦ä¹ æ›´æ·±å…¥çš„ä¼˜åŒ–ï¼ˆä¾‹å¦‚å¤šå¡è®­ç»ƒã€æ•°æ®å¹¶è¡Œã€é‡è®¡ç®—æŠ€æœ¯ç­‰ï¼‰ï¼Œæˆ‘ä¹Ÿå¯ä»¥ç»§ç»­ä¸ºä½ è§£é‡Šã€‚æ˜¯å¦éœ€è¦ï¼Ÿ


# Automatic Mixed Precision ä½œè€…æ¨èè¿™ä¸ª

https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html


# torch.compile, Python overhead, kernel fusion, 130ms

now okay so as we can see we are currently at about 300 milliseconds uh per iteration and we're now going to
reach for some really heavy weapons in the pie torch Arsenal and in particular we're going to introduce torch. compile
so torch. compile is really quite incredible infrastructure from the pytorch team and it's basically a
compiler for neural networks like it's almost like GCC for CN C++ code this is
just this GCC of neural nuts so came out a while ago and extremely simple to use
um the way to use torch compile is to do this it's a single line of code to compile your model and return it now
this line of code will cost you compilation time but as you might guess it's going to make the code a lot faster
so let's actually run that because this will take some time to run but currently remember we're at 300 milliseconds and
we'll see what happens now while this is running I'd like to explain a little bit of what torch. compile does under the
hood uh so feel free to read this page of P torch but basically there's no real
good reason for you to not use torch compile in your pie torch I kind of feel like you should be using almost by
default if you're not uh unless you're debugging and you want your code to run really fast and there's one line here in
torch compile that I found that actually kind of like gets to why this is faster speed up mainly comes from reducing
python overhead and GPU read wrs so let me unpack that a little bit um okay here
we are okay so we went from 300 milliseconds we're now running at 129
milliseconds so this is uh 300 129 about 2.3x Improvement from a single line of
code in py torch uh so quite incredible so what is happening what's happening under the hood well when you pass the
model to torch compile what we have here in this NN module this is really just the
algorithmic description of what we'd like to happen in our Network and torch
compile will analyze the entire thing and it will look at what operations You' like to use and with the benefit of
knowing exactly what's going to happen it doesn't have to run in What's called the e mode it doesn't have to just kind
of like go layer by layer like the python interpreter normally would start
at the forward and the python interpreter will go okay let's do this operation and then
let's do that operation and it kind of materializes all the operations as it goes through uh so these um calculations
are dispatched and run in this order and the python interpreter and this code doesn't know what kind of operations are
going to happen later but torch compile sees your entire code at the same time and it's able to know what operations
you intend to run and it will kind of optimize that process the first thing it will do is will it will take out the
python interpreter from the forward pass entirely and it will kind of compile this entire neural net as a single
object with no python interpreter involved so it knows exactly what's going to run and we'll just run that and
it's all going to be running in efficient code uh the second thing that happens is
uh this read write that they mentioned very briefly so a good example of that I think is the G nonlinearity that we've
been looking at so here we use the n and G now this here is me uh basically just
breaking up the inang Galu uh which you remember has this formula so this here
is the equivalent implementation to what's happening inside g algorithmic l it's identical Now by default if uh we just
we using this instead of ending. G here what would happen without torch compile
well the python interpreter would make its way here and then it would be okay well there's an input well let me first
let me raise this input to the third power and it's going to dispatch a kernel that takes your input and raises
it to the third power and that kernel will run and when this kernel runs what
ends up happening is this input is stored in the memory of the GPU so here's a helpful example of the layout
of what's happening right you have your CPU this is in every single computer there's a few cores in there and you
have your uh Ram uh your memory and the CPU can talk to the memory and this is
all well known but now we've added the GPU and the GPU is a slightly different architecture of course they can
communicate and it's different in that it's got a lot more course than a CPU
all of those cores are individually a lot simpler too but it also has memory right this high bandwidth memory I'm
sorry if I'm botching it hbm I don't even know what that stands for I'm just realizing that
but uh this is the memory and it's very equivalent to uh RAM basically in the
computer and what's happening is that input is living in the memory and when you do input
cubed this has to travel to the GPU to the course and to all the caches and
registers on the actual chip of this GPU and it has to calculate the all the
elements to the third and then it saves the result back to the memory and it's this uh travel time that actually causes
a lot of issues so here remember this memory bandwidth we can communicate
about 2 terabytes per second which is a lot but also we have to Traverse this
link and it's very slow so here on the GPU we're on chip and everything is super fast within the chip but going to
the memory is extremely expensive takes extremely long amount of time and so we load the input do the calculations and
load back the output and this round trip takes a lot of time and now right after we do that we
multiply by this constant so what happens then is we dispatch another kernel and then the result travels back
all the elements get multiplied by a constant and then the results travel back to the memory and then we take the
result and we add back input and so this entire thing again travels to the GPU
adds the inputs and gets written back so we're making all these round trips from
the memory to actually where the comput happens because all the tensor cores and alus and everything like that is all
stored on the chip in the GPU so we're doing a ton of round trips and pytorch uh without using torch compile doesn't
know to optimize this because it doesn't know what kind of operations you're running later you're just telling it
raise the power to the third then do this then do that and it will just do that in that sequence but torch compile
sees your entire code it will come here and it will realize wait all of these are elementwise operations and actually
what I'm going to do is I'm going to do a single trip of input to the GPU then for every single element I'm going to do
all of these operations while that memory is on the GPU or chunks of it
rather and then I'm going to write back a single time so we're not going to have these round trips and that's one example
of what's called kernel fusion and is a major way in which everything is sped up so basically if you have your benefit of
onet and you know exactly what you're going to compute you can optimize your round trips to the memory and you're not
going to pay the the memory bandwidth cost and that's fundamentally what makes some of these operations a lot faster
and what they mean by read writes here so let me erase this because we are
not using it and yeah we should be using torch compile and our code is now
significantly faster and we're doing about 125,000 tokens per second but we still
have a long way to go before we move on I wanted to supplement the discussion a little bit with a few more figures uh
because this is a complic topic but it's worth understanding on a high level uh what's happening here and I could
probably spend an entire video of like two hours on this but just the preview of that basically so this chip here that
is uh the GPU this chip is where all the calculations happen mostly but this chip
also does have some memory in it but most of the memory by far is here in the
high bandwidth memory hbm and is connected they're connected um but these
are two separate chips basically now here this is a zoom in of kind of
this cartoon diagram of a GPU and what we're seeing here is number one you see
this hbm I I realize it's probably very small for you but on the sides here it says hbm and so that that's the links to
the hbm now the hbm is again off chip on the chip there are a large number of
these streaming multiprocessors uh every one of these is an SM there's 120 of them in total and
this is where the a lot of the calculations happen and this is a zoom in of a single individual as it has
these four quadrants and see for example tensor core this is where a lot of the Matrix multiply stuff happens but
there's all these other units to do all different kinds of calculations for fp64 fp32 and for integers and so on now so
we have all this uh logic here to do the calculations but in addition to that on the chip there is memory sprinkled
throughout the chip so L2 cache is some amount of memory that lives on the chip
and then on the SMS themselves there's L1 cache I realized it's probably very small for you but this blue bar is L1
and there's also registers um and so there is memory stored here but the way
this memory is stored is very different from the way memory is stored in hbm uh this is a very different implementation
uh using um just in terms of like what the Silicon looks like it's a very different
implementation um so here you would using transistors and capacitors and here it's a very different
implementation uh with SRAM and what that looks like but long story short is
um there is um memory inside the chip but it's not a lot of memory that's the
critical point so this is some C this is a example diagram of a slightly different GPU just like here where it
shows that for example typical numbers for CPU Dam memory which is this thing here you might have one tab of this
right but it would be extremely expensive to access especially for a GPU you have to go through the CPU here now
next we have the hbm so we have tens of gigabytes of hbm memory on a typical GPU here but it's as I mentioned very
expensive to access and then on the chip itself everything is extremely fast
within the chip but we only have couple 10 megabytes of memory collectively
throughout the Chip And so there's just not enough space because the memory is very expensive on the chip and so
there's not a lot of it but it is lightning fast to access in relative terms and so basically whenever we have
these kernels um the more accurate picture of what's Happening Here is that
we take these inputs which live by default on the global memory and now we need to perform some calculation so we
start streaming the data from the um Global memory to the uh chip we perform
the calculations on the chip and then stream it back and store it back to the global memory right and so if we are if
we don't have torch compile we are streaming the data through the chip doing the calculations and saving to the memory and we're doing those round trips
many many times but uh if it's torch compiled then we start streaming the memory as before
but then while we're on the chip we're we're we have a chunk of the uh data
that we're trying to process so that chunk now lives on the chip while it's on the chip it's extremely fast to
operate on so if we have kernel Fusion we can do all the operations right there in an element-wise fashion and those are
very cheap and then we do a single round trip back to the global memory so
operator Fusion basically allows you to keep your chunk of data on the Chip And do lots of calculations on it before you
write it back and that gives huge savings and that's why torch compile ends up being a lot faster or that's one
of the major reasons uh so again just a very brief intro to the memory hierarchy and
roughly what torch compile does for you now torch compile is amazing but there are operations torch compile will not

å½“ç„¶å¯ä»¥ï¼Œä¸‹é¢æ˜¯å…³äº **`torch.compile`ã€Python å¼€é”€ã€Kernel èåˆã€ä»¥åŠæ€§èƒ½ä¼˜åŒ–æœºåˆ¶** çš„ä¸­æ–‡è§£é‡Šã€‚

---

## âœ… ä½¿ç”¨ `torch.compile` åŠ é€Ÿè®­ç»ƒ â€”â€” ä» 300ms é™åˆ° 130ms

---

### ğŸ“Œ `torch.compile` æ˜¯ä»€ä¹ˆï¼Ÿ

PyTorch å›¢é˜Ÿæ¨å‡ºçš„ä¸€ä¸ªé‡é‡çº§åŠŸèƒ½ï¼Œå®ƒæœ¬è´¨ä¸Šæ˜¯ï¼š

> â€œç¥ç»ç½‘ç»œçš„ç¼–è¯‘å™¨â€ï¼Œå°±åƒ `gcc` æ˜¯ C/C++ çš„ç¼–è¯‘å™¨ä¸€æ ·ã€‚

å®ƒå¯ä»¥ï¼š

* ç§»é™¤ Python è§£é‡Šå™¨çš„è¿è¡Œæ—¶å¼€é”€
* è‡ªåŠ¨ **åˆå¹¶å¤šä¸ª GPU kernel**ï¼ˆç§°ä¸º *kernel fusion*ï¼‰
* ä¼˜åŒ–è¯»å†™æ“ä½œï¼Œå‡å°‘å†…å­˜ä¼ è¾“
* è®©æ¨¡å‹æ‰§è¡Œå¾—æ›´å¿«ï¼Œç”šè‡³**æ— éœ€æ”¹åŠ¨æ¨¡å‹ä»£ç ** âœ…

---

### ğŸ§ª å¦‚ä½•ä½¿ç”¨ï¼Ÿ

ä»…éœ€ä¸€è¡Œä»£ç ï¼š

```python
model = torch.compile(model)
```

å³å¯å¯ç”¨æ‰€æœ‰ç¼–è¯‘ä¼˜åŒ–ã€‚

é¦–æ¬¡æ‰§è¡Œæ—¶ä¼šè¿›è¡Œä¸€æ¬¡ç¼–è¯‘ï¼ˆè€—æ—¶ç¨ä¹…ï¼‰ï¼Œä½†ä¹‹åæ‰€æœ‰ forward éƒ½æ˜¯é«˜åº¦ä¼˜åŒ–çš„æ‰§è¡Œã€‚

---

### â±ï¸ æé€Ÿæ•ˆæœï¼Ÿ

* ä½¿ç”¨ `bfloat16` å‰ï¼š**300ms/æ­¥**
* åŠ å…¥ `torch.compile` åï¼š**129ms/æ­¥**
* æé€Ÿçº¦ **2.3 å€**
* token ååé‡ä» **55,000 â 125,000 tokens/sec**

è¿™æ˜¯åœ¨ä¸æ”¹å˜æ¨¡å‹ç»“æ„ã€ä¸é‡å†™ä»£ç çš„å‰æä¸‹è·å¾—çš„åŠ é€Ÿï¼

---

## ğŸ’¡ ä¸ºä»€ä¹ˆä¼šåŠ é€Ÿï¼ŸèƒŒååšäº†ä»€ä¹ˆï¼Ÿ

### â‘  ç§»é™¤ Python è§£é‡Šå™¨çš„å¼€é”€

åœ¨æ™®é€š PyTorch æ¨¡å‹ä¸­ï¼Œæ¯ä¸€å±‚éƒ½ä¼šåœ¨ Python è§£é‡Šå™¨ä¸­é€æ¡æ‰§è¡Œï¼Œæ¯”å¦‚ï¼š

```python
x = layer1(x)
x = layer2(x)
x = layer3(x)
```

æ¯ä¸€å±‚éƒ½è¦é€šè¿‡ Python è°ƒåº¦ï¼Œè¿™å°±æœ‰å¤§é‡å‡½æ•°è°ƒç”¨ã€å¯¹è±¡ç®¡ç†ã€å†…å­˜åˆ†é…ç­‰å¼€é”€ã€‚

è€Œ `torch.compile` ä¼šæŠŠè¿™äº›æ“ä½œ**æå‰ç¼–è¯‘æˆä¸€ä¸ªæ•´ä½“æ‰§è¡Œçš„ç¨‹åº**ï¼Œåƒä¸€ä¸ªã€Œæ‰“åŒ…å¥½çš„ forward é€»è¾‘ã€ï¼Œè®© Python ä¸å†å¹²é¢„ï¼Œæé«˜æ‰§è¡Œæ•ˆç‡ã€‚

---

### â‘¡ Kernel Fusionï¼ˆç®—å­èåˆï¼‰

ä¸¾ä¾‹ï¼š

å‡è®¾ä½ å†™äº†å¦‚ä¸‹æ“ä½œï¼š

```python
y = x ** 3 * 0.0447 + x
```

åœ¨æ™®é€š PyTorch ä¸­ï¼Œè¿™ä¸‰æ­¥æ˜¯ä¸‰æ¬¡ **GPU kernel è°ƒç”¨**ï¼š

* ç¬¬ä¸€æ­¥ `x ** 3`ï¼šå¯åŠ¨ä¸€ä¸ª kernel
* ç¬¬äºŒæ­¥ `* 0.0447`ï¼šå†æ¥ä¸€æ¬¡
* ç¬¬ä¸‰æ­¥ `+ x`ï¼šå†å¯åŠ¨ä¸€æ¬¡

æ¯æ¬¡éƒ½è¦ï¼š

* æŠŠæ•°æ®ä» GPU å…¨å±€å†…å­˜ï¼ˆHBMï¼‰è¯»åˆ°èŠ¯ç‰‡ä¸Š
* è®¡ç®—
* å†™å›å†…å­˜ï¼ˆHBMï¼‰

è¿™æ˜¯å¾ˆæ…¢çš„ï¼Œ**å†…å­˜å¸¦å®½æ˜¯æœ€å¤§ç“¶é¢ˆ**ã€‚

> âš ï¸ å³ä½¿ä½ ç”¨äº† A100 æ˜¾å¡ï¼Œå…¶ HBM é€Ÿåº¦å†å¿«ï¼Œä¹Ÿè¿œä¸å¦‚èŠ¯ç‰‡å†…éƒ¨ç¼“å­˜ï¼ˆL1/L2ï¼‰å¿«ï¼

#### âœ… `torch.compile` èƒ½è¯†åˆ«è¿™äº›æ“ä½œæ˜¯ã€Œé€å…ƒç´ è®¡ç®—ã€ï¼ˆelement-wiseï¼‰

å®ƒä¼š**æŠŠå®ƒä»¬åˆå¹¶æˆä¸€ä¸ª GPU kernel**ï¼Œåœ¨æ•°æ®è¿˜æ²¡å†™å›ä¹‹å‰ï¼Œåœ¨èŠ¯ç‰‡ä¸Šè¿ç»­åšå®Œæ‰€æœ‰æ“ä½œï¼Œåªå†™å›ä¸€æ¬¡ã€‚

è¿™ä¸ªå« **ç®—å­èåˆï¼ˆkernel fusionï¼‰**ï¼Œå¤§å¹…å‡å°‘ GPU å†…å­˜è¯»å†™æ—¶é—´ï¼Œæ˜¯æ€§èƒ½æå‡çš„æ ¸å¿ƒæ¥æºä¹‹ä¸€ã€‚

---

### â‘¢ é¿å…ä¸å¿…è¦çš„è¯»å†™

PyTorch é»˜è®¤æ¯ä¸ªä¸­é—´ç»“æœéƒ½å†™å›å†…å­˜ã€‚
ä½† `torch.compile` èƒ½æ™ºèƒ½åˆ†æä¾èµ–å…³ç³»ï¼Œåªä¿ç•™å¿…è¦çš„ä¸­é—´å˜é‡ï¼Œå¤§å¤§å‡å°‘ä¸´æ—¶å†…å­˜åˆ†é…å’Œæ•°æ®æ¬è¿ã€‚

---

## ğŸ§  GPU å†…å­˜ç»“æ„å›é¡¾

* **HBMï¼ˆHigh Bandwidth Memoryï¼‰**ï¼šæ˜¾å¡ä¸»å†…å­˜ï¼ˆå‡ å GBï¼‰ï¼Œä½†è®¿é—®æ…¢ï¼ˆå°½ç®¡å¸¦å®½é«˜ï¼‰
* **L2/L1 cache & registers**ï¼šèŠ¯ç‰‡å†…å­˜ï¼Œé€Ÿåº¦æå¿«ï¼Œä½†å®¹é‡æå°ï¼ˆå‡  MBï¼‰
* **ç›®æ ‡ï¼šå°½é‡è®©æ•°æ®åœ¨èŠ¯ç‰‡å†…å®Œæˆè®¡ç®—ï¼Œåªåšä¸€æ¬¡ HBM è¯»å†™**

---

## âœ… æ€»ç»“

| ä¼˜åŒ–          | torch.compile å¸¦æ¥çš„æ•ˆæœ |
| ----------- | ------------------- |
| Python è°ƒåº¦   | âœ… ç§»é™¤                |
| kernel è°ƒç”¨æ¬¡æ•° | âœ… èåˆç®—å­ï¼Œåˆå¹¶ kernel    |
| å†…å­˜è®¿é—®        | âœ… å‡å°‘è¯»å†™ï¼Œæœ€å¤§åŒ–ä½¿ç”¨èŠ¯ç‰‡å†…ç¼“å­˜   |
| æ€§èƒ½æå‡        | âœ… çº¦ 2ï½3 å€ï¼Œä»…ä¸€è¡Œä»£ç      |

---

## ğŸ’¬ å°ç»“

ä½¿ç”¨ `torch.compile` æ˜¯ç°ä»£ PyTorch æ¨¡å‹è®­ç»ƒçš„æ¨èå®è·µä¹‹ä¸€ï¼š

* å‡ ä¹**é›¶ä»£ç æ”¹åŠ¨**
* å…è´¹è·å¾—å¤§å¹…åŠ é€Ÿ
* æé«˜æ˜¾å­˜åˆ©ç”¨æ•ˆç‡
* ç»“åˆ `autocast` + `bfloat16` ä½¿ç”¨æ•ˆæœæ›´ä½³

---

å¦‚æœä½ è¿˜æƒ³ç»§ç»­äº†è§£ï¼š

* `torch.compile` çš„è¿è¡Œæœºåˆ¶
* å®ƒç”¨åˆ°çš„åç«¯ï¼ˆå¦‚ TorchDynamoã€AOTAutogradã€Inductorï¼‰
* æˆ–è€…å¤š GPU åˆ†å¸ƒå¼ä¼˜åŒ–

æˆ‘ä¹Ÿå¯ä»¥è¿›ä¸€æ­¥å¸®ä½ è§£é‡Šã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ

å¥½é—®é¢˜ ğŸ‘
ä½ è´´çš„è¿™æ®µå†…å®¹æœ¬è´¨ä¸Šæ˜¯åœ¨è§£é‡Š **`torch.compile` ä¸ºä»€ä¹ˆèƒ½è®© PyTorch ä»£ç åŠ é€Ÿ**ã€‚æˆ‘æ¥å¸®ä½ æ‹†å¼€é‡ç‚¹ï¼Œé€å±‚è§£é‡Šã€‚

---

## 1. `torch.compile` æ˜¯ä»€ä¹ˆ

* PyTorch 2.0 å¼•å…¥çš„æ–°ç‰¹æ€§ã€‚
* ä½œç”¨ï¼šæŠŠæ¨¡å‹çš„ **Python ä»£ç ï¼ˆé€å±‚ forward è¿ç®—ï¼‰** ç¼–è¯‘æˆä¸€ä¸ªä¼˜åŒ–åçš„è®¡ç®—å›¾ï¼Œç„¶åæ‰§è¡Œã€‚
* ç±»ä¼¼äº **GCC ç¼–è¯‘ C ç¨‹åº** â†’ å®ƒæŠŠä½ çš„ PyTorch ä»£ç å˜æˆæ›´åº•å±‚ã€æ›´é«˜æ•ˆçš„æ‰§è¡Œä»£ç ã€‚

ä¸€å¥è¯ï¼š**`torch.compile` æ˜¯ PyTorch çš„ JIT ç¼–è¯‘å™¨ï¼Œå¯ä»¥å‡å°‘ Python å¼€é”€ã€èåˆç®—å­ï¼ˆkernel fusionï¼‰ã€å‡å°‘æ˜¾å­˜è¯»å†™ï¼Œä»è€Œæ˜¾è‘—åŠ é€Ÿæ¨¡å‹è¿è¡Œã€‚**

---

## 2. ä¸ºä»€ä¹ˆé»˜è®¤ PyTorch æ…¢

åœ¨æ²¡æœ‰ `torch.compile` æ—¶ï¼š

1. Forward Pass æ˜¯ Python è§£é‡Šå™¨é€è¡Œæ‰§è¡Œï¼š

   * `y = x ** 3` â†’ è°ƒç”¨ä¸€ä¸ª CUDA kernel
   * `y = y * const` â†’ åˆè°ƒä¸€ä¸ª CUDA kernel
   * `y = y + x` â†’ å†è°ƒä¸€ä¸ª CUDA kernel
     æ¯ä¸€æ­¥éƒ½ä¼šæ¶‰åŠ GPU å†…å­˜ â†’ GPU æ ¸å¿ƒ â†’ GPU å†…å­˜çš„å¾€è¿”ã€‚

2. è¿™äº›ã€Œå¾€è¿”ã€å¾ˆè´µï¼š

   * GPU èŠ¯ç‰‡å†…éƒ¨ï¼ˆSM, Tensor Coreï¼‰è®¡ç®—è¶…å¿«ã€‚
   * ä½†å¤§å¤šæ•°æ•°æ®å­˜åœ¨äº **æ˜¾å­˜ï¼ˆHBMï¼Œé«˜å¸¦å®½å†…å­˜ï¼‰**ã€‚
   * æ¯æ¬¡ç®—å®Œéƒ½è¦æŠŠç»“æœå†™å›æ˜¾å­˜ï¼Œå†ä»æ˜¾å­˜è¯»å‡ºæ¥ â†’ éå¸¸è€—æ—¶ã€‚

---

## 3. `torch.compile` åšäº†ä»€ä¹ˆä¼˜åŒ–

### (1) å»æ‰ Python è§£é‡Šå™¨å¼€é”€

* é»˜è®¤ï¼šPython ä¸€è¡Œè¡Œè°ƒåº¦è¿ç®—ï¼Œæ…¢ã€‚
* `torch.compile`ï¼šæŠŠæ•´ä¸ª forward ç¼–è¯‘æˆä¼˜åŒ–è¿‡çš„å›¾ï¼Œç›´æ¥è¿è¡Œ â†’ é¿å… Python overheadã€‚

### (2) Kernel Fusionï¼ˆç®—å­èåˆï¼‰

* é»˜è®¤ï¼šæ¯ä¸ªé€å…ƒç´ è¿ç®—ï¼ˆ+ã€-ã€\*ã€expã€relu ç­‰ï¼‰éƒ½è§¦å‘ä¸€ä¸ª CUDA kernelã€‚
* é—®é¢˜ï¼šæ¯ä¸ª kernel éƒ½è¦ã€Œä»æ˜¾å­˜ â†’ èŠ¯ç‰‡ â†’ å›å†™æ˜¾å­˜ã€ä¸€æ¬¡ã€‚
* ä¼˜åŒ–ï¼š`torch.compile` çœ‹åˆ°æ•´ä¸ªè®¡ç®—å›¾åï¼ŒæŠŠå¤šä¸ªé€å…ƒç´ è¿ç®—èåˆæˆ **ä¸€ä¸ª kernel**ã€‚

  * å†…å­˜åªéœ€è¦è¯»ä¸€æ¬¡ã€å†™ä¸€æ¬¡ã€‚
  * ä¸­é—´è®¡ç®—ç»“æœç›´æ¥å­˜åœ¨ **å¯„å­˜å™¨/L1/L2 cache**ï¼ˆåœ¨ GPU èŠ¯ç‰‡ä¸Šï¼‰ï¼Œä¸ç”¨å›åˆ°æ˜¾å­˜ã€‚

ğŸ‘‰ è¿™æ ·å°±çœä¸‹äº†å¤§é‡æ˜¾å­˜å¸¦å®½å’Œå»¶è¿Ÿã€‚

---

## 4. å®é™…æ•ˆæœ

* æ–‡ä¸­ä¾‹å­ï¼š

  * æœªä¼˜åŒ–ï¼š**300ms/iteration**
  * åŠ ä¸Š `torch.compile`ï¼š**129ms/iteration**
  * é€Ÿåº¦æå‡çº¦ **2.3x**

* ä¸€èˆ¬è®­ç»ƒä¸­ï¼Œæå‡å¹…åº¦å–å†³äºï¼š

  * æ¨¡å‹å¤§å°
  * è¿ç®—ç±»å‹ï¼ˆç®—å­å¤šä¸å¤šã€èƒ½ä¸èƒ½èåˆï¼‰
  * GPU æ¶æ„ï¼ˆAmpere/Hopper ä¸Šæ•ˆæœæ›´å¥½ï¼‰

---

## 5. èƒŒåçš„ç¡¬ä»¶åŸç†ï¼ˆç®€åŒ–ç‰ˆï¼‰

GPU å†…å­˜å±‚æ¬¡ï¼š

1. **HBM (High Bandwidth Memory)**ï¼šå‡ å GBï¼Œå­˜æ¨¡å‹å’Œæ•°æ®ï¼Œé€Ÿåº¦æ¯” CPU RAM å¿«å¾ˆå¤šï¼Œä½†è¿˜æ˜¯æ¯”èŠ¯ç‰‡å†…éƒ¨å­˜å–æ…¢ã€‚
2. **L2/L1 Cache & Registersï¼ˆåœ¨ GPU èŠ¯ç‰‡ä¸Šï¼‰**ï¼šå®¹é‡å°ï¼ˆMB çº§ï¼‰ï¼Œä½†æ˜¯éå¸¸å¿«ã€‚

ä¼˜åŒ–ç‚¹ï¼š

* **é»˜è®¤æ¨¡å¼**ï¼šæ¯ä¸ªè¿ç®—éƒ½è¦æŠŠæ•°æ®å†™å› HBMï¼Œå†è¯»å‡ºæ¥ã€‚
* **Kernel Fusion**ï¼šæ•°æ®åŠ è½½åˆ°èŠ¯ç‰‡ â†’ åœ¨å¯„å­˜å™¨é‡Œå®Œæˆä¸€ç³»åˆ—è®¡ç®— â†’ ä¸€æ¬¡æ€§å†™å›ã€‚

---

## 6. æ€»ç»“

* **ä¸ºä»€ä¹ˆå¿«ï¼Ÿ**

  1. å»æ‰ Python è§£é‡Šå™¨è°ƒåº¦ï¼ˆå‡å°‘ Python overheadï¼‰ã€‚
  2. Kernel Fusionï¼ˆå‡å°‘ GPU æ˜¾å­˜çš„åå¤è¯»å†™ï¼‰ã€‚
* **æ”¶ç›Š**ï¼š

  * æ›´é«˜ååï¼ˆè®­ç»ƒè¿­ä»£æ—¶é—´å‡å°‘ï¼‰ã€‚
  * æ˜¾å­˜å¸¦å®½å‹åŠ›å‡å°ã€‚
* **ä»£ä»·**ï¼š

  * ç¬¬ä¸€æ¬¡ç¼–è¯‘éœ€è¦é¢å¤–æ—¶é—´ï¼ˆcompilation overheadï¼‰ã€‚
  * å¹¶ä¸æ˜¯æ‰€æœ‰ç®—å­éƒ½èƒ½ä¼˜åŒ–ã€‚

---

âœ… ä¸€å¥è¯æ€»ç»“ï¼š
`torch.compile` å°±æ˜¯ PyTorch çš„ **ç¼–è¯‘ä¼˜åŒ–å™¨**ï¼Œé€šè¿‡æ¶ˆé™¤ Python overhead å’Œ **kernel fusion**ï¼Œé¿å… GPU æ˜¾å­˜é¢‘ç¹è¯»å†™ï¼Œè®©è®­ç»ƒ/æ¨ç†å¿«äº† 2\~3 å€ã€‚

---

è¦ä¸è¦æˆ‘ç»™ä½ ç”»ä¸€å¼  **ã€Œæ™®é€šæ‰§è¡Œ vs torch.compile æ‰§è¡Œã€çš„ GPU å†…å­˜æµè½¬ç¤ºæ„å›¾**ï¼Œè®©ä½ æ›´ç›´è§‚åœ°çœ‹åˆ°å·®åˆ«ï¼Ÿ



# flash attention, 96ms

find and an amazing example of that is Flash attention to which we turn next so
flash attention comes from this paper from uh Stanford in 2022 and it's this incredible algorithm
for performing attention so um and running it a lot faster so flash
attention will come here and we will take out these four lines and Flash attention implements
these four lines really really quickly and how does it do that well flash attention is a kernel Fusion operation
so you see here we have um in this diagram they're showing P torch and you
have these four operations uh they're including Dropout but we are not using Dropout here so we just have these four
lines of code here and instead of those we are fusing them into a single fused
kernel of flash attention so it's an it's a it's a kernel Fusion algorithm
but it's a kernel Fusion that torch compile cannot find and the reason that it cannot find it is
that it um requires an algorithmic rewrite of how attention is actually implemented here in this case and what's
remarkable about it is that uh flash attention actually if you just count the number of flops flash attention does
more flops than this attention here but flash attention is actually
significantly faster in fact they site 7. six times faster potentially and
that's because it is very mindful of the memory hierarchy as I described it just
now and so it's very mindful about what's in high bandwidth memory what's in the shared memory and it is very
careful with how it orchestrates the computation such that we have fewer reads and writes to the high bandwidth
memory and so even though we're doing more flops the expensive part is they load and store into hbm and that's what
they avoid and so in particular they do not ever materialize this end byend
attention Matrix this ATT here a flash attention is designed such that this
Matrix never gets materialized at any point and it never gets read or written to the hbm and this is a very large
Matrix right so um because this is where all the queries and keys interact and we're sort of getting
um for each head for each batch element we're getting a t BYT Matrix of
attention which is a Million numbers even for a single head at a single batch index at like so so basically this is a
ton of memory and and this is never materialized and the way that this is achieved is that basically the
fundamental algorithmic rewrite here relies on this online softmax trick which was proposed previously and I'll
show you the paper in a bit and the online softmax trick coming from a previous paper um shows how you can
incrementally evaluate a soft Max without having to sort of realize all of
the inputs to the softmax to do the normalization and you do that by having these intermediate variables M and L and
there's an update to them that allows you to evaluate the softmax in an online manner um now flash attention actually
so recently flash attention 2 came out as well so I have that paper up here as well uh that has additional gains to how
it calculates flash attention and the original paper that this is based on basically is this online normalizer
calculation for softmax and remarkably it came out of Nvidia and it came out of it like really early 2018 so this is 4
years before flash attention and this paper says that we propose a
way to compute the classical softmax with fewer memory accesses and hypothesize that this reduction in memory accesses should improve softmax
performance on actual hardware and so they are extremely correct in this
hypothesis but it's really fascinating to me that they're from Nvidia and that they had this realization but they
didn't actually take it to the actual flash attention that had to come four years later from Stanford so I don't
fully understand the historical how this happened historically um but they do basically propose this online update to
the softmax uh right here and this is fundamentally what they reuse here to
calculate the softmax in a streaming Manner and then they realize they can actually fuse all the other operations
with the online sofx calculation into a single fused kernel flash attention and that's what we are about to use so great
example I think of being aware of um memory hierarchy the fact that flops don't matter uh the entire memory access
pattern matters and that torch compile is amazing but there are many optimizations that are still available to us that potentially torch compile
cannot find maybe maybe one day it could but right now it seems like a lot to ask
so here's what we're going to do we're going to use Flash attention and the way to do that basically in pytorch is we
are going to comment out these four lines and we're going to replace them with a single line and here we are
calling this compound operation in pytorch called scale that product attention and uh pytorch will call flash
attention when you use it in this way I'm not actually 100% sure why torch
compile doesn't realize that these four lines should just call flash attention in this exact way we have to do it again
for it which in my opinion is a little bit odd but um here we are so you have
to use this compound up and uh let's wait for a few moments before torch comp
compile gets around to it and then let's remember that we achieved 6.05 661 I
have it here that's the loss we were expecting to see and we took 130 milliseconds uh before this change so
we're expecting to see the exact same result by iteration 49 but we expect to see faster runtime because Flash
attention is just a an algorithmic rewrite and it's a faster kernel but it doesn't actually change any of the computation and we should have the exact
same optimization so okay so we're a lot faster we're at about 95 milliseconds
and we achiev 6.58 okay so they're basically identical
up to a floating Point fudge Factor so it's the identical computation but it's
significantly faster going from 130 to roughly 90 96 and so this is um 96 divide
130ish so this is maybe 27 is% Improvement um so uh really interesting
and that is Flash retention okay we are now getting to one of my favorite optimizations and it is simultaneously

å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯å…³äº **Flash Attention** çš„ä¸­æ–‡è§£é‡Šï¼Œå¸®åŠ©ä½ ç†è§£å®ƒæ˜¯å¦‚ä½•å°†è®­ç»ƒé€Ÿåº¦ä» **130ms é™åˆ° 96ms** çš„ï¼š

---

## ğŸš€ Flash Attention â€”â€” æ›´å¿«çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ é€Ÿè‡³ 96msï¼

---

### ğŸ“Œ Flash Attention æ˜¯ä»€ä¹ˆï¼Ÿ

Flash Attention æ˜¯ 2022 å¹´æ–¯å¦ç¦å¤§å­¦æå‡ºçš„ä¸€ç§ **é«˜æ•ˆè®¡ç®—æ³¨æ„åŠ›ï¼ˆattentionï¼‰** çš„ç®—æ³•ï¼Œå®ƒï¼š

* **å®Œå…¨é‡å†™äº†åŸæœ¬çš„æ³¨æ„åŠ›è®¡ç®—æ–¹å¼**
* **å°† 4 ä¸ªç‹¬ç«‹çš„æ“ä½œåˆå¹¶æˆä¸€ä¸ªä¼˜åŒ–è¿‡çš„ GPU Kernel**
* **æ˜¾è‘—å‡å°‘äº†å†…å­˜è¯»å†™ï¼Œè®­ç»ƒé€Ÿåº¦æå¤§æå‡**

åŸå§‹æ³¨æ„åŠ›å®ç°æœ‰å››æ­¥ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š

```python
att = Q @ K.T
att = softmax(att)
att = att @ V
out = dropout(att)
```

Flash Attention æŠŠè¿™å‡ æ­¥èåˆæˆä¸€ä¸ª **ç»Ÿä¸€çš„ GPU å†…æ ¸æ“ä½œï¼ˆfused kernelï¼‰**ï¼Œä¸€æ¬¡æ€§å®Œæˆï¼Œé€Ÿåº¦æå¿«ã€‚

---

### ğŸ§  ä¸ºä»€ä¹ˆèƒ½æ›´å¿«ï¼Ÿ

#### åŸå› ä¸€ï¼š**å†…å­˜è¯»å†™æ˜¯æœ€å¤§ç“¶é¢ˆ**

æ³¨æ„åŠ›ä¸­æœ€è´¹å†…å­˜çš„æ˜¯è¿™ä¸€æ­¥ï¼š

```python
att = Q @ K.T
```

è¿™ä¸ª `att` æ˜¯ä¸€ä¸ª `T Ã— T` çš„å¤§çŸ©é˜µï¼ˆT æ˜¯åºåˆ—é•¿åº¦ï¼Œå¦‚ 1024ï¼‰ï¼Œå®ƒè¦å­˜åˆ° **GPU çš„é«˜å¸¦å®½å†…å­˜ï¼ˆHBMï¼‰** ä¸­ï¼Œè¿™ä¸€æ­¥éå¸¸æ…¢ã€‚

Flash Attention é€šè¿‡ **ç®—æ³•é‡å†™**ï¼š

* å®Œå…¨é¿å…ç”Ÿæˆè¿™ä¸ª `att` çŸ©é˜µ
* ä¸å†æŠŠå®ƒå†™å…¥ / è¯»å‡º GPU å†…å­˜
* è€Œæ˜¯åœ¨è®¡ç®—æ—¶ **åœ¨çº¿ï¼ˆstreamingï¼‰è®¡ç®— softmax**
* é¿å…äº†æœ€å¤§å†…å­˜å¼€é”€ï¼Œé€Ÿåº¦è‡ªç„¶æš´æ¶¨

---

#### åŸå› äºŒï¼š**åœ¨çº¿ Softmax è®¡ç®—ï¼ˆOnline Softmaxï¼‰**

Flash Attention åˆ©ç”¨äº†ä¸€ä¸ª 2018 å¹´ NVIDIA æå‡ºçš„æŠ€å·§ â€”â€” **åœ¨çº¿ softmax å½’ä¸€åŒ–**ï¼š

* åŸæœ¬çš„ softmax éœ€è¦ç­‰æ‰€æœ‰ `QÂ·K^T` å€¼ç®—å‡ºæ¥åï¼Œæ‰èƒ½å½’ä¸€åŒ–
* åœ¨çº¿ softmax å¯ä»¥åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ç»´æŠ¤ä¸­é—´å˜é‡ `M` å’Œ `L`ï¼Œä¸€æ­¥æ­¥è®¡ç®— softmaxï¼Œ**æ— éœ€ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´çŸ©é˜µ**
* è¿™æ ·å°±èƒ½**è¾¹è®¡ç®—ã€è¾¹å½’ä¸€åŒ–ã€è¾¹ç”Ÿæˆç»“æœ**

è¿™ä¸ª trick æ˜¯ Flash Attention çš„æ ¸å¿ƒç®—æ³•çªç ´ç‚¹ã€‚

---

### ğŸ§ª å®é™…åŠ é€Ÿæ•ˆæœ

* ä½¿ç”¨ `torch.compile` åæ˜¯ **130ms / step**
* ä½¿ç”¨ Flash Attention åæ˜¯ **96ms / step**
* æé€Ÿçº¦ **26%**

å¹¶ä¸” loss å‡ ä¹ä¸€è‡´ï¼Œè®¡ç®—ç»“æœç­‰æ•ˆï¼Œåªæ˜¯æ›´å¿«ã€‚

---

### âš ï¸ ä¸ºä»€ä¹ˆ PyTorch é»˜è®¤æ²¡ç”¨ï¼Ÿ

è™½ç„¶ `torch.compile` å·²ç»ä¼šè‡ªåŠ¨èåˆå¾ˆå¤šæ“ä½œï¼Œä½† Flash Attentionï¼š

* éœ€è¦ **æ”¹å†™ç®—æ³•ç»“æ„**ï¼Œè€Œä¸ä»…ä»…æ˜¯èåˆç°æœ‰ kernel
* PyTorch é»˜è®¤çš„æ³¨æ„åŠ›å†™æ³•ç”¨çš„æ˜¯åŸºæœ¬æ“ä½œï¼Œæ— æ³•è‡ªåŠ¨è¯†åˆ«å¯æ›¿æ¢ä¸º Flash Attention
* ä½ éœ€è¦æ‰‹åŠ¨è°ƒç”¨ PyTorch æä¾›çš„ **`scaled_dot_product_attention()`**ï¼š

```python
from torch.nn.functional import scaled_dot_product_attention

out = scaled_dot_product_attention(q, k, v, is_causal=True)
```

è¿™æ ·æ‰èƒ½è°ƒç”¨ Flash Attention æ ¸å¿ƒä¼˜åŒ–é€»è¾‘ã€‚

---

## âœ… æ€»ç»“ï¼šFlash Attention çš„æ ¸å¿ƒä»·å€¼

| ç‰¹æ€§        | è¯´æ˜                     |
| --------- | ---------------------- |
| ç®—æ³•ä¼˜åŒ–      | é‡‡ç”¨åœ¨çº¿ softmaxï¼Œé¿å…ç”Ÿæˆå¤§çŸ©é˜µ   |
| Kernel èåˆ | å°†å¤šæ­¥æ“ä½œåˆæˆä¸€ä¸ª GPU å†…æ ¸       |
| å†…å­˜æ•ˆç‡      | æå¤§å‡å°‘é«˜å¸¦å®½å†…å­˜ï¼ˆHBMï¼‰è¯»å†™       |
| å®é™…æé€Ÿ      | çº¦ 25-30%ï¼Œè®­ç»ƒæ›´å¿«ï¼Œç»“æœä¸å˜     |
| é€‚ç”¨èŒƒå›´      | é•¿åºåˆ—ã€é«˜ batchã€GPTç±»æ¨¡å‹å°¤ä¸ºæ˜æ˜¾ |

---

å¦‚æœä½ è¿˜æƒ³è¿›ä¸€æ­¥äº†è§£ Flash Attention çš„ CUDA å®ç°åŸç†ã€FlashAttention-2 çš„æ–°ç‰¹æ€§ï¼Œæˆ–å¦‚ä½•åœ¨è‡ªå·±æ¨¡å‹ä¸­é›†æˆ Flash Attentionï¼Œæˆ‘ä¹Ÿå¯ä»¥ç»§ç»­ä¸ºä½ è®²è§£ã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ

å¾ˆå¥½ ğŸ‘
è¿™ä¸€æ®µæ˜¯åœ¨è§£é‡Š **FlashAttention** â€”â€” ä¸€ä¸ªä¸“é—¨ä¸º **Transformer çš„ Attention è®¡ç®—** è®¾è®¡çš„é«˜æ•ˆç®—æ³•ã€‚å®ƒä¹‹æ‰€ä»¥å¿«ï¼ŒåŸå› å’Œ `torch.compile` ç±»ä¼¼ï¼šéƒ½æ˜¯ä¸ºäº†å‡å°‘ **æ˜¾å­˜ (HBM) è¯»å†™**ï¼Œä½† FlashAttention æ›´è¿›ä¸€æ­¥ï¼Œç›´æ¥å¯¹ **Attention ç®—æ³•æœ¬èº«åšäº†æ”¹å†™**ã€‚æˆ‘ç»™ä½ åˆ†å±‚æ‹†è§£ï¼š

---

## 1. å›é¡¾æ™®é€š Attention è®¡ç®—

æ ‡å‡† **Scaled Dot-Product Attention** æœ‰å‡ ä¸ªæ­¥éª¤ï¼š

1. è®¡ç®— **QKáµ€** ï¼ˆå¾—åˆ° Attention Scoresï¼Œå¤§å°æ˜¯ `seq_len Ã— seq_len`ï¼‰ã€‚
2. å¯¹è¿™ä¸ªå¤§çŸ©é˜µåš **softmax**ã€‚
3. å¯¹ç»“æœä¹˜ä¸Š **V**ã€‚
4. å¯èƒ½è¿˜æœ‰ **dropout**ã€‚

é—®é¢˜ï¼š

* è¿™ä¸ª `seq_len Ã— seq_len` çŸ©é˜µï¼ˆATTï¼‰å¾ˆå¤§ï¼Œå°¤å…¶å½“ `seq_len=1024/2048` æ—¶ï¼Œæ¯ä¸ª head éƒ½è¦å­˜å‡ ç™¾ä¸‡ä¸ªæµ®ç‚¹æ•°ã€‚
* PyTorch é»˜è®¤ä¼š **æ˜¾å¼ materializeï¼ˆå­˜å‚¨ï¼‰è¿™ä¸ªçŸ©é˜µåˆ° HBMï¼ˆæ˜¾å­˜ï¼‰**ã€‚
* ç»“æœæ˜¯ï¼šå¤§é‡ **æ˜¾å­˜ I/O**ï¼ˆè¯»å†™ï¼‰ï¼Œæ¯”ç®—åŠ›æ›´è´µã€‚

---

## 2. FlashAttention çš„å…³é”®æ€æƒ³

### (1) ä¸å­˜å‚¨å¤§çŸ©é˜µ

* FlashAttention **ä»ä¸æ˜¾å¼å­˜å‚¨** `QKáµ€` è¿™ä¸ª `seq_len Ã— seq_len` çŸ©é˜µã€‚
* å®ƒç”¨ **æµå¼ï¼ˆonlineï¼‰softmax æŠ€å·§**ï¼šé€å—è®¡ç®— softmaxï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§æ‹¿åˆ°å®Œæ•´çš„è¾“å…¥ã€‚

### (2) ç®—å­èåˆ (kernel fusion)

* æŠŠ **QKáµ€ â†’ softmax â†’ ä¹˜ V** è¿™äº›æ­¥éª¤ **èåˆæˆä¸€ä¸ªå†…æ ¸**ã€‚
* æ•°æ®åªéœ€ **ä¸€æ¬¡è¯»å…¥ã€ä¸€æ¬¡å†™å›**ï¼Œè€Œä¸æ˜¯æ¯ä¸€æ­¥éƒ½å›å†™æ˜¾å­˜ã€‚

### (3) å†…å­˜å±‚æ¬¡ä¼˜åŒ–

* GPU å†…å­˜å±‚æ¬¡ï¼š

  * **HBMï¼ˆæ˜¾å­˜ï¼‰**ï¼šå¤§ã€ä½†æ…¢ã€‚
  * **å¯„å­˜å™¨/L1/L2 cacheï¼ˆç‰‡ä¸Šå†…å­˜ï¼‰**ï¼šå°ã€ä½†æå¿«ã€‚
* FlashAttention è®¾è®¡æ—¶å°±è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼š

  * å°½å¯èƒ½æŠŠæ•°æ®å­˜åœ¨ **ç‰‡ä¸Š SRAM / å¯„å­˜å™¨**ï¼Œ
  * é¿å…é¢‘ç¹å†™å…¥ HBMã€‚

---

## 3. ä¸ºä»€ä¹ˆæ›´å¿«ï¼ˆå³ä½¿ FLOPs æ›´å¤šï¼‰

* æ™®é€š Attention FLOPs è¾ƒå°‘ï¼Œä½† **I/O æˆæœ¬æé«˜**ï¼ˆå†™è¯»å¤§çŸ©é˜µï¼‰ã€‚
* FlashAttention FLOPs æ›´å¤šï¼ˆè®¡ç®—æ›´å¤æ‚ï¼‰ï¼Œä½† **I/O æå°‘**ã€‚
* åœ¨ GPU ä¸Šï¼Œç®—åŠ›å…¶å®ä¸è´µï¼ˆTFLOPS å¾ˆé«˜ï¼‰ï¼Œè€Œ **HBM å¸¦å®½æ‰æ˜¯ç“¶é¢ˆ**ã€‚
* æ‰€ä»¥ï¼Œ**ç®—åŠ›æ¢å†…å­˜** = æ›´å¿«ã€‚

---

## 4. å®é™…æ•ˆæœ

* ç¤ºä¾‹é‡Œï¼š

  * `torch.compile` åï¼š**130 ms / iteration**ã€‚
  * åŠ ä¸Š FlashAttentionï¼š**96 ms / iteration**ã€‚
  * **å†å¿«äº† \~27%**ã€‚

* è®ºæ–‡æŠ¥å‘Šï¼šFlashAttention æœ€å¤šå¯ä»¥æ¯”æ™®é€šå®ç°å¿« **7.6x**ï¼ˆå–å†³äº seq\_len å’Œç¡¬ä»¶ï¼‰ã€‚

---

## 5. ä¸ `torch.compile` çš„åŒºåˆ«

* `torch.compile`ï¼šè‡ªåŠ¨å‘ç°å¯èåˆçš„ç®—å­ â†’ Kernel Fusionã€‚
* FlashAttentionï¼š**éœ€è¦ç®—æ³•å±‚é¢çš„é‡å†™**ï¼ˆonline softmaxï¼‰ã€‚
* å› æ­¤ï¼Œ`torch.compile` **ä¸èƒ½è‡ªå·±å‘ç°** FlashAttentionï¼Œå¿…é¡»æ˜¾å¼è°ƒç”¨ **`scaled_dot_product_attention`** APIã€‚

---

## 6. æ€»ç»“

* **æ™®é€š Attention**ï¼šå­˜å¤§çŸ©é˜µ â†’ æ˜¾å­˜ I/O ç“¶é¢ˆã€‚
* **FlashAttention**ï¼š

  1. ä¸å­˜çŸ©é˜µï¼ˆonline softmaxï¼‰ã€‚
  2. ç®—å­èåˆã€‚
  3. ä¼˜åŒ–å†…å­˜å±‚æ¬¡ã€‚
* **ç»“æœ**ï¼šå³ä½¿ FLOPs â†‘ï¼Œé€Ÿåº¦è¿˜æ˜¯æ›´å¿«ã€‚

ä¸€å¥è¯æ€»ç»“ï¼š
ğŸ‘‰ FlashAttention æ˜¯ **ä¸º Transformer Attention ç‰¹åŒ–çš„é«˜æ•ˆå†…æ ¸**ï¼Œé€šè¿‡ç®—æ³•é‡å†™å’Œå†…å­˜è®¿é—®ä¼˜åŒ–ï¼ŒæŠŠè®­ç»ƒé€Ÿåº¦æå‡äº† 1.2x \~ 7.6xã€‚

---

è¦ä¸è¦æˆ‘ç»™ä½ ç”»ä¸€å¼ å¯¹æ¯”å›¾ï¼ˆæ™®é€š Attention vs FlashAttention çš„æ•°æ®æµï¼‰ï¼Œç›´è§‚å±•ç¤º **æ˜¾å­˜æµé‡å·®åˆ«**ï¼Ÿ



# nice/ugly numbers. vocab size 50257 â†’ 50304, 93ms

the dumbest and the most brilliant optimization and it's always a little bit surprising to me um anyway so
basically I mentioned a few minutes ago that there are some numbers that are nice and some numbers that are ugly so
64 is a beautiful nice number 128 is even nicer 256 is beautiful what makes
these numbers beautiful is that there are many powers of two inside them you can divide by two many times and uh
examples of ugly numbers are like 13 and 17 and something like that prime numbers numbers that are not even and so on and
so pretty much you always want to use nice numbers in all of your code that deals with neural networks or Cuda
because everything in Cuda Works in sort of like powers of two and lots of kernels are written in terms of powers
of Two And there are lots of blocks of sizes 16 and uh 64 and so on so
everything is written in those terms and you always have special case handling for all kinds of uh logic that U when
your inputs are not made of nice numbers so let's see what that looks like basically scan your code and look for
ugly numbers is roughly theistic so three times is kind of ugly um I'm not
100% sure maybe this can be improved but this is uh this is ugly and not ideal um four times is nice so that's uh
that's nice 1024 is very nice that's a power of two
12 is a little bit suspicious um not too many powers of two 768 is great 50, 257
is a really really ugly number um it's first of all it's odd so uh and there's
no not too many powers of two in there so this is a very ugly number and it's highly suspicious and then when we
scroll down all these numbers are nice and then here we have mostly nice numbers except for 25 so in this
configuration of gpt2 XL a number of heads is 25 uh that's a really ugly number that's an odd number and um
actually this did cause a lot of headaches for us recently when we're trying to optimize some kernels uh to run this fast um and required a bunch of
special case handling so basically these numbers are we have some ugly numbers
and some of them are easier to fix than others and in particular the voap size being 50257 that's a very ugly number
very suspicious and we want to fix it now when you when you fix these things uh one of the easy ways to do that is
you basically um increase the number until it's the nearest power of two that
you like so here's a much nicer number it's 50304 and why is that because 50304 can
be divided by 8 or by 16 or by 32
64 it can even be divided by 128 I think yeah so it's a very nice number um so
what we're going to do here is the GPT config and you see that we initialized B cap size to
50257 Let's override just that um element to be
50304 okay so everything else stays the same we're just increasing our vocabulary size so we're adding it's
almost like we're adding fake tokens uh so that book up size has powers of two inside it now actually what I'm doing
here by the way is I'm increasing the amount of computation that our network will be doing if you just count the the flops on like do the math of how many
flops we're doing we're going to be doing more flops and we still have to think through whether this doesn't break
anything but if I just run this uh let's see what we get uh currently this ran in
maybe 96.5 milliseconds per step I'm just kind
of like eyeballing it and let's see what kind of a result we're going to get uh while this is compiling let's
think through whether our code actually works okay when we increase the vocap size like this let's look at where vocap
size is actually used so we swing up to the inet and we see that it's used inside the embedding
table of course so all the way at the bottom of the Transformer and it's used at the classifier layer all the way at the top of the Transformer so in two
places and let's take a look and we're running at 93 so 93 milliseconds instead
of 96.5 so we are seeing a roughly yeah 4%
Improvement here uh by doing more calculations and the reason for this is
we fixed we've made an ugly number into a nice number let's I'm going to come
into the explanation for that a little bit again but for now let's just convince ourselves that we're not breaking anything when we do this so
first of all we've made the the wte the embedding table for the tokens we've made it larger it's almost like we
introduced more tokens at the bottom and these tokens are never used because the
gbt tokenizer only has tokens up to $50,000 256 and so we'll never index into the
rows that we've added so we're wasting a little bit of space here by creating memory that's never going to be accessed
never going to be used Etc now that's not fully correct because this wte
weight ends up being shared and ends up being used in the classifier here at the end so what is that doing to the
classifier right here well what what that's doing is we're predicting additional Dimensions at the classifier
now and we're predicting probabilities for tokens that will of course never be present in the training set um and so
therefore the network has to learn that these probabilities uh have to be driven to zero and so the logits that the
network produces have to drive those dimensions of the output to negative Infinity but it that's no different from
all the other tokens that are already in our data set um or rather that are not in our data set so Shakespeare only
probably uses let's say a th000 tokens out of 50,000 to 57 tokens so most of the tokens are already being driven to
zero probability by the optimization we' just introduced a few more tokens now that in a similar manner will never be
used and have to be driven to zero in probability um so functionally though
nothing breaks we're using a bit more extra um memory but otherwise this is a
harmless operation as far as I can tell but and we're adding calculation but it's running faster and it's running
faster because as I mentioned in Cuda so many kernels use uh block tiles and
these block towels are usually nice numbers uh so powers of two so calculations are done in like chunks of
64 or chunks of 32 and when your um when your desired calculation doesn't neatly
fit into those block tiles um there are all kinds of boundary kernels that can
kick in to like do the last part so basically in a lot of kernels they will
chunk at up your input and they will do the nice part first and then they have a whole second second phase where they
come back to any that like uh remains uh and then they process the remaining part
and the kernels for that could be very inefficient and so you're basically um spinning up all this extra compute and
is extremely inefficient so you might as well pad your inputs and um make it fit
nicely and usually that empiric lens up actually running faster um so this is
another example of a 4% Improvement that we've added and this is something that
also torch compile did not find for us you would hope that torch compile at some point could figure an optimization
like this out uh but for now uh this is it and I also have to point out that we're using pytorch nightly so that's
why we're only seeing 4% if you're using pytorch 2.3.1 or earlier you would
actually see something like 30% Improvement just from this change from changing it to from 50,000 to 57 to
50304 so again one of my favorite examples also of having to understand
the under the hood and how it all works and to know what kinds of things to Tinker with to push the performance of your code okay so at this point we have

å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯å…³äº **ã€Œå°†è¯è¡¨å¤§å°ä» 50257 æ”¹ä¸º 50304ï¼Œæé«˜æ€§èƒ½ã€** çš„ä¸­æ–‡è§£é‡Šï¼š

---

## ğŸ§  å¥½æ•°ï¼ˆnice numbersï¼‰vs. åæ•°ï¼ˆugly numbersï¼‰

åœ¨æ·±åº¦å­¦ä¹ å’Œ GPU ç¼–ç¨‹ï¼ˆç‰¹åˆ«æ˜¯ CUDAï¼‰ä¸­ï¼Œæœ‰äº›æ•°å­—è¢«ç§°ä¸ºã€Œ**å¥½æ•°ï¼ˆnice numbersï¼‰**ã€ï¼Œæœ‰äº›æ˜¯ã€Œ**åæ•°ï¼ˆugly numbersï¼‰**ã€ã€‚

### ä»€ä¹ˆæ˜¯å¥½æ•°ï¼Ÿ

* å…·æœ‰å¾ˆå¤š **2 çš„å€æ•°å› å­ï¼ˆå³ï¼šå¯è¢« 2ã€4ã€8ã€16... æ•´é™¤ï¼‰**
* å¦‚ï¼š64ã€128ã€256ã€1024ï¼Œè¿™äº›éƒ½éå¸¸é€‚åˆ CUDA å†…æ ¸è¿è¡Œ
* å› ä¸º GPU çš„è®¡ç®—å•å…ƒã€å†…å­˜å—ç­‰éƒ½æ˜¯æŒ‰ç…§è¿™äº›ã€Œå—å¤§å°ã€è®¾è®¡çš„ï¼Œå¤„ç†è¿™ç±»æ•°éå¸¸é«˜æ•ˆ

### ä»€ä¹ˆæ˜¯åæ•°ï¼Ÿ

* **å¥‡æ•°æˆ–è´¨æ•°ï¼ˆå¦‚ 13ã€17ã€25ã€50257ï¼‰**
* **ä¸èƒ½æ•´é™¤å†…æ ¸å—å¤§å°ï¼Œä¼šå¯¼è‡´é¢å¤–å¼€é”€**
* ä¼šè§¦å‘ GPU å†…æ ¸ä¸­çš„ã€Œç‰¹æ®Šå¤„ç†é€»è¾‘ã€ï¼Œé™ä½æ•ˆç‡

---

## ğŸ§© é—®é¢˜ï¼šè¯è¡¨å¤§å°æ˜¯ 50257ï¼Œè¿™æ˜¯ä¸ªã€Œåæ•°ã€

GPT-2 çš„åŸå§‹è¯è¡¨å¤§å°æ˜¯ `50257`ï¼Œè¿™æ˜¯ä¸ªéå¸¸ä¸‘çš„æ•°å­—ï¼š

* ä¸æ˜¯ 2 çš„å€æ•°
* åœ¨ CUDA ä¸­ä¼šå¼•èµ·é¢å¤–çš„å¤„ç†å¼€é”€ï¼ˆä¾‹å¦‚ï¼šæ— æ³•æ•´é½åˆ’åˆ†æˆ GPU çš„ block tilesï¼‰

---

## ğŸ’¡ è§£å†³æ–¹æ³•ï¼šæŠŠè¯è¡¨å¤§å°ã€Œå‘ä¸Šè¡¥é½ã€ä¸ºä¸€ä¸ªå¥½æ•°

ä½œè€…å°†è¯è¡¨å¤§å°æ”¹ä¸ºï¼š

```python
vocab_size = 50304
```

è¿™ä¸ªæ•°å¾ˆå¥½ï¼š

* 50304 å¯ä»¥è¢« 8ã€16ã€32ã€64ã€128 æ•´é™¤
* éå¸¸é€‚åˆ GPU å†…æ ¸è¿è¡Œï¼Œé¿å…äº†é¢å¤–è¾¹ç•Œå¤„ç†ï¼ˆboundary kernelï¼‰

---

## âœ… ä¼šä¸ä¼šå‡ºé”™ï¼Ÿä¼šä¸ä¼šå½±å“æ¨¡å‹è®­ç»ƒï¼Ÿ

ä¸ä¼šå‡ºé”™ã€‚åŸå› å¦‚ä¸‹ï¼š

### è¯è¡¨æ‰©å±•å¸¦æ¥çš„å½±å“ï¼š

1. **embedding å±‚ï¼š**

   * åŸæœ¬æœ‰ 50257 ä¸ª tokenï¼Œç°åœ¨å¢åŠ äº†ä¸€äº›ã€Œæ°¸è¿œä¸ä¼šä½¿ç”¨ã€çš„ tokenã€‚
   * å› ä¸º tokenizer æ°¸è¿œä¸ä¼šç”Ÿæˆ ID è¶…è¿‡ 50256 çš„ tokenï¼Œæ‰€ä»¥è¿™äº› embedding æ°¸è¿œä¸ä¼šè¢«ç´¢å¼•åˆ°ã€‚

2. **åˆ†ç±»å±‚ï¼ˆè¾“å‡ºå±‚ï¼‰ï¼š**

   * è¾“å‡ºç°åœ¨å˜æˆäº† 50304 ç»´ï¼Œæ¨¡å‹éœ€è¦è¾“å‡ºä¸€äº›ã€Œæ°¸è¿œä¸ä¼šå‡ºç°çš„ token çš„æ¦‚ç‡ã€
   * ç½‘ç»œä¼šè‡ªåŠ¨å­¦ä¼šæŠŠè¿™äº›æ¦‚ç‡å‹åˆ°æå°ï¼ˆlogits â†’ è´Ÿæ— ç©· â†’ softmax â‰ˆ 0ï¼‰
   * è·ŸåŸæœ¬å°±ä»æ²¡å‡ºç°çš„ token æ˜¯ä¸€æ ·çš„å¤„ç†æ–¹å¼

3. **é¢å¤–å¼€é”€ï¼š**

   * å ç”¨å°‘é‡å†…å­˜å’Œè®¡ç®—ï¼Œä½†å¸¦æ¥äº†æ€§èƒ½æå‡ï¼Œ**æ˜¯å€¼å¾—çš„ trade-off**

---

## ğŸ“ˆ æ€§èƒ½æå‡æ•ˆæœ

* åŸæœ¬æ¯ä¸€æ­¥è®­ç»ƒè€—æ—¶ï¼š**96.5ms**
* ä¿®æ”¹åè€—æ—¶ï¼š**93ms**
* **æå‡çº¦ 4%**

  * åœ¨ PyTorch nightly ç‰ˆæœ¬ä¸‹æ˜¯ 4%
  * åœ¨ PyTorch 2.3.1 æˆ–æ›´æ—©ç‰ˆæœ¬ä¸­ï¼Œè¿™ä¸ªä¿®æ”¹å¯å¸¦æ¥ **é«˜è¾¾ 30% æå‡**ï¼

ä¸ºä»€ä¹ˆä¼šæ›´å¿«ï¼Ÿ

* GPU å†…æ ¸æ›´å®¹æ˜“ã€Œæ•´å—å¤„ç†ã€è€Œä¸æ˜¯ã€Œè¾¹ç•Œå¤„ç†ã€
* é¿å…ä½¿ç”¨é¢å¤–æ…¢é€Ÿçš„å°¾éƒ¨é€»è¾‘
* è®¡ç®—æ›´é«˜æ•ˆï¼Œååé‡æå‡

---

## ğŸ§ª æ€»ç»“

| ä¼˜åŒ–é¡¹        | åŸæ¥       | ä¿®æ”¹å      | æ˜¯å¦æ›´å¿«   | åŸå›                            |
| ---------- | -------- | -------- | ------ | ---------------------------- |
| vocab size | 50257ï¼ˆä¸‘ï¼‰ | 50304ï¼ˆç¾ï¼‰ | âœ… æ˜¯çš„   | å¯æ•´é™¤ã€å‡å°‘è¾¹ç•Œå¤„ç†ã€å†…æ ¸æ›´é«˜æ•ˆ             |
| æ€§èƒ½å˜åŒ–       | 96.5ms   | 93ms     | æå‡çº¦ 4% | æ›´å°‘çš„ memory accessï¼Œkernel æ›´é«˜æ•ˆ |
| æ˜¯å¦æœ‰å‰¯ä½œç”¨ï¼Ÿ    | å¦        | å¦        | å®‰å…¨     | å¤šå‡ºæ¥çš„ token æ°¸è¿œä¸ä¼šè¢«ç”¨åˆ°ï¼Œè¾“å‡ºä¼šå­¦ä¼šå¿½ç•¥å®ƒä»¬ |

---

å¦‚ä½ å¸Œæœ›ï¼Œæˆ‘è¿˜å¯ä»¥ç»§ç»­ä¸ºä½ æ€»ç»“åç»­ä¼˜åŒ–æ–¹æ³•æˆ–æ•´ä¸ªè®­ç»ƒåŠ é€Ÿæµç¨‹ã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ


# SECTION 3: hyperpamaters, AdamW, gradient clipping

improved the performance by about 11x right because we started at about 1,000 milliseconds per step and we're now down
to like 93 milliseconds so that's uh quite good and we're uh doing a much
better job of utilizing our GPU resources so I'm going to now turn to more algorithmic changes uh and
improvements to the actual optimization itself and what we would like to do is we would like to follow the hyper parameters that are mentioned in the GP
G2 or gpt2 gpt3 paper now sadly gpt2 is
uh doesn't actually say too much it's very nice of them that they released the model weights and the code but the paper
itself is extremely vague as to the optimization details uh the code itself that they released as well the code
we've been looking at this is just the inference code so there's no training code here and very few hyp parameters so
this doesn't also tell us too much so for that we have to turn to the gpt3 paper and um in the depending of the
gpt3 paper um they have a lot more hyper parameters here for us to use and the
gpt3 paper in general is a lot more detailed as to uh all of the you know
small details that go into the model training but gpt3 U models were never released so gbt2 we have the weights but
no details and gpt3 we have lots of details but no weights so um but roughly
speaking gpt2 and gpt3 architectures are very very similar and um basically there
are very few changes the context length was expanded from 1024 to 2048 and that's kind of like the major change uh
and some of the hyper parameters around the Transformer have changed but otherwise they're pretty much the same model it's just that gpt3 was trained
for a lot longer on a bigger data set and uh has a lot more thorough evaluations uh and the gpt3 model is 175
billion instead of 1.6 billion um in the gpt2 so long story short we're going to
go to gp3 paper to follow along some the hyper parameters so to train all the
versions of gpt3 we use atom with beta 1 beta 2 of9 and .95 so let's swing over
here and make sure that the betas parameter which you can see here defaults to 0.9 and
999 is actually set to 0.9 and .95 and then the Epsilon parameter uh
you can see is the default is 1 in8 and this is also one in8 let's just uh put
it in so that works expit uh now next up they say we clip
the gra Global Norm of the gradient at 1.0 so what this is referring to is that
once we calculate the gradients right after l. backward um we basically have
the gradients at all the parameter tensors and what people like to do is
basically uh clip them to have some kind of a maximum Norm so in pytor this is fairly easy to do uh it's one line of
code here that we have to insert right after we calcul Cal the gradients and what this utility function is doing is
um it's calculating the global Norm of the parameters so every single par um
gradient on all the parameters you square it and you add it all up and you take a big square root of that and
that's the norm of the parameter V Vector basically it's the it's the
length of it if you if you'd like to look at it that way and we are basically making sure that its length is no more
than 1.0 and we're going to clip it and the reason that people like to use this is that uh sometimes you can get
unlucky during your optimization maybe it's a bad data batch or something like that and if you get very unlucky in the
batch you might get really high loss and really high loss could lead to a really high gradient and this could basically
uh shock your model and shock the optimization so people like to use a gradient Norm clipping uh to prevent the
model from um basically getting too big of shocks in terms of the gradient magnet ude and uh the upper bound it in
this way it's a bit of a hacky solution it's about like a patch on top of like deeper issues uh but uh people still do
it fairly frequently now the clip grad Norm Returns the norm of the gradient
which I like to always visualize uh because um it is useful information and
sometimes you can look at the norm of the gradient and if it's well behaved things are good if it's climbing things
are bad and they're destabilizing during training sometimes you could get a spike in the norm and that means there's some
kind of an issue or an instability so the norm here will be a
norm uh and let's do a uh 4f or something like
that and I believe this is just a float and so we should be able to uh print
that uh so that's Global gradient clipping now they go into the details of
the learning rate uh scheduler so they don't just use a fixed learning rate like we do here for 3 E4 but there's
actually basically a cosine DK learning rate schedule um it's got a warm-up and
it's got a cosine DEC to 10% over some Horizon
um and so we're going to implement uh this in a second I just like to see Norm
printed here okay there we go so what happened here is the norm is actually really high in the beginning 30 or so
and you see that as we continue training it kind of like stabilizes um at values below one um and
this is not that crazy uncommon for the norm to be high in the very first few stages basically What's Happening Here
is the model is completely random and so there's a ton of learning happening very early in the network but that learning
is kind of like um you know it's mostly learning the biases of the output tokens
and so it's a bit of an unstable time uh but the network usually stabilizes in a very few iterations so this looks very
relatively reasonable to me except usually I would expect this looks a little bit funky that we go from 28 to 6
to 2 and then to 10 um it's not completely insane but it's just kind of
a little bit funky um okay so let's now get to the learning rate schuer so the learning

å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯è¿™ä¸€éƒ¨åˆ†å†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## ğŸ“Œ ç¬¬ 3 èŠ‚ï¼šè¶…å‚æ•°ã€AdamWã€æ¢¯åº¦è£å‰ªï¼ˆHyperparameters, AdamW, Gradient Clippingï¼‰

---

### ğŸš€ 1. **æ•´ä½“èƒŒæ™¯ï¼šæ€§èƒ½å·²æå‡çº¦ 11 å€**

* ä¸€å¼€å§‹æ¯æ­¥è¿­ä»£è€—æ—¶çº¦ **1000ms**
* ç»è¿‡ä¼˜åŒ–ï¼ˆå¦‚ TF32ã€æ··åˆç²¾åº¦ã€torch.compileã€Flash Attention ç­‰ï¼‰ï¼Œç°åœ¨åªéœ€ **93ms**
* æ˜¾è‘—æå‡äº† GPU çš„ä½¿ç”¨æ•ˆç‡ âœ…

---

## ğŸ”§ 2. ä¼˜åŒ–ç®—æ³•ï¼šä» GPT-3 è®ºæ–‡ä¸­æå–è¶…å‚æ•°

### ğŸ¤” ä¸ºä»€ä¹ˆå‚è€ƒ GPT-3 è€Œä¸æ˜¯ GPT-2ï¼Ÿ

* **GPT-2 è®ºæ–‡**ï¼šå‡ ä¹æ²¡æœ‰å†™ä¼˜åŒ–ç»†èŠ‚ï¼Œåªå…¬å¸ƒäº†æ¨¡å‹æƒé‡å’Œæ¨ç†ä»£ç 
* **GPT-3 è®ºæ–‡**ï¼šæ²¡æœ‰å…¬å¼€æ¨¡å‹ï¼Œä½†**è¯¦ç»†åˆ—å‡ºæ‰€æœ‰è®­ç»ƒç»†èŠ‚å’Œè¶…å‚æ•°**

æ‰€ä»¥å°½ç®¡ GPT-3 çš„æ¨¡å‹æ²¡å¼€æºï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥ç”¨å®ƒçš„è®­ç»ƒè®¾ç½®ã€‚

---

## ğŸ§® 3. Adam ä¼˜åŒ–å™¨è®¾ç½®ï¼ˆAdamWï¼‰

GPT-3 ä½¿ç”¨çš„å‚æ•°æ˜¯ï¼š

```python
betas = (0.9, 0.95)
epsilon = 1e-8
```

* é»˜è®¤çš„ Adam æ˜¯ `(0.9, 0.999)`ï¼Œè¿™é‡ŒæŠŠç¬¬äºŒä¸ª beta å‡å°äº†ï¼Œä»£è¡¨æ›´å¿«åœ°è·Ÿéšæ¢¯åº¦æ–¹å‘çš„å˜åŒ–
* `epsilon` ä¿æŒä¸å˜ï¼ˆæ˜¯é˜²æ­¢é™¤ä»¥ 0 çš„ä¸€ä¸ªæå°å€¼ï¼‰

---

## âœ‚ï¸ 4. æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰

### ğŸš¨ é—®é¢˜ï¼š

* å¦‚æœæŸä¸ª batch çš„ loss å¾ˆé«˜ï¼Œå¯èƒ½å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸
* è¿™ä¼šã€Œå†²å‡»ã€æ¨¡å‹å‚æ•°ï¼Œé€ æˆè®­ç»ƒä¸ç¨³å®š

### âœ… è§£å†³ï¼š

åŠ å…¥ **å…¨å±€æ¢¯åº¦è£å‰ª**ï¼ˆclip grad normï¼‰ï¼š

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

å«ä¹‰ï¼š

* å°†æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦å½“ä½œä¸€ä¸ªå¤§å‘é‡ï¼Œç®—å®ƒçš„ **L2 èŒƒæ•°**
* å¦‚æœå®ƒå¤§äº `1.0`ï¼Œå°±æ•´ä½“ç¼©æ”¾ä½¿å…¶å˜æˆ 1.0
* å°±åƒæ§åˆ¶ã€Œä¸€æ¬¡æœ€å¤šèµ°å¤šè¿œã€ï¼Œé˜²æ­¢æ¨¡å‹ã€Œç‹‚è·³ã€

> è™½ç„¶è¿™æœ‰ç‚¹åƒæ˜¯å¯¹ deeper é—®é¢˜çš„ä¸´æ—¶ä¿®è¡¥ï¼Œä½†åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­ä»è¢«å¹¿æ³›ä½¿ç”¨ã€‚

### ğŸ“Š æ‰“å°æ¢¯åº¦èŒƒæ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰

ä¸ºäº†äº†è§£è®­ç»ƒä¸­æ˜¯å¦ç¨³å®šï¼Œå¯ä»¥æ‰“å°å‡ºå½“å‰çš„æ¢¯åº¦èŒƒæ•°ï¼š

```python
print(f"Grad norm: {grad_norm:.4f}")
```

è§‚å¯Ÿè¾“å‡ºï¼š

* è®­ç»ƒåˆšå¼€å§‹æ—¶ï¼Œæ¢¯åº¦ norm å¯èƒ½å¾ˆå¤§ï¼ˆ30+ï¼‰ï¼Œæ˜¯æ­£å¸¸çš„
* å‡ æ­¥åå®ƒä¼šå˜å¾—æ¯”è¾ƒå°å¹¶ç¨³å®šï¼ˆ<1ï¼‰

å¦‚æœå‡ºç° **å¿½é«˜å¿½ä½ã€å¤§è·³å˜**ï¼Œè¯´æ˜è®­ç»ƒå¯èƒ½ä¸ç¨³å®šï¼Œéœ€è¦æ£€æŸ¥æ•°æ®æˆ–å­¦ä¹ ç‡ç­‰è®¾ç½®ã€‚

---

æ¥ä¸‹æ¥ï¼Œè§†é¢‘è¿˜ä¼šä»‹ç» **å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¦‚ warmup + cosine decayï¼‰**ï¼Œæ˜¯è®­ç»ƒä¸­éå¸¸é‡è¦çš„ä¸€éƒ¨åˆ†ã€‚å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥ç»§ç»­ä¸ºä½ ç¿»è¯‘å’Œæ€»ç»“è¿™ä¸€éƒ¨åˆ†å†…å®¹ã€‚

éœ€è¦ç»§ç»­å—ï¼Ÿ

å¥½ï¼Œè¿™ä¸€æ®µåœ¨è®² **è®­ç»ƒä¼˜åŒ–è¿‡ç¨‹ä¸­çš„è¶…å‚æ•° (hyperparameters)ã€AdamWã€æ¢¯åº¦è£å‰ª (gradient clipping)**ã€‚æˆ‘å¸®ä½ åˆ†ç‚¹æ‹†è§£ï¼š

---

## 1. èƒŒæ™¯

* ä¸€å¼€å§‹ä»£ç ä¼˜åŒ–åªåšäº†ç¡¬ä»¶çº§åˆ«çš„æå‡ï¼ˆæ¯”å¦‚ `torch.compile`ã€FlashAttentionï¼‰ï¼Œè®­ç»ƒé€Ÿåº¦ä» **1000ms â†’ 93ms/step**ï¼Œæé€Ÿäº† \~11xã€‚
* æ¥ä¸‹æ¥è¦åšçš„æ˜¯ **ç®—æ³•å±‚é¢çš„ä¼˜åŒ–**ï¼Œå³ **ä¼˜åŒ–å™¨å’Œè¶…å‚æ•°è®¾ç½®**ï¼Œä»¥ä¾¿è®­ç»ƒæ›´ç¨³å®šã€æ›´é«˜æ•ˆã€‚

---

## 2. GPT-2 vs GPT-3 è¶…å‚æ•°æ¥æº

* **GPT-2**ï¼šè™½ç„¶å¼€æºäº†æƒé‡å’Œæ¨ç†ä»£ç ï¼Œä½† **è®­ç»ƒç»†èŠ‚å†™å¾—å¾ˆå°‘**ã€‚
* **GPT-3**ï¼šæ²¡æœ‰å¼€æºæƒé‡ï¼Œä½† **è®ºæ–‡é‡Œå†™äº†å¾ˆå¤šè®­ç»ƒç»†èŠ‚ï¼ˆè¶…å‚æ•°ï¼‰**ã€‚
  ğŸ‘‰ å› æ­¤ï¼Œåœ¨å¤ç°æ—¶é€šå¸¸å‚è€ƒ GPT-3 çš„è®­ç»ƒé…ç½®ã€‚

---

## 3. AdamW è¶…å‚æ•°

GPT-3 ä½¿ç”¨ **Adam ä¼˜åŒ–å™¨**ï¼ˆä¸¥æ ¼æ¥è¯´æ˜¯ AdamW = Adam + weight decayï¼‰ï¼Œå¹¶ä¸”è®¾ç½®äº†ï¼š

* **Î²1 = 0.9**

* **Î²2 = 0.95**
  ï¼ˆPyTorch é»˜è®¤ Î²2=0.999ï¼ŒGPT-3 æŠŠå®ƒæ”¹å°äº†ï¼Œè®©æ¨¡å‹å¯¹è¿‘æœŸæ¢¯åº¦æ›´æ•æ„Ÿï¼Œæ›´æ–°æ›´å¿«ä¸€äº›ï¼‰

* **Îµ (epsilon) = 1e-8**
  ï¼ˆé˜²æ­¢é™¤é›¶é”™è¯¯ï¼Œå’Œé»˜è®¤å€¼ä¸€æ ·ï¼Œä½†ä»–ä»¬æ˜ç¡®å†™å‡ºæ¥äº†ï¼‰

---

## 4. æ¢¯åº¦è£å‰ª (Gradient Clipping)

* åœ¨æ¯æ¬¡åå‘ä¼ æ’­ (`loss.backward()`) å¾—åˆ°æ¢¯åº¦ä¹‹åï¼Œé€šå¸¸æ¢¯åº¦å¤§å°ï¼ˆnormï¼‰ä¼šæœ‰æ³¢åŠ¨ã€‚
* å¦‚æœæŸä¸ª batch äº§ç”Ÿäº† **å¼‚å¸¸å¤§çš„æ¢¯åº¦**ï¼ˆå¯èƒ½æ˜¯åæ•°æ®ï¼Œæˆ–è€… loss çªç„¶æš´æ¶¨ï¼‰ï¼Œä¼šâ€œå†²å‡»â€æ¨¡å‹ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šç”šè‡³å‘æ•£ã€‚
* **è§£å†³åŠæ³•**ï¼šè£å‰ªæ¢¯åº¦çš„å…¨å±€èŒƒæ•°ï¼ˆglobal normï¼‰ï¼Œæ¯”å¦‚è®¾ä¸Šé™ = 1.0ã€‚

### åŸç†

* æ‰€æœ‰æ¢¯åº¦ flatten æˆä¸€ä¸ªå¤§å‘é‡ `g`ã€‚
* è®¡ç®— `||g||â‚‚ = sqrt(Î£ gáµ¢Â²)`ã€‚
* å¦‚æœ `||g||â‚‚ > 1.0`ï¼Œå°±æŒ‰æ¯”ä¾‹ç¼©å°ï¼Œä½¿å…¶æœ€å¤§å€¼ä¸è¶…è¿‡ 1.0ã€‚

PyTorch ä¸€è¡Œæå®šï¼š

```python
norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
```

è¿™é‡Œ `norm` ä¼šè¿”å›è£å‰ªå‰çš„æ¢¯åº¦èŒƒæ•°ï¼Œæ–¹ä¾¿æˆ‘ä»¬ç›‘æ§ã€‚

---

## 5. ä¸ºä»€ä¹ˆæœ‰ç”¨

* é˜²æ­¢æ¢¯åº¦â€œçˆ†ç‚¸â€ã€‚
* æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚
* å±äºä¸€ä¸ª â€œå®ç”¨çš„å°æŠ€å·§â€ï¼Œè™½ç„¶æœ¬è´¨ä¸Šæ˜¯ **patchï¼ˆè¡¥ä¸ï¼‰**ï¼Œå¹¶ä¸æ˜¯è§£å†³æ·±å±‚åŸå› ã€‚

---

## 6. å®é™…æ•ˆæœ

* ä½œè€…æ‰“å°äº† `gradient norm`ï¼Œç»“æœå‘ç°ï¼š

  * è®­ç»ƒåˆæœŸ norm = 20\~30 å¾ˆå¤§ï¼Œå› ä¸ºæ¨¡å‹éšæœºåˆå§‹åŒ–ï¼Œloss ä¹Ÿå¾ˆå¤§ï¼Œå­¦ä¹ ç‡çˆ†ç‚¸ã€‚
  * å‡ ä¸ª step ä¹‹å norm ä¼šæ”¶æ•›åˆ° < 1ï¼Œè¯´æ˜è®­ç»ƒé€æ¸ç¨³å®šã€‚
* å¶å°” norm ä¼šçªç„¶è·³åŠ¨ï¼ˆä» 2 â†’ 10ï¼‰ï¼Œä»£è¡¨è®­ç»ƒä¸­æœ‰ä¸ç¨³å®šå› ç´ ã€‚

---

## 7. ä¸‹ä¸€æ­¥ï¼ˆå­¦ä¹ ç‡è°ƒåº¦ï¼‰

* ç›®å‰ç”¨çš„æ˜¯å›ºå®šå­¦ä¹ ç‡ `3e-4`ï¼Œä½† GPT-3 ç”¨äº†æ›´å¤æ‚çš„è°ƒåº¦ï¼š

  * **Warmup**ï¼šå‰æœŸé€æ¸å¢å¤§å­¦ä¹ ç‡ï¼ˆé˜²æ­¢åˆšå¼€å§‹æ¢¯åº¦å¤ªå¤§çˆ†ç‚¸ï¼‰ã€‚
  * **Cosine Decay**ï¼šåæœŸå­¦ä¹ ç‡é€æ¸è¡°å‡åˆ°åˆå§‹å€¼çš„ 10%ã€‚
    ğŸ‘‰ è¿™æ ·å¯ä»¥è®©æ¨¡å‹è®­ç»ƒæ›´å¹³ç¨³ã€æ”¶æ•›æ›´å¥½ã€‚

---

## âœ… æ€»ç»“

1. **AdamW è¶…å‚æ•°**ï¼šÎ²1=0.9, Î²2=0.95, Îµ=1e-8ã€‚
2. **æ¢¯åº¦è£å‰ª**ï¼šæŠŠå…¨å±€æ¢¯åº¦èŒƒæ•°é™åˆ¶åœ¨ â‰¤1.0ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚
3. **ç›‘æ§ norm**ï¼šå¯ä»¥åˆ¤æ–­è®­ç»ƒæ˜¯å¦ç¨³å®šã€‚
4. **å­¦ä¹ ç‡è°ƒåº¦**ï¼šGPT-3 ä½¿ç”¨ **warmup + cosine decay**ï¼Œæ¯”å›ºå®šå­¦ä¹ ç‡æ•ˆæœæ›´å¥½ã€‚

---

è¦ä¸è¦æˆ‘ç»™ä½ å†™ä¸€æ®µ **å¸¦æ¢¯åº¦è£å‰ª + å­¦ä¹ ç‡è°ƒåº¦** çš„ PyTorch ä»£ç ï¼ˆå¤ç° GPT-3 çš„è®­ç»ƒé…ç½®ï¼‰ï¼Ÿ

# ||g||â‚‚ æ˜¯ä»€ä¹ˆæ„æ€

å¥½é—®é¢˜ ğŸ‘

è¿™é‡Œçš„ `||g||â‚‚` è¡¨ç¤ºçš„æ˜¯ **å‘é‡ g çš„ Lâ‚‚ èŒƒæ•°ï¼ˆä¹Ÿå« Euclidean normï¼Œæ¬§å‡ é‡Œå¾—èŒƒæ•°ï¼‰**ã€‚

---

### 1. å‘é‡ g æ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨æ¢¯åº¦è£å‰ªé‡Œï¼Œ`g` ä»£è¡¨ **æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦æ‹¼æ¥èµ·æ¥çš„å¤§å‘é‡**ã€‚
æ¯”å¦‚ï¼š

* å¦‚æœæ¨¡å‹æœ‰ 3 ä¸ªå‚æ•°ï¼Œæ¢¯åº¦åˆ†åˆ«æ˜¯ `[0.5, -1.0, 2.0]`ï¼Œé‚£ `g = [0.5, -1.0, 2.0]`ã€‚
* å®é™…ä¸Šæ¨¡å‹å¯èƒ½æœ‰å‡ ç™¾ä¸‡ä¸ªå‚æ•°ï¼Œæ‰€æœ‰æ¢¯åº¦éƒ½ä¼šæ‹¼æˆä¸€ä¸ªè¶…é•¿çš„å‘é‡ã€‚

---

### 2. Lâ‚‚ èŒƒæ•°çš„å®šä¹‰

æ•°å­¦é‡Œï¼š

$$
||g||_{2} = \sqrt{\sum_{i=1}^n g_i^2}
$$

æ„æ€æ˜¯ï¼šæŠŠæ‰€æœ‰åˆ†é‡å¹³æ–¹ååŠ èµ·æ¥ï¼Œå†å¼€å¹³æ–¹æ ¹ã€‚
è¿™å…¶å®å°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„ **å‘é‡çš„é•¿åº¦**ï¼ˆå‡ ä½•æ„ä¹‰ä¸Šæ˜¯åˆ°åŸç‚¹çš„ç›´çº¿è·ç¦»ï¼‰ã€‚

ä¸¾ä¸ªä¾‹å­ï¼š

* `g = [3, 4]`
* `||g||â‚‚ = sqrt(3Â² + 4Â²) = sqrt(25) = 5`

---

### 3. åœ¨æ¢¯åº¦è£å‰ªé‡Œä¸ºä»€ä¹ˆç”¨ Lâ‚‚ èŒƒæ•°ï¼Ÿ

* å› ä¸ºå®ƒèƒ½è¡¡é‡æ•´ä¸ªæ¢¯åº¦å‘é‡çš„â€œæ•´ä½“å¤§å°â€ã€‚
* å¦‚æœ `||g||â‚‚` å¤ªå¤§ï¼Œè¯´æ˜æ¢¯åº¦â€œçˆ†ç‚¸â€ã€‚
* è£å‰ªæ–¹æ³•å°±æ˜¯ï¼š

  $$
  g \leftarrow \frac{g}{||g||_2} \cdot \text{clip\_value}
  $$

  æ¯”å¦‚ `clip_value = 1.0`ï¼Œå°±æŠŠæ¢¯åº¦ç¼©æ”¾åˆ°é•¿åº¦ä¸è¶…è¿‡ 1.

---

âš¡ ç®€å•è®°å¿†ï¼š

* `||g||â‚‚` = æ¢¯åº¦å‘é‡çš„ **é•¿åº¦**ã€‚
* è£å‰ªæ¢¯åº¦ï¼Œå°±æ˜¯ **é˜²æ­¢è¿™æ¡â€œæ¢¯åº¦å‘é‡â€å¤ªé•¿ï¼Œç»™æ¨¡å‹å¸¦æ¥å†²å‡»**ã€‚

---

è¦ä¸è¦æˆ‘ç»™ä½ å†™ä¸ª **å° Python ä¾‹å­**ï¼Œç®—ä¸€ç»„æ¢¯åº¦çš„ `||g||â‚‚` å¹¶æ¼”ç¤ºæ€ä¹ˆè£å‰ªï¼Ÿ

# å¦‚ä½•æŒ‰æ¯”ä¾‹ç¼©å°

å¯¹çš„ âœ…
æ‰€è°“ **â€œæŒ‰æ¯”ä¾‹ç¼©å°â€**ï¼Œå°±æ˜¯æŠŠæ¢¯åº¦å‘é‡æ•´ä½“ **é™¤ä»¥å®ƒçš„é•¿åº¦**ï¼Œå†ä¹˜ä¸Šä¸€ä¸ªå…è®¸çš„æœ€å¤§é•¿åº¦ï¼ˆæ¯”å¦‚ 1.0ï¼‰ã€‚

---

### æ•°å­¦å…¬å¼

å¦‚æœåŸå§‹æ¢¯åº¦æ˜¯ $g$ï¼Œå®ƒçš„ Lâ‚‚ èŒƒæ•°æ˜¯

$$
||g||_2 = \sqrt{\sum_{i} g_i^2}
$$

è£å‰ªåçš„æ¢¯åº¦ $g'$ ä¸ºï¼š

$$
g' = g \cdot \frac{\text{clip\_value}}{||g||_2}
\quad \text{å¦‚æœ } ||g||_2 > \text{clip\_value}
$$

å¦åˆ™å°±ä¿æŒä¸å˜ã€‚

---

### ä¸¾ä¸ªä¾‹å­

å‡è®¾ï¼š

* æ¢¯åº¦å‘é‡ $g = [3, 4]$
* é‚£ä¹ˆ $||g||_2 = \sqrt{3^2+4^2} = 5$
* è®¾å®šæœ€å¤§å…è®¸é•¿åº¦ `clip_value = 1.0`

é‚£ä¹ˆè£å‰ªåï¼š

$$
g' = [3, 4] \times \frac{1.0}{5}
   = [0.6, 0.8]
$$

æ­¤æ—¶ `||g'||â‚‚ = 1.0`ï¼Œåˆšå¥½è¢«ç¼©æ”¾åˆ°æœ€å¤§å€¼ã€‚

---

### Python æ¼”ç¤º

```python
import torch

g = torch.tensor([3.0, 4.0])
clip_value = 1.0

norm = torch.norm(g, p=2)  # è®¡ç®— ||g||â‚‚
if norm > clip_value:
    g = g * (clip_value / norm)

print(g)  # tensor([0.6000, 0.8000])
print(torch.norm(g, p=2))  # 1.0
```

---

ğŸ‘‰ è¿™æ ·å°±ä¿è¯äº†ï¼š

* **æ–¹å‘ä¸å˜**ï¼ˆæ¢¯åº¦æ–¹å‘æ²¡å˜ï¼‰ã€‚
* **é•¿åº¦è¢«ç¼©åˆ° clip\_value ä»¥å†…**ã€‚

è¦ä¸è¦æˆ‘ä¹Ÿç»™ä½ ç”»ä¸ª **äºŒç»´å‘é‡ç¼©å°å‰åçš„å›¾**ï¼Œæ›´ç›´è§‚å±•ç¤ºï¼Ÿ

# è¯¦ç»†è®²è§£ä¸€ä¸‹ AdamW

å¥½çš„ï¼ä¸‹é¢æŠŠ **AdamW** è®²é€ï¼šå®ƒæ˜¯ä»€ä¹ˆã€ä¸ºä»€ä¹ˆæ¯” Adam æ›´å¥½ã€å…¬å¼æ€ä¹ˆå†™ã€æ€ä¹ˆåœ¨ PyTorch é‡Œç”¨ã€ä»¥åŠè·Ÿè¶…å‚ï¼ˆÎ²â‚/Î²â‚‚/Îµ/weight\_decayï¼‰ã€æ¢¯åº¦è£å‰ªã€å‚æ•°åˆ†ç»„ä¹‹é—´çš„æœ€ä½³å®è·µã€‚

---

# AdamW æ˜¯ä»€ä¹ˆï¼Ÿ

**AdamW = Adamï¼ˆè‡ªé€‚åº”ä¼˜åŒ–ï¼‰ + â€œè§£è€¦â€çš„æƒé‡è¡°å‡ï¼ˆWeight Decayï¼‰**ã€‚
å…³é”®ç‚¹åœ¨â€œ**è§£è€¦**â€ï¼šæŠŠæƒé‡è¡°å‡å½“ä½œ**å¯¹å‚æ•°æœ¬èº«çš„ç¼©å‡**ï¼Œè€Œä¸æ˜¯æŠŠ `Î»Â·w` åŠ åˆ°æ¢¯åº¦é‡Œï¼ˆL2 æ­£åˆ™ï¼‰ã€‚è¿™ç‚¹å¯¹è‡ªé€‚åº”æ–¹æ³•ï¼ˆAdam/Adamax/Adagradâ€¦ï¼‰å¾ˆé‡è¦ï¼Œå› ä¸ºæŠŠ L2 åŠ åˆ°æ¢¯åº¦é‡Œä¼šå’Œè‡ªé€‚åº”ç¼©æ”¾çº ç¼ ï¼Œå¯¼è‡´â€œåä¸º weight decayã€å®ä¸ºåˆ«çš„ä¸œè¥¿â€çš„å‰¯ä½œç”¨ã€‚AdamW æŠŠå®ƒ**ä»æ¢¯åº¦æ›´æ–°ä¸­æ‹†å‡ºæ¥å•ç‹¬åš**ï¼Œæ³›åŒ–æ›´å¥½ã€æ›´å¯æ§ã€‚

---

# ä¸ Adam çš„æœ¬è´¨åŒºåˆ«

* **Adamï¼ˆå¸¦ L2 æ­£åˆ™ï¼‰**ï¼š
  å…ˆæŠŠ `Î»Â·w` åŠ åˆ°æ¢¯åº¦é‡Œï¼Œå†èµ° Adam çš„è‡ªé€‚åº”æ›´æ–°ã€‚
* **AdamWï¼ˆè§£è€¦è¡°å‡ï¼‰**ï¼š
  å…ˆæŒ‰ Adam çš„è‡ªé€‚åº”è§„åˆ™æ›´æ–°å‚æ•°ï¼›**å¦èµ·ä¸€æ¡ç‹¬ç«‹çš„â€œè¡°å‡â€æ­¥**å¯¹å‚æ•°åš `w â† w - lrÂ·Î»Â·w`ï¼ˆæˆ–ç­‰ä»·çš„ `w â† (1 - lrÂ·Î»)Â·w`ï¼‰ã€‚
  è¿™æ ·è¡°å‡å¹…åº¦ä¸è¢« `vÌ‚`/`Îµ` ç­‰è‡ªé€‚åº”é¡¹å½±å“ï¼Œåå‰¯å…¶å®å°±æ˜¯â€œweight decayâ€ã€‚

---

# æ•°å­¦æ›´æ–°ï¼ˆä¸€æ­¥ï¼‰

ä»¤å½“å‰æ­¥æ¢¯åº¦ä¸º $g_t$ï¼Œå‚æ•°ä¸º $w_t$ï¼Œè¶…å‚ $\beta_1,\beta_2,\epsilon, \lambda$ï¼ˆæƒé‡è¡°å‡ï¼‰ï¼Œå­¦ä¹ ç‡ $\alpha$ã€‚

**Adam çš„ä¸€éƒ¨åˆ†ï¼ˆå¸¦åç½®æ ¡æ­£ï¼‰ï¼š**

$$
\begin{aligned}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \\
\hat m_t &= \frac{m_t}{1-\beta_1^t},\quad
\hat v_t = \frac{v_t}{1-\beta_2^t} \\
\tilde g_t &= \frac{\hat m_t}{\sqrt{\hat v_t}+\epsilon}
\end{aligned}
$$

**AdamW æ›´æ–°ï¼š**

$$
\underbrace{w_{t+\frac12} = w_t - \alpha \,\tilde g_t}_{\text{Adam è‡ªé€‚åº”æ­¥}}
\qquad
\underbrace{w_{t+1} = w_{t+\frac12} - \alpha \lambda \, w_{t+\frac12}}_{\text{è§£è€¦æƒé‡è¡°å‡}}
$$

ï¼ˆå®ç°é‡Œå¸¸å†™æˆå¯¹ $w_t$ åš `w -= lr*wd*w`ï¼Œä¸ä¸Šå¼ç­‰ä»·åˆ°ä¸€é˜¶ï¼‰

> è¦ç‚¹ï¼š**è¡°å‡å¹…åº¦ä¸æ¢¯åº¦æ— å…³**ï¼Œä¸ä¼šè¢« $\hat v_t$ã€$\epsilon$ ç¨€é‡Šæˆ–æ”¾å¤§ã€‚

---

# è¶…å‚æ•°æ€ä¹ˆé€‰ï¼ˆä»¥ GPT ç±»æ¨¡å‹ä¸ºä¾‹ï¼‰

* **betas**ï¼š

  * ç»å…¸é»˜è®¤ `Î²1=0.9, Î²2=0.999`ã€‚
  * **GPT-3** ç”¨ **`Î²1=0.9, Î²2=0.95`**ï¼ˆæ›´â€œçµæ•â€ï¼Œå¯¹è¿‘æœŸæ¢¯åº¦æ›´æ•æ„Ÿï¼Œé€‚åˆå¤§æ‰¹é‡ã€é•¿è®­ç»ƒï¼‰ã€‚
* **epsilonï¼ˆÎµï¼‰**ï¼šå¸¸ç”¨ `1e-8`ï¼›å¤ªå¤§ä¼šè®©åˆ†æ¯åå¤§ã€æ­¥é•¿å˜å°ã€‚
* **weight\_decayï¼ˆÎ»ï¼‰**ï¼šé¢„è®­ç»ƒå¸¸è§ **0.1**ï¼›å¾®è°ƒå¯ç”¨ **0.01 \~ 0.1**ã€‚

  * **ä¸è¦**å¯¹ `bias` å’Œ `LayerNorm/BatchNorm çš„æƒé‡` ä½¿ç”¨ weight decayï¼ˆç»éªŒè§„åˆ™ï¼‰ã€‚
* **å­¦ä¹ ç‡ï¼ˆlrï¼‰**ï¼šä¸ batchã€æ¨¡å‹å¤§å°ã€schedule å¼ºç›¸å…³ï¼›GPT ç³»å¸¸é… **warmup + cosine decay**ã€‚
* **æ¢¯åº¦è£å‰ª**ï¼šé…åˆ **global norm clipping = 1.0** å¾ˆå¸¸è§ï¼ŒæŠ‘åˆ¶å¶å‘æ¢¯åº¦çˆ†å†²ã€‚

---

# PyTorch æ­£ç¡®ç”¨æ³•ï¼ˆå«å‚æ•°åˆ†ç»„ + è£å‰ªï¼‰

```python
import torch
from torch.nn.utils import clip_grad_norm_

# 1) å‚æ•°åˆ†ç»„ï¼šä¸ç»™ bias / LayerNorm æƒé‡åšè¡°å‡
decay, no_decay = [], []
for name, p in model.named_parameters():
    if not p.requires_grad:
        continue
    if p.ndim >= 2:   # ç»éªŒï¼šçŸ©é˜µ/å·ç§¯æ ¸åšè¡°å‡
        decay.append(p)
    else:             # biasã€LayerNorm.weight ç­‰ä¸åšè¡°å‡
        no_decay.append(p)

optim = torch.optim.AdamW(
    [
        {"params": decay,    "weight_decay": 0.1},
        {"params": no_decay, "weight_decay": 0.0},
    ],
    lr=3e-4,
    betas=(0.9, 0.95),  # GPT-3 é£æ ¼
    eps=1e-8
)

# 2) è®­ç»ƒæ­¥ï¼šå…ˆ backwardï¼Œå…ˆè£å‰ªï¼Œå† step
optim.zero_grad(set_to_none=True)
with torch.cuda.amp.autocast():   # è‹¥ç”¨æ··åˆç²¾åº¦
    logits, loss = model(x, y)
scaler.scale(loss).backward()     # è‹¥ç”¨ AMP çš„ GradScaler
# æˆ–è€…ï¼šloss.backward()

# å…¨å±€èŒƒæ•°è£å‰ªï¼ˆå…¸å‹é˜ˆå€¼ 1.0ï¼‰
clip_grad_norm_(model.parameters(), max_norm=1.0)

# å†æ›´æ–°
# è‹¥ç”¨ AMPï¼š
scaler.step(optim); scaler.update()
# è‹¥ä¸ç”¨ AMPï¼š
# optim.step()
```

> **ä¸ºä»€ä¹ˆè¦åˆ†ç»„ï¼Ÿ**
> å¯¹ `bias / LayerNorm.weight` åšè¡°å‡é€šå¸¸ä¼šä¼¤æ³›åŒ–ä¸æ”¶æ•›ï¼›æŠŠå®ƒä»¬æ”¾è¿› `no_decay` ç»„æ›´ç¨³ã€‚

---

# å¸¸è§é—®é¢˜ä¸å»ºè®®

* **Adam vs AdamW**ï¼šç”¨ **AdamW**ã€‚PyTorch çš„ `torch.optim.AdamW` å·²æ˜¯è§£è€¦ç‰ˆæœ¬ï¼›è€Œ `torch.optim.Adam(weight_decay=...)` åˆ™ç­‰ä»·äº L2 æ­£åˆ™ï¼ˆä¸è§£è€¦ï¼‰ã€‚
* **ä¸å­¦ä¹ ç‡è°ƒåº¦**ï¼šAdamW é€šå¸¸é… **warmupï¼ˆå¦‚ 1â€“2% æ­¥æ•°ï¼‰+ cosine decay åˆ° 10% ç»ˆå€¼**ã€‚
* **Î²â‚‚ è¯¥ä¸è¯¥ 0.95ï¼Ÿ**

  * å¤§æ¨¡å‹/å¤§ batchï¼š`0.95` å¸¸æ›´ç¨³ï¼›
  * å°æ¨¡å‹/å° batchï¼š`0.999` ä¹Ÿå¯ã€‚
* **Îµ è¦è°ƒå—ï¼Ÿ** å¤šæ•°æƒ…å†µä¸‹ä¿ç•™ `1e-8`ï¼›åªæœ‰åœ¨æ•°å€¼å¾ˆæŠ–æ—¶å†è€ƒè™‘å¾®è°ƒã€‚
* **æƒé‡è¡°å‡å¤šå¤§ï¼Ÿ** é¢„è®­ç»ƒå¸¸ `0.1`ï¼›å¾®è°ƒä»»åŠ¡å¯ `0.01~0.1` è¯•ç½‘æ ¼æœç´¢ã€‚
* **é…åˆæ¢¯åº¦è£å‰ª**ï¼šè£å‰ªæ”¾åœ¨ `backward()` ä¹‹åã€`optimizer.step()` ä¹‹å‰ï¼ˆä¸Šé¢çš„é¡ºåºå·²æ­£ç¡®ï¼‰ã€‚
* **ä¸ AMP/TF32**ï¼šæ•°å€¼ç²¾åº¦æ–¹æ¡ˆï¼ˆAMP/TF32ï¼‰ä¸æ”¹å˜ä¼˜åŒ–å™¨å®šä¹‰ï¼Œä½†ä¼šæ”¹å˜æ•°å€¼å™ªå£°ï¼›ç»“åˆæ¢¯åº¦è£å‰ªä¸åˆé€‚çš„ `Î²â‚‚/Îµ` æ›´ç¨³ã€‚

---

# ä¸€å¥è¯è®°å¿†

**AdamW = Adam çš„è‡ªé€‚åº”æ›´æ–° + ç‹¬ç«‹çš„â€œå‚æ•°ç¼©å‡â€æ­¥ã€‚**
æŠŠ weight decay ä»æ¢¯åº¦é‡Œè§£è€¦ï¼Œæ—¢**åå‰¯å…¶å®åœ°è¡°å‡æƒé‡**ï¼Œåˆ**æå‡æ³›åŒ–**ï¼Œæ˜¯ç°ä»£ Transformer è®­ç»ƒçš„é»˜è®¤é€‰æ‹©ã€‚


# learning rate scheduler: warmup + cosine decay

rate schedule that's used here in gpt3 is what's called a cosine Decay learning
schedule with warmup and the way this looks is that the learning rate is basically starts right at around zero
linearly rank s up over some amount of time and then comes down with this cosine sort of form and comes down to
some kind of a minimum learning rate that's up to you so here the minimum learning rate is zero but uh here in the
paper they said that they use cosine Decay for learning rate down to 10% of its value over the first 260 billion
tokens and then training continues 10% after and there's a linear warmup over
the first 375 million tokens so that's about the learn R so let's now implement
this uh so I already implemented it here and the way this works is let me scroll
down first here I changed our training Loop a little bit so this was a 4i in Max steps I just change it to step now
so that we have the notion of a step is a single optimization step in the in the
for Loop and then here I get the LR for this step of the optimization using a
new function I call get LR and then in pytorch to set the learning rate I think this is is the way to set the learning
rate it's a little bit gnarly um because you have to basically there's a notion of different par parameter groups that
could exist in the optimizer and so you actually have to iterate over them even though we currently have a single param
group only um and you have to set the LR in this for Loop kind of style is is my
impression right now so we have this look of LR we set the learning rate and then on the bottom I'm also printing it
uh so that's all the changes I made to this Loop and then of course the get LR is my scheduler now it's worth pointing
out that pytorch actually has learning rate schedulers and you can use them and I believe there's a cosine learning rate
schedule in pytorch I just don't really love using that code because honestly
it's like five lines of code and I fully understand what's happening inside these lines so I don't love to use
abstractions where they're kind of in screwable and then I don't know what they're doing so personal style so the
max learning rate here is let's say 3 E4 but we're going to see that in gpt3
here they have a table of what the maximum learning rate is for every model
size so um for for this one basically 12
12 layer 768 gpt3 so the gpt3 small is roughly like a GPT
2124m we see that here they use a learning rate of 6 E4 so we could actually go higher um in fact we may
want to try to follow that and just set the max LR here at six uh then the that's the maximum learning
rate the minum learning rate is uh 10% of that per description in the paper
some number of steps that we're going to warm up over and then the maximum steps of the optimization which I now use also
in the for Loop down here and then you can go over this code if you like it's not U it's not terribly inside Flor
interesting I'm just uh modulating based on the iteration number which learning rate uh there should be so this is the
warm-up region um this is the region after the optimization and then this is the region
sort of in between and this is where I calculate the cosine learning rate schedule and you can step through this
in detail if you'd like uh but this is basically implementing this curve and I ran this already and this is
what that looks like um so when we now run we start at
um some very low number now note that we don't start exactly at zero because that would be not useful to update with a
learning rate of zero that's why there's an it+ one so that on the zeroth iteration we are not using exactly zero
we're using something very very low then we linearly warm up to maximum learning rate which in this case was 34 when I
ran it but now would be 6 E4 and then it starts to decay all the way down to um 3
E5 which was at the time 10% of the original learning rate now one thing we are not following exactly is that they
mentioned that um let me see if I can find it
again we're not exactly following what they did because uh they mentioned that their
training Horizon is 300 billion tokens and they come down to 10% of the initial learning rate of at 260 billion and then
they train after 260 with 10% so basically their Decay time is less than
the max steps time whereas for us they're exactly equal so it's not exactly faithful but it's um it's an
okay um this is okay for us and for our purposes right now and um we're just
going to use this ourselves I don't think it makes too too big of a difference honestly I should point out that what learning rate schedule you use
is totally up to you there's many different types um coign learning rate has been popularized a lot by gpt2 and
gpt3 but people have come up with all kinds of uh other learning rate schedules um and this is kind of like an
active area of uh research as to which one is the most effective at train these networks okay next up the paper talks

å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯è¿™ä¸€éƒ¨åˆ†å†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## ğŸ§  å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š**é¢„çƒ­ï¼ˆwarmupï¼‰+ ä½™å¼¦é€€ç«ï¼ˆcosine decayï¼‰**

---

### ğŸ”º 1. ä»€ä¹ˆæ˜¯ã€Œä½™å¼¦é€€ç« + warmupã€å­¦ä¹ ç‡ç­–ç•¥ï¼Ÿ

å®ƒçš„å­¦ä¹ ç‡å˜åŒ–æ›²çº¿å¤§è‡´å¦‚ä¸‹ï¼š

```
ï¼ˆYè½´æ˜¯å­¦ä¹ ç‡ï¼ŒXè½´æ˜¯è®­ç»ƒæ­¥éª¤ï¼‰

 â†‘
 |                ï¼â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾ï¼¼
 |              ï¼                ï¼¼
 |            ï¼                    ï¼¼
 |__________/                        \___________â†’

      â†‘                            â†‘
   warmupé˜¶æ®µ         cosineé€æ¸å‡å°åˆ°æœ€ä½å€¼
```

* **Warmupé˜¶æ®µ**ï¼šä¸€å¼€å§‹ä»å¾ˆå°çš„å­¦ä¹ ç‡çº¿æ€§ä¸Šå‡åˆ°æœ€å¤§å€¼ï¼ˆé˜²æ­¢åˆšå¼€å§‹æ¨¡å‹ä¸ç¨³å®šï¼‰
* **Cosineé˜¶æ®µ**ï¼šä¹‹åæ²¿ç€ä¸€ä¸ª**ä½™å¼¦æ›²çº¿**ç¼“æ…¢ä¸‹é™ï¼ˆå¹³æ»‘é€€ç«ï¼‰ï¼Œç›´åˆ°è¾¾åˆ°æœ€ä½å­¦ä¹ ç‡

---

### ğŸ“– GPT-3 æ˜¯æ€ä¹ˆè®¾ç½®çš„ï¼Ÿ

æ ¹æ®è®ºæ–‡æè¿°ï¼š

* **æ€»è®­ç»ƒæ­¥æ•°**ï¼ˆä»¥ token æ•°è®¡ï¼‰ï¼š`3000äº¿`
* **Warmup æ­¥æ•°**ï¼š`3.75äº¿ tokens`
* **Cosine ä¸‹é™åŒºé—´**ï¼šä»æœ€å¤§å€¼ä¸‹é™åˆ° **åˆå§‹å­¦ä¹ ç‡çš„ 10%**ï¼ŒæŒç»­åˆ° `2600äº¿ tokens`
* å‰©ä¸‹çš„ `400äº¿ tokens` ä½¿ç”¨è¿™ä¸ª 10% çš„æœ€å°å­¦ä¹ ç‡ç»§ç»­è®­ç»ƒ

---

### ğŸ§ª å®é™…ä»£ç ä¸­æ€ä¹ˆå®ç°ï¼Ÿ

```python
# æ¯ä¸€æ­¥ä½¿ç”¨ä¸€ä¸ªå‡½æ•° get_lr() æ¥è®¡ç®—å½“å‰çš„å­¦ä¹ ç‡
lr = get_lr(step)

# ç„¶åé€šè¿‡ optimizer.param_groups è®¾ç½®å­¦ä¹ ç‡
for param_group in optimizer.param_groups:
    param_group['lr'] = lr
```

è¿™ä¸ªå¾ªç¯æ˜¯å› ä¸º PyTorch çš„ä¼˜åŒ–å™¨å¯èƒ½æœ‰å¤šä¸ªå‚æ•°ç»„ï¼ˆæ¯”å¦‚æœ‰çš„è¦ weight decayï¼Œæœ‰çš„ä¸éœ€è¦ï¼‰ï¼Œå°½ç®¡æˆ‘ä»¬è¿™é‡Œåªç”¨ä¸€ä¸ªç»„ï¼Œä½†ä»éœ€å¾ªç¯ã€‚

---

### ğŸ§® get\_lr() å‡½æ•°å¤§è‡´é€»è¾‘ï¼š

```python
if step < warmup_iters:
    # Warmup çº¿æ€§ä¸Šå‡é˜¶æ®µ
    return max_lr * step / warmup_iters
elif step > total_iters:
    # Cosine åæœŸï¼Œä¿æŒæœ€ä½å­¦ä¹ ç‡
    return min_lr
else:
    # ä¸­é—´é˜¶æ®µï¼Œæ‰§è¡Œä½™å¼¦é€€ç«
    decay_ratio = (step - warmup_iters) / (total_iters - warmup_iters)
    cosine_decay = 0.5 * (1 + cos(pi * decay_ratio))
    return min_lr + (max_lr - min_lr) * cosine_decay
```

### ç¤ºä¾‹å‚æ•°ï¼š

* `max_lr = 6e-4`ï¼ˆä» GPT-3 å¯¹åº”æ¨¡å‹å¤§å°ä¸­è·å–ï¼‰
* `min_lr = 6e-5`ï¼ˆ10%ï¼‰
* `warmup_iters = 200`
* `total_iters = 6000`

---

### ğŸ–¼ï¸ è¾“å‡ºæ ·ä¾‹ï¼ˆå¯è§†åŒ–è§‚å¯Ÿï¼‰ï¼š

* åˆå§‹å‡ æ­¥ï¼šå­¦ä¹ ç‡ä»æ¥è¿‘ 0 å¼€å§‹ï¼ˆä½†ä¸ä¼šæ˜¯ç»å¯¹çš„ 0ï¼‰
* å¿«é€Ÿä¸Šå‡è‡³ `max_lr`
* ç„¶åé€æ­¥ä¸‹é™åˆ° `min_lr`

---

### ğŸ“Œ å°ç»†èŠ‚ï¼š

> GPT-3 æ˜¯åœ¨ 2600 äº¿ tokens æ—¶ä¸‹é™åˆ° 10%ï¼Œç„¶åç»§ç»­ç”¨è¿™ä¸ª 10% çš„å€¼è®­ç»ƒåˆ° 3000 äº¿
> è€Œè¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬ç®€åŒ–äº†é€»è¾‘ï¼š**æ€»è®­ç»ƒæ­¥æ•°å†…ç›´æ¥ä» max\_lr é€€ç«åˆ° min\_lr**ï¼Œæ²¡æœ‰ç»§ç»­ä¿æŒå¸¸æ•°é˜¶æ®µ

è¿™æ˜¯ä¸€ä¸ªåˆç†çš„è¿‘ä¼¼ï¼Œä¸ä¼šæœ‰å¤ªå¤§é—®é¢˜ã€‚

---

### ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦æ˜¯ä¸€ä¸ªæ´»è·ƒç ”ç©¶æ–¹å‘

* GPT ç³»åˆ— popularize äº† cosine decay + warmup
* ä½†ä¹Ÿæœ‰å…¶ä»–ç­–ç•¥ï¼Œæ¯”å¦‚ï¼š

  * step decay
  * exponential decay
  * cyclical learning rates
  * one-cycle policy
* æ²¡æœ‰ç»Ÿä¸€æ ‡å‡†ï¼Œã€Œè°ƒå¾—å¥½ã€æ¯”ã€Œç†è®ºæœ€ä¼˜ã€æ›´é‡è¦ âœ…

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ ï¼š

* å¯è§†åŒ–è¿™æ®µä½™å¼¦å­¦ä¹ ç‡æ›²çº¿
* ç”¨ä¸­æ–‡å®ç°è¿™æ®µ PyTorch çš„è°ƒåº¦å™¨é€»è¾‘
* æˆ–è€…æ€»ç»“ä¸åŒå­¦ä¹ ç‡ç­–ç•¥çš„å¯¹æ¯”

éœ€è¦çš„è¯è¯·å‘Šè¯‰æˆ‘\~

å¥½é—®é¢˜ï¼è¿™æ®µåœ¨è®² **GPT-3 çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ (learning rate scheduler)** â€”â€”å®ƒä¸æ˜¯å›ºå®šçš„å­¦ä¹ ç‡ï¼Œè€Œæ˜¯ä¸€ä¸ªéšè®­ç»ƒè¿›åº¦å˜åŒ–çš„æ›²çº¿ã€‚æˆ‘ä»¬é€æ¡æ‹†å¼€ï¼š

---

## 1. ä¸ºä»€ä¹ˆè¦ç”¨å­¦ä¹ ç‡è°ƒåº¦ (LR scheduler)ï¼Ÿ

* **å¤§æ¨¡å‹è®­ç»ƒæ—©æœŸ**ï¼šç½‘ç»œå‚æ•°éšæœºåˆå§‹åŒ–ï¼Œç›´æ¥ç”¨è¾ƒå¤§å­¦ä¹ ç‡ä¼šâ€œç‚¸æ‰â€ï¼›æ‰€ä»¥å…ˆ **warmupï¼ˆçº¿æ€§å‡æ¸©ï¼‰**ï¼Œè®©æ¨¡å‹é€æ¸é€‚åº”ã€‚
* **ä¸­æœŸ**ï¼šä¿æŒåœ¨æœ€å¤§å€¼ä¸€æ®µæ—¶é—´ï¼Œé«˜æ•ˆå­¦ä¹ ã€‚
* **åæœŸ**ï¼šé€æ¸é™ä½å­¦ä¹ ç‡ï¼ˆcosine decayï¼‰ï¼Œé¿å…æ¨¡å‹åœ¨æ”¶æ•›é˜¶æ®µéœ‡è¡ï¼Œæé«˜æ³›åŒ–ã€‚

---

## 2. Warmup + Cosine Decay æ›²çº¿

å­¦ä¹ ç‡éšè®­ç»ƒæ­¥æ•° $t$ çš„å˜åŒ–å¤§è‡´æ˜¯è¿™æ ·ï¼š

1. **Warmup é˜¶æ®µï¼ˆçº¿æ€§ä¸Šå‡ï¼‰**

   * ä»æ¥è¿‘ 0 å¼€å§‹ï¼Œçº¿æ€§å‡åˆ° **æœ€å¤§å­¦ä¹ ç‡ $lr_{max}$**
   * GPT-3ï¼šåœ¨ **å‰ 375M tokens** å†…å®Œæˆ warmupã€‚

   $$
   lr(t) = lr_{max} \cdot \frac{t}{T_{warmup}}, \quad 0 \le t < T_{warmup}
   $$

2. **Cosine Decay é˜¶æ®µï¼ˆä½™å¼¦ä¸‹é™ï¼‰**

   * ä» $lr_{max}$ é€æ¸è¡°å‡åˆ° **æœ€å°å­¦ä¹ ç‡ $lr_{min}$**ã€‚
   * GPT-3ï¼šåœ¨ **260B tokens** å†…è¡°å‡åˆ° $0.1 \cdot lr_{max}$ã€‚

   $$
   lr(t) = lr_{min} + \tfrac{1}{2}(lr_{max} - lr_{min}) 
           \Big(1 + \cos\!\big(\pi \cdot \tfrac{t - T_{warmup}}{T_{decay}}\big)\Big)
   $$

3. **æ”¶å°¾é˜¶æ®µï¼ˆä¿æŒæœ€ä½å­¦ä¹ ç‡ï¼‰**

   * GPT-3ï¼š260B tokens ä»¥åï¼Œå­¦ä¹ ç‡å›ºå®šåœ¨ **$0.1 \cdot lr_{max}$**ï¼Œç›´åˆ°è®­ç»ƒç»“æŸã€‚

---

## 3. GPT-3 çš„å…·ä½“è¶…å‚æ•°

* **æœ€å¤§å­¦ä¹ ç‡**ï¼ˆlr\_maxï¼‰ï¼šä¾æ¨¡å‹å¤§å°å®šï¼ˆæ¯”å¦‚å°æ¨¡å‹ 6e-4ï¼‰ã€‚
* **æœ€å°å­¦ä¹ ç‡**ï¼ˆlr\_minï¼‰ï¼š$0.1 \cdot lr_{max}$ã€‚
* **warmup tokens**ï¼š3.75e8ï¼ˆ375M tokensï¼‰ã€‚
* **cosine decay tokens**ï¼š2.6e11ï¼ˆ260B tokensï¼‰ã€‚
* **è®­ç»ƒæ€»é•¿**ï¼š3.0e11ï¼ˆ300B tokensï¼‰ã€‚

æ¢å¥è¯è¯´ï¼š

* å‰ 375M token çº¿æ€§å‡æ¸©ï¼›
* ä¹‹ååˆ° 260B token é€æ¸è¡°å‡ï¼›
* å‰©ä¸‹ 40B token ä¿æŒ 0.1Ã— å­¦ä¹ ç‡ã€‚

---

## 4. PyTorch å®ç°æ€è·¯

GPT-3 ä»£ç é‡Œä½œè€…æ‰‹åŠ¨å†™äº† `get_lr(step)` å‡½æ•°ï¼Œè‡ªå·±è°ƒåº¦ã€‚PyTorch é‡Œä¹Ÿæœ‰ç°æˆçš„ï¼š

* `torch.optim.lr_scheduler.CosineAnnealingLR`ï¼ˆçº¯ cosineï¼‰
* `torch.optim.lr_scheduler.OneCycleLR`ï¼ˆå¸¦ warmup + decayï¼‰

ä½†æ‰‹å†™å¾ˆç®€å•ï¼Œæœ¬è´¨å°±æ˜¯å‡ è¡Œ if/elseï¼š

```python
def get_lr(step, warmup, total_steps, lr_max, lr_min):
    if step < warmup:
        # çº¿æ€§ warmup
        return lr_max * step / warmup
    elif step < total_steps:
        # cosine decay
        progress = (step - warmup) / (total_steps - warmup)
        return lr_min + 0.5 * (lr_max - lr_min) * (1 + math.cos(math.pi * progress))
    else:
        # è®­ç»ƒç»“æŸåä¿æŒæœ€ä½å­¦ä¹ ç‡
        return lr_min
```

---

## 5. ç›´è§‚å›¾åƒ

æ›²çº¿å¤§è‡´æ˜¯ï¼š

```
LR
â”‚        ________      (å¹³æ»‘ä½™å¼¦ä¸‹é™)
â”‚       /        \
â”‚      /          \
â”‚_____/             \___________
0   warmup         decay       end
```

---

âœ… **ä¸€å¥è¯æ€»ç»“**ï¼š
GPT-3 çš„å­¦ä¹ ç‡è°ƒåº¦ = **çº¿æ€§ warmupï¼ˆå‰ 375M tokensï¼‰+ cosine decay åˆ° 0.1Ã—ï¼ˆ260B tokensï¼‰+ ä¿æŒ 0.1Ã—ï¼ˆä½™ä¸‹è®­ç»ƒï¼‰**ã€‚è¿™æ ·æ—¢ç¨³ä½è®­ç»ƒå¼€å¤´ï¼Œåˆæå‡æ”¶æ•›å’Œæ³›åŒ–ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **cosine decay + warmup çš„å®é™…æ›²çº¿å›¾**ï¼ˆPython matplotlibï¼‰ï¼Œè¿™æ ·ç›´è§‚å±•ç¤ºå­¦ä¹ ç‡éšè®­ç»ƒæ­¥æ•°çš„å˜åŒ–ï¼Ÿ


```
max_lr = 6e-4
min_lr = max_lr * 0.1
warmup_steps = 10
max_steps = 50
def get_lr(it):
    # 1) linear warmup for warmup_iters steps
    if it < warmup_steps:
        return max_lr * (it+1) / warmup_steps
    # 2) if it > lr_decay_iters, return min learning rate
    if it > max_steps:
        return min_lr
    # 3) in between, use cosine decay down to min learning rate
    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)
    assert 0 <= decay_ratio <= 1
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0
    return min_lr + coeff * (max_lr - min_lr)
```

å¾ˆå¥½ï¼Œè¿™æ®µä»£ç å°±æ˜¯åœ¨å®ç°æˆ‘ä»¬åˆšåˆšè®²çš„ **Warmup + Cosine Decay å­¦ä¹ ç‡è°ƒåº¦å™¨**ã€‚æˆ‘æ¥é€è¡Œè¯¦ç»†æ‹†è§£ï¼š

---

### å‚æ•°è®¾å®š

```python
max_lr = 6e-4              # æœ€å¤§å­¦ä¹ ç‡ (0.0006)
min_lr = max_lr * 0.1      # æœ€å°å­¦ä¹ ç‡ (0.00006)ï¼Œæ˜¯æœ€å¤§å€¼çš„ 10%
warmup_steps = 10          # çº¿æ€§ warmup çš„æ­¥æ•°
max_steps = 50             # æ€»è®­ç»ƒæ­¥æ•°ï¼ˆç›¸å½“äº decay ç»“æŸçš„ç‚¹ï¼‰
```

---

### å‡½æ•° get\_lr(it)

è¾“å…¥ï¼šå½“å‰è®­ç»ƒæ­¥æ•° `it`
è¾“å‡ºï¼šæ­¤æ—¶çš„å­¦ä¹ ç‡

---

#### 1) Warmup é˜¶æ®µ

```python
if it < warmup_steps:
    return max_lr * (it+1) / warmup_steps
```

* å‰ 10 æ­¥ï¼ˆ`warmup_steps=10`ï¼‰ï¼Œå­¦ä¹ ç‡ä»æ¥è¿‘ **0** çº¿æ€§å‡åˆ° **max\_lr**ã€‚
* `(it+1)` æ˜¯ä¸ºäº†è®©ç¬¬ 0 æ­¥æ—¶å­¦ä¹ ç‡ä¸æ˜¯å®Œå…¨ 0ï¼Œè€Œæ˜¯ä¸€ä¸ªå¾ˆå°çš„æ•°ï¼ˆé¿å…è®­ç»ƒåœæ»ï¼‰ã€‚

ğŸ‘‰ **ä¸¾ä¾‹**ï¼š

* step=0 â†’ lr = 0.0006 \* 1/10 = 6e-5
* step=9 â†’ lr = 0.0006 \* 10/10 = 6e-4

---

#### 2) è¶…è¿‡æœ€å¤§æ­¥æ•°åçš„ä¿åº•å€¼

```python
if it > max_steps:
    return min_lr
```

* å¦‚æœè®­ç»ƒæ­¥æ•°è¶…è¿‡è®¾å®šçš„ `max_steps=50`ï¼Œå­¦ä¹ ç‡å›ºå®šåœ¨æœ€å°å€¼ `min_lr=0.00006`ï¼Œä¸å†ä¸‹é™ã€‚

---

#### 3) Cosine Decay é˜¶æ®µ

```python
decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)
assert 0 <= decay_ratio <= 1
coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
return min_lr + coeff * (max_lr - min_lr)
```

* **decay\_ratio**ï¼šå½’ä¸€åŒ–åçš„è¡°å‡è¿›åº¦ï¼ŒèŒƒå›´ 0\~1ã€‚

  * `it=warmup_steps=10` â†’ ratio=0
  * `it=max_steps=50` â†’ ratio=1

* **math.cos(Ï€ \* decay\_ratio)**ï¼šä½™å¼¦å‡½æ•°ä» `cos(0)=1` é€æ¸è¡°å‡åˆ° `cos(Ï€)=-1`ã€‚

* \**coeff = 0.5*(1+cos(...))\*\*ï¼šæŠŠèŒƒå›´ä» `[-1,1]` æ˜ å°„åˆ° `[0,1]`ã€‚

  * decay\_ratio=0 â†’ coeff=1
  * decay\_ratio=1 â†’ coeff=0

* **æœ€ç»ˆå…¬å¼**ï¼š

$$
lr(it) = lr_{min} + coeff \cdot (lr_{max} - lr_{min})
$$

è¿™æ ·å­¦ä¹ ç‡ä» **max\_lr** å¹³æ»‘è¡°å‡åˆ° **min\_lr**ï¼Œæ›²çº¿æ˜¯ä¸€ä¸ªåŠä¸ªä½™å¼¦æ³¢ã€‚

---

### æ€»ä½“æµç¨‹

1. **0\~9 æ­¥**ï¼šçº¿æ€§ warmupï¼Œä» 6e-5 å‡åˆ° 6e-4
2. **10\~50 æ­¥**ï¼šcosine decayï¼Œä» 6e-4 å¹³æ»‘ä¸‹é™åˆ° 6e-5
3. **50 æ­¥ä»¥å**ï¼šå›ºå®šåœ¨ 6e-5

---

### ç›´è§‚å›¾åƒ

å¦‚æœç”»å›¾çš„è¯ï¼Œæ›²çº¿å¤§æ¦‚æ˜¯è¿™æ ·çš„ï¼š

```
lr
â”‚        warmup       cosine decay
â”‚       /-----\________
â”‚      /       \        \______
â”‚_____/         \_______________
0    10         50              steps
```

---

è¦ä¸è¦æˆ‘å¸®ä½ å†™ä¸ª **matplotlib ä»£ç ç‰‡æ®µ**ï¼ŒæŠŠè¿™ä¸ª `get_lr` çš„æ›²çº¿ç”»å‡ºæ¥ï¼Ÿè¿™æ ·ä½ èƒ½ç›´è§‚çœ‹åˆ° warmup + cosine decay çš„æ•ˆæœã€‚


```
for param_group in optimizer.param_groups: param_group['lr'] = lr
```

å¥½é—®é¢˜ ğŸ‘ï¼Œè¿™å‡ è¡Œä»£ç çš„ä½œç”¨æ˜¯ **åœ¨è®­ç»ƒå¾ªç¯é‡ŒåŠ¨æ€æ›´æ–°ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡**ã€‚æˆ‘æ¥æ‹†è§£ä¸€ä¸‹ï¼š

---

### èƒŒæ™¯

åœ¨ PyTorch é‡Œï¼Œ`optimizer`ï¼ˆæ¯”å¦‚ `AdamW`, `SGD`ï¼‰é€šå¸¸æ˜¯è¿™æ ·åˆå§‹åŒ–çš„ï¼š

```python
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
```

è¿™é‡Œ `lr=3e-4` æ˜¯åˆå§‹å­¦ä¹ ç‡ã€‚
ä½†æ˜¯â€”â€”å¦‚æœæˆ‘ä»¬æƒ³ç”¨ **learning rate schedulerï¼ˆå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼‰**ï¼Œå°±å¿…é¡»åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ **ä¸æ–­ä¿®æ”¹ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡**ã€‚

---

### `optimizer.param_groups`

* PyTorch çš„ä¼˜åŒ–å™¨å…è®¸æˆ‘ä»¬å¯¹ **ä¸åŒå‚æ•°ç»„**ï¼ˆparameter groupsï¼‰è®¾ç½®ä¸åŒçš„è¶…å‚æ•°ï¼Œæ¯”å¦‚ï¼š

  * embedding å±‚ï¼šå­¦ä¹ ç‡å°ä¸€äº›
  * transformer å±‚ï¼šå­¦ä¹ ç‡å¤§ä¸€äº›
* å› æ­¤ï¼Œ`optimizer.param_groups` æ˜¯ä¸€ä¸ª **åˆ—è¡¨**ï¼Œé‡Œé¢çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ª **å­—å…¸ (dict)**ï¼Œå­˜æ”¾è¯¥å‚æ•°ç»„çš„è¶…å‚æ•°ï¼š

```python
[ 
  {'params': [...], 'lr': 0.0003, 'weight_decay': 0.01, ...},
  {'params': [...], 'lr': 0.0001, 'weight_decay': 0.00, ...},
]
```

---

### è¿™æ®µä»£ç 

```python
for param_group in optimizer.param_groups:
    param_group['lr'] = lr
```

æ„æ€æ˜¯ï¼š

* éå†ä¼˜åŒ–å™¨çš„æ¯ä¸ªå‚æ•°ç»„
* æŠŠå½“å‰ step è®¡ç®—å‡ºçš„ **æ–°å­¦ä¹ ç‡ `lr`** æ›´æ–°è¿›å»

è¿™æ ·ä¸‹ä¸€æ¬¡ `optimizer.step()` çš„æ—¶å€™ï¼Œå°±ä¼šç”¨æ–°çš„å­¦ä¹ ç‡æ›´æ–°å‚æ•°ã€‚

---

### ä¸¾ä¾‹

å‡è®¾ç°åœ¨ scheduler ç»™å‡ºçš„å­¦ä¹ ç‡æ˜¯ `1e-4`ï¼Œé‚£ä¹ˆæ‰§è¡Œåï¼š

```python
for param_group in optimizer.param_groups:
    param_group['lr'] = 1e-4
```

æ‰€æœ‰å‚æ•°ç»„çš„å­¦ä¹ ç‡éƒ½ä¼šè¢«è®¾ç½®ä¸º `1e-4`ã€‚
å¦‚æœä½ æœ‰å¤šä¸ªå‚æ•°ç»„ï¼Œå¯ä»¥é€‰æ‹©åªæ›´æ–°å…¶ä¸­çš„æŸå‡ ä¸ªã€‚

---

âœ… æ€»ç»“ä¸€å¥è¯ï¼š
è¿™æ®µä»£ç å°±æ˜¯ **æŠŠ scheduler ç®—å‡ºçš„å­¦ä¹ ç‡ä¼ ç»™ä¼˜åŒ–å™¨çš„å‚æ•°ç»„**ï¼Œä»è€Œå®ç° **åŠ¨æ€å­¦ä¹ ç‡è°ƒåº¦**ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ å†™ä¸ª **å®Œæ•´çš„ä¾‹å­**ï¼ˆå¸¦ `get_lr`ã€æ›´æ–° optimizerã€æ‰“å°å­¦ä¹ ç‡å˜åŒ–ï¼‰ï¼Œè¿™æ ·ä½ å°±èƒ½çœ‹åˆ°è°ƒåº¦å™¨åœ¨è®­ç»ƒä¸­æ˜¯æ€ä¹ˆä½œç”¨çš„ï¼Ÿ



# batch size schedule, weight decay, FusedAdamW, 90ms

about the gradual batch size increase so there's a ramp on the batch size that is
linear and you start with very small batch size and you ramp up to a big batch size over time uh we're going to
actually skip this and we're not going to work with it and the reason I don't love to use it is that it complicates a
lot of the arithmetic because you are changing the number of tokens that you're processing at every single step of the optimization and I like to keep
that math very very simple also my understanding is that that this is not like a major um Improvement and also my
understanding is that this is not like an algorithmic optimization Improvement it's more of a systems and speed
Improvement and roughly speaking this is because uh in the early stages of the
optimization uh again the model is in a very atypical setting and mostly what
you're learning is that um you're mostly learning to ignore the tokens uh that don't come up in your training set very
often you're learning very simple biases and and that kind of a thing and so
every single example that you put through your network is basically just telling you use these tokens and don't
use these tokens and so the gradients from every single example are actually extremely highly correlated they all
look roughly the same in the in the OR original parts of the optimization because they're all just telling you
that these tokens don't appear and these tokens do appear and so because the gradients are all very similar and
they're highly correlated then why are you doing batch sizes of like Millions when if you do a batch size of 32k
you're basically getting the exact same gradient early on in the training and then later in the optimization once
you've learned all the simple stuff that's where the actual work starts and that's where the gradients become more decorrelated per examples and that's
where they actually offer you sort of statistical power in some sense um so
we're going to skip this just because it kind of complicates things and we're going to go to uh data are sampled without
replacement during training um so until an Epoch boundary is reached so without
replacement means that they're not sampling from some fixed pool and then uh take a sequence train on it but then
also like return the sequence to the pool they are exhausting a pool so when they draw a sequence it's it's gone
until the next Epoch of training uh so we're already doing that because our data loader um iterates over chunks of
data so there's no replacement they don't become eligible to be drawn again until the next P so we're basically
already doing that um all models use a weight decay of
0.1 to provide a small amount of regularization so let's Implement a weight Decay and you see here that I've
already kind of made the changes and in particular instead of creating the optimizer right here um I I'm creating a
new configure optimizers function inside the model and I'm passing in some of the hyper parameters instead so let's look
at the configure optimizers which is supposed to return the optimizer
object okay so it looks complicated but it's actually really simple and it's just um we're just being very careful
and there's a few settings here to go through the most important thing with respect to this line is that you see
there's a weight Decay parameter here and I'm passing that into um well I'm passing that into
something called optim groups that eventually ends up going into the addom W Optimizer um and the weight Decay
that's by default used in Addam W here is 0.01 so it's it's u 10 times lower
than what's used in gpt3 paper here um so the weight dek basically ends up
making its way into the ADD and W through the optimizer groups now what else is going on here in this uh function so the two things that are
happening here that are important is that I'm splitting up the parameters into those that should be weight decayed
and those that should not be weight decayed so in particular it is common to not weight decay uh biases and any other
sort of one-dimensional tensors so the one-dimensional tensors are in the no Decay prams and these are also things
like uh layer Norm scales and biases it doesn't really make sense to weight Decay those you mostly want to weight
Decay uh the weights that participate in Matrix multiplications and you want to potentially weight Decay the
embeddings and uh We've covered in previous video why it makes sense to Decay the weights because you can sort
of the it as a regularization because when you're pulling down all the weights you're forcing the optimization to use
more of the weights um and you're not allowing any one of the weights individually to be way too large um
you're forcing you're forcing the network to kind of like distribute the work across more channels because there's sort of like a pull of gravity
on the weights themselves um so that's why we are separating it in those ways here we're
only decaying the embeddings and the mmal participating ways uh we're printing the number of uh
parameters that we decaying and not most of the parameters will be decayed and then one more thing that we're doing here is I'm doing another optimization
here and previous add and W did not have this option but later parts of pytorch
introduced it and that's why I'm guarding it with an inspect do signature which is basically checking if this
fused um quar is present inside atom W
and then if it is present I'm going to end up using it and passing it in here
because some earlier versions do not have fused equals so here's adamw fused
equals it did not used to exist and it was added later and there's some docks here for what's happening and basically
they say that by default they do not use fused because it is relatively new and we want to give it sufficient big time
so by default they don't use fused but fused is a lot faster when it is available and when you're running on
Cuda and what that does is in instead of iterating in a for Loop over all the
parameter tensors and updating them that would launch a lot of kernels right and so a fused just means that it's a um all
those kernels are fused into a single kernel you get rid of a lot of overhead and you a single time on all the
parameters call a uh kernel that updates them and so it's just basically a kernel
Fusion for the atom W update instead of iterating over all the tensors so that's the configure
optimizers function that I like to use and we can rerun and we're not going to see any major differences from what we
saw before but we are going to see some prints uh coming from here so let's just take a look at what they look
like so we see that number of Decay tensors is 50 and it's most of the parameters and number of non- deay
tensors is 98 and these are the biases and the layer Norm parameters mostly and that's there's only 100,000 of those so
most of it is decayed and then we are using the fused implementation of ATM W which will be a lot faster so if you
have it available I would advise you to use it I'm not actually 100% sure why they don't default to it it seems fairly
benign and harmless and also because we are using the fused implementation I think this is
why we have dropped um notice that the running time used to be 93 milliseconds
per step and we're now down to 90 milliseconds per step because of using the fused atom W Optimizer so in a
single commit here we are introducing fused atom getting improvements on the
time and we're adding or changing the weight Decay but we're only weight decaying the two dimensional parameters
the embeddings and the matrices that participate in linear so that is this
and we can take this out and uh yeah that is it for this line one more quick

å½“ç„¶å¯ä»¥ï¼ä¸‹é¢æ˜¯è¿™ä¸€æ®µå†…å®¹çš„ä¸­æ–‡è§£é‡Šï¼š

---

## ğŸ§ª **æ‰¹é‡å¤§å°å˜åŒ–ã€æƒé‡è¡°å‡ï¼ˆweight decayï¼‰ã€Fused AdamW ä¼˜åŒ–å™¨**

---

### ğŸš« 1. ä¸ºä»€ä¹ˆæˆ‘ä»¬**ä¸ä½¿ç”¨**ã€Œé€æ­¥å¢å¤§ batch sizeã€ï¼Ÿ

è™½ç„¶ GPT-3 è®­ç»ƒæ—¶ç”¨äº†ã€Œ**çº¿æ€§å¢é•¿çš„ batch size**ã€ç­–ç•¥ï¼ˆä»å°åˆ°å¤§ï¼‰ï¼Œä½†è¿™é‡Œæˆ‘ä»¬é€‰æ‹© **ä¸ä½¿ç”¨å®ƒ**ï¼Œç†ç”±æœ‰ä¸‰ï¼š

1. âœ… **å®ç°å¤æ‚åº¦é«˜**ï¼šæ¯ä¸€æ­¥çš„ token æ•°éƒ½å˜äº†ï¼Œå¾ˆå¤šä»£ç å’Œè®¡ç®—éƒ½è¦è°ƒæ•´ã€‚
2. ğŸ§  **ä¼˜åŒ–ä»·å€¼ä¸å¤§**ï¼šè¿™æ˜¯ã€Œç³»ç»Ÿå±‚ä¼˜åŒ–ã€ï¼Œä¸æ˜¯æ ¸å¿ƒç®—æ³•ä¼˜åŒ–ï¼Œå¯¹æœ€ç»ˆç²¾åº¦å½±å“æœ‰é™ã€‚
3. ğŸ¯ **å‰æœŸæ¢¯åº¦å‡ ä¹ä¸€æ ·**ï¼šæ¨¡å‹ä¸€å¼€å§‹æ˜¯éšæœºçš„ï¼Œæ¯ä¸ªæ ·æœ¬ç»™å‡ºçš„æ¢¯åº¦éƒ½å¾ˆç›¸ä¼¼ï¼Œbatch size å¤§å°å¯¹æ¢¯åº¦å¹³å‡å½±å“ä¸å¤§ã€‚åæœŸä¼˜åŒ–æ‰çœŸæ­£éœ€è¦å¤§ batchã€‚

---

### ğŸ“š 2. æ— æ”¾å›çš„æ•°æ®é‡‡æ ·ï¼ˆæ— é‡å¤æŠ½æ ·ï¼‰

GPT-3 è®­ç»ƒæ—¶ä½¿ç”¨ã€Œ**æ— æ”¾å›ï¼ˆwithout replacementï¼‰**ã€çš„æ•°æ®é‡‡æ ·ç­–ç•¥ï¼š

* æ„æ€æ˜¯ä¸€æ¬¡é‡‡æ ·åï¼Œä¸ä¼šå†ç”¨åŒä¸€æ•°æ®ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ª epoch
* âœ… æˆ‘ä»¬çš„ `DataLoader` å®é™…ä¸Šå·²ç»æ˜¯è¿™æ ·çš„å®ç°ï¼Œæ‰€ä»¥æ— éœ€ä¿®æ”¹

---

### ğŸ§² 3. æ·»åŠ æƒé‡è¡°å‡ï¼ˆweight decayï¼‰

#### âœ… GPT-3 è®¾ç½®ï¼š

* ä½¿ç”¨ `weight_decay = 0.1` æ¥å®ç°è½»å¾®çš„æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

#### âœ… æˆ‘ä»¬å®ç°é€»è¾‘ï¼š

1. æ–°å¢ä¸€ä¸ª `configure_optimizers()` å‡½æ•°æ¥æ„å»ºä¼˜åŒ–å™¨
2. æŒ‰ç…§ PyTorch ç¤¾åŒºæ¨èï¼ŒæŠŠæ¨¡å‹å‚æ•°åˆ†æˆä¸¤ç±»ï¼š

   * **éœ€è¦è¿›è¡Œ weight decay çš„å‚æ•°**ï¼š

     * æƒé‡çŸ©é˜µï¼ˆLinear å±‚ä¸­çš„ `weight`ï¼ŒEmbedding ç­‰ï¼‰
   * **ä¸éœ€è¦è¿›è¡Œ weight decay çš„å‚æ•°**ï¼š

     * Biasï¼ˆåç½®é¡¹ï¼‰
     * LayerNorm çš„å‚æ•°ï¼ˆä¾‹å¦‚ `weight`, `bias` ä¹Ÿæ˜¯ä¸€ç»´å¼ é‡ï¼‰

#### ä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼Ÿ

* **è¡°å‡ weight** æœ‰åŠ©äºæ§åˆ¶å‚æ•°è§„æ¨¡ï¼Œé˜²æ­¢æŸäº›æƒé‡å€¼çˆ†ç‚¸
* **ä¸è¡°å‡ bias / LayerNorm å‚æ•°**ï¼Œå› ä¸ºè¿™äº›å‚æ•°æ•°é‡å°‘ã€è¡Œä¸ºç‰¹æ®Šï¼Œé€šå¸¸ä¸å‚ä¸æ­£åˆ™åŒ–

---

### âš™ï¸ 4. ä½¿ç”¨ **Fused AdamW** ä¼˜åŒ–å™¨ï¼ˆèåˆç‰ˆæœ¬ï¼‰

åœ¨ PyTorch çš„è¾ƒæ–°ç‰ˆæœ¬ä¸­ï¼Œ`AdamW` æä¾›äº†ä¸€ä¸ªæ–°å‚æ•° `fused=True`ï¼š

* âœ… **æ›´å¿«**ï¼šæŠŠå¯¹æ¯ä¸ªå‚æ•°çš„æ›´æ–°æ“ä½œã€Œèåˆæˆä¸€ä¸ª kernelã€ï¼Œé¿å…é¢‘ç¹çš„ CUDA kernel å¯åŠ¨
* âœ… **æ¨èä½¿ç”¨**ï¼šä½†éœ€è¦ PyTorch æ”¯æŒè¿™ä¸ªå‚æ•°ï¼ˆç”¨ `inspect.signature()` åˆ¤æ–­æ˜¯å¦æ”¯æŒï¼‰
* ğŸ”§ é»˜è®¤ PyTorch å¹¶ä¸å¼€å¯ `fused`ï¼Œä½†æˆ‘ä»¬å¼ºçƒˆå»ºè®®åœ¨ CUDA å¯ç”¨æ—¶å¯ç”¨å®ƒ

---

### â±ï¸ 5. æ•ˆæœåˆ†æ

* åŸæ¥æ¯ä¸€æ­¥è¿è¡Œæ—¶é—´æ˜¯ **93ms**
* å¯ç”¨ FusedAdamW ä¹‹åæ˜¯ **90ms**
* æ‰€ä»¥æˆ‘ä»¬åœ¨å¼•å…¥æ­£åˆ™åŒ–çš„åŒæ—¶ï¼Œä¹Ÿè·å¾—äº† **â‰ˆ3% çš„é€Ÿåº¦æå‡**

---

### ğŸ§¾ å°ç»“

| å†…å®¹          | æ˜¯å¦å¯ç”¨ | è¯´æ˜                            |
| ----------- | ---- | ----------------------------- |
| æ‰¹é‡å¤§å°çº¿æ€§å¢é•¿    | âŒ    | å¤ªå¤æ‚ã€å¢ç›Šä¸å¤§                      |
| æ— æ”¾å›é‡‡æ ·       | âœ…    | å·²é»˜è®¤å¯ç”¨                         |
| æƒé‡è¡°å‡ `0.1`  | âœ…    | æ­£åˆ™åŒ–ï¼ŒæŠ‘åˆ¶è¿‡æ‹Ÿåˆï¼Œåªä½œç”¨äºæƒé‡çŸ©é˜µå’Œ Embedding |
| Fused AdamW | âœ…    | ä½¿ç”¨äº†æ›´å¿«çš„èåˆæ›´æ–°æ–¹æ³•ï¼Œåœ¨æ”¯æŒçš„å¹³å°ä¸‹æå‡æ€§èƒ½      |

---

å¦‚æœä½ å¸Œæœ›æˆ‘ï¼š

* ç»™å‡ºå®Œæ•´çš„ä¸­æ–‡æ³¨é‡Šå®ç°
* åˆ†ææƒé‡è¡°å‡å¯¹ loss æˆ–ç²¾åº¦çš„å½±å“
* å¯è§†åŒ–å¯ç”¨/ä¸å¯ç”¨ fused ä¼˜åŒ–å™¨çš„æ€§èƒ½å¯¹æ¯”

éƒ½å¯ä»¥å‘Šè¯‰æˆ‘\~

å¥½çš„ ğŸ‘ è¿™ä¸€å¤§æ®µä¸»è¦è®²äº† **GPT è®­ç»ƒé‡Œçš„ä¸€äº›ä¼˜åŒ–æŠ€å·§**ï¼šæ‰¹é‡å¤§å°è°ƒåº¦ï¼ˆbatch size scheduleï¼‰ã€æƒé‡è¡°å‡ï¼ˆweight decayï¼‰ã€FusedAdamWï¼Œä»¥åŠæ€§èƒ½ä¸Šçš„æå‡ï¼ˆ90ms â†’ 93msï¼‰ã€‚æˆ‘å¸®ä½ é€ç‚¹æ‹†å¼€è§£é‡Šï¼š

---

## 1. Batch size scheduleï¼ˆæ‰¹é‡å¤§å°è°ƒåº¦ï¼‰

* æ€æƒ³ï¼š**é€æ­¥å¢åŠ  batch size**ï¼Œä¸€å¼€å§‹ç”¨å°çš„ batch sizeï¼Œéšç€è®­ç»ƒè¿›å±•é€æ¸æ”¾å¤§ã€‚
* åŸå› ï¼š

  * è®­ç»ƒæ—©æœŸï¼Œæ¨¡å‹å‡ ä¹æ˜¯éšæœºçš„ï¼Œæ¢¯åº¦å˜åŒ–é«˜åº¦ç›¸å…³ï¼Œbatch size å¼€å¤§å…¶å®æ²¡å•¥ç»Ÿè®¡å­¦ä¸Šçš„å¥½å¤„ã€‚
  * è®­ç»ƒåæœŸï¼Œæ¨¡å‹æ›´ç¨³å®šï¼Œæ¢¯åº¦ä¹‹é—´å·®å¼‚å¢å¤§ï¼Œè¿™æ—¶å€™å¤§ batch size æ‰èƒ½å‘æŒ¥ç»Ÿè®¡å¹³å‡çš„ä¼˜åŠ¿ã€‚
* ä½œç”¨ï¼š**ä¸æ˜¯ä¼˜åŒ–ç®—æ³•ä¸Šçš„æ”¹è¿›**ï¼Œè€Œæ˜¯ **ç³»ç»Ÿå±‚é¢ï¼ˆååé‡ã€ç¡¬ä»¶åˆ©ç”¨ç‡ï¼‰ä¸Šçš„æé€ŸæŠ€å·§**ã€‚
* ä½œè€…é€‰æ‹© **è·³è¿‡**ï¼Œå› ä¸ºè¿™ä¼šè®©è®¡ç®—å¤æ‚åŒ–ï¼ˆæ¯ step token æ•°å˜äº†ï¼Œç®—å­¦ä¹ ç‡è¡°å‡æ—¶å¾ˆéº»çƒ¦ï¼‰ï¼Œæ”¶ç›Šä¹Ÿä¸æ˜¯å†³å®šæ€§çš„ã€‚

---

## 2. Weight decayï¼ˆæƒé‡è¡°å‡ï¼‰

* åœ¨ GPT-3 è®ºæ–‡ä¸­ï¼Œ**æ‰€æœ‰æ¨¡å‹éƒ½ç”¨ weight decay = 0.1**ã€‚
* åœ¨ AdamW é‡Œï¼Œweight decay è¡¨ç°ä¸ºåœ¨æ›´æ–°æ—¶å¯¹æƒé‡åŠ ä¸€ä¸ªâ€œé‡åŠ›â€å¾€ä¸‹æ‹‰ï¼š

  $$
  w \gets w - \eta \cdot (\nabla L(w) + \lambda w)
  $$

  å…¶ä¸­ $\lambda$ å°±æ˜¯ weight decay ç³»æ•°ã€‚
* ä½œç”¨ï¼š

  * é˜²æ­¢æƒé‡æ— é™å¢å¤§ï¼Œç±»ä¼¼æ­£åˆ™åŒ–ã€‚
  * è¿«ä½¿æ¨¡å‹â€œå‡åŒ€åˆ©ç”¨â€æ›´å¤šå‚æ•°ï¼Œè€Œä¸æ˜¯è®©æŸäº›å•ç‹¬çš„æƒé‡è¿‡å¤§ã€‚
* å®é™…åšæ³•ï¼š

  * ä¸å¯¹æ‰€æœ‰å‚æ•°éƒ½ decayï¼š

    * **ä¸ decay çš„å‚æ•°** â†’ biasã€LayerNorm ä¸­çš„ä¸€ç»´å‚æ•°
    * **è¦ decay çš„å‚æ•°** â†’ embedding çŸ©é˜µã€çº¿æ€§å±‚çš„æƒé‡ï¼ˆäºŒç»´çŸ©é˜µï¼‰

  å› ä¸ºå¯¹ bias / LayerNorm scale åšè¡°å‡é€šå¸¸æ²¡æœ‰æ„ä¹‰ã€‚

---

## 3. FusedAdamW

* **æ™®é€š AdamW** æ›´æ–°æµç¨‹ï¼šå¯¹æ¯ä¸ªå‚æ•°å¼ é‡å•ç‹¬è°ƒç”¨ CUDA kernel æ›´æ–° â†’ äº§ç”Ÿå¾ˆå¤š kernel launch å¼€é”€ã€‚
* **FusedAdamW**ï¼šæŠŠå¤šä¸ªæ›´æ–°æ“ä½œèåˆæˆä¸€ä¸ª CUDA kernel â†’ ä¸€æ¬¡æ€§æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚
* å¥½å¤„ï¼šå‡å°‘ kernel è°ƒåº¦å¼€é”€ â†’ æ›´å¿«ã€‚
* åœ¨æ–‡ä¸­ï¼š

  * æ™®é€š AdamW â†’ **93 ms / step**
  * FusedAdamW â†’ **90 ms / step**
  * è™½ç„¶åªå¿«äº†ä¸€ç‚¹ç‚¹ï¼Œä½†åœ¨å¤§è§„æ¨¡è®­ç»ƒé‡Œï¼ˆæˆåƒä¸Šä¸‡ GPUï¼Œå‡ ç™¾äº¿æ­¥ï¼‰å·®åˆ«éå¸¸å¯è§‚ã€‚

---

## 4. æ•°æ®é‡‡æ ·ï¼ˆwithout replacementï¼‰

* GPT-3 è®­ç»ƒæ—¶ï¼Œæ•°æ®æ˜¯ **ä¸æ”¾å›é‡‡æ ·**ï¼ˆwithout replacementï¼‰ï¼š

  * ä¸€ä¸ªæ ·æœ¬ç”¨è¿‡ï¼Œå°±ä¸ä¼šåœ¨å½“å‰ epoch å†æ¬¡ç”¨åˆ°ã€‚
  * ä¸‹ä¸€æ¬¡ epoch æ‰ä¼šé‡æ–°è¿›å…¥ã€‚
* è¿™æ ·ç¡®ä¿æ•°æ®ä¸ä¼šåœ¨çŸ­æ—¶é—´å†…é‡å¤ï¼Œè®­ç»ƒæ›´ç¨³å®šã€‚

---

## æ€»ç»“ä¸€å¥è¯

* **Batch size schedule**ï¼šæ—©å°åå¤§ï¼Œç³»ç»Ÿå±‚é¢åŠ é€Ÿï¼Œä½†å¤æ‚ï¼Œä½œè€…é€‰æ‹©è·³è¿‡ã€‚
* **Weight decay**ï¼š0.1ï¼Œæ­£åˆ™åŒ–æ•ˆæœï¼Œåº”ç”¨åœ¨æƒé‡çŸ©é˜µ/embeddingï¼Œä¸å¯¹ bias/LayerNorm åšè¡°å‡ã€‚
* **FusedAdamW**ï¼šæŠŠä¼˜åŒ–å™¨æ›´æ–° kernel èåˆï¼Œæé«˜ GPU åˆ©ç”¨ç‡ â†’ 93ms â†’ 90ms/stepã€‚
* **æ•°æ®é‡‡æ · without replacement**ï¼šæ¯ epoch è¦†ç›–æ•°æ®ä¸€éï¼Œé¿å…çŸ­æœŸé‡å¤ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼ å›¾ï¼ŒæŠŠ **learning rate schedule**ï¼ˆwarmup + cosine decayï¼‰å’Œ **batch size schedule** å¯¹æ¯”ä¸€ä¸‹ï¼Œè®©ä½ ç›´è§‚åœ°çœ‹åˆ°åŒºåˆ«ï¼Ÿ

```
    def configure_optimizers(self, weight_decay, learning_rate, device):
        # start with all of the candidate parameters (that require grad)
        param_dict = {pn: p for pn, p in self.named_parameters()}
        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}
        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.
        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.
        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]
        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]
        optim_groups = [
            {'params': decay_params, 'weight_decay': weight_decay},
            {'params': nodecay_params, 'weight_decay': 0.0}
        ]
        num_decay_params = sum(p.numel() for p in decay_params)
        num_nodecay_params = sum(p.numel() for p in nodecay_params)
        print(f"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters")
        print(f"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters")
        # Create AdamW optimizer and use the fused version if it is available
        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters
        use_fused = fused_available and 'cuda' in device
        print(f"using fused AdamW: {use_fused}")
        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)
        return optimizer
```

è¿™æ®µæ–¹æ³•ç”¨æ¥**æŒ‰æœ€ä½³å®è·µé…ç½® AdamW ä¼˜åŒ–å™¨**ï¼šæŠŠéœ€è¦åš weight decay çš„å‚æ•°å’Œä¸éœ€è¦åšçš„åˆ†å¼€ï¼›èƒ½ç”¨å°±å¯ç”¨ **Fused AdamW** æå‡é€Ÿåº¦ï¼›å¹¶è®¾ç½® GPT-3 é£æ ¼çš„è¶…å‚ã€‚é€è¡Œçœ‹ï¼š

---

### 1) å–å‡ºå‚ä¸è®­ç»ƒçš„å‚æ•°

```python
param_dict = {pn: p for pn, p in self.named_parameters()}
param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}
```

* æ”¶é›†æ‰€æœ‰å¸¦åå­—çš„å‚æ•°ï¼Œå¹¶**åªä¿ç•™ `requires_grad=True`** çš„ï¼ˆå†»ç»“çš„å±‚ä¼šè¢«å‰”é™¤ï¼‰ã€‚

---

### 2) åˆ†ç»„ï¼šå“ªäº›åš weight decayï¼Œå“ªäº›ä¸åš

```python
decay_params   = [p for n, p in param_dict.items() if p.dim() >= 2]
nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]
optim_groups = [
    {'params': decay_params,   'weight_decay': weight_decay},
    {'params': nodecay_params, 'weight_decay': 0.0}
]
```

* ç»éªŒæ³•åˆ™ï¼š**äºŒç»´åŠä»¥ä¸Š**ï¼ˆçº¿æ€§/å·ç§¯æƒé‡çŸ©é˜µã€embeddingï¼‰â†’ åš decayï¼›
  **ä¸€ç»´**ï¼ˆbiasã€LayerNorm/BatchNorm çš„ `weight`ï¼‰â†’ **ä¸åš** decayã€‚
* è¿™æ˜¯ Transformer é‡Œå¸¸è§åšæ³•ï¼Œèƒ½æå‡æ”¶æ•›å’Œæ³›åŒ–ã€‚

é¡ºä¾¿æ‰“å°ç»Ÿè®¡ï¼Œæ–¹ä¾¿æ ¸å¯¹ï¼š

```python
num_decay_params    = sum(p.numel() for p in decay_params)
num_nodecay_params  = sum(p.numel() for p in nodecay_params)
print(f"...")
```

---

### 3) å°è¯•å¯ç”¨ **Fused AdamW**

```python
fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters
use_fused = fused_available and 'cuda' in device
print(f"using fused AdamW: {use_fused}")
```

* ç”¨ `inspect.signature` æ£€æŸ¥æœ¬æœº PyTorch çš„ `AdamW` æ˜¯å¦æ”¯æŒ `fused` å‚æ•°ï¼ˆè¾ƒæ–°ç‰ˆæœ¬ CUDA æ„å»ºæ‰æœ‰ï¼‰ã€‚
* åªæœ‰åœ¨ **CUDA è®¾å¤‡** ä¸Šæ‰å¯ç”¨ï¼ˆ`'cuda' in device` çš„ç®€å•åˆ¤æ–­ï¼‰ã€‚

> æç¤ºï¼šå¦‚æœ `device` æ˜¯ `torch.device('cuda')` å¯¹è±¡è€Œä¸æ˜¯å­—ç¬¦ä¸²ï¼Œ`'cuda' in device` ä¸ç”Ÿæ•ˆï¼›æ›´ç¨³å¦¥å¯ç”¨
> `isinstance(device, torch.device) and device.type == 'cuda'` æˆ–ç›´æ¥ `torch.cuda.is_available()`ã€‚

---

### 4) æ„å»ºä¼˜åŒ–å™¨ï¼ˆGPT-3 é£æ ¼è¶…å‚ï¼‰

```python
optimizer = torch.optim.AdamW(
    optim_groups,
    lr=learning_rate,
    betas=(0.9, 0.95),
    eps=1e-8,
    fused=use_fused
)
return optimizer
```

* **å‚æ•°åˆ†ç»„**ä¼ å…¥ AdamWï¼šä¸€ç»„æœ‰ `weight_decay`ï¼Œä¸€ç»„ `0.0`ã€‚
* **betas=(0.9, 0.95)**ã€**eps=1e-8**ï¼šå‚è€ƒ GPT-3 è®ºæ–‡å¸¸ç”¨é…ç½®ã€‚
* `fused=True` æ—¶ä½¿ç”¨**èåˆå†…æ ¸**ä¸€æ¬¡æ€§æ›´æ–°å¤šä¸ªå‚æ•°ï¼Œå‡å°‘ kernel launch å¼€é”€ï¼Œ**æ›´å¿«**ã€‚

---

### å°ç»“ & æ³¨æ„ç‚¹

* è¿™å¥—å†™æ³• = **æ­£ç¡®çš„ AdamW + åˆ†ç»„ weight decay +ï¼ˆå¯é€‰ï¼‰Fused åŠ é€Ÿ**ã€‚
* `p.dim()>=2` æ˜¯å¸¸ç”¨**å¯å‘å¼**ï¼šå¤§å¤šæ•°æƒ…å†µæ­£ç¡®ï¼›è‹¥æ¨¡å‹é‡Œæœ‰**ç‰¹æ®Šä¸€ç»´æƒé‡**éœ€è¦ decayï¼Œè¦å•ç‹¬å¤„ç†ã€‚
* è‹¥ä½ åšäº†**æƒé‡å…±äº«**ï¼ˆå¦‚ `wte.weight = lm_head.weight`ï¼‰ï¼Œä¼˜åŒ–å™¨ä¼šåªç»´æŠ¤**ä¸€æ¬¡**å‚æ•°ï¼ˆåŒä¸€å¯¹è±¡ï¼‰ã€‚
* æ··åˆç²¾åº¦/TF32 ä¸æ­¤å…¼å®¹ï¼›æ¢¯åº¦è£å‰ªåº”åœ¨ `loss.backward()` åã€`optimizer.step()` å‰è¿›è¡Œã€‚

# ä¸€ç»´çš„ä¸ºä»€ä¹ˆä¸åš decay

è¿™æ˜¯ä¸ªå¾ˆç»å…¸çš„é—®é¢˜ ğŸ‘ã€‚åœ¨æ·±åº¦å­¦ä¹ é‡Œï¼Œ**ä¸ºä»€ä¹ˆå¸¸è§åšæ³•æ˜¯ä¸€ç»´å‚æ•°ï¼ˆbiasã€LayerNorm/BatchNorm çš„ Î³/Î² ç­‰ï¼‰ä¸åš weight decay**ï¼ŸåŸå› ä¸»è¦æœ‰ä¸‰ç±»ï¼š

---

## 1. æ•°å­¦/æ­£åˆ™åŒ–è§’åº¦

* **Weight Decay æœ¬è´¨** â‰ˆ åœ¨ç›®æ ‡å‡½æ•°ä¸­åŠ å…¥ä¸€ä¸ª `Î» * ||W||Â²` çš„ L2 æ­£åˆ™åŒ–é¡¹ã€‚
* ç›®çš„æ˜¯é˜²æ­¢æƒé‡çŸ©é˜µæ— é™å˜å¤§ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼ŒåŒæ—¶è®©å¤šä¸ªé€šé“/ç‰¹å¾â€œå‡è¡¡åœ°â€åˆ†æ‹…å·¥ä½œã€‚
* è¿™å¯¹ **å¤§çŸ©é˜µ/å·ç§¯æ ¸ï¼ˆdim â‰¥ 2ï¼‰** æœ‰æ„ä¹‰ï¼Œå› ä¸ºå®ƒä»¬å‚æ•°å¤šã€å®¹é‡å¤§ï¼Œå¾ˆå®¹æ˜“â€œå•ç‚¹çˆ†ç‚¸â€ã€‚

ä½†å¯¹ **bias å’Œå½’ä¸€åŒ–å±‚çš„ç¼©æ”¾å‚æ•°ï¼ˆ1D å‘é‡ï¼‰**ï¼š

* **bias**ï¼šåªæ˜¯åœ¨ç‰¹å¾ä¸ŠåŠ ä¸ªå¹³ç§»ï¼Œä¸ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆèƒ½åŠ›å¤§å¹…å¢åŠ ã€‚å¯¹å®ƒåš L2 æ­£åˆ™åŒ–æ„ä¹‰ä¸å¤§ï¼Œåè€Œä¼šå‹åˆ¶æ¨¡å‹å»æ‹Ÿåˆåˆç†çš„å‡å€¼åç§»ã€‚
* **LayerNorm/BatchNorm çš„ Î³ã€Î²**ï¼š

  * Î³ æ˜¯ç¼©æ”¾å› å­ï¼ŒÎ² æ˜¯å¹³ç§»å› å­ï¼Œå®ƒä»¬æ•°é‡éå¸¸å°‘ï¼ˆé€šå¸¸åªæ˜¯é€šé“æ•°ï¼‰ã€‚
  * Î³ çš„ä½œç”¨æ˜¯è°ƒèŠ‚å½’ä¸€åŒ–åçš„æ¿€æ´»å¹…åº¦ï¼Œå¦‚æœå¯¹å®ƒåš decayï¼Œç­‰ä»·äºå¼ºåˆ¶ç¼©æ”¾å›æ¥è¿‘ 1ï¼Œä¼š **å‰Šå¼±å½’ä¸€åŒ–å±‚çš„çµæ´»æ€§**ã€‚
  * Î² å°±å’Œ bias ä¸€æ ·ï¼Œåªæ˜¯å¹³ç§»ï¼Œä¸å½±å“å®¹é‡å¤æ‚åº¦ã€‚

---

## 2. å®è·µç»éªŒï¼ˆæ¥è‡ªè®ºæ–‡å’Œç¤¾åŒºç»éªŒï¼‰

* Transformerã€BERTã€GPT ç³»åˆ—éƒ½éµå¾ªè¿™ä¸ªåŸåˆ™ï¼š

  > **â€œåªå¯¹æƒé‡çŸ©é˜µï¼ˆâ‰¥2Dï¼‰å’Œ embedding åš weight decayï¼Œä¸å¯¹ biasã€LayerNorm/BatchNorm åš decayã€‚â€**
* è¿™æ˜¯å› ä¸ºå®éªŒè¯æ˜ï¼š

  * å¦‚æœå¯¹ä¸€ç»´å‚æ•°ä¹Ÿåš decayï¼Œæ”¶æ•›ä¼šå˜æ…¢ç”šè‡³æ€§èƒ½ä¸‹é™ã€‚
  * ä¸åš decay çš„æ”¶æ•›æ›´ç¨³å®šã€‚

---

## 3. ç›´è§‚ç†è§£

ä½ å¯ä»¥è¿™ä¹ˆæƒ³ï¼š

* **Weight Decay** â‰ˆ â€œé‡åŠ›â€ï¼Œæ‹‰ç€å¤§æƒé‡å¾€ 0 æ”¶ç¼©ã€‚
* å¯¹â€œå¤§å—å¤´â€ï¼ˆçŸ©é˜µæƒé‡ï¼‰ç”¨è¿™ä¸ªé‡åŠ› â†’ é˜²æ­¢å®ƒä»¬é•¿å¾—å¤ªå¤§ã€‚
* å¯¹â€œå°è§’è‰²â€ï¼ˆbiasã€ç¼©æ”¾å‚æ•°ï¼‰ç”¨è¿™ä¸ªé‡åŠ› â†’ ä¼šè®©å®ƒä»¬å¤±å»çµæ´»è°ƒèŠ‚ç‰¹å¾åˆ†å¸ƒçš„èƒ½åŠ›ï¼Œåè€Œåäº‹ã€‚

---

âœ… æ€»ç»“ï¼š
**ä¸€ç»´å‚æ•°ï¼ˆbiasã€LayerNorm/BatchNorm çš„ Î³/Î²ï¼‰é€šå¸¸ä¸åš weight decay**ï¼Œå› ä¸ºå®ƒä»¬å‚æ•°å°‘ã€åŠŸèƒ½æ˜¯å¹³ç§»/ç¼©æ”¾ï¼Œä¸ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼›å¯¹å®ƒä»¬åš decay åè€Œé™åˆ¶æ¨¡å‹è¡¨è¾¾èƒ½åŠ›å’Œè®­ç»ƒç¨³å®šæ€§ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **ç¤ºæ„å›¾**ï¼Œå±•ç¤ºçŸ©é˜µæƒé‡å’Œä¸€ç»´å‚æ•°åœ¨è®­ç»ƒä¸­å„è‡ªçš„â€œè§’è‰²â€å’Œä¸ºä»€ä¹ˆä¸€ä¸ªéœ€è¦æ”¶ç¼©ã€ä¸€ä¸ªä¸éœ€è¦ï¼Ÿ


# gradient accumulation

note before we continue here I just want to point out that the relationship between weight Decay learning rate batch
size the atom parameters beta 1 beta 2 the Epsilon and so on these are very complicated uh mathematical
relationships in the optimization literature and um for the most part I'm
in this video I'm just trying to copy paste the settings that open AI used but this is a complicated topic uh quite
deep and um yeah in this video I just want to copy the parameters because it's a whole different video to really talk
about that in detail and give it a proper Justice instead of just high level intuitions uh now the next thing that I
want to move on to is that uh this paragraph here by the way we're going to turn back around to when we improve our
data loader for now I want to swing back around to this
table where you will notice that um for different models we of course have
different U hyper parameters for the Transformer that dictate the size of the Transformer Network we also have a
different learning rate so we're seeing the pattern that the bigger networks are trained with slightly lower learning rates and we also see this batch size
where in in the small networks they use a smaller batch size and in the bigger networks they use a bigger batch size
now the problem with for us is we can't just use 0.5 million batch size because
uh if I just try to come in here and I try to set uh this uh B where is my
b um b
equals where where do I call the DAT okay b equal 16 if I try to set um
well well we have to be careful it's not 0.5 million because this is the badge size in the number of tokens every
single one of our rows is24 tokens so 0.5 E6 1 million divide 1024 this would
need about a 488 match size so the problem is I can't come in here and set this to 488 uh
because my GPU would explode um this would not fit for sure and so but we
still want to use this batch size because again as I mentioned the batch size is correlated with all the other
optimization hyper parameters and the learning rates and so on so we want to have a faithful representation of all
the hyper parameters and therefore we need to uh use a bat size of .5 million
roughly but the question is how do we use .5 million if we only have a small GPU well for that we need to use what's
called gradient accumulation uh so we're going to turn to that next and it allows us to simulate in a Serial way any
arbitrary batch size that we set and so we can do a batch size of .5 million we
just have to run longer and we have to process multiple sequences and basically
add up all the gradients from them to simulate a batch size of .5 million so let's turn to that next okay so I
started the implementation right here just by adding these lines of code and basically what I did is first I set the
total batch size that we desire so this is exactly .5 million and I used a nice
number a power of two uh because 2 to the 19 is 524 288 so it's roughly .5
million it's a nice number now our micro batch size as we call it now is 16 so
this is going to be we still have B BYT in the SE that go into the Transformer
and do forward backward but we're not going to do an update right we're going to do many forward backwards we're going
to and those gradients are all going to plus equals on the parameter gradients they're all going to add up so we're
going to do forward backward grad akum steps number of times and then we're going to do a single update once all
that is accumulated so in particular our micro batch size is just now controlling how
many tokens how many rows we're processing in a single go over a forward backward so um here we are doing 16 *
124 we're doing 16 384 um tokens per forward backward and
we are supposed to be doing 2 to the 19 whoops what am I doing 2 to the
19 in total so the grat Aon will be
32 uh so therefore gr AUM here will work out to 32 and we have to do 32 forward
backward um and then a single update now we see that we have about 100 milliseconds for a singer forward
backward so doing 32 of them will be will make every step roughly 3 seconds
just napkin math so that's grum steps but now we actually have to Implement that so we're
going to swing over to our training Loop because now this part
here and this part here the forward and the backward we have to now repeat this 32 times before we do everything else
that follows so let's uh see how we can Implement that so let's come over here and actually we do have to load a new
batch every single time so let me move that over here and now this is where we have the inner loop so for micro step in
range graum steps we do this and remember that l.
backward always deposits gradients so we're doing inside losta backward there's always a plus equals on the
gradients so in every single L of backward gradients will add up on the gradient
tensors um so we lost that backward and then we get all the gradients over there
and then we normalize and everything else should just follow um so we're very
close but actually there's like subtle and deep issue here and this is actually
incorrect so invite I invite you to think about why this is not yet sufficient um and uh let me fix it then
okay so I brought back the jupyter notebook so we can think about this carefully in a simple toy setting and
see what's happening so let's create a very simple neural nut that takes a 16 Vector of 16 numbers and returns a
single number and then here I'm creating some random uh examples X and some targets uh
y Y and then we are using the mean squared loss uh here to calculate the
loss so basically what this is is four individual examples and we're just doing
Simple regression with the mean squared loss over those four examples now when we calculate the loss
and we lost that backward and look at the gradient this is the gradient that we achieve now the loss objective here
notice that in MSE loss the default for the loss function is reduction is mean
so we're we're calculating the average mean loss um the the mean loss here over
the four examples so this is the exact loss objective and this is the average
the one over four because there are four independent examples here and then we have the four examples and their mean
squared error the squared error and then this makes it the mean squared error so
therefore uh we are we calculate the squared error and then we normalize it to make it the mean over the examples
and there's four examples here so now when we come to the gradient accumulation version of it this uh this
here is the gradient accumulation version of it where we have grad acum steps of four and I reset the gradient
we've grum steps of four and now I'm evaluating all the examples individually instead and calling L that backward on
them many times and then we're looking at the gradient that we achieve from that so basically now we forward our
function calculate the exact same loss do a backward and we do that four times
and when we look at the gradient uh you'll notice that the gradients don't match so here we uh did a single batch
of four and here we did uh four gradient accumulation steps of batch size one and
the gradients are not the same and basically the the reason that they're not the same is exactly because this
mean squared error gets lost this one quarter in this loss gets lost because what happens here is the loss of
objective for every one of the loops is just a mean squ error um which in this
case because there's only a single example is just this term here so that was the loss in the zeroth eration same
in the first third and so on and then when you do the loss. backward we're accumulating gradients and what happens
is that accumulation in the gradient is basically equivalent to doing a sum in
the loss so our loss actually here is this
without the factor of one quarter outside of it so we're missing the normalizer and therefore our gradients
are off and so the way to fix this or one of them is basically we can actually come here and we can say loss equals
loss divide 4 and what happens now is that we're introducing we're we're scaling our loss
we're introducing a one quarter in front of all of these
places so all the individual losses are now scaled by one quarter and and then when we backward all of these accumulate
with a sum but now there's a one quarter inside every one of these components and now our losses will be
equivalent so when I run this you see that the U gradients are now identical
so long story short with this simple example uh when you step through it you can see that basically the reason that
this is not correct is because in the same way as here in the MSE loss the
loss that we're calculating here in the model is using a reduction of mean as
well uh so where's the loss after that cross entropy and by default the reduction uh
here in Cross entropy is also I don't know why they don't show it but it's the mean uh the mean uh loss at all the B
BYT elements right so there's a reduction by mean in
there and if we're just doing this gradient accumulation here we're missing that and so the way to fix this is to
simply compensate for the number of gradient accumulation steps and we can in the same way divide this loss so in
particular here the number of steps that we're doing is loss equals loss divide
gradient accumulation steps so even uh co-pilot s gets the modification but in
the same way exactly we are scaling down the loss so that when we do loss that backward which basically corresponds to
a sum in the objective we are summing up the already normalized um loss and and therefore
when we sum up the losses divided by grum steps we are recovering the additional normalizer uh and so now
these two will be now this will be equivalent to the original uh sort of optimization because the gradient will
come out the same okay so I had to do a few more touch-ups and I launched launched the optimization here so in
particular one thing we want to do because we want to print things nicely is well first of all we need to create
like an accumulator over the loss we can't just print the loss because we'd be printing only the final loss at the
final micro step so instead we have loss ofon which I initialize at zero and then I accumulate a uh the loss into it and
I'm using detach so that um uh I'm detaching the tensor uh from the graph
and I'm just trying to keep track of the values so I'm making these Leaf nodes when I add them so that's lakum and then
we're printing that here instead of loss and then in addition to that I had to account for the grum steps inside the
tokens processed because now the tokens processed per step is B * T * gradient
accumulation so long story short here we have the optimization it looks uh
reasonable right we're starting at a good spot we calculated the grum steps to be 32 and uh we're getting about 3 seconds
here right um and so this looks pretty good now if
you'd like to verify that uh your optimization and the implementation here is correct and your working on a side
well now because we have the total patch size and the gradient accumulation steps our setting of B is purely a performance
optimization kind of setting so if you have a big GPU you can actually increase this to 32 and you'll probably go a bit
faster if you have a very small GPU you can try eight or four but in any case you should be getting the exact same
optimization and the same answers up to like a floating Point error because the gradient accumulation kicks in and um
and can um handle everything serially as an Neary so uh that's it for gradient
accumulation I think okay so now is the time to bring out the heavy weapons uh you've noticed that so far we've only

å½“ç„¶å¯ä»¥ï¼ä»¥ä¸‹æ˜¯å…³äºã€Œæ¢¯åº¦ç´¯ç§¯ï¼ˆgradient accumulationï¼‰ã€çš„ä¸­æ–‡è§£é‡Šï¼š

---

## ğŸ§  **æ¢¯åº¦ç´¯ç§¯ï¼šè§£å†³å°æ˜¾å­˜ä¹Ÿèƒ½æ¨¡æ‹Ÿå¤§ batch çš„æŠ€å·§**

---

### ğŸ“Œ é—®é¢˜èƒŒæ™¯

åœ¨ GPT-3 è®ºæ–‡ä¸­ï¼Œä½œè€…ä½¿ç”¨äº† **è¶…å¤§çš„ batch sizeï¼ˆå¦‚ 50 ä¸‡ä¸ª tokenï¼‰** æ¥è®­ç»ƒæ¨¡å‹ã€‚ä½†é—®é¢˜æ˜¯ï¼š

* ğŸ§¨ **æˆ‘ä»¬æ˜¾å¡æ˜¾å­˜å°**ï¼Œä¸èƒ½ä¸€æ¬¡æ€§å¡è¿›è¿™ä¹ˆå¤š tokenï¼›
* ğŸ§ª **æˆ‘ä»¬åˆæƒ³ç”¨ä¸€æ ·çš„ batch size**ï¼Œæ¥æ¨¡æ‹Ÿ GPT-3 çš„è¶…å¤§è®­ç»ƒè®¾ç½®ï¼›
* âœ… **è§£å†³æ–¹æ¡ˆå°±æ˜¯ã€Œæ¢¯åº¦ç´¯ç§¯ã€**ï¼ˆGradient Accumulationï¼‰ã€‚

---

### ğŸ§© æ¢¯åº¦ç´¯ç§¯çš„åŸç†

* æŠŠä¸€ä¸ªå¤§ batchï¼ˆå¦‚ 50 ä¸‡ tokensï¼‰åˆ†æˆå¤šä¸ªå°æ‰¹ï¼ˆå¦‚æ¯æ¬¡ 16 ä¸ª batchï¼‰ï¼š

  * æ¯æ¬¡ **forward + backward** ä½†**ä¸æ›´æ–°æƒé‡**
  * å¤šæ¬¡ç´¯åŠ æ¢¯åº¦
  * **ç´¯åŠ  N æ¬¡ä¹‹åå†æ›´æ–°ä¸€æ¬¡æƒé‡**

**ä¸¾ä¸ªä¾‹å­ï¼š**

* ç›®æ ‡æ€» batch size = `524288`ï¼ˆçº¦ç­‰äº 50 ä¸‡ï¼Œé€‰äº†ä¸ª 2 çš„å¹‚ï¼Œæ–¹ä¾¿ CUDA ä¼˜åŒ–ï¼‰
* æ¯æ¬¡å¤„ç†ä¸€ä¸ªã€Œå¾®å°æ‰¹ã€`B = 16`ï¼Œæ¯ä¸ªæ ·æœ¬é•¿åº¦ `T = 1024`ï¼Œåˆ™æ¯è½®å¤„ç† `16 Ã— 1024 = 16384` tokens
* ä¸ºäº†ç´¯è®¡åˆ° 524288 ä¸ª tokenï¼š

  * `524288 Ã· 16384 = 32`
  * æ‰€ä»¥æˆ‘ä»¬éœ€è¦ **32 æ¬¡ forward + backward æ‰æ›´æ–°ä¸€æ¬¡æƒé‡**

è¿™å°±æ˜¯ `gradient_accumulation_steps = 32`

---

### âš ï¸ å°ç»†èŠ‚ï¼šloss è¦é™¤ä»¥ç´¯ç§¯æ­¥æ•°

è¿™é‡Œæœ‰ä¸€ä¸ªå¾ˆå®¹æ˜“å‡ºé”™çš„ç»†èŠ‚ï¼š

* é€šå¸¸æˆ‘ä»¬çš„ loss æ˜¯ `mean`ï¼Œæ¯”å¦‚ `CrossEntropyLoss(reduction='mean')`
* å¦‚æœæˆ‘ä»¬åš 32 æ¬¡ `loss.backward()`ï¼Œå…¶å®æ˜¯æŠŠæ¯ä¸€æ­¥çš„æ¢¯åº¦ **ç›´æ¥ç›¸åŠ äº†**
* é‚£ç­‰äºæŸå¤±å‡½æ•°ä»ã€Œå¹³å‡å€¼ã€å˜æˆäº†ã€Œæ€»å’Œã€äº†ï¼âŒ

æ‰€ä»¥ï¼Œ**æ¯æ¬¡éƒ½è¦æŠŠ loss é™¤ä»¥ `gradient_accumulation_steps`**ï¼Œæ‰èƒ½å¾—åˆ°ç­‰æ•ˆçš„æ¢¯åº¦ï¼š

```python
loss = loss / gradient_accumulation_steps
loss.backward()
```

è¿™æ ·ç´¯ç§¯å‡ºæ¥çš„æ¢¯åº¦ä¸ã€Œä¸€æ¬¡å¤§ batch çš„æ¢¯åº¦ã€å®Œå…¨ç­‰ä»· âœ…

---

### ğŸ–¨ï¸ å…¶ä»–ä»£ç æ”¹åŠ¨

1. ç´¯è®¡ loss ç”¨æ¥æ‰“å°ï¼Œä¸èƒ½åªæ‰“å°æœ€åä¸€æ¬¡çš„ï¼š

   ```python
   loss_accum += loss.detach()
   ```

2. token æ•°çš„ç»Ÿè®¡ä¹Ÿè¦ä¹˜ä¸Šç´¯ç§¯æ­¥æ•°ï¼š

   ```python
   tokens += B * T * gradient_accumulation_steps
   ```

3. å¤–å±‚ loop æ¯æ¬¡åšä¸€æ¬¡å®Œæ•´çš„ 32 æ¬¡ç´¯ç§¯æ›´æ–°

---

### ğŸ’¡ æ¢¯åº¦ç´¯ç§¯çš„ä½œç”¨æ€»ç»“

| åŠŸèƒ½                | æè¿°                                    |
| ----------------- | ------------------------------------- |
| æ¨¡æ‹Ÿå¤§ batch         | ç”¨å°æ˜¾å¡å®ç°å¤§æ¨¡å‹è®­ç»ƒ                           |
| ä¿æŒå­¦ä¹ ç‡ç­‰è¶…å‚æ•°ä¸å˜       | æ›´è´´è¿‘ GPT-3 ç­‰å¤§å‹æ¨¡å‹çš„ä¼˜åŒ–è®¾ç½®                  |
| ä¸ã€Œä¸€æ¬¡æ€§å¤§ batchã€ç»“æœç›¸åŒ | é™¤äº†æµ®ç‚¹è¯¯å·®å¤–ï¼Œæœ€ç»ˆæ•ˆæœæ˜¯ç­‰ä»·çš„                      |
| å¯è°ƒèŠ‚ B å€¼åšæ€§èƒ½ä¼˜åŒ–      | æœ‰å¤§ GPU å°±è°ƒå¤§ Bï¼Œæœ‰å° GPU å°±è°ƒå° Bï¼Œä½†ä¼˜åŒ–ç»“æœä¿æŒä¸€è‡´ âœ… |

---

å¦‚æœä½ è¿˜æƒ³è¿›ä¸€æ­¥å¯è§†åŒ–ï¼š

* **ä¸ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ vs ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯** çš„è®­ç»ƒæ›²çº¿å¯¹æ¯”
* è°ƒæ•´ `B` æˆ– `accum_steps` å¯¹è®­ç»ƒç¨³å®šæ€§çš„å½±å“

æˆ‘ä¹Ÿå¯ä»¥å¸®ä½ å±•ç¤ºï¼ğŸŒŸ

å¥½çš„ï¼Œè¿™ä¸€å¤§æ®µå†…å®¹çš„æ ¸å¿ƒå°±æ˜¯ **Gradient Accumulationï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰**ã€‚æˆ‘ç»™ä½ æ¢³ç†ä¸€ä¸‹é‡ç‚¹ï¼Œå¹¶åˆ†å±‚æ¬¡è§£é‡Šï¼š

---

## ğŸ”‘ èƒŒæ™¯é—®é¢˜

* **å¤§æ¨¡å‹è®­ç»ƒéœ€è¦è¶…å¤§çš„ batch size**ï¼ˆæ¯”å¦‚ 50 ä¸‡ tokens ä¸€æ¬¡ï¼‰ã€‚
* ä½†æ˜¯ **å•å¡ GPU æ˜¾å­˜æœ‰é™**ï¼Œæ ¹æœ¬æ”¾ä¸ä¸‹è¿™ä¹ˆå¤§çš„ batchã€‚
* æ€ä¹ˆåŠï¼Ÿâ€”â€”ç”¨ **æ¢¯åº¦ç´¯ç§¯** æ¥â€œæ¨¡æ‹Ÿâ€å¤§ batchã€‚

---

## âš™ï¸ æ¢¯åº¦ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰çš„åŸºæœ¬åŸç†

### 1. æ­£å¸¸è®­ç»ƒ

è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬çš„å¾ªç¯æ˜¯è¿™æ ·çš„ï¼š

```python
for step in range(max_steps):
    optimizer.zero_grad()
    logits, loss = model(x, y)
    loss.backward()       # è®¡ç®—æ¢¯åº¦ï¼ˆç´¯åŠ åˆ°å‚æ•° .grad ä¸Šï¼‰
    optimizer.step()      # å‚æ•°æ›´æ–°
```

è¿™é‡Œä¸€ä¸ª step å°±ç­‰äºä¸€ä¸ª batch çš„å‰å‘ã€åå‘å’Œæ›´æ–°ã€‚

---

### 2. æ˜¾å­˜ä¸å¤Ÿæ—¶ï¼ˆæ¢¯åº¦ç´¯ç§¯æ€è·¯ï¼‰

å¦‚æœ batch å¤ªå¤§ï¼Œæ”¾ä¸è¿› GPUï¼Œæˆ‘ä»¬å¯ä»¥ï¼š

* æŠŠå®ƒ **æ‹†æˆå¤šä¸ªå° batchï¼ˆmicro-batchï¼‰**ã€‚
* æ¯ä¸ª micro-batch åš **forward + backward**ï¼Œæ¢¯åº¦ä¼šè‡ªåŠ¨ç´¯åŠ åˆ° `.grad` é‡Œï¼ˆPyTorch é»˜è®¤æ˜¯ +=ï¼‰ã€‚
* ä½† **æš‚æ—¶ä¸è°ƒç”¨ `optimizer.step()`**ã€‚
* å½“ç´¯ç§¯äº† N ä¸ª micro-batch åï¼Œæ‰åšä¸€æ¬¡å‚æ•°æ›´æ–°ã€‚

è¿™æ ·ä¸€æ¥ï¼š

```
å¤§ batch = N Ã— å° batch
```

å°±æ¨¡æ‹Ÿå‡ºäº†å¤§ batch çš„æ•ˆæœã€‚

---

### 3. ä¸ºä»€ä¹ˆè¦é™¤ä»¥ `grad_accum_steps`

è¿™é‡Œæœ‰ä¸€ä¸ªå‘ç‚¹ï¼š

* `loss.backward()` é»˜è®¤å¯¹ loss çš„ **å‡å€¼ (mean)** è¿›è¡Œåå‘ä¼ æ’­ã€‚
* å¦‚æœä½ ç´¯ç§¯äº† 32 æ¬¡å° batchï¼Œæ¯æ¬¡ loss éƒ½æ˜¯å¹³å‡çš„ï¼Œä½†ä½ ç›´æ¥ç›¸åŠ ï¼Œå®é™…ä¸Šä¼šæ”¾å¤§ 32 å€ã€‚
* æ­£ç¡®åšæ³•ï¼š**åœ¨åå‘ä¼ æ’­å‰ï¼ŒæŠŠ loss é™¤ä»¥ç´¯ç§¯æ­¥æ•°**ï¼š

```python
loss = loss / grad_accum_steps
loss.backward()
```

è¿™æ ·æ¯æ¬¡ç´¯åŠ çš„æ¢¯åº¦æ˜¯ç¼©æ”¾è¿‡çš„ï¼Œæœ€ç»ˆæ•ˆæœå’Œä¸€æ¬¡æ€§è·‘å¤§ batch ä¸€è‡´ã€‚

---

## ğŸ“Š å°ä¾‹å­

å‡è®¾ï¼š

* æƒ³è¦çš„ batch size = 64
* GPU åªèƒ½æ”¾ä¸‹ batch size = 16
* é‚£ä¹ˆï¼š

  * `grad_accum_steps = 64 / 16 = 4`
  * æ¯ 4 ä¸ªå° batch ä¹‹åæ›´æ–°ä¸€æ¬¡å‚æ•°ã€‚

ä»£ç å¤§æ¦‚æ˜¯ï¼š

```python
grad_accum_steps = 4
optimizer.zero_grad()
for micro_step in range(grad_accum_steps):
    x, y = next(data_iter)               # å–ä¸€ä¸ª micro batch
    logits, loss = model(x, y)
    (loss / grad_accum_steps).backward() # ç¼©æ”¾æŸå¤±ï¼Œç´¯ç§¯æ¢¯åº¦
optimizer.step()
```

---

## ğŸ§  å…³é”®ç†è§£

1. **ä¸ºä»€ä¹ˆè¦æ¢¯åº¦ç´¯ç§¯ï¼Ÿ**

   * ä¸ºäº†åœ¨æ˜¾å­˜æœ‰é™çš„æƒ…å†µä¸‹æ¨¡æ‹Ÿæ›´å¤§çš„ batch sizeï¼Œæå‡ç¨³å®šæ€§ã€‚
   * å¤§ batch size é€šå¸¸éœ€è¦è°ƒæ•´å­¦ä¹ ç‡ã€weight decay ç­‰è¶…å‚æ•°ã€‚

2. **ä¸ºä»€ä¹ˆè¦å¯¹ loss ç¼©æ”¾ï¼Ÿ**

   * å› ä¸º `loss.backward()` æœ¬è´¨æ˜¯â€œæŠŠ loss å¯¹å‚æ•°çš„æ¢¯åº¦æ±‚å‡ºæ¥å¹¶åŠ åˆ° .grad ä¸Šâ€ã€‚
   * å¦‚æœä¸ç¼©æ”¾ï¼Œç­‰ä»·äºâ€œåšäº† N æ¬¡ sumâ€ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡ meanã€‚
   * è¿™ä¼šè®©æ¢¯åº¦å˜å¤§ N å€ï¼Œè®­ç»ƒå‘æ•£ã€‚

3. **è®¡ç®—å¼€é”€**

   * æ—¶é—´ä¼šå˜æ…¢ï¼ˆè¦å¤šæ¬¡ forward/backwardï¼‰ï¼Œä½†æ˜¾å­˜å‹åŠ›å¤§å¤§å‡å°ã€‚
   * æœ¬è´¨æ˜¯ç”¨ **æ—¶é—´æ¢ç©ºé—´**ã€‚

---

âœ… **ä¸€å¥è¯æ€»ç»“ï¼š**
**Gradient Accumulation = ç”¨å¤šä¸ªå° batch çš„æ¢¯åº¦ç´¯åŠ ï¼Œæ¥æ¨¡æ‹Ÿä¸€ä¸ªå¤§ batch çš„è®­ç»ƒæ•ˆæœã€‚**
å…³é”®ç‚¹æ˜¯è¦åœ¨ `.backward()` å‰å¯¹ loss é™¤ä»¥ç´¯ç§¯æ­¥æ•°ï¼Œå¦åˆ™æ¢¯åº¦ä¼šæ”¾å¤§ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **å¯¹æ¯”æµç¨‹å›¾**ï¼ˆä¸€æ¬¡å¤§ batch vs å¤šæ¬¡å° batch + æ¢¯åº¦ç´¯ç§¯ï¼‰ï¼Œè¿™æ ·ç›´è§‚å±•ç¤ºåŒºåˆ«ï¼Ÿ

```
import torch

# super simple little MLP
net = torch.nn.Sequential(
    torch.nn.Linear(16, 32),
    torch.nn.GELU(),
    torch.nn.Linear(32, 1)
)
torch.random.manual_seed(42)
x = torch.randn(4, 16)
y = torch.randn(4, 1)
net.zero_grad()
yhat = net(x)
loss = torch.nn.functional.mse_loss(yhat, y)
loss.backward()
print(net[0].weight.grad.view(-1)[:10])
```

å¥½ï¼Œè¿™æ®µä»£ç æ˜¯ä¸€ä¸ª **PyTorch çš„æœ€å°åŒ–ç¤ºä¾‹**ï¼Œæ¼”ç¤ºäº†å¦‚ä½•æ„å»ºä¸€ä¸ªå°çš„ MLPã€è®¡ç®—æŸå¤±ã€åšä¸€æ¬¡åå‘ä¼ æ’­ï¼Œå¹¶æŸ¥çœ‹æ¢¯åº¦ã€‚é€è¡Œæ‹†è§£ä¸€ä¸‹ï¼š

---

### ä»£ç è§£æ

```python
import torch
```

å¯¼å…¥ PyTorchã€‚

---

```python
# super simple little MLP
net = torch.nn.Sequential(
    torch.nn.Linear(16, 32),
    torch.nn.GELU(),
    torch.nn.Linear(32, 1)
)
```

è¿™é‡Œå®šä¹‰äº†ä¸€ä¸ª **é¡ºåºæ¨¡å‹ï¼ˆMLPï¼Œå¤šå±‚æ„ŸçŸ¥æœºï¼‰**ï¼š

* `torch.nn.Linear(16, 32)`ï¼šè¾“å…¥ 16 ç»´ â†’ è¾“å‡º 32 ç»´ã€‚
* `torch.nn.GELU()`ï¼šæ¿€æ´»å‡½æ•°ã€‚
* `torch.nn.Linear(32, 1)`ï¼šè¾“å…¥ 32 ç»´ â†’ è¾“å‡º 1 ç»´ï¼ˆä¸€ä¸ªæ ‡é‡å›å½’ï¼‰ã€‚

æ•´ä½“ç»“æ„ï¼š
**16 â†’ 32 â†’ GELU â†’ 1**

---

```python
torch.random.manual_seed(42)
```

è®¾ç½®éšæœºæ•°ç§å­ï¼Œä¿è¯æ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´ï¼ˆæƒé‡åˆå§‹åŒ–ã€è¾“å…¥æ ·æœ¬éƒ½ä¸€æ ·ï¼‰ã€‚

---

```python
x = torch.randn(4, 16)
y = torch.randn(4, 1)
```

ç”Ÿæˆæ•°æ®ï¼š

* `x`ï¼šshape = `(4, 16)`ï¼Œ4 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬ 16 ç»´ã€‚
* `y`ï¼šshape = `(4, 1)`ï¼Œ4 ä¸ªå›å½’ç›®æ ‡å€¼ã€‚

---

```python
net.zero_grad()
```

æ¸…ç©ºç½‘ç»œä¸­æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼ˆå¦åˆ™æ¢¯åº¦ä¼šç´¯ç§¯ï¼‰ã€‚

---

```python
yhat = net(x)
```

å‰å‘ä¼ æ’­ï¼š

* `net(x)` æŠŠè¾“å…¥ `x` å–‚ç»™ MLPï¼Œå¾—åˆ°é¢„æµ‹å€¼ `yhat`ï¼Œå½¢çŠ¶ `(4, 1)`ã€‚

---

```python
loss = torch.nn.functional.mse_loss(yhat, y)
```

è®¡ç®— **å‡æ–¹è¯¯å·®æŸå¤± (MSE loss)**ï¼š

$$
\text{loss} = \frac{1}{N} \sum_i (yhat_i - y_i)^2
$$

---

```python
loss.backward()
```

**åå‘ä¼ æ’­**ï¼š

* è‡ªåŠ¨è®¡ç®— loss å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚
* æ¢¯åº¦ä¼šå­˜æ”¾åœ¨ `param.grad` é‡Œã€‚

---

```python
print(net[0].weight.grad.view(-1)[:10])
```

å–å‡º **ç¬¬ä¸€å±‚çš„æƒé‡çŸ©é˜µçš„æ¢¯åº¦**ï¼š

* `net[0]` å°±æ˜¯ç¬¬ä¸€ä¸ª `Linear(16, 32)` å±‚ã€‚
* `weight` æ˜¯å½¢çŠ¶ `(32, 16)` çš„çŸ©é˜µã€‚
* `.grad` æ˜¯åŒæ ·å¤§å°çš„æ¢¯åº¦çŸ©é˜µã€‚
* `.view(-1)` æŠŠå®ƒæ‘Šå¹³æˆä¸€ç»´å‘é‡ã€‚
* `[:10]` æ‰“å°å‰ 10 ä¸ªå…ƒç´ ã€‚

---

### æ€»ç»“

è¿™æ®µä»£ç åšçš„äº‹æƒ…æ˜¯ï¼š

1. å®šä¹‰ä¸€ä¸ªå° MLPã€‚
2. éšæœºç”Ÿæˆ 4 ä¸ªè¾“å…¥æ ·æœ¬å’Œç›®æ ‡ã€‚
3. å‰å‘ä¼ æ’­ â†’ å¾—åˆ°é¢„æµ‹ã€‚
4. è®¡ç®— MSE æŸå¤±ã€‚
5. åå‘ä¼ æ’­ â†’ å¾—åˆ°ç¬¬ä¸€å±‚æƒé‡çš„æ¢¯åº¦ã€‚
6. æ‰“å°æ¢¯åº¦çš„å‰ 10 ä¸ªæ•°ã€‚

---

ğŸ’¡ ç”¨ä¸€å¥è¯æ¥è¯´ï¼š**è¿™æ˜¯ä¸€ä¸ªæœ€ç®€ç¤ºä¾‹ï¼Œå±•ç¤ºäº†â€œå‰å‘ + æŸå¤± + åå‘ä¼ æ’­ + æŸ¥çœ‹æ¢¯åº¦â€çš„å®Œæ•´è®­ç»ƒæ­¥éª¤**ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸ªå›¾ï¼ŒæŠŠ **æ•°æ®æµåŠ¨ï¼ˆå‰å‘ï¼‰** å’Œ **æ¢¯åº¦æµåŠ¨ï¼ˆåå‘ï¼‰** åœ¨è¿™ä¸ªå° MLP é‡Œæ ‡å‡ºæ¥ï¼Ÿè¿™æ ·æ›´ç›´è§‚ã€‚


# loss.detach() è§£é‡Šä¸€ä¸‹

å¥½é—®é¢˜ ğŸ‘ã€‚æˆ‘ä»¬æ¥çœ‹ **`loss.detach()`** åœ¨ PyTorch é‡Œçš„ä½œç”¨ã€‚

---

### èƒŒæ™¯

åœ¨ PyTorch é‡Œï¼Œ**Tensor æœ‰ä¸¤ç§å½¢æ€**ï¼š

* æ™®é€šå¼ é‡ï¼ˆä¸è¿½è¸ªæ¢¯åº¦ï¼‰
* è®¡ç®—å›¾é‡Œçš„å¼ é‡ï¼ˆ`requires_grad=True`ï¼Œä¼šè®°å½•å®ƒæ˜¯ç”±å“ªäº›æ“ä½œè®¡ç®—å¾—åˆ°çš„ï¼‰

æ¯”å¦‚ï¼š

```python
loss = torch.nn.functional.mse_loss(yhat, y)
```

è¿™é‡Œçš„ `loss` æ˜¯ä¸€ä¸ª **æ ‡é‡ Tensor**ï¼Œå®ƒå±äºè®¡ç®—å›¾çš„ä¸€éƒ¨åˆ†ï¼Œå’Œ `yhat`ã€`net` çš„å‚æ•°éƒ½æœ‰å…³è”ã€‚
è¿™æ ·å½“ä½ è°ƒç”¨ `loss.backward()` æ—¶ï¼ŒPyTorch å°±èƒ½æ ¹æ® `loss` çš„è®¡ç®—å›¾ï¼Œå»ç®—å‡ºæ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ã€‚

---

### `detach()` çš„ä½œç”¨

`loss.detach()` ä¼šï¼š

1. **åˆ‡æ–­è®¡ç®—å›¾** â€”â€” è¿”å›ä¸€ä¸ªæ–° Tensorï¼Œå®ƒæœ‰ç›¸åŒçš„æ•°å€¼ï¼Œä½†ä¸å†è·Ÿè¸ªæ¢¯åº¦ã€‚
2. **åœæ­¢åå‘ä¼ æ’­** â€”â€” è¿™ä¸ªæ–° Tensor ä¸ä¼šå†å¾€å‰ä¼ é€’æ¢¯åº¦ã€‚

ä¸¾ä¸ªä¾‹å­ï¼š

```python
import torch

x = torch.randn(3, requires_grad=True)
y = x * 2
loss = y.sum()

print(loss.requires_grad)  # Trueï¼Œå› ä¸ºå®ƒä¾èµ–äº x
loss_detached = loss.detach()
print(loss_detached.requires_grad)  # False
```

---

### ä¸ºä»€ä¹ˆè¦ç”¨ `detach()`ï¼Ÿ

å¸¸è§ç”¨é€”æœ‰ï¼š

1. **è®°å½•æˆ–æ‰“å° loss**

   ```python
   running_loss += loss.detach()
   ```

   è¿™æ ·ä¸ä¼šæŠŠ `running_loss` å˜æˆè®¡ç®—å›¾çš„ä¸€éƒ¨åˆ†ï¼ˆé¿å…æ˜¾å­˜æ³„æ¼ï¼‰ã€‚

2. **é¿å…æ¢¯åº¦ä¼ æ’­**
   æ¯”å¦‚ä½ æƒ³åœ¨è®­ç»ƒæ—¶ï¼Œç”¨ `loss` çš„æ•°å€¼åšä¸€äº›ç»Ÿè®¡æˆ– logï¼Œä½†æ˜¯è¿™äº›æ“ä½œä¸åº”è¯¥å½±å“æ¢¯åº¦ï¼Œå°±è¦å…ˆ `detach()`ã€‚

3. **å†»ç»“éƒ¨åˆ†ç½‘ç»œ**
   å¦‚æœä½ åªæƒ³ç”¨æŸä¸ªå­ç½‘ç»œçš„è¾“å‡ºå½“ä½œå¸¸é‡è¾“å…¥å¦ä¸€ä¸ªç½‘ç»œï¼Œä¸å¸Œæœ›åå‘ä¼ æ’­è¿›å»ï¼Œå¯ä»¥ `detach()`ã€‚

   ```python
   with torch.no_grad():
       features = backbone(x)  # æˆ–è€… backbone(x).detach()
   ```

---

### æ€»ç»“ä¸€å¥è¯

`loss.detach()` çš„æ„æ€æ˜¯ï¼š**å¾—åˆ° loss çš„æ•°å€¼å‰¯æœ¬ï¼Œä¸å†è¿½è¸ªæ¢¯åº¦ï¼Œä¹Ÿä¸ä¼šå‚ä¸åå‘ä¼ æ’­**ã€‚

---

è¦ä¸è¦æˆ‘ç»™ä½ å†™ä¸ª**å°å®éªŒ**ï¼Œæ¼”ç¤ºå¦‚æœä¸ç”¨ `detach`ï¼Œä¼šå¯¼è‡´ `running_loss` é‡Œâ€œå·å·ç§¯ç´¯è®¡ç®—å›¾â€ï¼Œä»è€Œæ˜¾å­˜çˆ†ç‚¸çš„æƒ…å†µï¼Ÿ



# distributed data parallel (DDP)

been using a single GPU for training but actually I am paying for eight gpus here
and so uh we should be putting all of them to work and in particular they are going to collaborate and uh you know
optimize over tokens at the same time and communicate so that um uh they're
all kind of collaborating on the optimization for this we are going to be using the distributed data parallel from
pytorch there's also a legacy data parallel which I recommend you not use and that's kind of like you know Legacy
distributed data parallel Works in a very simple way we have eight gpus so we're going to uh launch eight processes
and each process is going to be assigned to GPU and for each process the training
Loop and everything we've worked on so far is going to look pretty much the same H GPU as far as it's concerned is
just working on exactly what we've built so far but now Secret L there's eight of them and they're all going to be
processing slightly different parts of the data and we're going to add one more
part where once they all calculate their gradients there's one more part where we do a average of those
gradients and so that's how they're going to be collaborating on uh the computational workload here so to use
all eight of them we're not going to be launching our script anymore with just um pytorch train
gbt2 piy we're going to be running it with a special command called torrun in pytorch we'll see that in a bit and
torrun uh when it runs our python script we'll actually make sure to run eight eight of them in parallel and it creates
these environmental variables where each of these processes can look up which uh
basically which one of the processes it is so for example torron will set rank
local Rank and World size environmental variables and so this is a bad way to
detect whether uh DDP is running so if we're using torch run if DDP is
running then uh we have to make sure that K is available because I don't know that you can run this on CPU anymore or
that that makes sense to do um this is some um setup code here the important
part is that there's a world size which for us will be eight that's the total number of processes running there's a
rank which is um each process will basically run the ex exact same code at
the exact same time roughly but all the process the only difference between these processes is that they all have a
different dtp rank so the um gpu0 will have DDP rank of zero GPU 1 will have uh
rank of one Etc so otherwise they're all running the exact same script it's just
that DDP rank will be a slightly different integer and that is the way for us to coordinate that they don't for
example run on the same data we want to we want them to run on different parts of the data and so on
now local rank is something that is only used in a multi- node setting we only have a single node with ag gpus and so
local rank is the rank of the GPU on a single node so from 0 to seven as an
example but for us we're mostly going to be running on a single box so the things we care about are Rank and World size
this is eight and this will be whatever it is depending on the GPU uh that uh that this particular instantiation of
the script runs on now here we make sure that according to
the local rank we are setting the device to be Cuda colon and colon indicates
which GPU to use if there are more than one gpus so depending on the local rank
of this process it's going to use just the appropriate GPU so there's no collisions on which GPU is being used by
which process and finally there's a Boolean variable that I like to create which is the DDP rank equ equal Z so the master
process is arbitrarily process number zero and it does a lot of the printing logging checkpointing Etc and the other
processes are thought of mostly as a compute processes that are assisting and so Master process zero will have some
additional work to do all the other processes will uh will mostly just be doing forward backwards and if we're not using DDP and
none of these variables are set we revert back to single GPU training so that means that we only have rank zero
the world size is just one uh and and we are the master process and we try to autodetect the device and this is world
as normal so so far all we've done is we've initialized DDP and uh in the case where we're
running with torrun which we'll see in a bit there's going to be eight copies running in parallel each one of them
will have a different Rank and now we have to make sure that everything happens uh correctly afterwards so the
tricky thing with running multiple processes is you always have to imagine that there's going to be eight processes
running in parallel so as you read the code now you have to imagine there's eight you know eight python interpreters
running down these lines of code and the only difference between them is that they have a different DDP rank so they
all come here they all pick the exact same seed they all make all of these calculations completely unaware of the
other copies running roughly speaking right so they all make the exact same calculations and now we have to adjust
these calculations to take into account that there's actually like a certain world size and certain ranks so in
particular these micro batches and sequence lengths these are all just per GPU right so now there's going to be num
processes of them running in parallel so we have to adjust this right because the
grum steps now is going to be total B size divide B * T time U DDP R
size because each um process will will
do B * T and there's this many of them and so in addition to that we we
want to make sure that this fits nicely into total batch size which for us it will because 16 * 124 * 8 8 gpus is
131 uh K and so 524288 this means that our gratum will
be four with the current settings right so there's going to be 16 * 124 process
on each GPU and then there's a GP pus so we're going to be doing 131,000 tokens in a single forward
backward on the 8 gpus so we want to make sure that this
fits nicely so that we can derive a nice gradient accumulation steps and uh yeah let's just adjust the
comments here times uh DDP World size okay so each GPU calculates this now
this is where we start to get run into issues right so we are each process is going to come by a print and they're all
going to print so we're going to have eight copies of these prints so one way to deal with this is exactly this master
process variable that we have so if Master process then guard this and
that's just so that we just print this a single time because otherwise all the processes would have computed the exact same variables and there's no need to
print this eight times um before getting into the data loader and we're going to have to
refactor it obviously maybe at this point is uh we should do some prints and
uh just take it out for a spin and exit at this point so import
sis and S start exit and print IM
GPU um DDP
rank IM GPU DDP Rank and that um print
by so uh so now let's try to run this and just see how this works so let's
take it for a spin just so we see what it looks like so normally we use to launch python train gpd2 P like this now
we're going to run with torch run and this is what it looks like so torch run Standalone number of processes for
example is eight for us because we have eight gpus uh and then change of2 Pi so
this is what the command would look like and torch run again we'll run eight of these so let's just see what happens so
first it gets a little busy so there's a lot going on here so first of all there's
some warnings from distributed and I don't actually know that these mean anything I think this is just like the
code is setting up and the processes are coming online and we're seeing some preliminary failure to collect while the
processes come up I'm not 100% sure about that but we start to then get into
actual prints so all the processes went down and then the first print actually comes from
process 5 uh just by chance and then it printed so process 5 basically got here
first it said I'm process on GPU 5 buy and then this these prints come from the
master process so process 5 just finished first for whatever reason it just depends on
how the operating system scheduled the processes to run uh then gpu0 ended then GPU 3 and two and then uh probably
process 5 or something like that has uh exited and and DDP really doesn't like
that because we didn't properly dispose of uh the multi-gpus um setting and so
process group has not been destroyed before we destruct uh so it really doesn't like that and in an actual
application we would want to call destroy process group uh so that we clean up DDP properly and so it doesn't
like that too much and then the rest of the gpus finish and that's it so
basically we can't guarantee when these processes are running it's totally but they are running in parallel we
don't want them to be printing um and next up let's erase
this next up we want to make sure that when we create data loader light we need to now make it aware of this
multi-process um setting because we don't want all the processes to be loading the exact same data we want
every process to get its own chunk of data so that they're all working on different parts of the data set of course so let's adjust that so one
particular particularly simple and a naive way to do this is we have to make sure that we pass in the rank and the
size to the data loader and then when we come up here we see that we now take Rank and processes
and we save them now the current position will not be zero uh because
what we want is we want to stride out all the processes so one way to do this
is we basically take S.B times salt. T and then multiply it by the process
rank so proc process rank 0 will start at zero but process rank one now starts
at B * T process rank two is starts at 2 * B * D Etc so that is the
initialization now we still they still do this identically but now when we
advance we don't Advance by B * T we advance by B * T times number of
processes right so basically um the total number of tokens that we're um
consuming is B * T * number processes and they all go off to a different Rank
and the position has to advance by the entire chunk and then here B * T time uh s. num
processes + one would be to exceed number of tokens then we're going to Loop and when we Loop we want to of
course Loop in the exact same way so we sort of like reset back uh so this is
the simplest change that I can uh find for kind of a very simple distributed data Lo light and um you can notice that
if process rank is zero and non processes is one then uh the whole thing will be identical to what we had before
but now we can have actually multiple processes uh running and this should work fine um so that's the data loader okay
so next up once they've all initialized the data loader they come here and they all create a GPT model uh so we create
eight GPT models on eight processes but because the seeds are fixed here they all create the same identical model they
all move it to the device of their Rank and they all compile the model and because the models are identical there
are eight identical compilations happening in parallel but that's okay now none of this uh changes because that
is on a per step basis and we're currently working kind of within step because we need to um just uh all the
all the changes we're making are kind of like a within step changes now the important thing here is
when we construct the M model we actually have a bit of work to to do here get loits is deprecated so uh
create model we need to actually wrap the model into the distributed data parallel
container so um this is how we wrap the model into the DDP container and these
are the docs for DDP and they're quite extensive and there's a lot of caveats and a lot of things to be careful with
because everything complexifies times 10 when multiple processes are involved but
roughly speaking this device IDs I believe has to be passed in now unfortunately the docs for what device
IDs is is is extremely unclear uh so when you actually like come here this
comment for what device IDs is is roughly nonsensical um but I'm pretty sure it's
supposed to be the DDP local rank so not the DDP rank the local rank uh so this
is what you pass in here this wraps the model and in particular what DDP does for you is in a forward pass it actually
behaves identically so um my understanding of it is nothing should be changed in the forward pass but in the
backward pass as you are doing the backward pass um in the simpl setting
once the backp passes over on each independent GPU each independent GPU has
the gradient for all the parameters and what DDP does for you is once the backward pass is over it will call
what's called all reduce and it basically does an average across all the
uh ranks of their gradients and and then it will deposit that average on every
single rank so every sing Single rank will end up with the average on it and so basically that's the communication it
just synchronizes and averages the gradients and that's what DDP offers you now DDP actually is a little bit more um
it is a little bit more involved than that because as you are doing the backward pass through the layers of the Transformer it actually can dispatch
Communications for the gradient while the backward pass is still happening so there's overlap of the uh communication
of the gradient and the synchronization of them and uh the backward pass and uh this is just more efficient and um uh to
do it that way so that's what DDP does for you um forward is unchanged and
backward is mostly unchanged and we're tacking on this average as we'll see in a bit okay so now let's go to the uh
optimization nothing here changes let's go to the optimization here the inner loop and think through the
synchronization of uh these gradients in the DP so basically by default what happens as I mentioned is when you do l.
backward here it will do the backward pass and then it will synchronize the gradients um the problem here is because
of the gradient accumulation steps Loop here we don't actually want to do the
synchronization after every single La step backward because we are just depositing gradients and we're doing
that serially and we just want them adding up and we don't want to synchronize every single time that would be extremely wasteful so basically we
want to add them up and then on the the very last uh it's only on the very last step when micro when micro step becomes
gratak steps minus one only at that last step do we want to actually do the
alberu uh to average up the gradients so to do that we come here and um the
official sanctioned way by the way is to do this no sync context manager so
pytorch says this is a context manager to disable gradient synchronization across DDP processes So within this
context gradient will be accumulated and basically when you do no sync there will be no communication so
they are telling us to do with DDP no sync uh do the gradient accumulation accumulate grats and then they are
asking us to do DDP again with another input and that backward and I just really don't love this I I just really
don't like it uh the fact that you have to copy paste your code here and use a context manager and this is just super
ugly so when I went to this source code here you can see that when you enter
you simply toggle this variable this require backward grat sync and this is
uh being toggled around and changed and this is the variable that basically uh
if you step through it is being toggled to determine if the gradient is going to be synchronized so I actually just kind
of like to use that directly uh so instead what I like to do is the
following right here before the L back backward if we are using the DDP then um
then basically we only want to synchronize we only want this variable to be true when it is the final
iteration in all the other iterations inside the micr steps we want to be false so I just toggle it like this so
required backward graph sync should only turn on when the micro step is the last step and so I'm toggling this variable
directly and I hope that that impacts last St backwards and this is a naughty thing to do
because you know they could probably change the DDP and this variable will go away but for now I believe this this
works and it allows me to avoid the use of context managers and code duplication I'm just toggling the variable and then
Lop backward will not synchronize most of the steps and it will synchronize the very last step and so once this is over
uh and we come out every single um rank will suddenly magically have the average
of all the gradients that were stored on all the ranks so now we have to think
through whether that is what we want and also um if this suffices and whether how
it works with the loss and what is loss AUM so let's think through through that now and the problem I'm getting at is
that we've averaged the gradients which is great but the loss AUM has not been impacted yet and the and this is outside
of the DDP container so that is not being averaged um and so here when when we are printing Los AUM well presumably
we're only going to be printing on the master process uh rank zero and it's just going to be printing the losses
that it saw on its process but instead we want it to print the loss over all
the processes and the average of that loss because we did average of gradients so we want the average of loss as well
so simply here after this uh this is the code that I've used in the past um and
instead of LF we want Lum so if
DDP again then this is a p torch distributed I import it where do I
import it uh oh gosh so this file is starting
to get out of control huh so if uh so import torch. distributed as dist
so dist. ALU and we're doing the average on Lum
and so this lakum tensor exists on all the ranks when we call all use of average it creates the average of those
numbers and it deposits that average on all the ranks so all the ranks after this um call will now contain L AUM uh
averaged up and so when we print here on the master process the L AUM is identical in all the other ranks as well
so here if Master process oops we want to print like this okay and
finally we have to be careful because we're not processing even more tokens so times DDP World size
that's number of tokens that we've processed up above
and everything else should be fine uh the only other thing to be careful with is as I mentioned you want to destroy
the process group so that we are nice to nickel and it's not going to uh to uh to DDP and it's not going to complain to us
uh when we exit here so that should be it let's try to take it for a spin okay so I launched
the script and it should be uh printing here imminently we're now training with 8 gpus at the same time so the gradient
accumulation steps is not 32 it is now divide 8 and it's just four uh so um
otherwise this is what the optimization now looks like and wow we're going really fast so we're processing 1.5
million tokens uh per second now so these are some serious numbers and the
tiny shakespare data set is so tiny that we're just doing like so many Epoch over it most likely but this is roughly what
looks like um one thing that I had to fix by the way is that this was model.
configure optimizers which Now doesn't work because model now is a DDP model so instead this has to become raw
model. configure optimizers where raw model is something I create here so
right after I wrap the model into DDP uh I have to create the raw model which in
the case of DDP is a model. module is where it stores the raw and then module
of gpt2 as we have it which contains the uh configure optimizers function that we want to call so that's one thing that I
have to fix otherwise this seems to run now one thing you'll notice is that when you actually compare this run and the
numbers in it to the just running a single GPU you'll notice that this is single GPU run with 32 gratum the
numbers won't exactly match up and uh that's kind of a boring reason for why that happens uh the reason for
that is that in the data loader we're basically just iterating through batches and slightly different way because now
we're looking for an entire page of data and if that page uh for all the gpus if
that chunk exceeds the number of tokens we just Loop and so actually the single GPU and the H GPU process will end up um
resetting in a slightly different Manner and so our batches are slightly different and so we get slightly
different numbers but one way to convince yourself that this is okay it just make the total batch size much
smaller and the b and a t and then um so I think I used uh 4 * 124 * 8 so I
used 32768 as a total patch size and then um so I made sure that the single
GPU will do eight creting accumulation steps and then the multi-gpu and then you're reducing the boundary effects of
the data loader and you'll see that the numbers match up so long story short we're now going really really fast the
optimization is mostly consistent with gpt2 and three hyper parameters and uh
we have outgrown our tiny Shakespeare file and we want to upgrade it so let's move to next to that next so let's now

è¿™æ®µæ–‡å­—æè¿°äº†å¦‚ä½•ä½¿ç”¨ PyTorch çš„åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰æ¥åŠ é€Ÿæ¨¡å‹è®­ç»ƒï¼Œå°¤å…¶æ˜¯åœ¨å¤š GPU ç¯å¢ƒä¸‹ã€‚ä»¥ä¸‹æ˜¯ä¸­æ–‡æ€»ç»“ï¼š

### **1. åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œç®€ä»‹ï¼š**

* **å• GPU è®­ç»ƒï¼š** åœ¨å• GPU ä¸Šè®­ç»ƒæ—¶ï¼Œæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹å°±æ˜¯å•çº¿ç¨‹è¿è¡Œçš„ï¼Œæ‰€æœ‰è®¡ç®—éƒ½åœ¨ä¸€ä¸ªè®¾å¤‡ä¸Šè¿›è¡Œã€‚
* **å¤š GPU è®­ç»ƒï¼š** ç°åœ¨ä½¿ç”¨äº† 8 ä¸ª GPUï¼Œå› æ­¤éœ€è¦è®©è¿™äº› GPU ååŒå·¥ä½œè¿›è¡Œä¼˜åŒ–ã€‚PyTorch æä¾›äº† **Distributed Data Parallelï¼ˆDDPï¼‰** æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚DDP é€šè¿‡å°†è®­ç»ƒè¿‡ç¨‹æ‹†åˆ†æˆå¤šä¸ªè¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹åˆ†é…ç»™ä¸€ä¸ª GPUï¼Œè¿™äº›è¿›ç¨‹å¹¶è¡Œæ‰§è¡Œï¼Œä½†å®ƒä»¬è®¡ç®—çš„æ¢¯åº¦ä¼šè¢«åŒæ­¥å’Œå¹³å‡ã€‚

### **2. å¯åŠ¨è®­ç»ƒï¼š**

* **torch.run**ï¼šä¸ºäº†å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œæˆ‘ä»¬ä¸å†é€šè¿‡ä¼ ç»Ÿçš„ `python train.py` æ¥å¯åŠ¨ï¼Œè€Œæ˜¯ä½¿ç”¨ `torch run` å‘½ä»¤ï¼Œå®ƒä¼šå¹¶è¡Œå¯åŠ¨å¤šä¸ªè¿›ç¨‹ï¼Œå¹¶ä¸ºæ¯ä¸ªè¿›ç¨‹è®¾ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡ï¼ˆä¾‹å¦‚ rank å’Œ world sizeï¼‰ã€‚

  * **rank**ï¼šè¡¨ç¤ºå½“å‰è¿›ç¨‹åœ¨ 8 ä¸ª GPU ä¸­çš„ç¼–å·ã€‚
  * **world size**ï¼šè¡¨ç¤ºæ€»å…±å‚ä¸è®­ç»ƒçš„è¿›ç¨‹æ•°ã€‚
  * **local rank**ï¼šç”¨äºåœ¨å¤šèŠ‚ç‚¹è®­ç»ƒä¸­æ ‡è¯†å½“å‰èŠ‚ç‚¹ä¸Šçš„ GPU ç¼–å·ã€‚

### **3. æ•°æ®åŠ è½½å™¨çš„è°ƒæ•´ï¼š**

* **åˆ†å¸ƒå¼æ•°æ®åŠ è½½ï¼š** ç”±äºæˆ‘ä»¬æœ‰å¤šä¸ªè¿›ç¨‹å¹¶è¡Œå·¥ä½œï¼Œæ‰€ä»¥æ¯ä¸ªè¿›ç¨‹éœ€è¦åŠ è½½ä¸åŒçš„æ•°æ®éƒ¨åˆ†ã€‚éœ€è¦é€šè¿‡ `rank` å’Œ `world size` æ¥è°ƒæ•´æ•°æ®åŠ è½½å™¨ï¼Œä½¿æ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸åŒçš„æ•°æ®å—ã€‚
* è¿™æ ·ï¼Œæˆ‘ä»¬ç¡®ä¿æ¯ä¸ªè¿›ç¨‹å¤„ç†çš„æ•°æ®æ˜¯ç‹¬ç«‹çš„ï¼Œé¿å…äº†å¤šä¸ªè¿›ç¨‹è¯»å–ç›¸åŒçš„æ•°æ®ã€‚

### **4. æ¨¡å‹è®­ç»ƒï¼š**

* **æ¨¡å‹åˆå§‹åŒ–ï¼š** æ¯ä¸ªè¿›ç¨‹éƒ½ä¼šåˆå§‹åŒ–ä¸€ä¸ªç›¸åŒçš„æ¨¡å‹å¹¶å°†å…¶ç§»åŠ¨åˆ°ç›¸åº”çš„ GPU ä¸Šã€‚æ¨¡å‹çš„åˆ›å»ºæ˜¯ç›¸åŒçš„ï¼Œä½†æ˜¯æ¯ä¸ªè¿›ç¨‹éƒ½ä¼šåœ¨ä¸åŒçš„ GPU ä¸Šæ‰§è¡Œã€‚
* **DDPåŒ…è£…æ¨¡å‹ï¼š** æ¨¡å‹éœ€è¦è¢« `DistributedDataParallel` (DDP) å®¹å™¨åŒ…è£…ï¼Œè¿™æ ·åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒDDP ä¼šå¤„ç†æ¢¯åº¦åŒæ­¥ã€‚å…·ä½“æ¥è¯´ï¼Œå½“ä¸€ä¸ªè¿›ç¨‹å®Œæˆåå‘ä¼ æ’­è®¡ç®—åï¼ŒDDP ä¼šè¿›è¡Œæ¢¯åº¦çš„ **all-reduce** æ“ä½œï¼Œå¹³å‡æ‰€æœ‰è¿›ç¨‹çš„æ¢¯åº¦ï¼Œå¹¶å°†å¹³å‡ç»“æœæ›´æ–°åˆ°æ¯ä¸ªè¿›ç¨‹çš„æ¨¡å‹ä¸­ã€‚

### **5. æ¢¯åº¦åŒæ­¥ä¸ç´¯ç§¯ï¼š**

* **æ¢¯åº¦åŒæ­¥ï¼š** é»˜è®¤æƒ…å†µä¸‹ï¼ŒPyTorch åœ¨æ¯æ¬¡åå‘ä¼ æ’­åä¼šè‡ªåŠ¨åŒæ­¥æ¢¯åº¦ã€‚ä½†åœ¨æ¢¯åº¦ç´¯ç§¯ï¼ˆgradient accumulationï¼‰è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›æ¯ä¸€æ­¥éƒ½åŒæ­¥æ¢¯åº¦ï¼Œå› ä¸ºé‚£æ ·ä¼šæµªè´¹è®¡ç®—èµ„æºã€‚ä¸ºæ­¤ï¼Œä½¿ç”¨ `no_sync` ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¥é¿å…åœ¨æ¯ä¸ªæ¢¯åº¦ç´¯ç§¯æ­¥éª¤åè¿›è¡ŒåŒæ­¥ã€‚
* **æ¢¯åº¦ç´¯ç§¯ï¼š** åªæœ‰åœ¨æœ€åä¸€æ¬¡ç´¯ç§¯æ­¥éª¤æ—¶æ‰ä¼šåŒæ­¥æ¢¯åº¦ï¼Œè¿™æ ·å¯ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚

### **6. æŸå¤±è®¡ç®—ä¸åŒæ­¥ï¼š**

* åœ¨å¤š GPU ç¯å¢ƒä¸‹ï¼ŒæŸå¤±ï¼ˆlossï¼‰æ˜¯æ¯ä¸ªè¿›ç¨‹è®¡ç®—çš„ï¼Œä½†æˆ‘ä»¬éœ€è¦è®¡ç®—æ‰€æœ‰è¿›ç¨‹çš„å¹³å‡æŸå¤±ã€‚é€šè¿‡ä½¿ç”¨ `all_reduce` æ“ä½œï¼Œå¯ä»¥å°†æ¯ä¸ªè¿›ç¨‹çš„æŸå¤±å€¼åŒæ­¥å¹¶å¹³å‡ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½èƒ½çœ‹åˆ°ç›¸åŒçš„æŸå¤±å€¼ã€‚

### **7. ä»£ç è°ƒæ•´ä¸ä¼˜åŒ–ï¼š**

* **ä¼˜åŒ–å™¨é…ç½®ï¼š** ç”±äºæˆ‘ä»¬ä½¿ç”¨äº† DDPï¼Œä¼˜åŒ–å™¨çš„é…ç½®æ–¹å¼ç¨æœ‰ä¸åŒã€‚æˆ‘ä»¬éœ€è¦åœ¨ `DDP` æ¨¡å‹çš„ `module` å±æ€§ä¸‹è°ƒç”¨ä¼˜åŒ–å™¨é…ç½®å‡½æ•°ï¼Œè€Œä¸æ˜¯ç›´æ¥åœ¨æ¨¡å‹ä¸Šè°ƒç”¨ã€‚
* **æ€§èƒ½æå‡ï¼š** é€šè¿‡å¤š GPU ååŒå·¥ä½œï¼Œè®­ç»ƒé€Ÿåº¦å¤§å¤§æé«˜ï¼Œå¤„ç†çš„ tokens æ•°é‡ä¹Ÿæ˜¾è‘—å¢åŠ ã€‚

### **æ€»ç»“ï¼š**

é€šè¿‡ä½¿ç”¨ PyTorch çš„ DDPï¼Œæˆ‘ä»¬èƒ½å¤Ÿè®©å¤šä¸ª GPU ååŒå·¥ä½œï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡é€‚å½“è°ƒæ•´æ•°æ®åŠ è½½ã€æ¢¯åº¦åŒæ­¥å’ŒæŸå¤±è®¡ç®—ç­‰æ–¹é¢çš„ä»£ç ï¼Œå¯ä»¥ç¡®ä¿æ¯ä¸ªè¿›ç¨‹ååŒå·¥ä½œä¸”ä¸å‡ºç°æ•°æ®å†²çªï¼Œä»è€Œå®ç°é«˜æ•ˆçš„åˆ†å¸ƒå¼è®­ç»ƒã€‚


# datasets used in GPT-2, GPT-3, FineWeb (EDU)

take a look at what data sets were used by gpt2 and gpt3 so gbt2 used this web
Text data set that was never released um there's an attempt at reproducing it called open web text uh so basically
roughly speaking what they say here in the paper is that they scraped all outbound links from Reddit and then uh
with at least three Karma and that was kind of like their starting point and they collected all the web P all the web pages and all the text in them and so
this was 45 million links and this ended up being 40 GB of text so uh so that's
roughly what gpt2 says about its data set so it's basically outbound links from Reddit now when we go over to gpt3
there's a training data set section and that's where they start to talk about um common coll which is a lot more uh used
actually I think even gpt2 talked about common coll um but basically it's not a
very high quality data set all by itself because it is extremely noisy this is a completely random subset of the internet
and it's much worse than you think so people go into Great Lengths to filter common craw because there's good stuff
in it but most of it is just like ad spam random tables and numbers and stock tickers and uh it's just total mess
so that's why people like to train on these data mixtures that they curate and
uh are careful with so a large chunk of these data mixtures typically will be common C like for example 50% of the
tokens will be comic but then here in gpt3 they're also using web text to from before so that's Reddit outbound but
they're also adding for example books and they're adding Wikipedia there's many other things you can decide to add
now this data set for gpt3 was also never released so today some of the data sets that I'm familiar with that are
quite good and would be representative of something along these lines are number one the red pajama data set or
more specifically for example the slim pajama subset of the red pajama data set which is a cleaned and D duplicated
version of it and just to give you a sense again it's a bunch of common crawl um C4 which is also as far as I know
more common craw but processed differently and then we have GitHub books archive Wikipedia stack exchange
these are the kinds of data sets that would go into these data mixtures now specifically the one that I like that
came out recently is called Fine web data set uh so this is an attempt to basically collect really high quality
common coll data and filter it in this case to 15 trillion tokens and then in
addition to that more recently huggingface released this fine web edu subset which is 1.3 trillion of
educational and 5.4 trillion of high educational content so basically they're
trying to filter common C to very high quality educational subsets and uh this
is the one that we will use there's a long uh web page here on fine web and
they go into a ton of detail about how they process the data which is really fascinating reading by the way and I would definitely recommend if you're
interested into Data mixtures and so on and how data gets processed at these scales a look at this uh page and more
specifically we'll be working with the fine web edu I think and it's basically educational content from the
internet uh they show that training on educational content in in their metrics
um uh works really really well and we're going to use this sample 10 billion
tokens subsample of it because we're not going to be training on trillions of tokens uh we're just going to train on
uh 10 billion sample of the fine web edu because empirically in my previous few
experiments this actually suffices to really get close to gpt2 Performance and it's um simple enough to work with and
so let's work with the sample 10 uh BT so our goal will be to download it
process it and make sure that our data loader can work with it so let's get to that okay so I introduced another um
file here that will basically download Fine web edu from huging face data sets
it will pre-process and pre- tokenize all of the data and it will save data shards to a uh folder on um local disk
and so while this is running uh just wanted to briefly mention that you can
kind of look through the data set viewer here just to get a sense of what's in here and it's kind of interesting I mean it's a it basically looks like it's
working fairly well like it's talking about nuclear energy in France it's talking
about Mexican America some mac PJs Etc so actually it
seems like their filters are working pretty well uh the filters here by the way were applied automatically using um
llama 370b I believe and so uh basically llms are judging which content is
educational and that ends up making it through the filter uh so that's pretty cool now in terms of the script itself
I'm not going to go through the full script because it's not as interesting and not as llm Centric but when you run
this basically number one we're going to load the data set uh which this is all huging face code running this you're
going to need to uh pip install data sets um so it's downloading the data set
then it is tokenizing all of the documents inside this data set now when we tokenize the documents you'll notice
that um to tokenize a single document uh we first
start the tokens with the end of text token and this is a special token in the gpt2 tokenizer as you know so
50256 is the ID of the end of text and this is what begins a document even
though it's called end of text but this is uh the first token that begins a document then we extend with all of the
tokens of that document then we create a numpy array out of that we make sure
that all the tokens are between oh okay let me debug this
okay so apologies for that uh it just had to do with me using a float division in Python it must be integer division so
that this is an INT and everything is nice um okay but basically the
tokenization here is relatively straightforward returns tokens in mp. un6 uh we're using .16 to save a little
bit of space because 2 to the 16us 1 is 65,000 so the gpt2 max token ID is well
below that and then here there's a bunch of multiprocessing code and it's honestly not that exciting so I'm not
going to step through it but we're loading the data set we're tokenizing it and we're saving everything to shards
and the shards are numpy files uh so just storing a numpy array and uh which
is very very similar to torch tensors and the first Shard 0000 is a
Val a validation Shard and all the other shards are uh training shards and as I
mentioned they all have 100 million tokens in them exactly um and and that
just makes it easier to work with as to Shard the files because if we just have a single massive file sometimes they can
be hard to work with on the disk and so sharting it is just kind of um nicer from that
perspective and uh yeah so we'll just let this run this will be probably um
30ish minutes or so and then we're going to come back to actually train on this data and we're going to be actually doing some legit pre-training in this
case this is a good data set we're doing lots of tokens per second we have 8 gpus
the code is ready and so we're actually going to be doing a serious training run so let's get P it back in a bit okay so
we're back so uh if we LS edu fine web we see that there's now 100 charts in it
um and that makes sense because each chart is 100 million tokens so 100 charts of that is 10 billion tokens in
total now swinging over to the main file I made some adjustments to our data loader again and that's because we're
not running with uh Shakespeare anymore we want to use the fine web shards and
so you'll see some code here that additionally basically can load these shards uh we load the um un6 numpy file
we convert it to a torch. long tensor which is what a lot of the layers up top expect by default and then here we're
just enumerating all the shards I also added a split to data load of light so
we can uh load the split train but also the split Val uh the zero split and then we can load the shards
and then here we also have not just the current position now but also the current Shard so we have a position
inside A Shard and then when we uh run out of tokens in A Single Shard we first
Advance The Shard and loop if we need to and then we get the tokens and readjust the position so this data loader will
now iterate all the shards as well so I Chang that and then the other thing that
I did while uh the data was processing is our train loader now has split train
of course and down here I set up some I set up some numbers so we are doing 2 to the
9 uh tokens per uh per um per step and
we want to do roughly 10 billion tokens um because that's how many unique tokens
we have so if we did 10 billion tokens then divide that by 29 we see that this
is 1973 steps so that's where that's from and then the GPT three paper says
that they warm up the learning rate over 375 million tokens so I came here and
375 E6 tokens divide uh 2 to the 19 is 715 steps so that's why warm-up
steps is set to 715 so this will exactly match um the warm-up schedule that gpt3
used and I think 715 by the way is very uh mild and this could be made significantly more aggressive probably
even like 100 is good enough um but it's okay let's leave it for now so that we have the exact hyper parameters
of gpt3 so I fix that and then um that's
pretty much it we can we can run so we have our script here and we can
launch and actually sorry let me do one more
thing excuse me for my GPU I can actually fit more
batch size and I believe I can fat I can fit 60 4 on my GPU as a micro bash size
so let me try that I could be misremembering but that
means 64 * 124 per GPU and then we have a gpus so that means we would not even
be doing gradient accumulation if this fits because uh this just multi multiplies out to uh the full total bat
size so no gradient accumulation and that would run pretty quickly if that fits
let's go let's go I mean if this works then this is basically a serious pre-training run um we're not logging
we're not evaluating the validation split we're not running any evaluations yet so it's not we haven't crossed our
te's and dotted our eyes but uh if we let this run for a while we're going to actually get a pretty good model and the
model that might even be on par with or better than gpt2 124 M okay so it looks
like everything is going great we're processing 1.5 million tokens per second uh everything here looks good
we're doing 330 milliseconds per iteration and we have to do a total
of uh where are we printing that 1973 so 19073 times 0.33
is this many seconds this many minutes so this will run for 1.7
hours uh so one and a half hour run uh like this and uh we don't even have to
use gradient accumulation which is nice and you might not have that luxury in your GPU in that case just start decreasing the batch size until things
fit but keep it to nice numbers um so that's pretty exciting
we're currently warming up the learning rate so you see that it's still very low one4 so this will ramp up over the next
few steps all the way to 6 e Nega uh 4
here very cool so now what I'd like to do is uh let's cross the T and do our eyes let's evaluate on the validation
split and let's try to figure out how we can run evals how we can do logging how we can visualize our losses and all the
good stuff so let's get to that before we actually do the run okay so I've adjusted the code so that we're

### **æ•°æ®é›†æ¦‚è¿°ï¼š**

åœ¨è¿™æ®µå†…å®¹ä¸­ï¼Œä¸»è¦ä»‹ç»äº† GPT-2ã€GPT-3 å’Œ FineWebï¼ˆæ•™è‚²ç±»å­é›†ï¼‰ä½¿ç”¨çš„æ•°æ®é›†ä»¥åŠå¦‚ä½•å¤„ç†è¿™äº›æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒã€‚

### **GPT-2 æ•°æ®é›†ï¼š**

* **Open Web Text**ï¼šGPT-2 ä½¿ç”¨äº†ä¸€ä¸ªæ¥è‡ª Reddit çš„ Web æ–‡æœ¬æ•°æ®é›†ï¼Œè™½ç„¶è¿™ä¸ªæ•°æ®é›†ä»æœªå‘å¸ƒï¼Œä½†æœ‰å°è¯•é‡ç°å®ƒçš„ç‰ˆæœ¬å«åš Open Web Textã€‚å¤§è‡´æµç¨‹æ˜¯ï¼šæŠ“å– Reddit ä¸Šå¤–é“¾çš„ç½‘é¡µï¼ˆè‡³å°‘ 3 ç‚¹ Karma çš„é“¾æ¥ï¼‰ï¼Œç„¶åä»è¿™äº›ç½‘é¡µæŠ“å–æ–‡æœ¬æ•°æ®ã€‚æ€»å…±æŠ“å–äº† 4500 ä¸‡ä¸ªé“¾æ¥ï¼Œæœ€ç»ˆå½¢æˆäº† 40GB çš„æ–‡æœ¬æ•°æ®ã€‚

### **GPT-3 æ•°æ®é›†ï¼š**

* **Common Crawl**ï¼šGPT-3 ä½¿ç”¨çš„è®­ç»ƒæ•°æ®é›†åŒ…æ‹¬ Common Crawl æ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†åŒ…å«äº’è”ç½‘ä¸Šçš„å¤§é‡éšæœºç½‘é¡µã€‚ç”±äºæ•°æ®å™ªå£°è¾ƒå¤§ï¼ŒCommon Crawl æ•°æ®é›†åŒ…å«äº†å¾ˆå¤šä¸ç›¸å…³çš„å†…å®¹ï¼Œæ¯”å¦‚å¹¿å‘Šã€åƒåœ¾ä¿¡æ¯å’Œè‚¡ç¥¨æ•°æ®ï¼Œå› æ­¤é€šå¸¸éœ€è¦è¿›è¡Œè¿‡æ»¤ã€‚
* **Web Text**ï¼šé™¤äº† Common Crawlï¼ŒGPT-3 è¿˜ä½¿ç”¨äº† Web Text æ•°æ®é›†ï¼ˆæ¥æºäº Reddit çš„å¤–é“¾ï¼‰ï¼Œå¹¶ä¸”è¿˜åŠ å…¥äº†å…¶ä»–çš„æ•°æ®é›†ï¼Œå¦‚ä¹¦ç±ã€ç»´åŸºç™¾ç§‘ç­‰ã€‚
* GPT-3 çš„æ•°æ®é›†å¹¶æœªå…¬å¼€å‘å¸ƒï¼Œä½†å®ƒåŒ…å«äº†å¤§é‡çš„ Web æ•°æ®ã€ä¹¦ç±ã€ç»´åŸºç™¾ç§‘ã€Stack Exchangeã€GitHub ç­‰å†…å®¹ã€‚

### **FineWeb æ•°æ®é›†ï¼š**

* **Fine Web Data**ï¼šFine Web æ•°æ®é›†æ˜¯ä¸€ä¸ªå°è¯•æ”¶é›†é«˜è´¨é‡çš„ Common Crawl æ•°æ®çš„é¡¹ç›®ï¼Œæ•°æ®è¢«è¿‡æ»¤åå½¢æˆäº† 15 ä¸‡äº¿ä¸ª tokenã€‚è¿™ä¸ªæ•°æ®é›†çš„ç›®æ ‡æ˜¯å»é™¤æ‰ä½è´¨é‡çš„å†…å®¹ï¼Œä¿ç•™æ›´é«˜è´¨é‡çš„æ•™è‚²æ€§æ–‡æœ¬ã€‚
* **Fine Web EDU å­é›†**ï¼šHugging Face å‘å¸ƒäº† Fine Web æ•°æ®é›†çš„æ•™è‚²å­é›†ï¼ˆFine Web EDUï¼‰ï¼Œå…¶ä¸­åŒ…å«äº† 1.3 ä¸‡äº¿çš„æ•™è‚²ç±»å†…å®¹å’Œ 5.4 ä¸‡äº¿çš„é«˜ç­‰æ•™è‚²å†…å®¹ã€‚è¿™äº›å†…å®¹ç»è¿‡ Llama 370B è¿™æ ·çš„ LLM è¿‡æ»¤å™¨ç­›é€‰ï¼Œç¡®ä¿å…¶æ•™è‚²æ€§å’Œé«˜è´¨é‡ã€‚

### **Fine Web EDU æ•°æ®å¤„ç†ï¼š**

1. **æ•°æ®é¢„å¤„ç†ä¸ä¸‹è½½ï¼š**

   * ä½¿ç”¨ Hugging Face çš„ `datasets` åº“æ¥ä¸‹è½½å’Œå¤„ç† Fine Web EDU æ•°æ®é›†ã€‚
   * åœ¨ä¸‹è½½è¿‡ç¨‹ä¸­ï¼Œæ•°æ®é›†ä¼šè¢«åˆ†æˆå¤šä¸ªâ€œshardâ€ï¼ˆæ•°æ®ç‰‡æ®µï¼‰ï¼Œæ¯ä¸ªç‰‡æ®µåŒ…å« 1 äº¿ä¸ª tokenã€‚è¿™æ ·åšæ˜¯ä¸ºäº†é¿å…å¤„ç†å¤§æ–‡ä»¶æ—¶å‡ºç°æ€§èƒ½ç“¶é¢ˆã€‚
   * æ•°æ®ç»è¿‡ token åŒ–å¤„ç†ï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æ¡£çš„å¼€å¤´éƒ½ä¼šåŠ ä¸Šç‰¹æ®Šçš„ "end of text" tokenï¼ˆ50256ï¼‰ã€‚

2. **æ•°æ®åŠ è½½å™¨ï¼š**

   * ä½¿ç”¨å¤šçº¿ç¨‹å’Œæ•°æ®åˆ†ç‰‡çš„æ–¹å¼è¿›è¡Œæ•°æ®åŠ è½½ï¼Œç¡®ä¿æ¯ä¸ªè¿›ç¨‹å¯ä»¥ç‹¬ç«‹åœ°åŠ è½½å’Œå¤„ç†ä¸åŒçš„ shardã€‚
   * è®­ç»ƒæ—¶ï¼Œæ¯ä¸ªè¿›ç¨‹ä¼šè¯»å–ä¸åŒçš„ shardï¼Œè¿™æ ·å°±èƒ½æœ‰æ•ˆåœ°åˆ©ç”¨æ‰€æœ‰çš„ GPU è¿›è¡Œè®­ç»ƒã€‚

3. **è®­ç»ƒå’Œä¼˜åŒ–ï¼š**

   * è®­ç»ƒè¿‡ç¨‹ä½¿ç”¨äº† GPT-3 ä¸­çš„è¶…å‚æ•°è®¾ç½®ï¼Œå°¤å…¶æ˜¯åœ¨å­¦ä¹ ç‡é¢„çƒ­é˜¶æ®µï¼Œè®¾å®šäº† 715 æ­¥çš„ warm-up æ­¥éª¤ï¼Œè¿™ä¸ªå€¼æ¥æºäº GPT-3 è®ºæ–‡ä¸­çš„è®¾å®šã€‚
   * åœ¨ 8 GPU ç¯å¢ƒä¸‹ï¼Œè®­ç»ƒå¯ä»¥éå¸¸å¿«é€Ÿåœ°å¤„ç†æ¯ç§’ 150 ä¸‡ä¸ª tokenï¼Œè€Œæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹å¯èƒ½éœ€è¦çº¦ 1.7 å°æ—¶å®Œæˆã€‚

### **è®­ç»ƒè¿‡ç¨‹å’Œä¼˜åŒ–ï¼š**

1. **å¾®æ‰¹æ¬¡ä¸æ¢¯åº¦ç´¯ç§¯ï¼š**

   * ä½¿ç”¨å¾®æ‰¹æ¬¡ï¼ˆmicro-batchï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”é‡‡ç”¨äº†åˆé€‚çš„æ‰¹æ¬¡å¤§å°ï¼ˆå¦‚æ¯ä¸ª GPU ä¸Šçš„æ‰¹æ¬¡å¤§å°ä¸º 64ï¼‰ï¼Œé¿å…äº†æ¢¯åº¦ç´¯ç§¯çš„éœ€æ±‚ã€‚
   * å¦‚æœ GPU å†…å­˜ä¸è¶³ï¼Œå¯ä»¥å‡å°‘æ‰¹æ¬¡å¤§å°æ¥é€‚é…ï¼Œä½†è¦ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸ºåˆç†çš„æ•°å­—ï¼Œä»¥é¿å…å†…å­˜æº¢å‡ºã€‚

2. **å­¦ä¹ ç‡é¢„çƒ­ï¼š**

   * å­¦ä¹ ç‡åœ¨è®­ç»ƒåˆæœŸä¼šæœ‰ä¸€ä¸ªé¢„çƒ­é˜¶æ®µï¼ŒåˆæœŸçš„å­¦ä¹ ç‡è¾ƒå°ï¼Œç„¶åé€æ¸å¢åŠ ï¼Œç›´åˆ°è®¾å®šçš„æœ€å¤§å€¼ã€‚è¿™æœ‰åŠ©äºé¿å…åœ¨è®­ç»ƒåˆæœŸæ¨¡å‹ä¸ç¨³å®šã€‚

3. **è®­ç»ƒæ—¥å¿—ä¸è¯„ä¼°ï¼š**

   * åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé¦–å…ˆè¿›è¡Œå­¦ä¹ ç‡é¢„çƒ­ï¼Œç„¶åå¼€å§‹æ­£å¼çš„è®­ç»ƒã€‚åœ¨ä¸€å®šçš„è®­ç»ƒæ­¥éª¤åï¼Œä»£ç ä¼šå¯åŠ¨å¯¹éªŒè¯é›†çš„è¯„ä¼°ï¼Œå¹¶è®°å½•æ—¥å¿—ä»¥ä¾¿åˆ†ææŸå¤±å’Œå…¶ä»–æŒ‡æ ‡ã€‚

### **æ€»ç»“ï¼š**

é€šè¿‡ Fine Web EDU æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆåœ°è®­ç»ƒä¸€ä¸ªé«˜è´¨é‡çš„è¯­è¨€æ¨¡å‹ã€‚æ•°æ®é¢„å¤„ç†ã€åˆ†ç‰‡å­˜å‚¨å’Œå¤šè¿›ç¨‹åŠ è½½æ˜¯è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹æ—¶çš„å…³é”®æ­¥éª¤ã€‚ä½¿ç”¨ Hugging Face çš„ `datasets` åº“æ¥ç®¡ç†è¿™äº›æ•°æ®é›†ï¼Œä½¿å¾—è®­ç»ƒè¿‡ç¨‹æ›´åŠ é«˜æ•ˆã€‚æ­¤å¤–ï¼Œé€šè¿‡è°ƒæ•´æ‰¹æ¬¡å¤§å°å’Œå­¦ä¹ ç‡é¢„çƒ­ç­‰è¶…å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦å’Œæ¨¡å‹æ€§èƒ½ã€‚


# validation data split, validation loss, sampling revive

evaluating on the validation split so creating the Val loader just by passing in Split equals Val that will basically
create a data loader just for the uh validation Shard um the other thing I did is in the
data loader I introduced a new function reset which is called at init and it basically resets the data loader and
that is very useful because when we come to the main training Loop now so this is
the code that I've added and basically every 100th iteration including the zeroth iteration we put the model into
evaluation mode we reset the Val loader and then um no gradients involved we're
going to basically accumulate the gradients over say 20 steps and then average it all up
and print out the validation loss and so that basically is the exact same logic
as the training Loop roughly but there's no loss that backward it's only inference we're just measuring the loss
we're adding it up everything else otherwise applies and is exactly as we've seen it before and so this will
print the validation laws um every 100th iteration including on the very first
iteration uh so that's nice that will tell us some amount some a little bit about how much we're overfitting that
said like uh we have roughly Infinity data so we're mostly expecting our train and Val loss to be about the same but
the other reason I'm kind of interested in this is because we can take the GPT 2124m as openi released it we can
initialize from it and we can basically see what kind of loss it achieves on the validation loss as well and that gives
us kind of an indication as to uh how much that model would generalize to 124 M but it's not an sorry to fine web edu
validation split that said it's not a super fair comparison to gpt2 because it was trained on a very different data
distribution but it's still kind of like an interesting data point and in any case you would always want to have a
validation split in a training run like this so that you can make sure that you are not um overfitting and this is
especially a concern if we were to make more Epoch in our training data um so
for example right now we're just doing a single Epoch but if we get to a point where we want to train on 10 epochs or something like that we would be really
careful with maybe we are memorizing that data too much if we have a big enough model and our validation split
would be one way to tell whether that is happening okay and in addition to that if you remember at bottom of our script
we had all of this orphaned code for sampling from way back when so I deleted that code and I moved it up um to here
so once in a while we simply value validation once in a while we sample we generate
samples and then uh we do that only every 100 steps and we train on every
single step so that's how I have a structure right now and I've been running this for 10,000 iterations so
here are some samples on neration 1,000 um hello I'm a language model and I'm
not able to get more creative I'm a language model and languages file you're learning about
here is or is the beginning of a computer okay so this is all like pretty uh this
is still a garble uh but we're only at ration 1,000 and we've only just barely reached maximum learning rate uh so this
is still learning uh we're about to get some more samples coming up in
1,00 okay um okay this is you know the model is
still is still a young baby okay so uh basically all of this sampling code that
I've put here everything should be familiar with to you and came from before the only thing that I did is I created a generator object in pytorch so
that I have a direct control over the sampling of the random numbers don't because I don't want to impact the RNG
state of the random number generator that is the global one used for training I want this to be completely outside of
the training Loop and so I'm using a special sampling RNG and then I make
sure to seed it that every single rank has a different seed and then I pass in
here where we sort of consumer in the numbers in multinomial where the sampling happens I make sure to pass in
the generator object there otherwise this is identical uh now the other thing is um you'll notice that we're running a
bit slower that's because I actually had to disable torch. compile to get this to sample and um so we're running a bit
slower so for some reason it works with no torch compile but when I torch compile my model I get a really scary
error from pytorch and I have no idea how to resolve it right now so probably by the time you see this code released
or something like that maybe it's fixed but for now I'm just going to do end false um and I'm going to bring back
toor compile and you're not going to get samples and I I think I'll fix this
later uh by the way um I will be releasing all this code and actually I've been very careful about making get
commits every time we add something and so I'm going to release the entire repo that starts completely from scratch all
the way to uh now and after this as well and so everything should be exactly documented in the git commit history um
um and so I think that will be nice so hopefully by the time you go to GitHub uh this is removed and it's working and
I will have fixed the bug okay so I have the optimization running here and it's stepping and we're on step 6,000 or so

åœ¨è¿™æ®µå†…å®¹ä¸­ï¼Œä¸»è¦è®²è§£äº†å¦‚ä½•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ **éªŒè¯æ•°æ®é›†** è¿›è¡Œ **éªŒè¯æŸå¤±** è¯„ä¼°ã€**é‡‡æ ·** ç”Ÿæˆç»“æœï¼Œå¹¶å¦‚ä½•é€šè¿‡è¿™äº›è¯„ä¼°æ¥ä¼˜åŒ–æ¨¡å‹è®­ç»ƒã€‚

### **éªŒè¯æ•°æ®é›†ï¼ˆValidation Splitï¼‰ä¸éªŒè¯æŸå¤±ï¼ˆValidation Lossï¼‰ï¼š**

1. **éªŒè¯æ•°æ®é›†åŠ è½½å™¨ï¼ˆValidation Loaderï¼‰**ï¼š

   * åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯100æ­¥ï¼ˆåŒ…æ‹¬ç¬¬ä¸€æ¬¡è¿­ä»£ï¼‰ï¼Œä¼šå°†æ¨¡å‹åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆ`eval()`ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¼šé‡ç½®éªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆ`Val loader`ï¼‰ï¼Œå¹¶ç¡®ä¿åœ¨æ­¤æœŸé—´ä¸è®¡ç®—æ¢¯åº¦ï¼ˆå³åªæœ‰æ¨ç†è€Œæ²¡æœ‰åå‘ä¼ æ’­ï¼‰ã€‚
   * æˆ‘ä»¬ä¼šç´¯ç§¯æ¯ä¸ªéªŒè¯æ­¥éª¤çš„æŸå¤±ï¼Œå¹¶è®¡ç®—å‡º **éªŒè¯æŸå¤±**ï¼Œæ¯100æ­¥æ‰“å°ä¸€æ¬¡ï¼Œè¿™æœ‰åŠ©äºäº†è§£æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°ï¼Œå¹¶ç›‘æ§æ˜¯å¦å­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ã€‚
   * é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹ä¼šåœ¨éªŒè¯é›†ä¸Šè¿›è¡Œæ£€æŸ¥ï¼Œç¡®ä¿å…¶åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šä»æœ‰è¾ƒå¥½çš„è¡¨ç°ã€‚

2. **éªŒè¯æŸå¤±çš„ä½œç”¨**ï¼š

   * éªŒè¯æŸå¤±æœ‰åŠ©äºåˆ¤æ–­æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å°¤å…¶æ˜¯å½“è®­ç»ƒæ•°æ®é‡å·¨å¤§æ—¶ï¼Œæˆ‘ä»¬ä¼šæ‹…å¿ƒæ¨¡å‹æ˜¯å¦åªæ˜¯åœ¨è®°ä½è®­ç»ƒæ•°æ®ï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼Œè€Œä¸æ˜¯å­¦ä¼šä»æ•°æ®ä¸­æŠ½è±¡å‡ºä¸€èˆ¬åŒ–çš„çŸ¥è¯†ã€‚
   * å¦‚æœè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æŸå¤±å·®å¼‚è¿‡å¤§ï¼Œå°±å¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå› æ­¤å®šæœŸæ£€æŸ¥éªŒè¯æŸå¤±éå¸¸é‡è¦ã€‚

3. **ä¸ GPT-2 çš„å¯¹æ¯”**ï¼š

   * ä½ å¯ä»¥å°†å½“å‰æ¨¡å‹ä¸ GPT-2ï¼ˆ124Mï¼‰è¿›è¡Œå¯¹æ¯”ï¼ŒæŸ¥çœ‹éªŒè¯æŸå¤±ï¼Œå¹¶ä½œä¸ºä¸€ç§å‚è€ƒï¼Œçœ‹çœ‹å½“å‰æ¨¡å‹å¦‚ä½•ä¸å·²æœ‰çš„æ¨¡å‹ç›¸æ¯”è¾ƒã€‚è™½ç„¶è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸åŒï¼Œç›´æ¥å¯¹æ¯”ä¸å®Œå…¨å…¬å¹³ï¼Œä½†ä»ç„¶æ˜¯ä¸€ä¸ªæœ‰ä»·å€¼çš„å‚è€ƒç‚¹ã€‚

4. **è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ³¨æ„äº‹é¡¹**ï¼š

   * åœ¨å½“å‰çš„è®­ç»ƒä¸­ï¼Œåªè¿›è¡Œäº† **ä¸€ä¸ª Epoch** çš„è®­ç»ƒã€‚å¦‚æœæœªæ¥å¸Œæœ›è¿›è¡Œå¤šæ¬¡ Epochï¼ˆä¾‹å¦‚ 10 ä¸ª Epochï¼‰ï¼Œåˆ™éœ€è¦éå¸¸å°å¿ƒæ˜¯å¦ä¼šå‡ºç° **è¿‡æ‹Ÿåˆ**ï¼Œç‰¹åˆ«æ˜¯å½“æ¨¡å‹éå¸¸å¤§çš„æ—¶å€™ã€‚

### **é‡‡æ ·ä¸ç”Ÿæˆï¼ˆSampling and Generationï¼‰**ï¼š

1. **é‡‡æ ·ä»£ç **ï¼š

   * åœ¨æ¯100æ­¥è®­ç»ƒåï¼Œä¼šç”Ÿæˆä¸€äº›æ–‡æœ¬æ ·æœ¬ï¼Œä»¥ä¾¿è§‚å¯Ÿæ¨¡å‹çš„ç”Ÿæˆæ•ˆæœã€‚å°½ç®¡åœ¨è®­ç»ƒåˆæœŸï¼ˆä¾‹å¦‚ç¬¬ 1000 æ­¥ï¼‰ï¼Œç”Ÿæˆçš„æ–‡æœ¬ä»ç„¶å¾ˆæ‚ä¹±ã€éš¾ä»¥ç†è§£ï¼Œä½†è¿™æ˜¯å› ä¸ºæ¨¡å‹è¿˜å¤„äºæ—©æœŸé˜¶æ®µï¼Œå­¦ä¹ ç‡åˆšåˆšå¼€å§‹å¢åŠ ã€‚
   * ç”Ÿæˆçš„æ–‡æœ¬ç¤ºä¾‹åŒ…æ‹¬åƒâ€œHello, Iâ€™m a language model...â€è¿™æ ·çš„è¯ï¼Œä½†å› ä¸ºæ¨¡å‹è¿˜æ²¡æœ‰å®Œå…¨è®­ç»ƒå¥½ï¼Œè¿™äº›è¾“å‡ºä»ç„¶æ˜¯æ··ä¹±çš„ã€‚

2. **éšæœºæ•°ç”Ÿæˆå™¨ï¼ˆRNGï¼‰**ï¼š

   * ä¸ºäº†ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­ä¸å½±å“éšæœºæ•°ç”Ÿæˆå™¨çš„çŠ¶æ€ï¼Œé‡‡æ ·æ—¶ä½¿ç”¨äº†ä¸€ä¸ªç‹¬ç«‹çš„ **PyTorch éšæœºæ•°ç”Ÿæˆå™¨ï¼ˆRNGï¼‰**ã€‚è¿™å¯ä»¥ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„éšæœºæ•°ç”Ÿæˆä¸é‡‡æ ·è¿‡ç¨‹åˆ†å¼€ï¼Œé¿å…äº’ç›¸å¹²æ‰°ã€‚
   * æ¯ä¸ªè¿›ç¨‹éƒ½ä½¿ç”¨ä¸åŒçš„ç§å­ï¼ˆseedï¼‰æ¥ä¿è¯ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§ã€‚

3. **Torch Compile é—®é¢˜**ï¼š

   * åœ¨ä½¿ç”¨ **`torch.compile`** è¿›è¡Œç¼–è¯‘æ—¶ï¼Œå‡ºç°äº†æŸäº›é”™è¯¯ï¼Œå¯¼è‡´é‡‡æ ·ä»£ç æ— æ³•æ­£å¸¸å·¥ä½œã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç›®å‰ç¦ç”¨äº† `torch.compile`ï¼Œä»¥ç¡®ä¿é‡‡æ ·ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œã€‚ä¸è¿‡ï¼Œè¿™ä¸ªé—®é¢˜é¢„è®¡ä¼šåœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­å¾—åˆ°ä¿®å¤ã€‚

### **æ€»ç»“**ï¼š

* è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨éªŒè¯æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œå®šæœŸè¯„ä¼°ï¼Œä»¥é˜²è¿‡æ‹Ÿåˆï¼Œå¹¶é€šè¿‡è§‚å¯ŸéªŒè¯æŸå¤±æ¥åˆ¤æ–­æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
* åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸç”Ÿæˆæ ·æœ¬æœ‰åŠ©äºè§‚å¯Ÿæ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå°½ç®¡åˆæœŸç”Ÿæˆçš„æ–‡æœ¬å¯èƒ½ä¸å¤ªæœ‰æ„ä¹‰ï¼Œä½†å®ƒä»¬å¯ä»¥å¸®åŠ©æˆ‘ä»¬äº†è§£æ¨¡å‹çš„å­¦ä¹ è¿›åº¦ã€‚
* ä½¿ç”¨ç‹¬ç«‹çš„éšæœºæ•°ç”Ÿæˆå™¨æ¥è¿›è¡Œé‡‡æ ·ï¼Œä»¥é¿å…ä¸è®­ç»ƒä¸­çš„å…¨å±€ RNG çŠ¶æ€å†²çªã€‚
* é‡åˆ° `torch.compile` çš„é—®é¢˜æ—¶ï¼Œæš‚æ—¶ç¦ç”¨è¯¥åŠŸèƒ½ï¼Œç¡®ä¿é‡‡æ ·èƒ½å¤Ÿæ­£å¸¸è¿è¡Œã€‚

æ€»ä¹‹ï¼ŒéªŒè¯æŸå¤±å’Œç”Ÿæˆæ ·æœ¬æ˜¯è¯„ä¼°æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­éå¸¸é‡è¦çš„æ­¥éª¤ï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬ç¡®ä¿æ¨¡å‹åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚


# evaluation: HellaSwag, starting the run

so we're about 30% through training now while this is training I would like to introduce one evaluation that we're
going to use to supplement the validation set and that is the H swag eval so hos swag comes from this paper
back in 2019 so it's a 5-year-old eval now and the way H swag works is there is basically a sentence completion data set
so it's a multiple choice for every one of these questions we have uh basically a shared context like a woman is outside
with a bucket and a dog the dog is running around trying to avoid bath she
a Rises the bucket off with soap and blow dry the dog's head B uses a hose to
keep it from getting soapy C gets the dog wet and it runs away again or D gets
into a bathtub with the dog and so basically the idea is that these
multiple choice are constructed so that one of them is a natural continuation of
the um sentence and the others are not and uh the others might not make
sense like uses the host to keep it from getting soaped that makes no sense and so what happens is that models that are
not trained very well are not able to tell these apart but models that have a lot of World Knowledge and can tell uh
which um and can tell a lot about the world will be able to create these completions and these sentences are
sourced from activity net and from Wiki how and at the bottom of the uh
paper there's kind of like a cool chart of the kinds of domains in Wiki house so
there's a lot of sentences from computers and electronics and Homes and Garden and it has kind of a broad
coverage of the kinds of things you need to know about the world in order to find the most likely completion and um the
identity of that of that completion one more thing that's kind of interesting about H swag is the way it was
constructed is that the incorrect um options are deliberately um
adversarially sourced so they're not just random sentences they're actually sentences generated by language models
and they're generated in such a way that language models basically find them difficult but humans find them easy and
so they mentioned that humans have a 95% accuracy on this set but at the time the state-of-the-art language models had
only 48% and so at the time this was a good Benchmark now you can read the
details of this paper to to learn more um the thing to point out though is that this is 5 years ago and since then what
happened to H swag is that it's been totally just uh um solved and so now the language models
here are 96% so basically the 4% the last 4% is probably errors in the data
set or the questions are really really hard and so basically this data set is kind of crushed with respect to language
models but back then the best language model was only at about 50% uh but this
is how far things got but still the the reason people like H swag and it's not
used by the way in gpt2 but in gpt3 there is H swag eval and lots of people
use H swag and so for gpt3 we have results
here that are cited so we know what percent accuracies gpt3 um attains at all these
different model checkpoints for H swag eval and the reason people like it is because H swag is a smooth eval and it
is an eval that offers quote unquote early signal uh so early signal means that even small language models are
going to start at the random chance of 25% but they're going to slowly improve and you're going to see 25 26 27 Etc and
uh you can see slow Improvement even when the models are very small and it's very early so it's smooth it has early
signal and um it's been around for a long time so that's why people kind of
like this eval uh now the way that we're going to evaluate this is as
follows as I mentioned we have a shared context and this is kind of like a multiple choice task but instead of
giving the model a multiple choice question and asking it for A B C or D uh we can't do that because these models
when they are so small as we are seeing here the models can't actually do multiple choice they don't understand
the concept of associating a label to one of the options of multiple choice uh they don't understand that so we have to
give it to them in a native form and the native form is a token completion so
here's what we do we construct a batch of four rows and uh T tokens whatever
that t happens to be then the shared context that is basically the context for the for choices the tokens of that
are shared across all of the rows and then we have the four options so we kind of like lay them out and then only one
of the options is correct in this case label three option three and so um this
is the correct option and option one two and for are incorrect now these options might be of
different lengths so what we do is we sort of like take the longest length and that's the size of the batch B BYT and
then some of these uh here are going to be pded Dimensions so they're going to be unused and so we need the tokens we
need the correct label and we need a mask that tells us which tokens are active and the mask is then zero for
these uh padded areas so that's how we construct these batches and then in
order to get the language model to predict A B C or D the way this works is basically we're just going to look at
the tokens their probabilities and we're going to pick the option that gets the
lowest or the highest average probability for the token so for the
tokens because that is the most likely completion according to the language model so we're just going to look at the
um probabilities here and average them up across the options and pick the one
with the highest probability roughly speaking so this is how we're going to do H swag
um and this is I believe also how uh gpt3 did it um this is how gpt3 did it
as far as I know but you should note that some of the other evals where you might see H swag may not do it this way
they may do it in a multiple choice format where you sort of uh give the the context a single time and then the four
completions and so the model is able to see all the four options before it picks the best possible option and that's
actually an easier task for a model because you get to see the other options when you're picking your choice um but
unfortunately models at our size can't do that only models at a bigger size are able to do that and so our models are
actually slightly handicapped in this way that they are not going to see the other options they're only going to see
one option at a time and they just have to assign probabilities and the correct option has to win out in this metric all
right so let's now implement this very briefly and incorporate it into our script okay so what I've done here is
I've introduced a new file called hell swag. py that you can take a look into and I'm not going to to step through all
of it because uh this is not exactly like deep code deep code it's kind of
like a little bit tedious honestly because what's happening is I'm downloading hsac from GitHub and I'm
rendering all of its examples and there are a total of 10,000 examples I am rendering them into this format um and
so here at the end of this render example function you can see that I'm
returning the tokens uh the tokens of this um 4xt
uh array of Tokens The Mask which tells us which parts are the options and
everything else is zero and the label that is the correct label and so that
allows us to then iterate the examples and render them and I have an evaluate function here which can load a um gpt2
from huging face and it runs the eval here um and it basically just calculates
uh just as I described it predicts the option that has the lowest or the highest prob ility and the way to do
that actually is we can basically evaluate the cross entropy loss so we're basically evaluating the loss of
predicting the next token in a sequence and then we're looking at the row that has the lowest average loss and that's
the uh option that we pick as the prediction and then we do some stats and
prints and stuff like that so that is a way to evaluate L swag now if you go up here I'm showing that for GPT 2124m if
you run this script you're going to see that H swag gets
29.5% um so that's the performance we get here now remember that random Chan is 25% so we haven't gone too far and
gpt2 XL which is the biggest the gpt2 gets all the way up to 49% roughly so uh
these are pretty low values considering that today's state-ofthe-art is more like 95% uh so these are definitely
older models by now and then there's one more thing called Uther harness which is a very piece of infrastructure for
running evals for language models and they get slightly different numbers and I'm not 100% sure what the discrepancy
is for these um it could be that they actually do the multiple choice uh instead of just the completions and that
could be the um uh the discrepancy but I'm not 100% sure about that i' have to take a look but for now our script
reports 2955 and so that is the number that we'd like to beat if we are training a GPD 2124m from scratch and
ourselves um so now I'm going to go into actually
incorporating this eval into our main training script and um and basically
because we want to evaluate it in a periodic manner so that we can track H swag and how it evolves over time and
see when when and if we cross uh this 2955 um sort of region so let's now walk
through some of the changes to train gpt2 thatp the first thing I did here is I actually made use compile optional
kind of and I disabled it by default and the problem with that is the problem
with compile is that unfortunately it does make our code faster but it actually breaks the evaluation code and
the sampling code it gives me a very gnarly message and I don't know why so hopefully by the time you get to the
codebase when I put it up on GitHub uh we're going to fix that by then but for now I'm running without torch compile
which is why you see this be a bit slower so we're running without torch compile I also create cre a log
directory log where we can place our log.txt which will record the train loss
validation loss and the H swag accuracies so a very simple text file and we're going to uh open for writing
so that it sort of starts empty and then we're going to append to it I created a simple variable that um
helps tell us when we have a last step and then basically periodically inside this Loop every 250th iteration or at
the last step we're going to evaluate the validation loss and then every 250th
iteration um we are going to evaluate H swag but only if we are not using
compile because compile breaks it so I'm going to come back to this code for evaluating H swag in a second and then
every 250th iteration as well we're also going to sample from the model and so you should recognize this as our ancient
code from way back when we started the video and we're just sampling from the model
and then finally here um these are if we're not after we validate sample and
evaluate hell swag we actually do a training step here and so this is one step of uh training and you should be
pretty familiar with all of what this does and at the end here once we get our training laws we write it to the file so
the only thing that changed that I really added is this entire section for H swag eval and the way this works is
I'm trying to get all the gpus to collaborate on the H swag and so we're iterating all the examples and then each
process only picks the examples that assigned to it so we sort of take I and
moded by the world size and we have to make it equal to rank otherwise we continue and then we render an example
put it on the GPU we get the low jits then I create a helper function that helps us basically predict the option
with the lowest loss so this comes here the prediction and then if it's correct we sort of keep count and then if
multiple processes were collaborating on all this then we need to synchronize their stats and so the way one way to do
that is to package up our statistics here into tensors which we can then call
this. alberon and sum and then here we sort of um unwrap
them from tensors so that we just have ins and then here the master process will print and log the hellis swag
accuracy so that's kind of the that's kind of it
and that's what I'm running right here so you see this optimization here and uh we just had a generation and this is
Step 10,000 out of about 20,000 right so we are halfway done and these are the
kinds of samples that uh we are getting at this stage so let's take a look hello I'm a language model so I'd like to use
it to generate some kinds of output hello I'm a language model and I'm a developer for a lot of
companies Al language model uh let's see if I can find fun
one
um I don't know you can go through this yourself but certainly the predictions are getting less and less random uh it
seems like the model is a little bit more self-aware and using language uh that is a bit
more uh specific to it being language model hello I'm a language model and
like how the language is used to communicate I'm a language model and I'm going to be speaking English and German
okay I don't know so let's just wait until this optimization finishes and uh we'll see what kind of samples we get
and we're also going to look at the train Val and the hway accuracy and see
how we're doing with respect to gpt2 okay good morning so focusing For a

è¿™æ®µå†…å®¹ä»‹ç»äº†å¦‚ä½•åœ¨æ¨¡å‹è®­ç»ƒä¸­åŠ å…¥ **HellaSwag** è¯„ä¼°ï¼ˆç®€ç§° **H-Swag**ï¼‰ï¼Œä»¥åŠå¦‚ä½•å°†å…¶ä¸è®­ç»ƒè¿‡ç¨‹ç»“åˆï¼Œè¿›è¡Œå‘¨æœŸæ€§è¯„ä¼°ï¼Œç›‘æ§æ¨¡å‹çš„æ€§èƒ½ã€‚

### **HellaSwag è¯„ä¼°ç®€ä»‹**

1. **HellaSwag** æ˜¯ä¸€ç§ **å¥å­å®Œæˆ** ä»»åŠ¡çš„è¯„ä¼°æ•°æ®é›†ï¼Œä»»åŠ¡æ ¼å¼æ˜¯ç»™å®šä¸€ä¸ªä¸Šä¸‹æ–‡ï¼ˆæ¯”å¦‚ï¼š**â€œä¸€ä¸ªå¥³äººç«™åœ¨å¤–é¢ï¼Œæ‰‹é‡Œæ‹¿ç€ä¸€ä¸ªæ¡¶å’Œç‹—ï¼Œç‹—æ­£åœ¨å››å¤„è·‘ï¼Œè¯•å›¾é¿å¼€æ´—æ¾¡â€**ï¼‰ï¼Œç„¶åç»™å‡º 4 ä¸ªé€‰é¡¹ï¼Œè¦æ±‚æ¨¡å‹é€‰æ‹©ä¸€ä¸ªæœ€ç¬¦åˆä¸Šä¸‹æ–‡çš„ç­”æ¡ˆã€‚ä¾‹å¦‚ï¼š

   * A. ç”¨è‚¥çš‚æŠŠæ¡¶è£…æ»¡
   * B. ç”¨æ°´ç®¡æŠŠç‹—å¼„æ¹¿
   * C. æŠŠç‹—å¼„æ¹¿åå®ƒå†æ¬¡è·‘å¼€
   * D. è·Ÿç‹—ä¸€èµ·è¿›æµ´ç¼¸

2. **æ•°æ®é›†æ„é€ **ï¼š

   * æ¯ä¸ªé—®é¢˜çš„ 4 ä¸ªé€‰é¡¹ä¸­ï¼Œåªæœ‰ä¸€ä¸ªæ˜¯åˆç†çš„å›ç­”ï¼Œå…¶ä»–ä¸‰ä¸ªé€‰é¡¹æ˜¯æ— æ„ä¹‰æˆ–è€…ä¸ç¬¦åˆä¸Šä¸‹æ–‡çš„ç­”æ¡ˆã€‚
   * è¿™äº›é—®é¢˜æ¥è‡ª **ActivityNet** å’Œ **WikiHow**ï¼Œæ¶µç›–äº†è®¸å¤šé¢†åŸŸï¼Œå¦‚è®¡ç®—æœºã€ç”µå­äº§å“ã€å®¶åº­å›­è‰ºç­‰ï¼Œéœ€è¦æ¨¡å‹å…·å¤‡ä¸€å®šçš„ä¸–ç•ŒçŸ¥è¯†æ¥é€‰æ‹©æ­£ç¡®ç­”æ¡ˆã€‚
   * å…¶ä¸­ï¼Œé”™è¯¯é€‰é¡¹ä¸æ˜¯éšæœºç”Ÿæˆçš„ï¼Œè€Œæ˜¯é€šè¿‡ **è¯­è¨€æ¨¡å‹** ç‰¹æ„ç”Ÿæˆçš„ï¼Œè¿™æ ·å®ƒä»¬å¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ä¼šå¾ˆéš¾å¤„ç†ï¼Œä½†å¯¹äººç±»æ¥è¯´åˆ™å¾ˆå®¹æ˜“åˆ¤æ–­ã€‚

3. **è¯„ä¼°è¿‡ç¨‹**ï¼š

   * è¯­è¨€æ¨¡å‹éœ€è¦æ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆæœ€æœ‰å¯èƒ½çš„å¥å­å®Œæˆï¼Œé€‰æ‹©æœ€åˆé€‚çš„é€‰é¡¹ã€‚H-Swagçš„ä¸€ä¸ªé‡è¦ç‰¹æ€§æ˜¯ **å¹³æ»‘çš„è¯„ä¼°æ›²çº¿**ï¼ˆsmooth evalï¼‰ï¼Œå³ä½¿æ˜¯å°å‹æ¨¡å‹ä¹Ÿèƒ½é€æ­¥æé«˜å‡†ç¡®åº¦ï¼Œèƒ½å¤Ÿæ—©æœŸç»™å‡ºæ¨¡å‹å­¦ä¹ çš„ä¿¡å·ã€‚
   * ç„¶è€Œï¼Œç”±äº H-Swag æ˜¯åœ¨ 5 å¹´å‰è®¾è®¡çš„ï¼Œå®ƒçš„ä»»åŠ¡å¯¹ç°ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´å·²ç»â€œè¿‡äºç®€å•â€ï¼Œå¾ˆå¤šæ¨¡å‹çš„å‡†ç¡®ç‡å·²ç»è¾¾åˆ° 96%ï¼Œä½†å®ƒä»ç„¶è¢«ç”¨ä½œ **æ—©æœŸä¿¡å·** è¯„ä¼°ï¼Œå¸®åŠ©è§‚å¯Ÿå°æ¨¡å‹çš„è®­ç»ƒè¿›å±•ã€‚

### **å¦‚ä½•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç° H-Swag è¯„ä¼°**

1. **æ¨¡å‹ä¸æ•°æ®çš„å‡†å¤‡**ï¼š

   * ä¸ºäº†ä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç† H-Swag ä»»åŠ¡ï¼Œé€šå¸¸æˆ‘ä»¬å°†æ¯ä¸ªé—®é¢˜å’Œ 4 ä¸ªé€‰é¡¹çš„ä¸Šä¸‹æ–‡è½¬æ¢ä¸º **token å®Œæˆ** å½¢å¼ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä¼šå°†æ¯ä¸ªé€‰é¡¹çš„å†…å®¹ä½œä¸ºæ¨¡å‹è¾“å…¥çš„ä¸€éƒ¨åˆ†ï¼Œæ¨¡å‹ä¼šæ ¹æ®è¾“å…¥çš„ä¸Šä¸‹æ–‡æ¥é¢„æµ‹æœ€åˆç†çš„é€‰é¡¹ã€‚
   * ç”±äºæˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹è¾ƒå°ï¼Œå› æ­¤æ— æ³•ç›´æ¥ä½¿ç”¨å¸¸è§çš„å¤šé¡¹é€‰æ‹©æ–¹å¼ï¼ˆå³ç›´æ¥ç»™å‡º 4 ä¸ªé€‰é¡¹è®©æ¨¡å‹ä¸€æ¬¡æ€§é€‰æ‹©ï¼‰ã€‚å–è€Œä»£ä¹‹çš„æ˜¯ï¼Œæ¯æ¬¡ç»™æ¨¡å‹å‘ˆç°ä¸€ä¸ªé€‰é¡¹ï¼Œé€šè¿‡ **token çš„æ¦‚ç‡åˆ†å¸ƒ** æ¥å†³å®šå“ªä¸ªé€‰é¡¹æ˜¯æœ€å¯èƒ½çš„ã€‚

2. **è¯„ä¼°å®ç°**ï¼š

   * **è¯„ä¼°ä»£ç **ï¼šç¼–å†™ä¸€ä¸ª `hellaswag.py` æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶è´Ÿè´£åŠ è½½ H-Swag æ•°æ®ï¼Œæ¸²æŸ“é—®é¢˜å¹¶å°†å…¶è½¬åŒ–ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ ¼å¼ï¼ˆä¾‹å¦‚ token åŒ–ï¼‰ã€‚æ¯ä¸ªé—®é¢˜çš„æ­£ç¡®ç­”æ¡ˆä¼šè¢«æ ‡è®°ï¼Œå¹¶ä¸”ä¼šæœ‰ä¸€ä¸ª **mask** ç”¨æ¥æ ‡è®°å“ªäº›éƒ¨åˆ†æ˜¯æœ‰æ•ˆçš„ã€‚
   * ç„¶åï¼Œä½¿ç”¨ **GPT-2 æ¨¡å‹**ï¼ˆä» Hugging Face åŠ è½½ï¼‰è¿›è¡Œè¯„ä¼°ï¼Œè®¡ç®—æ¨¡å‹å¯¹æ¯ä¸ªé€‰é¡¹çš„é¢„æµ‹æŸå¤±ï¼ˆcross-entropy lossï¼‰ã€‚æ ¹æ®è¿™ä¸ªæŸå¤±å€¼ï¼Œé€‰æ‹©é¢„æµ‹æŸå¤±æœ€å°çš„é€‰é¡¹ä½œä¸ºç­”æ¡ˆã€‚

3. **å‘¨æœŸæ€§è¯„ä¼°**ï¼š

   * åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯ **250 æ­¥** ä¼šè¿›è¡Œä¸€æ¬¡è¯„ä¼°ï¼ŒåŒ…æ‹¬ï¼š

     * **éªŒè¯æŸå¤±**ï¼šç”¨æ¥æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ã€‚
     * **H-Swag è¯„ä¼°**ï¼šç”¨æ¥æ£€æŸ¥æ¨¡å‹åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚
     * **é‡‡æ ·**ï¼šæ¯ 250 æ­¥é‡‡æ ·ä¸€æ¬¡æ¨¡å‹è¾“å‡ºï¼Œè§‚å¯Ÿç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚
   * æ‰€æœ‰è¯„ä¼°ç»“æœï¼ˆè®­ç»ƒæŸå¤±ã€éªŒè¯æŸå¤±ã€H-Swag å‡†ç¡®ç‡ï¼‰éƒ½ä¼šå†™å…¥æ—¥å¿—æ–‡ä»¶ï¼Œä¾¿äºè·Ÿè¸ªè®­ç»ƒè¿›åº¦ã€‚

4. **å¤š GPU åä½œ**ï¼š

   * ç”±äºè®­ç»ƒæ˜¯åˆ†å¸ƒå¼çš„ï¼Œéœ€è¦ç¡®ä¿å¤šä¸ª GPU ååŒå·¥ä½œæ¥è¿›è¡Œ H-Swag è¯„ä¼°ã€‚æ¯ä¸ªè¿›ç¨‹ä¼šå¤„ç†ä¸åŒçš„æ ·æœ¬ï¼Œå¹¶åœ¨è®¡ç®—å®Œæˆåè¿›è¡Œç»Ÿè®¡æ±‡æ€»ã€‚æœ€ç»ˆï¼Œä¸»è¿›ç¨‹ä¼šè¾“å‡ºå¹¶è®°å½•è¯„ä¼°ç»“æœã€‚

### **è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ€§èƒ½**

* åœ¨ **GPT-2 124M** æ¨¡å‹ä¸Šè¿›è¡Œ H-Swag è¯„ä¼°æ—¶ï¼Œå‡†ç¡®ç‡å¤§çº¦ä¸º 29.5%ã€‚å¯¹äº GPT-2 XL æ¨¡å‹ï¼Œå‡†ç¡®ç‡æå‡åˆ°çº¦ 49%ã€‚å°½ç®¡è¿™äº›å€¼ç›¸å¯¹è¾ƒä½ï¼Œä½†ä¸éšæœºçŒœæµ‹ï¼ˆ25%ï¼‰ç›¸æ¯”ï¼Œå·²ç»æœ‰äº†æ˜æ˜¾çš„è¿›æ­¥ã€‚
* ç„¶è€Œï¼Œç°åœ¨çš„ **ä¸»æµæ¨¡å‹** çš„ H-Swag å‡†ç¡®ç‡å·²ç»æ¥è¿‘ 96%ï¼Œè¿™æ„å‘³ç€ H-Swag å¯¹äºå½“å‰çš„æ¨¡å‹æ¥è¯´å·²ç»ä¸å†æ˜¯ä¸€ä¸ªéš¾é¢˜ï¼Œè€Œæ›´å¤šçš„æ˜¯ä¸€ä¸ª â€œåŸºå‡†â€ ç”¨æ¥è¯„ä¼°æ¨¡å‹çš„è¿›å±•ã€‚

### **æ€»ç»“**

é€šè¿‡å‘¨æœŸæ€§åœ°è¿›è¡Œ H-Swag è¯„ä¼°ï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬è·Ÿè¸ªæ¨¡å‹åœ¨å¥å­å®Œæˆä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è™½ç„¶è¿™ä¸ªä»»åŠ¡å¯¹äºç°ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´å·²ç»å˜å¾—å®¹æ˜“ï¼Œä½†å®ƒä»ç„¶æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒåˆæœŸèƒ½å¤Ÿç»™å‡º **æ—©æœŸä¿¡å·**ï¼Œå¸®åŠ©æˆ‘ä»¬åˆ¤æ–­æ¨¡å‹çš„å­¦ä¹ è¿›å±•ã€‚åŒæ—¶ï¼Œé€šè¿‡è°ƒæ•´è®­ç»ƒè„šæœ¬ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ–¹ä¾¿åœ°è·Ÿè¸ªè®­ç»ƒæŸå¤±ã€éªŒè¯æŸå¤±å’Œ H-Swag å‡†ç¡®ç‡ï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹è®­ç»ƒã€‚


# SECTION 4: results in the morning! GPT-2, GPT-3 repro

Moment On The jupyter Notebook here on the right I created a new cell that basically allows us to visualize the the
train Val and Hela and um the hel score and you can step through this it
basically like parses the log file that we are writing and um a lot of this is just like boring ma plot lip code but
basically this is what our optimization looks like so we ran for
19,731 billion tokens which is whoops oh my gosh which is one Epoch of the sample
10B of webd on the left we have the loss and the in blue we have the training
loss in Orange we have the validation loss and red as a horizontal line we
have the opening IG gpt2 124 M model checkpoint when it's just evaluated on
the validation set of um of this fine web edu uh so you can see that we are
surpassing this orange is below the red so we're surpassing the validation set of this data set and like I mentioned
the data set distribution is very different from what gpt2 trained on so this is not an exactly fair comparison
but it's a good cross check uh to uh to look at now we would ideally like
something that is withheld and comparable and somewhat standard um and
so for us that is helis swag and so on here we see the H swag progress we made from 25% all the way here in red we see
the open gpt2 124 M model in red so it
achieves this h bag here and the the gpt3 model 124 M which was trained on
300 billion tokens achieves green so that's over here so you see that we
basically surpassed the gbt2 24m uh model right here uh which is uh really
nice now interestingly we were able to do so with only training on 10 billion tokens while gpt2 was trained on 100
billion tokens so uh for some reason we were able to get away with significantly fewer tokens for training there are many
possibilities to as to why we could match or surpass this accuracy um with
only 10 million training so number one um it could be that opening gbt2 was
trained on a much wider data distribution so in particular fine web edu is all English it's not multilingual
and there's not that much math and code um and so math and code and multilingual
could have been stealing capacity from the original gpt2 model and um basically
that could be partially the reason why uh this is not working out there's many other reasons um so for example the H
swag eval is fairly old uh maybe 5 years or so it is possible that aspects of H
swag in some way or even identically have made it into the training Set uh of fine web we don't know for sure but if
that was the case then we are basically looking at the training curve instead of the validation curve so long story short this is not a perfect eval and there's
some caveats here uh but at least we have some confidence that that we're not doing something completely wrong and
um and uh it's probably the case that when people try to create these data sets they try to make sure that test
sets that are very common are not part of the training set for example uh when hugging face created the fine web BDU
they use H swag as an eval so I would hope that they make sure that they D duplicate and that there's no hella swag
in the training set but we can't be sure uh the other thing I wanted to address briefly is look at this loss curve this
looks really this looks really wrong here I don't actually know 100% what this is and I suspect it's because the
uh 10 billion sample of fine web edu was not properly shuffled um and there's
some issue here uh with the data that I don't fully understand yet and there's some weird periodicity to it um and
because we are in a very lazy way sort of serializing all the tokens and just iterating all them from scratch without
doing any permutation or any random sampling ourselves I think we're inheriting some of the ordering that
they have in the data set so uh this is not ideal but hopefully by the time you
get to this repo uh some of these things by the way will hopefully be fixed and I
will release this build n GPT repo and right now it looks a little ugly and
preliminary uh so hopefully by the time you get here it's nicer but down here I'm going to show aada and I'm going to
talk about about some of the things that happened after the video and I expect that we will have fixed uh the small
issue uh but for now basically this shows that uh our training is not uh completely wrong and it shows that uh
we're able to surpass the accuracy with only 10x the token budget um and
possibly it could be also that the data set may have improved so uh the original
uh gpt2 data set was web text it's possible that not a lot of care and attention went into the data set this
was very early in llms whereas now there's a lot more scrutiny on good practices around uh D duplication
filtering uh quality filtering and so on and it's possible that the data that we're training on is just of higher quality per token and that could be
giving us a boost as well so a number of cave has to think about but for now uh we're pretty happy with this um and yeah
now the next thing I was interested in is as you see it's a morning now so there was an overnight and I wanted to
basically see how far I could push the result so uh to do an overnight run I
basically did instead of one Epoch which took roughly two hours I just did a times four so that that would take eight
hours while I was sleeping and so we did four Epoch or roughly 40 billion uh tokens of training and I was trying to
see how far we could get um and so this was the only change and I reran the script and when I point uh and read the
log file at uh at the 40b uh this is what the curve look like okay so to narrate this number one
we are seeing this issue here here with the periodicity through the different Epoch and something really weird with
the fine web edu data set and that is to be determined uh but otherwise we are
seeing that the H swag actually went up by a lot and we almost we almost made it
uh to the GPT 324m accuracy uh up here uh but not quite so uh it's too bad that
I didn't sleep slightly longer um and uh I think if this was an uh five Epoch run
we may have gotten here now one thing to point out is that if you're doing multi Epoch runs uh we're not actually being
very careful in our data loader and we're not um I this data loader goes
through the data in exactly the same format and exactly the same order and
this is kind of suboptimal and you would want to look into extensions where you actually permute the data uh randomly
you permute the documents around in Every Single Shard on every single new Epoch um and po even permute the
shards and that would go a long way into decreasing the pricity and it's also better for the optimization so that
you're not seeing things ident in the identical format and you're introducing some of the some uh Randomness in how
the documents follow each other because you have to remember that in every single row these documents follow each
other and then there's the end of text token and then the next document so the documents are currently glued together
in the exact same identical manner but we actually want to break break up the documents and shuffle them around
because the order of the documents shouldn't matter and they shouldn't um basically we want to break up that
dependence because it's a kind of a spous correlation and so our data lad is not currently doing that and that's one
Improvement uh you could think of making um the other thing to point out
is we're almost matching gpt3 accuracy with only 40 billion tokens gpt3 trained on 300 billion tokens so again we're
seeing about a 10x um Improvement here with respect to learning efficiency uh
the other thing I wanted to and I don't actually know exactly what to attribute this to other than some of the things
that I already mentioned previously for the previous run uh the other thing I wanted to briefly mention is uh the max
LR here I saw some people already play with this a little bit in a previous related repository um and it turns out
that you can actually almost like three xas so it's possible that the maximum learning rate can be a lot higher and
for some reason the gpt3 hyper parameters that we are inheriting are actually extremely conservative and you can actually get away with a Higher
Learning rate and it would train faster so a lot of these hyper parameters um are quite tunable and feel free to play
with them and they're probably not set precisely correctly and um it's possible
that you can get away with doing this basically and if you wanted to exactly be faithful to gpt3 you would also want
to make the following difference you'd want to come here and the sequence length of gpt3 is 2x it's 20 48 instead
of 1,24 so you would come here change this to 248 for T and then if you want
the exact same number of tokens uh half a million per iteration or per step you
want to then decrease this to 32 so they still multiply to half a mil so that
would give your model sequence length equal to that of gpt3 and in that case basically the
um the models would be roughly identical as far as I'm as far as I'm aware
because again gpt2 and gpt3 are very very similar models now we can also look at some of the samples here from the
model that was trained overnight so this is the optimization and you see that here
we stepped all the way to 76290 also or so and these are the hos
mag we achieved was 33.2 4 and these are some of the samples from the model and
you can see that if you read through this and pause the video briefly you can see that they are a lot more coherent uh
so um and they're actually addressing the fact that it's a language model almost
so uh hello I'm a language model and I try to be as accurate as
possible um I'm a language model not a programming language I know how to communicate uh I
use Python um I don't know if you pause this and
look at it and then compare it to the one to the model that was only trained for 10 billion uh you will see that
these are a lot more coherent and you can play with this uh yourself one more thing I added to The Code by the way is this chunk of code
here so basically right after we evaluate the validation loss if we are the master process in addition to
logging the validation loss every 5,000 steps we're also going to save the checkpoint which is really just the
state dictionary of the model and so checkpointing is nice just because uh you can save the model and later you can
uh use it in some way if you wanted to resume the optimiz ation then in addition to saving the model we have to
also save the optimizer State dict because remember that the optimizer has a few additional buffers because of adom
so it's got the m and V and uh you need to also resume the optimizer properly
you have to be careful with your RNG seeds uh random number generators and so on so if you wanted to exactly be able
to resume optimization you have to think through the state of the of the training process but if you just want to save the
model this is how you would do it and one one nice reason why you might want to do this is because you may want to
evaluate the model a lot more carefully so here we are only kind of like winging
the hell swag eval but you may want to use something um nicer like for example
the Luther uh Luther evaluation hardness evaluation hardness hardness um so this
is a way to also evaluate language models and um so it's possible that um
you may want to use basically different infrastructure to more thoroughly evaluate the models on different um
evaluations and compare it to the opening gbt2 model on many other um tasks like for example that involve math
code or different languages and so on so this is a nice functionality to have as well
um and then the other thing I wanted to mention is that everything we've built here this is only the pre-training step
so um the GPT here is a it dreams documents it just predicts the next to
you can't talk to it like you can talk to chat GPT uh chat GPT if you wanted to talk to the model we have to fine-tune
it into the chat format and it's not actually like that complicated if you're looking at supervised fine-tuning or sft
really what that means is we're just swapping out a data set into a data set that is a lot more conversational and there's a user assistant user assistant
kind of structure and we just fine-tune on it and then we um we basically fill in the user tokens and we sample the
assistant tokens it's not a lot more deeper than that uh but basically we swap out the data set and continue
training uh but for now we're going to stop at uh pre-training one more thing that I wanted to briefly show you is

è¿™æ®µå†…å®¹æ˜¯è§†é¢‘æˆ–æ•™å­¦ç¬”è®°çš„ç¬¬ 4 éƒ¨åˆ†ï¼Œä½œè€…åœ¨è®­ç»ƒå®Œæˆåè®²è§£äº† **è®­ç»ƒç»“æœçš„å¯è§†åŒ–ã€å¯¹æ¯” GPT-2 å’Œ GPT-3 è®­ç»ƒæ•ˆæœ**ï¼Œå¹¶è®¨è®ºäº†ä¸€äº›å½±å“è®­ç»ƒæ•ˆç‡å’Œè´¨é‡çš„å› ç´ ã€‚ä¸‹é¢æ˜¯é€æ®µä¸­æ–‡è§£é‡Šï¼š

---

## ğŸ§ª **1. è®­ç»ƒç»“æœå¯è§†åŒ–ï¼šè®­ç»ƒæŸå¤± / éªŒè¯æŸå¤± / HellaSwag è¡¨ç°**

ä½œè€…åœ¨ Jupyter Notebook ä¸­åˆ›å»ºäº†ä¸€ä¸ª cellï¼Œè¯»å–å‰é¢è®­ç»ƒæ—¶è®°å½•çš„æ—¥å¿—æ–‡ä»¶ï¼ˆlog.txtï¼‰ï¼Œå¹¶ä½¿ç”¨ `matplotlib` ç»˜å›¾ï¼Œå±•ç¤ºè®­ç»ƒè¿›åº¦ï¼š

* **è“çº¿ï¼šè®­ç»ƒæŸå¤±ï¼ˆtrain lossï¼‰**
* **æ©™çº¿ï¼šéªŒè¯æŸå¤±ï¼ˆval lossï¼‰**
* **çº¢çº¿ï¼šGPT-2 å®˜æ–¹æ¨¡å‹ï¼ˆ124Mï¼‰åœ¨éªŒè¯é›†ä¸Šçš„æŸå¤±ï¼ˆä½œä¸ºå‚è€ƒçº¿ï¼‰**

### ğŸ” è§‚å¯Ÿç»“æœï¼š

* æ©™çº¿ä½äºçº¢çº¿ï¼Œè¯´æ˜æˆ‘ä»¬è‡ªå·±è®­ç»ƒçš„æ¨¡å‹åœ¨è¿™ä¸ªéªŒè¯é›†ä¸Š**å·²ç»è¶…è¿‡äº† GPT-2 çš„è¡¨ç°**ã€‚
* è™½ç„¶ä¸æ˜¯å®Œå…¨å…¬å¹³çš„å¯¹æ¯”ï¼ˆæ•°æ®é›†åˆ†å¸ƒä¸åŒï¼‰ï¼Œä½†è¿™ä»æ˜¯ä¸€ä¸ªå€¼å¾—å‚è€ƒçš„æŒ‡æ ‡ã€‚

---

## ğŸ“Š **2. HellaSwag å‡†ç¡®ç‡æå‡æƒ…å†µ**

å›¾ä¸­ä¹Ÿå±•ç¤ºäº† **HellaSwag å‡†ç¡®ç‡** çš„æå‡æƒ…å†µï¼š

* ä»æœ€åˆçš„éšæœºçŒœæµ‹ï¼ˆ25%ï¼‰æå‡åˆ°äº† **å¤§çº¦ 30%+**
* çº¢çº¿æ˜¯ GPT-2 çš„å‡†ç¡®ç‡ï¼ˆ29.5%ï¼‰
* ç»¿çº¿æ˜¯ GPT-3 åœ¨åŒä¸€ä»»åŠ¡ä¸Šå–å¾—çš„å‡†ç¡®ç‡ï¼ˆçº¦ 33.5%ï¼‰
* æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹åœ¨ä»…ä½¿ç”¨ **10Bï¼ˆåäº¿ï¼‰token** çš„æƒ…å†µä¸‹**å°±è¶…è¶Šäº† GPT-2**

---

## ğŸ“‰ **3. ä¸ºä»€ä¹ˆåªç”¨äº† GPT-2 çš„ 1/10 æ•°æ®å°±è¶…è¿‡äº†å®ƒï¼Ÿ**

ä½œè€…åˆ†æäº†å‡ ç‚¹å¯èƒ½çš„åŸå› ï¼š

1. **æ•°æ®è´¨é‡æå‡**ï¼šGPT-2 ç”¨çš„æ˜¯ WebTextï¼Œæˆ‘ä»¬ç”¨çš„æ˜¯ FineWeb Eduï¼Œè¿™ä¸ªæ•°æ®é›†æ›´å¹²å‡€ã€èšç„¦äºè‹±è¯­å’Œé«˜è´¨é‡æ–‡æœ¬ã€‚
2. **æ•°æ®èŒƒå›´æ›´é›†ä¸­**ï¼šGPT-2 æ˜¯å¤šè¯­è¨€+ä»£ç +æ··åˆæ–‡æœ¬ï¼Œè®­ç»ƒç›®æ ‡æ›´æ‚ã€‚è€Œæˆ‘ä»¬åªç”¨è‹±æ–‡é€šè¯†æ•°æ®ï¼Œæ›´å®¹æ˜“è®­ç»ƒå‡ºç¨³å®šçš„è¯­è¨€æ¨¡å‹ã€‚
3. **å¯èƒ½çš„æ•°æ®æ³„éœ²ï¼Ÿ**

   * HellaSwag æ˜¯å…¬å¼€æ•°æ®é›†ï¼Œæœ‰å¯èƒ½æŸäº›æ ·æœ¬åœ¨ FineWeb ä¸­å‡ºç°è¿‡ï¼ˆä½† HuggingFace åº”è¯¥åšè¿‡å»é‡å¤„ç†ï¼‰ã€‚
   * æ‰€ä»¥ HellaSwag å‡†ç¡®ç‡å¹¶é 100% å¯é çš„æ³›åŒ–è¯„ä¼°ï¼Œåªèƒ½ä½œä¸º**å‚è€ƒæŒ‡æ ‡**ã€‚

---

## âš ï¸ **4. å¥‡æ€ªçš„è®­ç»ƒæŸå¤±å‘¨æœŸæ€§æ³¢åŠ¨ï¼Ÿ**

å›¾ä¸­è®­ç»ƒæŸå¤±æ›²çº¿å­˜åœ¨æ˜æ˜¾çš„æ³¢åŠ¨ï¼ˆå‘¨æœŸæ€§éœ‡è¡ï¼‰ï¼š

* åŸå› å¯èƒ½æ˜¯ **æ•°æ®é›†æœªæ‰“ä¹±**ï¼ˆæ²¡æ´—ç‰Œ shuffleï¼‰ï¼š

  * æˆ‘ä»¬çš„ data loader æ˜¯é¡ºåºåŠ è½½æ‰€æœ‰ tokenï¼Œæ²¡æœ‰å¯¹æ–‡æ¡£æˆ– shard åšæ‰“ä¹±ã€‚
  * å¯¼è‡´æ¯è½®çœ‹åˆ°çš„æ–‡æ¡£é¡ºåºå‡ ä¹ä¸€è‡´ï¼Œè¿™ä¼šå¯¼è‡´æ¨¡å‹â€œè®°ä½é¡ºåºâ€ï¼Œè€Œä¸æ˜¯æ³›åŒ–ã€‚
  * **æ”¹è¿›å»ºè®®**ï¼šåœ¨æ¯è½®è®­ç»ƒå‰æ‰“ä¹±æ–‡æ¡£é¡ºåºï¼Œç”šè‡³æ‰“ä¹±æ¯ä¸ª shard çš„é¡ºåºï¼Œä»¥å‡å°‘â€œé¡ºåºä¾èµ–â€ã€‚

---

## ğŸŒ™ **5. è¿‡å¤œè®­ç»ƒç»“æœï¼ˆ40B tokensï¼‰**

ä½œè€…å°†è®­ç»ƒæ‰©å±•ä¸º 4 ä¸ª epochï¼ˆæ€»è®¡çº¦ 40B tokensï¼‰ï¼Œè®­ç»ƒäº†ä¸€æ•´å¤œï¼š

* **HellaSwag å‡†ç¡®ç‡è¾¾åˆ° 33.2%**
* å·²éå¸¸æ¥è¿‘ GPT-3 124M æ¨¡å‹çš„å‡†ç¡®ç‡ï¼ˆ33.5%ï¼‰
* åŒæ—¶è®­ç»ƒæŸå¤±è¿›ä¸€æ­¥é™ä½ï¼Œä½†å‘¨æœŸæ€§æ³¢åŠ¨ä»ç„¶å­˜åœ¨ï¼Œè¯´æ˜ shuffle é—®é¢˜æœªè§£å†³

---

## ğŸ’¡ **6. å…³äºå­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰å’Œè¶…å‚è®¾ç½®**

* GPT-3 ä½¿ç”¨çš„å­¦ä¹ ç‡å…¶å®éå¸¸ä¿å®ˆï¼ˆå¯èƒ½ä½ä¼°äº†è®­ç»ƒèƒ½åŠ›ï¼‰
* å®éªŒå‘ç°å¯ä»¥å°†æœ€å¤§å­¦ä¹ ç‡æé«˜ 3 å€ï¼Œ**è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œæ•ˆæœä¹Ÿæ›´å¥½**
* **æç¤º**ï¼šè¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€batch sizeã€seq lengthï¼‰å¹¶ä¸æ˜¯å›ºå®šçš„ï¼Œå¯ä»¥å¤§èƒ†å°è¯•è°ƒæ•´

---

## ğŸ§  **7. æ¨¡å‹é‡‡æ ·è´¨é‡æ˜¾è‘—æå‡**

å±•ç¤ºäº†ä¸€äº›æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æ ·æœ¬ï¼Œç›¸è¾ƒäº 10B tokens çš„æ¨¡å‹ï¼Œ**è¯­è¨€æ›´è¿è´¯ã€è¯­ä¹‰æ›´æ˜ç¡®**ï¼Œè¯´æ˜æ¨¡å‹çš„ç†è§£å’Œè¡¨è¾¾èƒ½åŠ›éƒ½åœ¨ç¨³æ­¥ä¸Šå‡ã€‚

---

## ğŸ’¾ **8. æ¨¡å‹ä¿å­˜ï¼ˆcheckpointingï¼‰**

æ–°å¢åŠŸèƒ½ï¼šæ¯è®­ç»ƒ 5000 æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹çŠ¶æ€ï¼š

* ä¿å­˜å†…å®¹åŒ…æ‹¬ï¼š

  * `model.state_dict()`ï¼šæ¨¡å‹çš„æƒé‡
  * `optimizer.state_dict()`ï¼šä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚ Adam çš„åŠ¨é‡å˜é‡ `m` å’Œ `v`ï¼‰
* ä¿å­˜è¿™äº›å†…å®¹å¯ä»¥ç”¨äº **ç»§ç»­è®­ç»ƒæˆ–åç»­è¯„ä¼°**

---

## ğŸ“‹ **9. HellaSwag è¯„ä¼°æ–¹å¼ä»æœ‰æ”¹è¿›ç©ºé—´**

ç›®å‰çš„ HellaSwag è¯„ä¼°æ¯”è¾ƒâ€œç®€åŒ–â€ï¼š

* ä»…æ”¯æŒå•ä¸ªé€‰é¡¹çš„ token å®Œæˆè¯„åˆ†
* æ²¡æœ‰ä½¿ç”¨æ›´å®Œå–„çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¯”å¦‚ [EleutherAI çš„ evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness)ï¼Œè¿™ä¸ªå¯ä»¥æ”¯æŒå¤šä¸ªæ ‡å‡†ä»»åŠ¡
* å¦‚æœéœ€è¦æ›´å‡†ç¡®å¯¹æ¯” GPT-2ã€GPT-3 çš„æ³›åŒ–èƒ½åŠ›ï¼Œå»ºè®®åç»­ä½¿ç”¨æ›´é«˜çº§çš„è¯„ä¼°æ¡†æ¶

---

## ğŸ’¬ **10. æƒ³åšæˆåƒ ChatGPT ä¸€æ ·ï¼Ÿéœ€è¦ SFT**

ç›®å‰è®­ç»ƒçš„æ¨¡å‹æ˜¯ä¸€ä¸ªâ€œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹â€ï¼ˆåªå­¦ä¼šç»­å†™æ–‡æœ¬ï¼‰ï¼Œè¿˜ä¸æ˜¯â€œå¯¹è¯æ¨¡å‹â€ï¼š

* å¦‚æœæƒ³è®©å®ƒåƒ ChatGPT ä¸€æ ·èƒ½å¯¹è¯ï¼Œéœ€è¦è¿›è¡Œ SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰ï¼š

  * æ¢ä¸€ä¸ªå¯¹è¯æ•°æ®é›†ï¼ˆuser-assistant æ ¼å¼ï¼‰
  * ç”¨ç›¸åŒçš„è®­ç»ƒæ–¹å¼ç»§ç»­è®­ç»ƒ
  * å¹¶ç”Ÿæˆ assistant çš„å›ç­”
* å®è´¨ä¸Šå°±æ˜¯æ¢äº†æ•°æ®é›†ã€ç»§ç»­è®­ç»ƒ

---

## âœ… æ€»ç»“

| é¡¹ç›®     | å†…å®¹                                                                   |
| ------ | -------------------------------------------------------------------- |
| ğŸ¯ ç›®æ ‡  | ç”¨ 124M çš„ GPT æ¨¡å‹ï¼Œåœ¨å°æ•°æ®é‡ï¼ˆ10\~40B tokenï¼‰ä¸‹å¤ç°ç”šè‡³è¶…è¶Š GPT-2                    |
| ğŸ“ˆ ç»“æœ  | - HellaSwag è¶…è¿‡ GPT-2ï¼Œé€¼è¿‘ GPT-3ï¼ˆä»…ç”¨ 1/7 æ•°æ®é‡ï¼‰<br>- éªŒè¯æŸå¤±ä½äº GPT-2 baseline |
| âš ï¸ é—®é¢˜  | - æ•°æ®æœªæ‰“ä¹±ï¼Œè®­ç»ƒæ›²çº¿æœ‰å‘¨æœŸæ€§æ³¢åŠ¨<br>- æ¨¡å‹è¯„ä¼°æ–¹å¼ç®€åŒ–ï¼Œéœ€è¦è¿›ä¸€æ­¥å®Œå–„                             |
| ğŸ’¡ å»ºè®®  | - å¢å¼ºæ•°æ® shuffle<br>- å°è¯•æ›´é«˜å­¦ä¹ ç‡<br>- ç”¨æ›´é«˜çº§çš„è¯„ä¼°æ–¹å¼                           |
| ğŸ”œ ä¸‹ä¸€æ­¥ | - è¿›è¡Œ SFT è½¬åŒ–ä¸ºå¯¹è¯æ¨¡å‹<br>- æ›´å¤§æ¨¡å‹/æ•°æ®é›†è®­ç»ƒ                                     |

å¦‚æœä½ æƒ³æˆ‘ç»§ç»­æ•´ç†åé¢çš„å†…å®¹æˆ–å¸®ä½ æ•´ç†æˆä¸€ä»½å®Œæ•´çš„å­¦ä¹ ç¬”è®°ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘ï¼


# shoutout to llm.c, equivalent but faster code in raw C/CUDA

that of course what we've built up today was building towards nanog GPT which is this repository from earlier uh but also
there's actually another nanog GPT implementation and it's hiding in a more recent project that I've been working on
called llm Doc and lm. C is a pure Cuda
implementation of gpt2 or gpt3 training and it just directly uses uh Cuda and is
written as Cuda now the nanog gbt here acts as reference code in pytorch to the
C implementation so we're trying to exactly match up the two but we're hoping that the C Cuda is faster and of
course currently that seems to be the case um because it is a direct optimized implementation so train gpt2 Pi in LL
M.C is basically the nanog GPT and when you scroll through this file you'll find
a lot of things that very much look like um things that we've built up in this
lecture and then when you look at train gpt2 docu uh this is the C Cuda
implementation so there's a lot of MPI nickel GPU Cuda
cc++ and you have to be familiar with that but uh um when this is built up we
can actually run the two side by side and they're going to produce the exact same results but lm. C actually runs
faster so let's see that so on the left I have pytorch a nanog GPT looking thing
on the right I have the llmc call and here I'm going to launch the two both of these are going to be
running on a single GPU and here I'm putting the lm. C on GPU 1 and this one will grab uh gpu0 by default and
then we can see here that lm. c compiled and then allocate space and
it's stepping so basically uh meanwhile P torch is still
compiling because torch compile is a bit slower here than the lm. C nbcc Cuda
compile and so this program has already started running and uh we're still waiting here for torch compile now of
course uh this is a very specific implementation to gpt2 and 3 a pytorch is a very general neural network
framework so they're not exactly comparable but if you're only interested in training gpt2 and 3 lm. C is very
fast it takes less space it's faster to start and it's faster per
step and so P started to Stepping here and as you can see we're running at about 223,000 tokens per second here and
about 185,000 tokens per second here um
so quite a bit slower but I don't have full confidence that I exactly squeezed
out all the juice from the pytorch implementation but the important thing here is notice that if I Aline up the
steps you will see that the losses and Norms that are printed between these two are
identical so on the left we have the pie torch and on the right this C implementation and they're the same
except this one runs faster uh so that's kind of I wanted to show you also briefly lm. C and this is a parallel
implementation and it's also something that you may want to uh play with or look at and um it's kind of interesting

è¿™æ®µå†…å®¹è®²çš„æ˜¯ä½œè€…é™¤äº†ç”¨ PyTorch å†™äº† `nanoGPT` æ•™å­¦ç‰ˆï¼Œè¿˜**è‡ªå·±ç”¨çº¯ C/CUDA å®ç°äº†ä¸€ä¸ªæ›´å¿«çš„ GPT è®­ç»ƒå™¨é¡¹ç›®ï¼Œå«åš `llm.c`**ã€‚ä¸‹é¢æ˜¯é€æ®µçš„ä¸­æ–‡è§£é‡Šï¼š

---

## ğŸ› ï¸ `llm.c` æ˜¯ä»€ä¹ˆï¼Ÿ

* `llm.c` æ˜¯ä¸€ä¸ª **ç”¨ C å’Œ CUDA ç¼–å†™çš„ GPT-2/GPT-3 æ¨¡å‹è®­ç»ƒå®ç°**ã€‚
* å®ƒå’Œæˆ‘ä»¬ä¸€ç›´è®²è§£çš„ `nanoGPT` å¾ˆåƒï¼Œ**åŠŸèƒ½ä¸€æ ·ï¼ˆç­‰ä»·ï¼‰ï¼Œä½†é€Ÿåº¦æ›´å¿«**ï¼Œå› ä¸ºæ²¡æœ‰ä½¿ç”¨ PyTorchï¼Œè€Œæ˜¯ç›´æ¥ç”¨åº•å±‚ CUDA å†™çš„ã€‚
* PyTorch æ˜¯é€šç”¨ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œå®¹æ˜“ç”¨ï¼Œä½†æ•ˆç‡æ²¡é‚£ä¹ˆæè‡´ï¼›è€Œ `llm.c` æ˜¯æ‰‹å†™åº•å±‚çš„ä¸“ç”¨å®ç°ï¼Œæ›´å¿«ä½†æ›´å¤æ‚ã€‚

---

## ğŸ§© æ–‡ä»¶ç»“æ„å¯¹æ¯”

åœ¨ `llm.c` é¡¹ç›®ä¸­ï¼š

* `train_gpt2.py`ï¼šPyTorch ç‰ˆçš„å‚è€ƒå®ç°ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯ nanoGPT çš„ä»£ç ã€‚
* `train_gpt2.cu`ï¼ˆæˆ– `.cpp/.cu`ï¼‰ï¼šæ˜¯**CUDA ç‰ˆæœ¬çš„è®­ç»ƒä¸»ç¨‹åº**ï¼Œå®ƒä½¿ç”¨äº†å¤§é‡çš„ C/C++ å’Œ CUDA ä»£ç ï¼Œç”šè‡³åŒ…å«äº†åˆ†å¸ƒå¼è®¡ç®—ç›¸å…³çš„å†…å®¹ï¼ˆå¦‚ MPIã€NCCL ç­‰ï¼‰ã€‚

---

## ğŸ§ª è¿è¡Œå¯¹æ¯”å®éªŒ

ä½œè€…æ¼”ç¤ºäº†åŒæ—¶è¿è¡Œä¸¤ä¸ªç¨‹åºï¼š

| å·¦è¾¹                      | å³è¾¹                 |
| ----------------------- | ------------------ |
| `nanoGPT`ï¼ˆPyTorch ç‰ˆï¼‰    | `llm.c`ï¼ˆCUDA åŸç”Ÿç‰ˆï¼‰  |
| ä½¿ç”¨ GPU 0                | ä½¿ç”¨ GPU 1           |
| ä½¿ç”¨ `torch.compile()` ç¼–è¯‘ | ä½¿ç”¨ `nvcc` CUDA ç¼–è¯‘å™¨ |
| ç¼–è¯‘æ…¢ã€è¿è¡Œæ…¢                 | ç¼–è¯‘å¿«ã€è¿è¡Œå¿«            |

### â±ï¸ æ€§èƒ½å¯¹æ¯”ï¼š

| æŒ‡æ ‡           | PyTorchï¼ˆå·¦ï¼‰         | llm.cï¼ˆå³ï¼‰           |
| ------------ | ------------------ | ------------------ |
| æ¯ç§’è®­ç»ƒ token æ•° | \~185,000 tokens/s | \~223,000 tokens/s |
| ç¼–è¯‘è€—æ—¶         | æ…¢                  | å¿«                  |
| å ç”¨å†…å­˜         | é«˜                  | ä½                  |

---

## âœ… ç»“æœä¸€è‡´æ€§

* **è™½ç„¶è¿è¡Œé€Ÿåº¦ä¸ä¸€æ ·ï¼Œä¸¤è¾¹æ‰“å°å‡ºçš„ loss å€¼å’Œ normï¼ˆæ¢¯åº¦èŒƒæ•°ï¼‰æ˜¯å®Œå…¨ä¸€æ ·çš„**ã€‚
* è¯´æ˜è¿™ä¸¤ä¸ªå®ç°å®Œå…¨å¯¹é½ï¼šè®­ç»ƒè¿‡ç¨‹æ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯ä¸€ä¸ªç”¨ Python+PyTorch å®ç°ï¼Œä¸€ä¸ªç”¨ C/CUDA å®ç°ã€‚

---

## ğŸš€ æ€»ç»“ï¼šä¸ºä»€ä¹ˆå…³æ³¨ `llm.c`ï¼Ÿ

* å¦‚æœä½ åªå…³å¿ƒè®­ç»ƒ GPT-2 / GPT-3 ç±»å‹çš„æ¨¡å‹ï¼Œå¹¶ä¸”æƒ³è¿½æ±‚ **æé™é€Ÿåº¦**ï¼Œé‚£ä¹ˆ `llm.c` æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚
* å®ƒæœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼š

  * å¯åŠ¨å¿«
  * å†…å­˜å ç”¨å°‘
  * æ¯æ­¥è®­ç»ƒé€Ÿåº¦å¿«
* ä½†ç¼ºç‚¹æ˜¯ï¼š

  * **éœ€è¦æ‡‚ C/CUDA**
  * ä¸å¦‚ PyTorch çµæ´»ï¼Œè°ƒè¯•ä¹Ÿæ›´éš¾

---

## ğŸ“Œ æ€»ç»“ä¸€å¥è¯ï¼š

> `llm.c` æ˜¯ä¸€ä¸ªç”¨ C/CUDA å†™çš„ â€œæ›´å¿«ç‰ˆæœ¬çš„ nanoGPTâ€ï¼Œä¸“ä¸º GPT-2/3 è®­ç»ƒä¼˜åŒ–ï¼Œè¿è¡Œé€Ÿåº¦æ¯” PyTorch å¿«ï¼Œä½†éœ€è¦æ›´é«˜çš„ç¼–ç¨‹åŸºç¡€ã€‚

---

å¦‚æœä½ æœ‰å…´è¶£æˆ‘å¯ä»¥å¸®ä½ åˆ†æ `llm.c` çš„ä»£ç ç»“æ„å’Œå…³é”®å®ç°ç‚¹ï¼ˆæ¯”å¦‚æ¨¡å‹ forward/backward æ˜¯æ€ä¹ˆæ‰‹å†™çš„ï¼‰ï¼Œæˆ–è€…å‘Šè¯‰ä½ å¦‚ä½•ä¸Šæ‰‹è¿è¡Œå®ƒã€‚éœ€è¦çš„è¯éšæ—¶å‘Šè¯‰æˆ‘ï¼


# summary, phew, build-nanogpt github repo

okay so at this point I should probably start wrapping up the video because I think it's getting way longer than I anticipated uh but we did Cover a lot of
ground and we built everything from scratch so as a brief summary we were looking at the gpt2 and GPT 3
papers we were looking at how you set up these training runs uh and all the considerations involved we wrote
everything from scratch and then we saw that over the duration of either a 2-hour training run or an overnight run
we can actually match the 124 million parameter checkpoints of gbt2 and gpt3
uh to a very large extent um in principle the code that we wrote would be able to train even bigger
models if you have the patients or the Computing resources uh and so you could potentially think about training some of
the bigger checkpoints as well um there are a few remaining issues to address
what's happening with the loss here which I suspect has to do with the fine web edu data sampling uh why can't we
turn on Torch compile uh it currently breaks generation and H swag what's up with that in the data loader we should
probably be permuting our data when we reach boundaries so there's a few more issues like that and I expect to be
documenting some of those over time in the uh build n GPT repository here which
I'm going to be releasing with this video if you have any questions or like to talk about anything that we covered
please go to discussions tab uh so we can talk here uh or please go to issues or pull request pull requests um
depending on what you'd like to contribute or also have a look at the uh Zero to Hero Discord and uh I'm going to
be hanging out here on N GPT um otherwise for now I'm pretty happy
about where we got um and I hope you enjoyed the video and I will see you later


