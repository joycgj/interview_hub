# Residual blocks — Building blocks of ResNet


理解残差块其实很简单。在传统神经网络中，每一层的输出都会传递到下一层。而在包含残差块的网络中，每一层不仅将输出传递到下一层，还会直接传递给相隔 2–3 层的后续层。就是这样。
但要真正理解为什么最初需要它、它为什么如此重要，以及它与一些其他先进架构的相似之处，这才是我们接下来要关注的重点。关于为什么残差块如此优秀、以及它是如何成为让神经网络在广泛任务中取得最先进性能的关键思想之一，其实有不止一种解释。
在深入细节之前，这里先给出一张残差块的示意图，帮助你直观地了解它的结构。

![](pictures/single_residual_block.webp "")

我们知道，神经网络是通用函数逼近器，并且随着网络层数的增加，准确率也会提高。但层数增加到一定程度后，准确率提升会遇到瓶颈。既然神经网络是通用函数逼近器，那它本应能够学习任何简单或复杂的函数。然而，受困于梯度消失和维度灾难等问题，当网络足够深时，它甚至可能无法学习像恒等函数这样简单的函数。这显然是不可取的。

此外，如果我们不断增加网络层数，会发现准确率在某个点会趋于饱和，并最终下降。而且，这通常不是由过拟合引起的。因此，看起来浅层网络的学习效果反而优于更深层的网络，这有些反直觉。但这正是实践中观察到的现象，并且这种现象被广泛称为**退化问题（degradation problem）**。

在不深究退化问题以及深度神经网络无法学习恒等函数的具体根因之前，我们先来思考一些可能的解决方案。在退化问题中，我们知道浅层网络的性能优于仅多加了几层的深层网络。那么，为什么不直接跳过这些额外的层，至少让性能与浅层子网络持平呢？但是，要如何跳过这些层呢？


你可以通过跳跃连接（skip connections）**或**残差连接（residual connections）**来跳过部分层的训练。这正是我们在上图中看到的结构。事实上，如果仔细观察，你会发现仅依靠跳跃连接，我们就能直接学习一个恒等函数。这正是为什么跳跃连接也被称为**恒等快捷连接（identity shortcut connections）的原因——一个方案，解决所有问题！

但是，为什么叫它“残差（residual）”呢？“残差”又在哪里？现在，是时候让我们内心的数学家出场了。假设我们有一个神经网络块，它的输入是 $x$，而我们希望学习到真实的分布 $H(x)$。我们定义它们之间的差（即残差）为：

$$
R(x) = \text{Output} - \text{Input} = H(x) - x
$$

将其重新排列，可以得到：

$$
H(x) = R(x) + x
$$

我们的残差块整体上就是在尝试学习真实输出 $H(x)$。如果仔细看上面的图，你会发现，由于有一条从 $x$ 直接传递的恒等连接，这些层实际上是在学习残差 $R(x)$。
所以，总结来说：**传统网络**中的层在学习真实输出 $H(x)$，而**残差网络**中的层在学习残差 $R(x)$。这也正是名字“Residual Block（残差块）”的由来。

还有研究发现，相比只学习输入本身，学习**输出与输入的残差**更容易。作为额外的好处，我们的网络只需将残差设为零，就能直接学习到恒等函数。

如果你真正理解反向传播，并且知道随着网络层数增加，梯度消失问题会变得多么严重，那么你就会很清楚地看到：由于这些跳跃连接（skip connections）的存在，我们可以将更大的梯度传回到初始层，使这些初始层的学习速度与最后几层相当，从而具备训练更深网络的能力。

下图展示了如何排列残差块与恒等连接，以实现最佳的梯度流动效果。研究发现，带有批归一化（Batch Normalization）的预激活（pre-activation）通常能带来最佳效果（也就是说，图中最右侧的残差块结构往往表现最优）。

![](pictures/types_of_residual_block.webp "")

除了上面已经讨论过的内容之外，其实对残差块（Residual Block）和残差网络（ResNet）还有更多的解读。
在训练 ResNet 时，我们要么训练残差块中的层，要么通过跳跃连接（skip connections）跳过这些层的训练。这样一来，网络的不同部分会根据误差在网络中反向传播的方式，在不同的训练数据点上以不同的速度进行训练。我们可以把这种现象看作是在数据集上训练了多个不同模型的集合（ensemble），从而获得最佳的准确率。

从乐观的角度来看，跳过某些残差块中的层的训练也是有益的。通常，我们并不知道一个神经网络所需的最优层数（或残差块数量），因为它可能取决于数据集的复杂度。与其把网络层数当作一个需要调优的重要超参数，不如通过在网络中加入跳跃连接，让网络在训练过程中自动跳过那些无用、对整体准确率没有贡献的层。
某种意义上，跳跃连接让神经网络具备了**在训练中动态调整层数**的能力，从而达到最优。

下图展示了对残差块的多种不同解读方式。

![](pictures/different_interpretations_of_residual_block.webp "")

让我们简单回顾一下**跳跃连接（skip connections）**的历史。
在层与层之间引入跳跃连接的想法，最早出现在 **Highway Networks** 中。Highway 网络在跳跃连接中引入了**门控机制（gates）**，用来控制信息通过的多少，并且这些门是可训练的，可以选择性地打开。

这一思想同样可以在 **LSTM 网络** 中看到，它们通过门控机制来控制来自过去数据点的信息流动。这些门的作用与控制从先前看到的数据点中流出的“记忆”流非常相似。下图展示了这一相同的思想。

![](pictures/Similar_to_LSTM_Block.webp "")

残差块（Residual Block）本质上就是**没有在跳跃连接中使用门控机制**的 Highway 网络的特殊情况。
从根本上说，残差块允许记忆（或信息）从网络的初始层一直流向最后的层。
尽管它们的跳跃连接中没有门控机制，但在实际应用中，残差网络的表现依然与任何其他 Highway 网络一样出色。

在结束本文之前，下面这张图展示了**所有残差块组合在一起**是如何构成一个完整的 ResNet 的。

![](pictures/ResNet_architectures.webp "")

如果你对 ResNet 及其不同变体有兴趣了解更多内容，可以查看这篇文章。
