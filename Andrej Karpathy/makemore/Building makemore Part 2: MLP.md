We implement a multilayer perceptron (MLP) character-level language model. In this video we also introduce many basics of machine learning (e.g. model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.).

Links:
- makemore on github: https://github.com/karpathy/makemore
- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-t...
- collab notebook (new)!!!: https://colab.research.google.com/dri...
- Bengio et al. 2003 MLP language model paper (pdf): https://www.jmlr.org/papers/volume3/b...
- my website: https://karpathy.ai
- my twitter:   / karpathy  
- (new) Neural Networks: Zero to Hero series Discord channel:   / discord   , for people who'd like to chat more and go beyond youtube comments

Useful links:
- PyTorch internals ref http://blog.ezyang.com/2019/05/pytorc...

Exercises:
- E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2
- E02: I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the - predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the - initialization to get a starting loss that is much more similar to (1)?
- E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?

Chapters:
- 00:00:00 intro
- 00:01:48 Bengio et al. 2003 (MLP language model) paper walkthrough
- 00:09:03 (re-)building our training dataset
- 00:12:19 implementing the embedding lookup table
- 00:18:35 implementing the hidden layer + internals of torch.Tensor: storage, views
- 00:29:15 implementing the output layer
- 00:29:53 implementing the negative log likelihood loss
- 00:32:17 summary of the full network
- 00:32:49 introducing F.cross_entropy and why
- 00:37:56 implementing the training loop, overfitting one batch
- 00:41:25 training on the full dataset, minibatches
- 00:45:40 finding a good initial learning rate
- 00:53:20 splitting up the dataset into train/val/test splits and why
- 01:00:49 experiment: larger hidden layer
- 01:05:27 visualizing the character embeddings
- 01:07:16 experiment: larger embedding size
- 01:11:46 summary of our final code, conclusion
- 01:13:24 sampling from the model
- 01:14:55 google collab (new!!) notebook advertisement

ä»¥ä¸‹æ˜¯ä½ æä¾›å†…å®¹çš„ä¸­æ–‡ç¿»è¯‘ï¼ˆå«ç« èŠ‚å’Œç»ƒä¹ è¯´æ˜ï¼‰ï¼Œé€‚åˆå¯¹æœºå™¨å­¦ä¹ å’Œç¥ç»ç½‘ç»œæœ‰åˆæ­¥äº†è§£çš„å­¦ä¹ è€…ï¼š

---

æˆ‘ä»¬å®ç°äº†ä¸€ä¸ª**å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰å­—ç¬¦çº§è¯­è¨€æ¨¡å‹**ã€‚æœ¬è§†é¢‘è¿˜ä»‹ç»äº†è®¸å¤š**æœºå™¨å­¦ä¹ çš„åŸºç¡€çŸ¥è¯†**ï¼ŒåŒ…æ‹¬ï¼šæ¨¡å‹è®­ç»ƒã€å­¦ä¹ ç‡è°ƒæ•´ã€è¶…å‚æ•°è®¾ç½®ã€æ¨¡å‹è¯„ä¼°ã€è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†åˆ’åˆ†ï¼Œä»¥åŠæ¬ æ‹Ÿåˆ/è¿‡æ‹Ÿåˆç­‰å†…å®¹ã€‚

ğŸ”— **ç›¸å…³é“¾æ¥ï¼š**

* makemore é¡¹ç›® GitHubï¼š [https://github.com/karpathy/makemore](https://github.com/karpathy/makemore)
* è§†é¢‘ä¸­æˆ‘åˆ¶ä½œçš„ Jupyter Notebookï¼š [https://github.com/karpathy/nn-zero-t](https://github.com/karpathy/nn-zero-t)...
* \[æ–°] Google Colab Notebookï¼ˆå¯åœ¨çº¿è¿è¡Œï¼‰ï¼š[https://colab.research.google.com/dri](https://colab.research.google.com/dri)...
* Bengio ç­‰äºº 2003 å¹´ MLP è¯­è¨€æ¨¡å‹è®ºæ–‡ï¼ˆPDFï¼‰ï¼š[https://www.jmlr.org/papers/volume3/b](https://www.jmlr.org/papers/volume3/b)...
* æˆ‘çš„ä¸ªäººç½‘ç«™ï¼š[https://karpathy.ai](https://karpathy.ai)
* æˆ‘çš„æ¨ç‰¹è´¦å·ï¼š[@karpathy](https://twitter.com/karpathy)
* \[æ–°] â€œç¥ç»ç½‘ç»œï¼šä»é›¶å¼€å§‹â€ç³»åˆ— Discord é¢‘é“ï¼š/discordï¼Œæ¬¢è¿å¤§å®¶äº¤æµå­¦ä¹ ã€æ·±å…¥æ¢è®¨

ğŸ“š **æœ‰ç”¨çš„å‚è€ƒèµ„æ–™ï¼š**

* PyTorch åº•å±‚å®ç°å‚è€ƒåšå®¢ï¼š[http://blog.ezyang.com/2019/05/pytorc](http://blog.ezyang.com/2019/05/pytorc)...

---

### ğŸ§ª **ç»ƒä¹ é¢˜ï¼š**

* **E01ï¼š** è°ƒæ•´è®­ç»ƒçš„è¶…å‚æ•°ï¼Œå°è¯•å‡»è´¥æˆ‘ç›®å‰æœ€å¥½çš„éªŒè¯é›†æŸå¤± 2.2
* **E02ï¼š** æœ¬è§†é¢‘ä¸­åˆå§‹åŒ–åšå¾—ä¸å¤Ÿä»”ç»†ã€‚

  * (1) å¦‚æœåˆå§‹é¢„æµ‹æ¦‚ç‡æ˜¯å®Œå…¨å‡åŒ€åˆ†å¸ƒï¼Œç†è®ºæŸå¤±æ˜¯å¤šå°‘ï¼Ÿæˆ‘ä»¬å®é™…åˆå§‹åŒ–æ—¶çš„æŸå¤±æ˜¯å¤šå°‘ï¼Ÿ
  * (2) èƒ½å¦é€šè¿‡è°ƒæ•´åˆå§‹åŒ–æ–¹æ³•ï¼Œè®©åˆå§‹æŸå¤±æ›´æ¥è¿‘ç†è®ºå€¼ï¼Ÿ
* **E03ï¼š** é˜…è¯»ä¸Šé¢æåˆ°çš„ Bengio 2003 å¹´è®ºæ–‡ï¼ŒæŒ‘é€‰å…¶ä¸­çš„ä¸€ä¸ªæƒ³æ³•ï¼Œå°è¯•å®ç°å¹¶æµ‹è¯•å…¶æ•ˆæœã€‚çœ‹çœ‹æ˜¯å¦æœ‰æå‡ã€‚

---

### ğŸ“º **è§†é¢‘ç« èŠ‚ï¼š**

```
00:00:00  å¼•è¨€  
00:01:48  Bengio ç­‰äºº 2003 å¹´ MLP è¯­è¨€æ¨¡å‹è®ºæ–‡è§£æ  
00:09:03  ï¼ˆé‡æ–°ï¼‰æ„å»ºè®­ç»ƒæ•°æ®é›†  
00:12:19  å®ç°åµŒå…¥æŸ¥æ‰¾è¡¨ï¼ˆembedding lookup tableï¼‰  
00:18:35  å®ç°éšè—å±‚ + torch.Tensor çš„åº•å±‚æœºåˆ¶ï¼šstorageã€è§†å›¾ï¼ˆviewsï¼‰  
00:29:15  å®ç°è¾“å‡ºå±‚  
00:29:53  å®ç°è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼ˆNLL Lossï¼‰  
00:32:17  ç½‘ç»œç»“æ„æ€»ç»“  
00:32:49  å¼•å…¥ F.cross_entropy ä»¥åŠç†ç”±  
00:37:56  å®ç°è®­ç»ƒå¾ªç¯ï¼Œé¦–å…ˆåœ¨ä¸€ä¸ªå°æ‰¹æ•°æ®ä¸Šè¿‡æ‹Ÿåˆ  
00:41:25  åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œä½¿ç”¨å°æ‰¹é‡ï¼ˆminibatchesï¼‰  
00:45:40  å¯»æ‰¾åˆé€‚çš„åˆå§‹å­¦ä¹ ç‡  
00:53:20  å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼Œå¹¶è§£é‡ŠåŸå›   
01:00:49  å®éªŒï¼šå¢å¤§éšè—å±‚è§„æ¨¡  
01:05:27  å¯è§†åŒ–å­—ç¬¦åµŒå…¥ï¼ˆembeddingï¼‰  
01:07:16  å®éªŒï¼šå¢å¤§åµŒå…¥ç»´åº¦  
01:11:46  æœ€ç»ˆä»£ç æ€»ç»“ä¸ç»“è¯­  
01:13:24  ä»æ¨¡å‹ä¸­é‡‡æ ·æ–‡æœ¬  
01:14:55  Google Colab notebook å¹¿å‘Š
```

å¦‚æœä½ éœ€è¦æˆ‘æ•´ç†è¿™æ®µè§†é¢‘å†…å®¹çš„å­¦ä¹ è·¯çº¿ã€ä»£ç ç¬”è®°ã€æˆ–è€…å°†å†…å®¹ç®€åŒ–ä¸º HSK3 çº§ä¸­æ–‡ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘ï¼
