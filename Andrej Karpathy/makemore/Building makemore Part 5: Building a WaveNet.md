We take the 2-layer MLP from previous video and make it deeper with a tree-like structure, arriving at a convolutional neural network architecture similar to the WaveNet (2016) from DeepMind. In the WaveNet paper, the same hierarchical architecture is implemented more efficiently using causal dilated convolutions (not yet covered). Along the way we get a better sense of torch.nn and what it is and how it works under the hood, and what a typical deep learning development process looks like (a lot of reading of documentation, keeping track of multidimensional tensor shapes, moving between jupyter notebooks and repository code, ...).

Links:
- makemore on github: https://github.com/karpathy/makemore
- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-t...
- collab notebook: https://colab.research.google.com/dri...
- my website: https://karpathy.ai
- my twitter:   / karpathy  
- our Discord channel:   / discord  

Supplementary links:
- WaveNet 2016 from DeepMind https://arxiv.org/abs/1609.03499
- Bengio et al. 2003 MLP LM https://www.jmlr.org/papers/volume3/b... 

```
Chapters:
intro
00:00:00 intro
00:01:40 starter code walkthrough
00:06:56 letâ€™s fix the learning rate plot
00:09:16 pytorchifying our code: layers, containers, torch.nn, fun bugs
implementing wavenet
00:17:11 overview: WaveNet
00:19:33 dataset bump the context size to 8
00:19:55 re-running baseline code on block_size 8
00:21:36 implementing WaveNet
00:37:41 training the WaveNet: first pass
00:38:50 fixing batchnorm1d bug
00:45:21 re-training WaveNet with bug fix
00:46:07 scaling up our WaveNet
conclusions
00:46:58 experimental harness
00:47:44 WaveNet but with â€œdilated causal convolutionsâ€
00:51:34 torch.nn
00:52:28 the development process of building deep neural nets
00:54:17 going forward
00:55:26 improve on my loss! how far can we improve a WaveNet on this data?
```

å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯è¿™æ®µè¯çš„ä¸­æ–‡ç¿»è¯‘ï¼š

---

æˆ‘ä»¬å°†ä¹‹å‰è§†é¢‘ä¸­çš„ 2 å±‚ MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰åšäº†â€œåŠ æ·±â€ï¼Œæ„å»ºæˆæ ‘çŠ¶ç»“æ„ï¼Œæœ€ç»ˆå½¢æˆä¸€ç§ç±»ä¼¼äº DeepMind 2016 å¹´ WaveNet è®ºæ–‡ä¸­çš„å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„ã€‚åœ¨ WaveNet è®ºæ–‡ä¸­ï¼ŒåŒæ ·çš„å±‚æ¬¡åŒ–ç»“æ„é€šè¿‡\*\*å› æœæ‰©å¼ å·ç§¯ï¼ˆcausal dilated convolutionsï¼‰\*\*æ¥æ›´é«˜æ•ˆåœ°å®ç°ï¼ˆæœ¬è§†é¢‘æš‚æœªæ¶‰åŠè¯¥éƒ¨åˆ†ï¼‰ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œä½ å¯ä»¥æ›´å¥½åœ°ç†è§£ `torch.nn` æ˜¯ä»€ä¹ˆï¼Œå®ƒèƒŒåçš„å·¥ä½œåŸç†ï¼Œä»¥åŠä¸€ä¸ªå…¸å‹çš„æ·±åº¦å­¦ä¹ å¼€å‘è¿‡ç¨‹é€šå¸¸æ˜¯ä»€ä¹ˆæ ·å­ï¼ˆå¤§é‡é˜…è¯»æ–‡æ¡£ã€å…³æ³¨å¤šç»´å¼ é‡çš„ shape å˜åŒ–ã€é¢‘ç¹åˆ‡æ¢ Jupyter notebook å’Œä»£ç ä»“åº“ç­‰ï¼‰ã€‚

**é“¾æ¥ï¼š**

* makemore é¡¹ç›® GitHub: [https://github.com/karpathy/makemore](https://github.com/karpathy/makemore)
* æœ¬è§†é¢‘å¯¹åº”çš„ Jupyter notebook: [https://github.com/karpathy/nn-zero-t](https://github.com/karpathy/nn-zero-t)...
* Colab notebook: [https://colab.research.google.com/dri](https://colab.research.google.com/dri)...
* æˆ‘çš„ä¸ªäººç½‘ç«™: [https://karpathy.ai](https://karpathy.ai)
* æˆ‘çš„æ¨ç‰¹:   / karpathy
* æˆ‘ä»¬çš„ Discord é¢‘é“:   / discord

**è¡¥å……é˜…è¯»ï¼š**

* DeepMind 2016 å¹´ WaveNet è®ºæ–‡: [https://arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499)
* Bengio ç­‰äºº 2003 å¹´çš„ MLP è¯­è¨€æ¨¡å‹è®ºæ–‡: [https://www.jmlr.org/papers/volume3/b](https://www.jmlr.org/papers/volume3/b)...

```
**è§†é¢‘ç« èŠ‚ï¼š**
**intro**
00:00:00 ä»‹ç»
00:01:40 èµ·å§‹ä»£ç è®²è§£
00:06:56 ä¿®æ­£å­¦ä¹ ç‡æ›²çº¿çš„ç»˜å›¾
00:09:16 ä½¿ç”¨ PyTorch æ”¹é€ ä»£ç ï¼šlayers, containers, torch.nnï¼Œé‡åˆ°çš„æœ‰è¶£å° bug

**å®ç° WaveNet**
00:17:11 WaveNet æ€»è§ˆ
00:19:33 æ•°æ®é›†ï¼šå°† context size å¢å¤§åˆ° 8
00:19:55 ä½¿ç”¨ block\_size=8 é‡æ–°è¿è¡Œ baseline ä»£ç 
00:21:36 å®ç° WaveNet ç½‘ç»œç»“æ„
00:37:41 ç¬¬ä¸€æ¬¡è®­ç»ƒ WaveNet
00:38:50 ä¿®å¤ batchnorm1d çš„ bug
00:45:21 ä¿®å¤åé‡æ–°è®­ç»ƒ WaveNet
00:46:07 æ‰©å±•æˆ‘ä»¬çš„ WaveNet è§„æ¨¡

**æ€»ç»“**
00:46:58 å®éªŒæµç¨‹å·¥å…·
00:47:44 ä½¿ç”¨â€œæ‰©å¼ å› æœå·ç§¯â€çš„ WaveNet
00:51:34 è¿›ä¸€æ­¥ç†è§£ torch.nn
00:52:28 æ„å»ºæ·±åº¦ç¥ç»ç½‘ç»œçš„å¼€å‘è¿‡ç¨‹
00:54:17 å±•æœ›æœªæ¥
00:55:26 æ”¹è¿› lossï¼çœ‹çœ‹æˆ‘ä»¬èƒ½æŠŠ WaveNet åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šä¼˜åŒ–åˆ°ä»€ä¹ˆç¨‹åº¦ï¼Ÿ
```

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ æ€»ç»“æˆæ›´ç®€å•çš„ä¸­æ–‡ï¼Œæˆ–è€…é‡ç‚¹è§£é‡Š "å› æœæ‰©å¼ å·ç§¯"ã€"WaveNet" è¿™äº›æ¦‚å¿µï½è¦ä¸è¦ç»§ç»­ï¼Ÿ ğŸŒŸ


# intro

hi everyone today we are continuing our implementation of make more our favorite character level language model
now you'll notice that the background behind me is different that's because I am in Kyoto and it is awesome so I'm in
a hotel room here now over the last few lectures we've built up to this architecture that is a
multi-layer perceptron character level language model so we see that it receives three previous characters and
tries to predict the fourth character in a sequence using a very simple multi perceptron using one hidden layer of
neurons with 10ational neuralities so we'd like to do now in this lecture is I'd like to complexify this
architecture in particular we would like to take more characters in a sequence as an input not just three and in addition
to that we don't just want to feed them all into a single hidden layer because that squashes too much information too
quickly instead we would like to make a deeper model that progressively fuses this information to make its guess about
the next character in a sequence and so we'll see that as we make this architecture more complex we're actually
going to arrive at something that looks very much like a wavenet the witness is this paper published by
the point in 2016 and it is also a language model basically but it tries to
predict audio sequences instead of character level sequences or Word level sequences but fundamentally the modeling
setup is identical it is an auto aggressive model and it tries to predict next character in a sequence and the
architecture actually takes this interesting hierarchical sort of approach to predicting the next
character in a sequence uh with the street-like structure and this is the architecture and we're going to
implement it in the course of this video so let's get started so the starter code for part five is very similar to where

å½“ç„¶ï¼Œè¿™ä¸€æ®µæˆ‘ç»™ä½ ç¿»è¯‘ä¸€ä¸‹ï¼š

---

å¤§å®¶å¥½ï¼Œä»Šå¤©æˆ‘ä»¬ç»§ç»­å®ç°æˆ‘ä»¬çš„ `makemore`â€”â€”æˆ‘ä»¬æœ€å–œæ¬¢çš„**å­—ç¬¦çº§è¯­è¨€æ¨¡å‹**ã€‚
ä½ å¯èƒ½æ³¨æ„åˆ°æˆ‘èº«åçš„èƒŒæ™¯æ¢äº†ï¼Œå› ä¸ºæˆ‘ç°åœ¨åœ¨äº¬éƒ½ï¼Œéå¸¸æ£’ï¼Œæˆ‘ç°åœ¨æ˜¯åœ¨é…’åº—æˆ¿é—´é‡Œå½•çš„ã€‚

åœ¨å‰å‡ èŠ‚è¯¾é‡Œï¼Œæˆ‘ä»¬å·²ç»æ­å»ºäº†è¿™æ ·ä¸€ä¸ªæ¶æ„ï¼š
ä¸€ä¸ª**å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰**çš„å­—ç¬¦çº§è¯­è¨€æ¨¡å‹ã€‚å®ƒçš„è¾“å…¥æ˜¯**å‰é¢ä¸‰ä¸ªå­—ç¬¦**ï¼Œç„¶åå°è¯•é¢„æµ‹**ç¬¬å››ä¸ªå­—ç¬¦**ï¼Œæ¨¡å‹çš„æ ¸å¿ƒæ˜¯ä¸€å±‚éšè—å±‚ï¼ˆhidden layerï¼‰ï¼Œé‡Œé¢æœ‰ 10 ä¸ªç¥ç»å…ƒï¼ˆneuronsï¼‰ã€‚

ä»Šå¤©è¿™èŠ‚è¯¾æˆ‘å¸Œæœ›èƒ½**å¤æ‚åŒ–**è¿™ä¸ªæ¶æ„ï¼Œå…·ä½“åŒ…æ‹¬ä¸¤ç‚¹ï¼š

1. **è¾“å…¥æ›´å¤šçš„å­—ç¬¦**ï¼Œä¸ä»…ä»…æ˜¯ä¸‰ä¸ªå­—ç¬¦ä½œä¸ºè¾“å…¥ï¼›
2. ä¸å¸Œæœ›æŠŠè¿™äº›å­—ç¬¦ä¸€æ¬¡æ€§å…¨é€è¿›ä¸€ä¸ªéšè—å±‚â€”â€”å› ä¸ºé‚£æ ·ä¼šæŠŠä¿¡æ¯å‹ç¼©å¾—å¤ªå¿«ã€å¤ªç‹ ã€‚æˆ‘ä»¬å¸Œæœ›æ„å»ºä¸€ä¸ª**æ›´æ·±çš„æ¨¡å‹**ï¼Œé€æ­¥èåˆä¿¡æ¯ï¼Œé€æ­¥åšå‡ºå¯¹ä¸‹ä¸€ä¸ªå­—ç¬¦çš„é¢„æµ‹ã€‚

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šå‘ç°ï¼Œéšç€æ¶æ„çš„å¤æ‚åŒ–ï¼Œæœ€ç»ˆæˆ‘ä»¬åšå‡ºæ¥çš„ç½‘ç»œä¼šéå¸¸åƒ 2016 å¹´ DeepMind å‘è¡¨çš„**WaveNet è®ºæ–‡**ä¸­çš„ç»“æ„ã€‚
WaveNet æœ¬è´¨ä¸Šä¹Ÿæ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œä¸è¿‡å®ƒæ˜¯ç”¨æ¥é¢„æµ‹**éŸ³é¢‘åºåˆ—**ï¼Œè€Œä¸æ˜¯å­—ç¬¦åºåˆ—æˆ–è€…å•è¯åºåˆ—ã€‚
ä½†å…¶å®è¿™ä¸¤è€…çš„å»ºæ¨¡æ–¹å¼æ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯**è‡ªå›å½’æ¨¡å‹ï¼ˆautoregressive modelï¼‰**ï¼Œéƒ½æ˜¯åœ¨é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªâ€œå…ƒç´ â€ï¼ˆå­—ç¬¦æˆ–è€…éŸ³é¢‘é‡‡æ ·ç‚¹ï¼‰ã€‚

WaveNet çš„æ¶æ„æœ‰ä¸€ä¸ªå¾ˆæœ‰æ„æ€çš„**åˆ†å±‚ï¼ˆhierarchicalï¼‰ç»“æ„**ï¼Œæ˜¯æ ‘çŠ¶ï¼ˆtree-likeï¼‰çš„ã€‚
ä»Šå¤©æˆ‘ä»¬å°±è¦åœ¨è¿™ä¸ªè§†é¢‘é‡Œï¼Œæ‰‹æŠŠæ‰‹å»å®ç°è¿™æ ·ä¸€ä¸ªç±»ä¼¼ WaveNet çš„æ¨¡å‹æ¶æ„ã€‚
å¥½ï¼Œé‚£æˆ‘ä»¬å°±å¼€å§‹å§ï½ è¿™èŠ‚è¯¾ç”¨çš„ starter codeï¼Œå’Œæˆ‘ä»¬ä¹‹å‰è¯¾ç¨‹ç»“æŸæ—¶çš„ä»£ç éå¸¸ç›¸ä¼¼ã€‚

---

å¦‚æœéœ€è¦çš„è¯ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ æ€»ç»“å‡º**è¿™ä¸ªæ¨¡å‹çš„æ€è·¯å’Œæ¼”å˜è¿‡ç¨‹**ï¼Œè¿™æ ·ä½ å­¦ä¹ çš„æ—¶å€™ä¼šæ›´æ¸…æ¥šï½ è¦ä¸è¦ç»§ç»­ï¼ŸğŸŒŸ


# starter code walkthrough

we ended up in in part three recall that part four was the manual black replication exercise that is kind of an
aside so we are coming back to part three copy pasting chunks out of it and that is our starter code for part five
I've changed very few things otherwise so a lot of this should look familiar to if you've gone through part three so in
particular very briefly we are doing Imports we are reading our our data set of words and we are processing their set
of words into individual examples and none of this data generation code has changed and basically we have lots and
lots of examples in particular we have 182 000 examples of three characters try
to predict the fourth one and we've broken up every one of these words into little problems of given three
characters predict the fourth one so this is our data set and this is what we're trying to get the neural lot to do
now in part three we started to develop our code around these layer modules
um that are for example like class linear and we're doing this because we want to think of these modules as
building blocks and like a Lego building block bricks that we can sort of like stack up into neural networks and we can
feed data between these layers and stack them up into a sort of graphs now we also developed these layers to
have apis and signatures very similar to those that are found in pytorch so we
have torch.nn and it's got all these layer building blocks that you would use in practice and we were developing all
of these to mimic the apis of these so for example we have linear so there will also be a torch.nn.linear and its
signature will be very similar to our signature and the functionality will be also quite identical as far as I'm aware
so we have the linear layer with the Bass from 1D layer and the 10h layer that we developed previously
and linear just as a matrix multiply in the forward pass of this module batch
number of course is this crazy layer that we developed in the previous lecture and what's crazy about it is
well there's many things number one it has these running mean and variances that are trained outside of back
propagation they are trained using exponential moving average inside this
layer when we call the forward pass in addition to that there's this training plug because the
behavior of bathroom is different during train time and evaluation time and so suddenly we have to be very careful that bash form is in its correct state that
it's in the evaluation state or training state so that's something to now keep track of something that sometimes introduces bugs
uh because you forget to put it into the right mode and finally we saw that Bachelor couples the statistics or the
the activations across the examples in the batch so normally we thought of the bat as just an efficiency thing but now
we are coupling the computation across batch elements and it's done for the
purposes of controlling the automation statistics as we saw in the previous video so it's a very weird layer at least a
lot of bugs partly for example because you have to modulate the training in eval phase and
so on um in addition for example you have to wait for uh the mean and the variance to
settle and to actually reach a steady state and so um you have to make sure that you basically there's state in this
layer and state is harmful uh usually now I brought out the generator object
previously we had a generator equals g and so on inside these layers I've discarded that in favor of just
initializing the torch RNG outside here use it just once globally just for
Simplicity and then here we are starting to build out some of the neural network elements this should look very familiar we are we
have our embedding table C and then we have a list of players and uh it's a linear feeds to Bachelor feeds to 10h
and then a linear output layer and its weights are scaled down so we are not confidently wrong at the initialization
we see that this is about 12 000 parameters we're telling pytorch that the parameters require gradients
the optimization is as far as I'm aware identical and should look very very familiar
nothing changed here uh loss function looks very crazy we
should probably fix this and that's because 32 batch elements are too few and so you can get very lucky lucky or
unlucky in any one of these batches and it creates a very thick loss function um so we're going to fix that soon
now once we want to evaluate the trained neural network we need to remember because of the bathroom layers to set
all the layers to be training equals false so this only matters for the bathroom layer so far
and then we evaluate we see that currently we have validation
loss of 2.10 which is fairly good but there's still ways to go but even at
2.10 we see that when we sample from the model we actually get relatively name-like results that do not exist in a
training set so for example Yvonne kilo Pros
Alaia Etc so certainly not reasonable not unreasonable I would say
but not amazing and we can still push this validation loss even lower and get much better samples that are even more
name-like so let's improve this model okay first let's fix this graph because

å½“ç„¶ï¼Œè¿™ä¸€æ®µæˆ‘ç»™ä½ è¯¦ç»†ç¿»è¯‘ä¸€ä¸‹ï¼š

---

**# starter code walkthroughï¼ˆèµ·å§‹ä»£ç è®²è§£ï¼‰**

æˆ‘ä»¬ç°åœ¨å›åˆ°äº†**ç¬¬ä¸‰éƒ¨åˆ†**ç»“æŸæ—¶çš„ä»£ç ï¼ˆæ³¨æ„ï¼š**ç¬¬å››éƒ¨åˆ†**æ˜¯ä¸€æ¬¡â€œæ‰‹åŠ¨åå‘ä¼ æ’­â€ç»ƒä¹ ï¼Œç®—æ˜¯ä¸ªç•ªå¤–ç¯‡ï¼‰ï¼Œæ‰€ä»¥è¿™èŠ‚è¯¾æ˜¯ä»ç¬¬ä¸‰éƒ¨åˆ†çš„ä»£ç ä¸­**å¤åˆ¶éƒ¨åˆ†ä»£ç **ä½œä¸ºæ–°çš„èµ·ç‚¹ï¼ˆstarter codeï¼‰å¼€å§‹çš„ï¼Œæ•´ä½“å˜åŒ–ä¸å¤§ã€‚
å¦‚æœä½ ä¹‹å‰çœ‹è¿‡ç¬¬ä¸‰éƒ¨åˆ†çš„å†…å®¹ï¼Œè¿™äº›ä»£ç ä½ åº”è¯¥ä¼šå¾ˆç†Ÿæ‚‰ã€‚

å¤§è‡´æµç¨‹æ˜¯è¿™æ ·çš„ï¼š

* å…ˆåš **import**ï¼Œç„¶åè¯»å–æ•°æ®é›†ï¼ˆwordsï¼‰ï¼Œå¹¶æŠŠå®ƒä»¬æ‹†è§£æˆå•ä¸ªçš„è®­ç»ƒæ ·æœ¬ï¼ˆexamplesï¼‰ï¼Œè¿™éƒ¨åˆ†çš„æ•°æ®å¤„ç†é€»è¾‘æ²¡æœ‰å˜åŒ–ã€‚
* æ•°æ®é‡ä¹ŸæŒºå¤§ï¼Œæ€»å…±ç”Ÿæˆäº† **182,000** ä¸ªå°æ ·æœ¬ï¼Œæ¯ä¸ªå°æ ·æœ¬æ˜¯â€œç»™å®š 3 ä¸ªå­—ç¬¦ï¼Œé¢„æµ‹ç¬¬ 4 ä¸ªå­—ç¬¦â€ã€‚
* è¿™ä¸ªæ•°æ®é›†å°±æ˜¯æˆ‘ä»¬å¸Œæœ›ç”¨ç¥ç»ç½‘ç»œå»å­¦ä¹ çš„ç›®æ ‡ï¼š**é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦**ã€‚

åœ¨ **ç¬¬ä¸‰éƒ¨åˆ†**ï¼Œæˆ‘ä»¬å¼•å…¥äº† â€œlayer moduleâ€ è¿™ç§è®¾è®¡æ–¹å¼ï¼š

* ä¾‹å¦‚å†™äº†ä¸€ä¸ª `class Linear`ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³æŠŠè¿™äº›æ¨¡å—ï¼ˆmodulesï¼‰åƒ\*\*ä¹é«˜ç§¯æœ¨ï¼ˆlego blocksï¼‰\*\*ä¸€æ ·æ‹¼è£…èµ·æ¥ï¼Œæ­å»ºæˆå®Œæ•´çš„ç¥ç»ç½‘ç»œï¼Œæ•°æ®å¯ä»¥åœ¨ä¸åŒå±‚ä¹‹é—´ä¼ é€’ï¼Œå½¢æˆä¸€ä¸ªè®¡ç®—å›¾ï¼ˆgraphï¼‰ã€‚
* åŒæ—¶ï¼Œæˆ‘ä»¬å†™è¿™äº›æ¨¡å—æ—¶ï¼Œå°½é‡è®©å®ƒä»¬çš„ API å’Œ **PyTorch çš„ torch.nn** æ¥å£ä¿æŒä¸€è‡´ï¼Œè¿™æ ·ä»¥åå¯ä»¥æ— ç¼åˆ‡æ¢ã€å­¦ä¹  PyTorch ä¹Ÿæ›´é¡ºæ‰‹ã€‚
* ä¸¾ä¾‹æ¥è¯´ï¼Œæˆ‘ä»¬å†™çš„ `Linear` å±‚ï¼Œå…¶å®åŠŸèƒ½å’Œ `torch.nn.Linear` å·®ä¸å¤šï¼Œæ¥å£ç­¾åï¼ˆsignatureï¼‰ä¹Ÿå°½é‡å¯¹é½ã€‚

å›é¡¾ä¹‹å‰å¼€å‘çš„å‡ ç§ layerï¼š

* **Linear** å±‚ï¼šå°±æ˜¯ä¸€å±‚çŸ©é˜µä¹˜æ³•ï¼ˆforward è¿‡ç¨‹å°±æ˜¯ matmulï¼‰
* **BatchNorm1d** å±‚ï¼šè¿™æ˜¯ä¹‹å‰å¼€å‘è¿‡ç¨‹ä¸­ä¸€ä¸ªâ€œç¥å¥‡â€çš„å±‚ï¼Œå› ä¸ºï¼š

  1. å®ƒå†…éƒ¨ç»´æŠ¤äº†**å‡å€¼ï¼ˆmeanï¼‰å’Œæ–¹å·®ï¼ˆvarianceï¼‰**ï¼Œå¹¶ä¸”è¿™äº›æ˜¯é€šè¿‡\*\*æŒ‡æ•°æ»‘åŠ¨å¹³å‡ï¼ˆEMAï¼‰\*\*æ¥æ›´æ–°çš„ï¼Œä¸æ˜¯ç›´æ¥ç”¨åå‘ä¼ æ’­å­¦åˆ°çš„ï¼›
  2. è®­ç»ƒé˜¶æ®µï¼ˆtrainingï¼‰å’Œè¯„ä¼°é˜¶æ®µï¼ˆevaluationï¼‰çš„è¡Œä¸ºä¸åŒï¼Œæ‰€ä»¥éœ€è¦æ‰‹åŠ¨åˆ‡æ¢ `training=True/False`ï¼Œå¦åˆ™å®¹æ˜“å‡º bugï¼›
  3. è¿™ä¸ªå±‚çš„è®¡ç®—æ˜¯**è·¨ batch çš„**ï¼Œä¹Ÿå°±æ˜¯ä¸åŒ batch å…ƒç´ ä¹‹é—´ä¼šäº’ç›¸å½±å“ï¼ˆå› ä¸ºéœ€è¦è®¡ç®—æ•´ä½“çš„å‡å€¼æ–¹å·®ï¼‰ï¼Œä¸åƒæ™®é€šçš„ Linear å±‚æ˜¯ç‹¬ç«‹è®¡ç®—çš„ã€‚

BatchNorm1d æ˜¯ä¸€ä¸ªæœ‰â€œçŠ¶æ€ï¼ˆstateï¼‰â€çš„å±‚ï¼Œè¿™ç§ state å¾ˆå®¹æ˜“å¼•å…¥ bugï¼Œæ¯”å¦‚éœ€è¦ç­‰å¾…å‡å€¼æ–¹å·®æ”¶æ•›ï¼ˆsettleï¼‰ï¼Œè€Œä¸”è®­ç»ƒå’Œæ¨ç†é˜¶æ®µåˆ‡æ¢å®¹æ˜“å¿˜è®°ã€‚

ä»¥å‰æˆ‘ä»¬æŠŠ `torch.Generator` æ”¾åˆ°æ¯å±‚é‡Œï¼Œç°åœ¨ç®€åŒ–äº†ï¼Œæ”¹æˆåªåœ¨å¤–é¢ç»Ÿä¸€åˆå§‹åŒ– RNGï¼ˆéšæœºæ•°ç”Ÿæˆå™¨ï¼‰ï¼Œæ•´ä½“ä»£ç æ›´å¹²å‡€ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬æ­å»ºç¥ç»ç½‘ç»œçš„â€œå‰åŠéƒ¨åˆ†â€ï¼š

* æœ‰ä¸€ä¸ª **embedding table C**ï¼Œ
* ç„¶åä¸€å±‚ `Linear` -> `BatchNorm1d` -> `Tanh` -> è¾“å‡ºå±‚ `Linear`ï¼Œ
* åˆå§‹åŒ–æ—¶ç‰¹æ„æŠŠè¾“å‡ºå±‚çš„æƒé‡ç¼©å°ï¼Œé˜²æ­¢ä¸€å¼€å§‹å°±é¢„æµ‹å¾—â€œä¿¡å¿ƒè¿‡é«˜ä½†é”™è¯¯â€ï¼ˆconfidently wrongï¼‰ã€‚
* ç›®å‰æ¨¡å‹å¤§æ¦‚æœ‰ **12,000 ä¸ªå‚æ•°**ï¼Œæˆ‘ä»¬å‘Šè¯‰ PyTorch è¿™äº›å‚æ•°éœ€è¦ `requires_grad=True`ï¼Œè¿™æ ·å¯ä»¥å‚ä¸åå‘ä¼ æ’­ã€‚

ä¼˜åŒ–å™¨éƒ¨åˆ†ï¼ˆoptimizerï¼‰å’Œä»¥å‰ä¸€æ ·ï¼Œæ²¡å˜ã€‚

**æŸå¤±æ›²çº¿ï¼ˆloss functionï¼‰** ç°åœ¨çœ‹èµ·æ¥å¾ˆâ€œè·³â€å¾ˆâ€œä¹±â€ï¼Œå› ä¸º batch size å¤ªå°ï¼ˆåªæœ‰ 32 ä¸ªæ ·æœ¬ï¼‰ï¼Œå®¹æ˜“å‡ºç° batch çš„ç»“æœå¤ªå¥½/å¤ªåï¼Œå¯¼è‡´ loss æ›²çº¿å¾ˆåšé‡ï¼Œä¸å¹³æ»‘ï¼Œè¿™ä¸ªé©¬ä¸Šä¼šè°ƒæ•´ã€‚

è®­ç»ƒå®Œæ¨¡å‹åï¼Œè¦åšè¯„ä¼°ï¼ˆevaluateï¼‰æ—¶è¦æ³¨æ„ï¼š

* å› ä¸ºæœ‰ BatchNorm å±‚ï¼Œè¯„ä¼°å‰ä¸€å®šè¦æŠŠæ‰€æœ‰å±‚è®¾æˆ `training=False`ï¼Œå¦åˆ™ç»“æœä¼šä¸ç¨³å®šã€‚

å½“å‰æ¨¡å‹çš„ **éªŒè¯é›† loss å¤§æ¦‚æ˜¯ 2.10**ï¼Œè¿˜å¯ä»¥ç»§ç»­ä¼˜åŒ–ã€‚
å°½ç®¡ loss è¿˜ä¸æ˜¯å¾ˆä½ï¼Œä½†å·²ç»èƒ½ç”Ÿæˆä¸€äº›â€œåƒåå­—çš„â€å­—ç¬¦åºåˆ—ï¼ˆè™½ç„¶è®­ç»ƒé›†ä¸­ä¸å­˜åœ¨ï¼‰ï¼Œæ¯”å¦‚ï¼š
`Yvonne kilo Pros`
`Alaia` ç­‰ç­‰ã€‚

ç›®å‰æ¥è¯´ï¼Œæ•ˆæœè¿˜ä¸é”™ï¼Œä½†ä¹Ÿä¸æ˜¯ç‰¹åˆ«æƒŠè‰³ï¼Œæˆ‘ä»¬å¸Œæœ›ç»§ç»­æŠŠ loss æ‹‰ä½ï¼Œæå‡ç”Ÿæˆæ•ˆæœã€‚
ä¸‹ä¸€æ­¥ï¼Œæˆ‘ä»¬å…ˆæ¥è°ƒæ•´ä¸€ä¸‹ loss æ›²çº¿çš„ç»˜å›¾é€»è¾‘â€”â€”ç°åœ¨å®ƒå¤ªä¹±äº†ã€‚

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ æ•´ç†ä¸€ä¸‹è¿™ä¸ªâ€œstarter code è®²è§£â€çš„**æ ¸å¿ƒé‡ç‚¹**ï¼Œæ–¹ä¾¿ä½ å¤ä¹ ï½è¦ä¸è¦ï¼Ÿ ğŸš€


# letâ€™s fix the learning rate plot

it is daggers in my eyes and I just can't take it anymore um so last I if you recall is a python
list of floats so for example the first 10 elements
now what we'd like to do basically is we need to average up um some of these values to get a more
sort of Representative uh value along the way so one way to do this is the following
in part torch if I create for example a tensor of the first 10 numbers
then this is currently a one-dimensional array but recall that I can view this array as two-dimensional so for example
I can use it as a two by five array and this is a 2d tensor now two by five and
you see what petroch has done is that the first row of this tensor is the first five elements and the second row
is the second five elements I can also view it as a five by two as an example
and then recall that I can also use negative one in place of one of
these numbers and pytorch will calculate what that number must be in order to make the number of elements work out so this can
be this or like that but it will work of course this would not work
okay so this allows it to spread out some of the consecutive values into rows so that's very helpful because what we
can do now is first of all we're going to create a torshot tensor out of the a
list of floats and then we're going to view it as whatever it is but we're going to
stretch it out into rows of 1000 consecutive elements so the shape of this now becomes 200 by 1000. and each
row is one thousand um consecutive elements in this list so that's very helpful because now we
can do a mean along the rows and the shape of this will just be 200.
and so we've taken basically the mean on every row so plt.plot of that should be something nicer
much better so we see that we basically made a lot of progress and then here this is the
learning rate Decay so here we see that the learning rate Decay subtracted a ton of energy out of the system and allowed
us to settle into sort of the local minimum in this optimization so this is a much nicer plot let me come
up and delete the monster and we're going to be using this going forward now next up what I'm bothered by is that you

å½“ç„¶ï¼Œè¿™æ®µæˆ‘æ¥å¸®ä½ ç¿»è¯‘è§£é‡Šä¸€ä¸‹ï¼š

---

**# ä¿®æ­£å­¦ä¹ ç‡æ›²çº¿**

â€œè¿™æ¡æ›²çº¿çœ‹å¾—æˆ‘çœ¼ç›éƒ½ç–¼äº†ï¼Œå®åœ¨å—ä¸äº†äº†â€ï¼Œæ‰€ä»¥æˆ‘ä»¬æ¥ä¿®ä¸€ä¸‹ã€‚

ä¹‹å‰è®°å½• loss çš„å˜é‡ `lossi` æ˜¯ä¸€ä¸ª **Python çš„ list**ï¼Œé‡Œé¢å­˜çš„æ˜¯æ¯æ¬¡è®­ç»ƒ step çš„æµ®ç‚¹æ•°ï¼ˆfloatï¼‰ã€‚
æ¯”å¦‚å‰ 10 ä¸ªå…ƒç´ çœ‹èµ·æ¥åƒè¿™æ · `[2.3, 2.1, 2.05, ...]`ã€‚

ç°åœ¨æˆ‘ä»¬å¸Œæœ›å¯¹è¿™äº› loss å€¼**è¿›è¡Œå¹³æ»‘å¤„ç†**ï¼Œä¹Ÿå°±æ˜¯å¯¹å®ƒä»¬åšä¸€ä¸‹**å¹³å‡**ï¼Œè®©æ›²çº¿æ›´æœ‰ä»£è¡¨æ€§ï¼Œä¸è¦å¤ªä¹±ã€‚

å¯ä»¥æ€ä¹ˆåšå‘¢ï¼Ÿ
ä¸¾ä¾‹æ¥è¯´ï¼Œåœ¨ PyTorch é‡Œæˆ‘å¯ä»¥æŠŠè¿™ä¸ª list è½¬æ¢æˆä¸€ä¸ª tensorï¼Œå˜æˆä¸€ç»´çš„æ•°ç»„ï¼ˆ1D arrayï¼‰ã€‚
ç„¶åï¼Œtensor æ˜¯å¯ä»¥\*\*reshapeï¼ˆé‡å¡‘å½¢çŠ¶ï¼‰\*\*çš„ï¼Œæ¯”å¦‚æˆ‘å¯ä»¥æŠŠå®ƒ reshape æˆ 2 è¡Œ 5 åˆ—çš„äºŒç»´ tensorï¼ˆ2x5ï¼‰ï¼Œ
PyTorch ä¼šè‡ªåŠ¨æŠŠå‰ 5 ä¸ªå…ƒç´ æ”¾åˆ°ç¬¬ä¸€è¡Œï¼Œæ¥ä¸‹æ¥çš„ 5 ä¸ªå…ƒç´ æ”¾åˆ°ç¬¬äºŒè¡Œã€‚

è¿˜å¯ä»¥ reshape æˆ 5x2ï¼Œå½“ç„¶ï¼Œåªè¦å…ƒç´ ä¸ªæ•°å¯¹å¾—ä¸Šå°±å¯ä»¥ã€‚
è€Œä¸”ï¼ŒPyTorch çš„ reshape é‡Œï¼Œshape å‚æ•°å¯ä»¥å†™ `-1`ï¼Œè¿™æ · PyTorch ä¼šè‡ªåŠ¨å¸®ä½ è®¡ç®—è¿™ä¸€ç»´çš„å¤§å°ã€‚
æ¯”å¦‚ `.view(-1, 1000)`ï¼Œå°±ä¼šè‡ªåŠ¨æ ¹æ®æ€»å…ƒç´ ä¸ªæ•°ç®—å‡ºè¡Œæ•°ã€‚

è¿™ä¸ªç‰¹æ€§å¾ˆæœ‰ç”¨ï¼š
æˆ‘ä»¬ç°åœ¨è¦æŠŠ `lossi` è¿™ä¸ª list å˜æˆä¸€ä¸ª PyTorch tensorï¼Œç„¶å reshape æˆ â€œæ¯è¡Œ 1000 ä¸ªå…ƒç´ â€ï¼Œ
è¿™æ ·æ¯è¡Œå°±ä»£è¡¨**è¿ç»­çš„ 1000 ä¸ªè®­ç»ƒ step**ã€‚
æ¯”å¦‚ reshape æˆ `200 x 1000`ï¼Œè¡¨ç¤ºæ€»å…±æœ‰ 200 ç»„ï¼Œæ¯ç»„ 1000 ä¸ª loss æ•°æ®ã€‚

æ¥ä¸‹æ¥å°±å¯ä»¥å¯¹æ¯ä¸€è¡Œæ±‚å¹³å‡ï¼ˆmeanï¼‰ï¼Œä¹Ÿå°±æ˜¯**æ¯ 1000 ä¸ª step æ±‚ä¸€ä¸ªå¹³å‡ loss**ï¼Œ
è¿™æ ·ç”»å‡ºæ¥çš„æ›²çº¿å°±ä¸ä¼šé‚£ä¹ˆä¹±äº†ï¼Œå˜å¾—å¹³æ»‘å¾ˆå¤šã€‚

* ç”¨ `plt.plot` ç”»å‡ºè¿™ä¸ªå¹³æ»‘åçš„æ›²çº¿ï¼Œæ•ˆæœå°±å¥½å¤šäº†ï¼
* æ›²çº¿å·¦ä¾§éƒ¨åˆ†æ˜¯ loss ä¸‹é™çš„è¿‡ç¨‹ï¼Œå³ä¾§ä½ å¯ä»¥çœ‹åˆ° learning rate å¼€å§‹ decayï¼ˆè¡°å‡ï¼‰ï¼Œç³»ç»Ÿé‡Œçš„èƒ½é‡ä¸‹é™ï¼Œä¼˜åŒ–å™¨æ”¶æ•›åˆ°ä¸€ä¸ª local minimumã€‚
* è¿™æ ·ç”»å‡ºæ¥çš„ loss æ›²çº¿æ›´æ¸…æ¥šï¼Œæ•´ä½“è¶‹åŠ¿ä¸€ç›®äº†ç„¶ã€‚

æœ€åä½œè€…è¯´ï¼šâ€œæˆ‘è¦æŠŠåŸæ¥é‚£æ¡ä¹±ä¸ƒå…«ç³Ÿçš„æ›²çº¿åˆ æ‰ï¼Œæˆ‘ä»¬ä»Šåå°±ç”¨è¿™ç§æ›´å¥½çœ‹çš„ç‰ˆæœ¬â€ã€‚

---

å¦‚æœä½ æƒ³è¦ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ å†™ä¸€æ®µ**å¯¹åº”çš„ PyTorch ä»£ç ç¤ºä¾‹**ï¼Œè¿™æ ·ä½ å¯ä»¥ç›´æ¥å‚è€ƒæˆ–è€…ç”¨åœ¨ä½ è‡ªå·±çš„ notebook é‡Œï¼Œè¦ä¸è¦ï¼Ÿ ğŸŒŸ


# pytorchifying our code: layers, containers, torch.nn, fun bugs

see our forward pass is a little bit gnarly and takes way too many lines of code
so in particular we see that we've organized some of the layers inside the layers list but not all of them uh for
no reason so in particular we see that we still have the embedding table a special case outside of the layers and
in addition to that the viewing operation here is also outside of our layers so let's create layers for these
and then we can add those layers to just our list so in particular the two things that we
need is here we have this embedding table and we are indexing at the integers inside uh the batch XB uh
inside the tensor xB so that's an embedding table lookup just done with indexing and then here we see
that we have this view operation which if you recall from the previous video Simply rearranges the character
embeddings and stretches them out into a row and effectively what print that does
is the concatenation operation basically except it's free because viewing is very cheap in pytorch no no memory is being
copied we're just re-representing how we view that tensor so let's create um
modules for both of these operations the embedding operation and flattening operation
so I actually wrote the code in just to save some time so we have a module embedding and a
module pattern and both of them simply do the indexing operation in the forward pass and the flattening operation here
and this C now will just become a salt dot weight inside an embedding module
and I'm calling these layers specifically embedding a platinum because it turns out that both of them actually exist in pi torch so in
phytorch we have n and Dot embedding and it also takes the number of embeddings and the dimensionality of the bedding
just like we have here but in addition python takes in a lot of other keyword arguments that we are not using for our
purposes yet and for flatten that also exists in pytorch and it also takes additional
keyword arguments that we are not using so we have a very simple platform but both of them exist in pytorch
they're just a bit more simpler and now that we have these we can simply take
out some of these special cased um things so instead of C we're just
going to have an embedding and of a cup size and N embed
and then after the embedding we are going to flatten so let's construct those modules and now
I can take out this the and here I don't have to special case anymore because now C is the embeddings
weight and it's inside layers so this should just work
and then here our forward pass simplifies substantially because we don't need to do these now outside of
these layer outside and explicitly they're now inside layers
so we can delete those but now to to kick things off we want this little X which in the beginning is
just XB uh the tensor of integers specifying the identities of these characters at the input
and so these characters can now directly feed into the first layer and this should just work so let me come here and insert a break
because I just want to make sure that the first iteration of this runs and then there's no mistake so that ran
properly and basically we substantially simplified the forward pass here okay I'm sorry I changed my microphone so
hopefully the audio is a little bit better now one more thing that I would like to do in order to pytortify our code even
further is that right now we are maintaining all of our modules in a naked list of layers and we can also
simplify this uh because we can introduce the concept of Pi torch containers so in tors.nn which we are
basically rebuilding from scratch here there's a concept of containers and these containers are basically a way of organizing layers into
lists or dicts and so on so in particular there's a sequential which
maintains a list of layers and is a module class in pytorch and it basically
just passes a given input through all the layers sequentially exactly as we are doing here
so let's write our own sequential I've written a code here and basically
the code for sequential is quite straightforward we pass in a list of layers which we keep here and then given
any input in a forward pass we just call all the layers sequentially and return the result in terms of the parameters
it's just all the parameters of the child modules so we can run this and we can again
simplify this substantially because we don't maintain this naked list of layers we now have a notion of a model which is
a module and in particular is a sequential of all these layers
and now parameters are simply just a model about parameters and so that list comprehension now lives
here and then here we are press here we are doing all the things we used to do
now here the code again simplifies substantially because we don't have to do this forwarding here instead of just
call the model on the input data and the input data here are the integers inside xB so we can simply do logits which are
the outputs of our model are simply the model called on xB
and then the cross entropy here takes the logits and the targets so this simplifies substantially
and then this looks good so let's just make sure this runs that looks good
now here we actually have some work to do still here but I'm going to come back later for now there's no more layers
there's a model that layers but it's not a to access attributes of these classes
directly so we'll come back and fix this later and then here of course this simplifies substantially as well because logits are
the model called on x and then these low Jets come here
so we can evaluate the train and validation loss which currently is terrible because we just initialized the
neural net and then we can also sample from the model and this simplifies dramatically as well because we just want to call the model
onto the context and outcome logits and these logits go into softmax and get
the probabilities Etc so we can sample from this model what did I screw up
okay so I fixed the issue and we now get the result that we expect which is gibberish because the model is not
trained because we re-initialize it from scratch the problem was that when I fixed this cell to be modeled out layers instead of
just layers I did not actually run the cell and so our neural net was in a training mode and what caused the issue
here is the bathroom layer as bathroom layer of the likes to do because Bachelor was in a training mode and here
we are passing in an input which is a batch of just a single example made up of the context
and so if you are trying to pass in a single example into a bash Norm that is in the training mode you're going to end
up estimating the variance using the input and the variance of a single number is is not a number because it is
a measure of a spread so for example the variance of just the single number five you can see is not a number and so
that's what happened in the master basically caused an issue and then that polluted all of the further processing
so all that we have to do was make sure that this runs and we basically made the
issue of again we didn't actually see the issue with the loss we could have evaluated
the loss but we got the wrong result because basharm was in the training mode and uh and so we still get a result it's
just the wrong result because it's using the uh sample statistics of the batch whereas we want to use the running mean
and running variants inside the bachelor and so again an example of introducing a bug
inline because we did not properly maintain the state of what is training or not okay so I Rewritten everything

å½“ç„¶ï¼Œè¿™æ®µå†…å®¹æˆ‘æ¥å¸®ä½ è¯¦ç»†ç¿»è¯‘å’Œè§£é‡Šï¼š

---

**# PytorchåŒ–ä»£ç ï¼šå±‚ã€å®¹å™¨ã€torch.nnã€é‡åˆ°çš„å° bug**

ç°åœ¨æˆ‘ä»¬ç¥ç»ç½‘ç»œçš„ forward è¿‡ç¨‹ï¼ˆå‰å‘ä¼ æ’­ï¼‰å†™å¾—å¤ªå•°å—¦äº†ï¼Œä»£ç è¡Œæ•°å¤ªå¤šã€ç»“æ„ä¸æ¸…æ™°ã€‚
ç›®å‰æœ‰ä¸€éƒ¨åˆ† layer è¢«æ”¾åœ¨ `layers` åˆ—è¡¨é‡Œç®¡ç†ï¼Œä½†è¿˜æœ‰ä¸€äº›æ²¡æœ‰æ”¾è¿›å»ï¼Œæ¯”å¦‚ï¼š

* **embedding table C** æ˜¯å•ç‹¬å†™åœ¨å¤–é¢çš„ï¼Œ
* **viewï¼ˆreshapeï¼‰æ“ä½œ** ä¹Ÿæ˜¯å†™åœ¨å¤–é¢çš„ï¼Œ
  è¿™å…¶å®æ˜¯æ²¡æœ‰å¿…è¦çš„ã€‚

æ‰€ä»¥ç°åœ¨çš„æ”¹è¿›ç›®æ ‡å°±æ˜¯ï¼š
æŠŠè¿™äº›â€œç‰¹ä¾‹â€æ“ä½œä¹Ÿå°è£…æˆ layerï¼Œç„¶åç»Ÿä¸€ç®¡ç†ï¼Œä¾¿äºåé¢ç»´æŠ¤å’Œæ‰©å±•ã€‚

å…·ä½“æ¥è¯´ï¼Œå½“å‰éœ€è¦å¤„ç†çš„ä¸¤ä¸ªæ“ä½œï¼š

1. **embedding lookup**ï¼Œä¹Ÿå°±æ˜¯ä» embedding table æŸ¥è¡¨ï¼Œè¿™ä¸ªæ˜¯é€šè¿‡ `C[xB]` åšçš„ç´¢å¼•ï¼›
2. **view æ“ä½œ**ï¼Œä¹Ÿå°±æ˜¯æŠŠ embedding ç»“æœå±•å¹³ï¼ˆflattenï¼‰æˆä¸€è¡Œï¼Œç›¸å½“äºæ˜¯â€œæ‹¼æ¥â€æ“ä½œï¼Œä¸è¿‡ view æ˜¯â€œé›¶æˆæœ¬â€çš„ï¼ˆä¸å¤åˆ¶å†…å­˜ï¼Œåªæ˜¯æ¢ä¸€ä¸ª tensor è§†å›¾ï¼‰ã€‚

äºæ˜¯ä½œè€…æå‰å†™å¥½ä¸¤ä¸ªæ¨¡å—ï¼š

* `Embedding` æ¨¡å—ï¼šå°è£…äº† embedding lookup çš„æ“ä½œï¼›
* `Flatten` æ¨¡å—ï¼šå°è£…äº† flatten æ“ä½œã€‚

PyTorch é‡Œå…¶å®æœ¬æ¥ä¹Ÿæœ‰å¯¹åº”çš„æ¨¡å—ï¼š

* `torch.nn.Embedding`
* `torch.nn.Flatten`
  åªä¸è¿‡ PyTorch ç‰ˆæœ¬åŠŸèƒ½æ›´ä¸°å¯Œï¼Œå‚æ•°æ›´å¤šï¼Œæˆ‘ä»¬ç›®å‰å…ˆå®ç°ä¸€ä¸ªç®€ç‰ˆçš„å¤Ÿç”¨å³å¯ã€‚

æœ‰äº†è¿™ä¸¤ä¸ªæ¨¡å—ä¹‹åï¼Œä¹‹å‰ä»£ç é‡Œå•ç‹¬å¤„ç† `C` å’Œ `view` çš„åœ°æ–¹å°±å¯ä»¥åˆ æ‰ï¼Œç›´æ¥æ”¾å…¥ layers ç»Ÿä¸€ç®¡ç†ï¼š

* `Embedding` -> `Flatten` -> å…¶ä»–å±‚ï¼Œå½¢æˆä¸€ä¸ªæ›´å¹²å‡€çš„ forward æµç¨‹ã€‚

è¿™æ · forward è¿‡ç¨‹å°±å˜å¾—æ›´ç®€å•äº†ï¼Œæ•°æ®ï¼ˆxBï¼‰ç›´æ¥è¾“å…¥ç¬¬ä¸€å±‚ layerï¼Œåé¢å±‚å±‚ä¼ é€’ã€‚

---

æ¥ä¸‹æ¥è¿›ä¸€æ­¥â€œPyTorch åŒ–â€ï¼š
ç›®å‰æˆ‘ä»¬çš„ `layers` è¿˜æ˜¯è£¸ listï¼Œè¿™æ ·ä¸å¥½ã€‚
PyTorch æä¾›äº†â€œå®¹å™¨ï¼ˆcontainersï¼‰â€ï¼Œå¯ä»¥æ›´å¥½åœ°ç®¡ç† layerï¼Œæ¯”å¦‚ï¼š

* `torch.nn.Sequential` å°±æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„å®¹å™¨ï¼Œå†…éƒ¨æ˜¯ä¸€ä¸ª layer åˆ—è¡¨ï¼Œforward è¿‡ç¨‹ä¼š**è‡ªåŠ¨æŒ‰é¡ºåºè°ƒç”¨æ‰€æœ‰ layer**ã€‚

æ‰€ä»¥æˆ‘ä»¬ä¹Ÿå†™äº†ä¸€ä¸ªç®€ç‰ˆçš„ `Sequential` å®¹å™¨ï¼š

* åˆå§‹åŒ–æ—¶ä¼ å…¥ä¸€ä¸ª layer listï¼›
* forward æ—¶ä¾æ¬¡è°ƒç”¨è¿™äº› layerï¼Œæœ€åè¿”å›ç»“æœï¼›
* åŒæ—¶å¯ä»¥å¾ˆæ–¹ä¾¿åœ°æ”¶é›†æ‰€æœ‰çš„ parametersï¼ˆæ¨¡å‹å‚æ•°ï¼‰ã€‚

ç„¶åæ¨¡å‹å°±å‡çº§ä¸ºï¼š

```
model = Sequential([...layers...])  
```

* è¿™æ ·å°±ä¸ç”¨å•ç‹¬ç®¡ç† `layers`ï¼Œä¹Ÿä¸ç”¨æ‰‹åŠ¨ forward äº†ï¼Œç›´æ¥ `model(xB)` å°±å¯ä»¥è·‘ forwardï¼Œä»£ç å¤§å¤§ç®€åŒ–ã€‚

---

ç„¶åä½œè€…æ”¹å†™äº† loss è®¡ç®—å’Œ sample éƒ¨åˆ†ï¼š

* logits = model(xB)
* loss = cross\_entropy(logits, yB)

è¯„ä¼°ï¼ˆevaluateï¼‰å’Œ sample çš„æµç¨‹ä¹Ÿç®€åŒ–äº†ï¼Œç›´æ¥è°ƒç”¨ model å³å¯ï¼Œä¸ç”¨å¤„ç† layers ç»†èŠ‚ã€‚

---

è¿™é‡Œé‡åˆ°äº†ä¸€ä¸ªå° bugï¼š
æ”¹å®Œä»¥åï¼Œè·‘ sample ç»“æœå‡ºç°é—®é¢˜ï¼ŒåŸå› æ˜¯ï¼š

* æ”¹å®Œ `model = Sequential(layers)` åï¼Œæ²¡ rerun è¿™ä¸ª cellï¼Œå¯¼è‡´ BatchNorm layer è¿˜å¤„äº training æ¨¡å¼ï¼›
* ç»“æœåœ¨ç”¨å•ä¸ª example åš sample æ—¶ï¼ŒBatchNorm è¯•å›¾ç”¨ä¸€ä¸ª batch çš„ statisticsï¼ˆå‡å€¼æ–¹å·®ï¼‰ï¼Œä½†æ˜¯ batch size=1ï¼Œæ–¹å·®æ— æ³•è®¡ç®—ï¼ˆvariance of single number = NaNï¼‰ï¼›
* è¿™å°±æ±¡æŸ“äº†åç»­è®¡ç®—ï¼Œç»“æœé”™è¯¯ã€‚

æ­£ç¡®åšæ³•æ˜¯ sample æ—¶è¦æŠŠ model è®¾æˆ `eval()` æ¨¡å¼ï¼ˆtraining=Falseï¼‰ï¼ŒBatchNorm æ‰ä¼šç”¨ running mean/varianceï¼Œç»“æœæ‰ä¼šæ­£å¸¸ã€‚

---

æ€»ç»“ï¼š

1. æŠŠ embedding / flatten å°è£…æˆ layerï¼Œforward æ›´å¹²å‡€ï¼›
2. å¼•å…¥ Sequential å®¹å™¨ï¼Œè‡ªåŠ¨ç®¡ç† layer åˆ—è¡¨å’Œ forward æµç¨‹ï¼›
3. é‡åˆ°äº† BatchNorm è®­ç»ƒ/æ¨ç†æ¨¡å¼åˆ‡æ¢çš„å°å‘ï¼Œè¦å°å¿ƒ state çš„ç®¡ç†ï¼›
4. æ•´ä½“ä»£ç  PyTorch åŒ–ç¨‹åº¦æ›´é«˜ï¼Œæ›´æ¥è¿‘å®é™…å·¥ç¨‹å†™æ³•äº†ã€‚

---

å¦‚æœä½ éœ€è¦çš„è¯ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ å†™å‡ºè¿™éƒ¨åˆ†çš„ **å…³é”®ä»£ç ç¤ºä¾‹**ï¼Œæ–¹ä¾¿ä½ å‚è€ƒå’Œè‡ªå·±ç»ƒä¹ ï½ è¦ä¸è¦ï¼ŸğŸŒŸ


# overview: WaveNet

and here's where we are as a reminder we have the training loss of 2.05 and validation 2.10
now because these losses are very similar to each other we have a sense that we are not overfitting too much on
this task and we can make additional progress in our performance by scaling up the size of the neural network and
making everything bigger and deeper now currently we are using this architecture here where we are taking in
some number of characters going into a single hidden layer and then going to the prediction of the next character
the problem here is we don't have a naive way of making this bigger in a productive way we could of course use
our layers sort of building blocks and materials to introduce additional layers here and make the network deeper but it
is still the case that we are crushing all of the characters into a single layer all the way at the beginning
and even if we make this a bigger layer and add neurons it's still kind of like silly to squash all that information so
fast in a single step so we'd like to do instead is we'd like our Network to look a lot more like this
in the wavenet case so you see in the wavenet when we are trying to make the prediction for the next character in the
sequence it is a function of the previous characters that are feeding that feed in but not all of these
different characters are not just crushed to a single layer and then you have a sandwich they are crushed slowly
so in particular we take two characters and we fuse them into sort of like a diagram representation and we do that
for all these characters consecutively and then we take the bigrams and we fuse those into four character level chunks
and then we fuse that again and so we do that in this like tree-like hierarchical manner so we fuse the information from
the previous context slowly into the network as it gets deeper and so this is
the kind of architecture that we want to implement now in the wave Nets case this is a visualization of a stack of dilated
causal convolution layers and this makes it sound very scary but actually the idea is very simple and the fact that
it's a dilated causal convolution layer is really just an implementation detail to make everything fast we're going to
see that later but for now let's just keep the basic idea of it which is this Progressive Fusion so we want to make
the network deeper and at each level we want to fuse only two consecutive elements two characters then two bigrams
then two four grams and so on so let's unplant this okay so first up let me scroll to where we built the data set

å½“ç„¶ï¼Œè¿™æ®µæˆ‘å¸®ä½ ç¿»è¯‘è§£é‡Šä¸€ä¸‹ï¼š

---

**# æ¦‚è¿°ï¼šWaveNet**

æˆ‘ä»¬ç°åœ¨çš„è®­ç»ƒ loss æ˜¯ **2.05**ï¼ŒéªŒè¯é›† loss æ˜¯ **2.10**ã€‚
è¿™ä¸¤ä¸ª loss å¾ˆæ¥è¿‘ï¼Œè¯´æ˜ç›®å‰æ¨¡å‹**æ²¡æœ‰ä¸¥é‡ overfittingï¼ˆè¿‡æ‹Ÿåˆï¼‰**ï¼Œå¯ä»¥é€šè¿‡æ‰©å¤§æ¨¡å‹è§„æ¨¡ï¼ˆåŠ å¤§ç½‘ç»œæ·±åº¦ã€å®½åº¦ï¼‰è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

ç›®å‰ç”¨çš„æ¨¡å‹ç»“æ„æ˜¯è¿™æ ·çš„ï¼š

* è¾“å…¥ä¸€ä¸²å­—ç¬¦ï¼ˆæŸä¸ª context sizeï¼‰
* è¿›åˆ°ä¸€ä¸ªéšè—å±‚
* ç„¶åè¾“å‡ºä¸‹ä¸€ä¸ªå­—ç¬¦çš„é¢„æµ‹

ä½†æ˜¯è¿™ä¸ªæ¶æ„æœ‰ä¸€ä¸ªé—®é¢˜ï¼š

* è™½ç„¶å¯ä»¥ç®€å•é€šè¿‡åŠ å±‚ã€åŠ ç¥ç»å…ƒæ¥â€œåšå¤§â€æ¨¡å‹ï¼Œ
* ä½†æ˜¯å®ƒçš„æœ¬è´¨æ˜¯ï¼š**ä¸€å¼€å§‹å°±æŠŠæ‰€æœ‰è¾“å…¥å­—ç¬¦ç›´æ¥â€œå‹ç¼©â€æˆä¸€å±‚çš„è¡¨ç¤º**ï¼Œä¿¡æ¯èåˆå¾—å¤ªå¿«äº†ï¼
* å°±ç®—åŠ å¤§ hidden layerï¼Œä¿¡æ¯çš„èåˆé€Ÿåº¦è¿˜æ˜¯å¤ªå¿«ï¼Œè¿™æ ·å…¶å®ä¸å¤ªåˆç†ï¼Œç½‘ç»œéš¾ä»¥æœ‰æ•ˆå»ºæ¨¡â€œé•¿è·ç¦»ä¾èµ–â€ã€‚

æˆ‘ä»¬å¸Œæœ›çš„æ¶æ„æ˜¯åƒ WaveNet è¿™æ ·çš„ï¼Œæ€è·¯æ˜¯ï¼š

* é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ—¶å€™ï¼Œæ˜¯æ‰€æœ‰å‰é¢å­—ç¬¦çš„å‡½æ•°ï¼Œ
* ä½†è¿™äº›å­—ç¬¦ä¸æ˜¯ä¸€ä¸‹å­å…¨â€œå‹â€åˆ°ä¸€ä¸ªå±‚é‡Œï¼Œ
* è€Œæ˜¯é€šè¿‡**é€æ­¥èåˆï¼ˆProgressive Fusionï¼‰**ï¼Œä¿¡æ¯é€æ­¥æµå…¥æ›´æ·±å±‚ã€‚

ä¸¾ä¾‹æ¥è¯´ï¼ŒWaveNet æ˜¯è¿™æ ·åšçš„ï¼š

1. å…ˆæŠŠç›¸é‚»ä¸¤ä¸ªå­—ç¬¦èåˆæˆ bigram çš„è¡¨ç¤ºï¼›
2. ç„¶åæŠŠ bigram èåˆæˆå››ä¸ªå­—ç¬¦çš„è¡¨ç¤ºï¼ˆ4-gramï¼‰ï¼›
3. å†ç»§ç»­èåˆ... å½¢æˆä¸€ç§**æ ‘çŠ¶çš„å±‚æ¬¡ç»“æ„**ï¼ˆtree-like hierarchical mannerï¼‰ï¼Œ
4. æ¯ä¸€å±‚èåˆçš„æ˜¯æ›´å¤§ç²’åº¦çš„ä¿¡æ¯ï¼Œç›´åˆ°æœ€ç»ˆåšå‡ºé¢„æµ‹ã€‚

WaveNet è®ºæ–‡å›¾ç¤ºæ˜¯ä¸€ä¸ª**å †å çš„ dilated causal convolution layersï¼ˆæ‰©å¼ å‹å› æœå·ç§¯å±‚ï¼‰**ï¼Œ
å¬èµ·æ¥å¾ˆå“äººï¼Œä½†å…¶å®æ ¸å¿ƒæ€æƒ³å¾ˆç®€å•ï¼Œå°±æ˜¯ä¸ºäº†**åŠ å¿«å®ç°**ï¼Œåº•å±‚ç”¨äº†æ‰©å¼ å·ç§¯ï¼ˆdilated convï¼‰è¿™ç§æŠ€å·§ã€‚
æˆ‘ä»¬æš‚æ—¶ä¸ç”¨ç®¡å®ç°ç»†èŠ‚ï¼Œé‡ç‚¹æ˜¯è¿™ä¸ªâ€œä¿¡æ¯é€å±‚èåˆâ€çš„æ¶æ„æ€æƒ³ã€‚

ç›®æ ‡ï¼š

* åšä¸€ä¸ª**æ›´æ·±**çš„ç½‘ç»œï¼Œ
* æ¯ä¸€å±‚åªèåˆç›¸é‚»ä¸¤ä¸ªå…ƒç´ ï¼ˆå­—ç¬¦ï¼Œbigramï¼Œ4-gramï¼Œ...ï¼‰ï¼Œ
* é€å±‚ä¼ é€’ï¼Œé€å±‚èåˆï¼Œæœ€ç»ˆåšå‡ºä¸‹ä¸€ä¸ªå­—ç¬¦çš„é¢„æµ‹ã€‚

---

ä½œè€…è¯´ï¼šâ€œé‚£æˆ‘ä»¬æ¥å¼€å§‹å®ç°è¿™ä¸ªæ¶æ„å§ï¼â€
é¦–å…ˆè¦å›å»çœ‹çœ‹ä¹‹å‰æ€ä¹ˆæ„å»ºçš„æ•°æ®é›†ï¼Œç„¶åå†å¾€ä¸‹æ”¹è¿›æ¨¡å‹ã€‚

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ ç”»ä¸€ä¸ªç®€å•çš„å›¾ï¼Œ**MLP vs. WaveNet çš„ç»“æ„å¯¹æ¯”**ï¼Œ
è¿™æ ·ä½ ç†è§£è¿™ä¸ªâ€œé€æ­¥èåˆâ€æ€æƒ³ä¼šæ›´ç›´è§‚ï½ è¦ä¸è¦ï¼Ÿ ğŸš€


# dataset bump the context size to 8

and let's change the block size from 3 to 8. so we're going to be taking eight characters of context to predict the
ninth character so the data set now looks like this we have a lot more context feeding in to predict any next
character in a sequence and these eight characters are going to be processed in this tree like structure
now if we scroll here everything here should just be able to work so we should be able to redefine the network

å½“ç„¶ï¼Œè¿™æ®µå†…å®¹æˆ‘æ¥å¸®ä½ ç¿»è¯‘ä¸€ä¸‹ï¼š

---

**# æ•°æ®é›†è°ƒæ•´ï¼šæŠŠ context size æå‡åˆ° 8**

æˆ‘ä»¬æŠŠ `block_size`ï¼ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼‰ä» 3 æ”¹æˆ **8**ï¼Œ
ä¹Ÿå°±æ˜¯è¯´ï¼š

* ç°åœ¨è¾“å…¥æ˜¯ 8 ä¸ªå­—ç¬¦ï¼Œ
* ç›®æ ‡æ˜¯é¢„æµ‹**ç¬¬ 9 ä¸ªå­—ç¬¦**ã€‚

è¿™æ ·ä¸€æ¥ï¼Œæ•°æ®é›†çš„æ ¼å¼å°±å˜æˆäº†ï¼š

* è¾“å…¥æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼ˆ8 ä¸ªå­—ç¬¦ï¼‰å»é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œ
* ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸€ä¸ªè®­ç»ƒæ ·æœ¬éƒ½ä¼šåŒ…å«æ›´å¤šä¿¡æ¯ã€‚

ç„¶åï¼Œè¿™ 8 ä¸ªå­—ç¬¦ä¼šåœ¨æ¨¡å‹é‡Œé€šè¿‡**æ ‘çŠ¶ç»“æ„**ï¼ˆtree-like structureï¼‰æ¥å¤„ç†ï¼ˆä¹‹å‰è®²è¿‡çš„ Progressive Fusionï¼‰ã€‚

æ¥ä¸‹æ¥æ»šåŠ¨åˆ°ä¸‹é¢ï¼Œå¯ä»¥çœ‹åˆ°å…¶å®ƒä»£ç åŸºæœ¬ä¸ç”¨å¤§æ”¹ï¼Œç›´æ¥å°±èƒ½è·‘ï¼š

* é‡æ–°å®šä¹‰ç½‘ç»œç»“æ„ä¹‹åï¼Œæ•´ä¸ªè®­ç»ƒæµç¨‹åº”è¯¥æ˜¯å¯ä»¥æ­£å¸¸å·¥ä½œçš„ã€‚

---

ç®€å•è¯´ï¼Œå°±æ˜¯â€œæ‰©å¤§äº†ä¸Šä¸‹æ–‡çª—å£â€ï¼Œè¿™æ ·æ¨¡å‹å¯ä»¥å­¦ä¹ æ›´å¤æ‚çš„åºåˆ—ä¾èµ–å…³ç³»ã€‚

---

è¦ä¸è¦æˆ‘é¡ºä¾¿ä¹Ÿå¸®ä½ æ•´ç†ä¸€ä¸‹ï¼š
1ï¸âƒ£ **ä¸ºä»€ä¹ˆæå‡ context size æœ‰æ„ä¹‰**
2ï¸âƒ£ **ä¼šå¸¦æ¥ä»€ä¹ˆæŒ‘æˆ˜**
è¿™æ ·ä½ ç†è§£èµ·æ¥ä¼šæ›´ç³»ç»Ÿï½ ğŸš€


# re-running baseline code on block_size 8

you see the number of parameters has increased by 10 000 and that's because the block size has grown so this first
linear layer is much much bigger our linear layer now takes eight characters into this middle layer so there's a lot
more parameters there but this should just run let me just break right after
the very first iteration so you see that this runs just fine it's just that this network doesn't make too much sense
we're crushing way too much information way too fast so let's now come in and see how we
could try to implement the hierarchical scheme now before we dive into the detail of the re-implementation here I
was just curious to actually run it and see where we are in terms of the Baseline performance of just lazily scaling up the context length so I'll
let it run we get a nice loss curve and then evaluating the loss we actually see quite a bit of improvement just from
increasing the context line length so I started a little bit of a performance log here and previously where we were is
we were getting a performance of 2.10 on the validation loss and now simply scaling up the contact length from 3 to
8 gives us a performance of 2.02 so quite a bit of an improvement here and
also when you sample from the model you see that the names are definitely improving qualitatively as well
so we could of course spend a lot of time here tuning um uh tuning things and making it even bigger and scaling up the network
further even with the simple um sort of setup here but let's continue
and let's Implement here model and treat this as just a rough Baseline performance but there's a lot of
optimization like left on the table in terms of some of the hyper parameters that you're hopefully getting a sense of
now okay so let's scroll up now and come back up and what I've done here

å½“ç„¶ï¼Œè¿™æ®µå†…å®¹æˆ‘æ¥å¸®ä½ ç¿»è¯‘è§£é‡Šä¸€ä¸‹ï¼š

---

**# ç”¨ block\_size = 8 é‡æ–°è·‘ baseline ä»£ç **

ç°åœ¨æŠŠ `block_size` æå‡åˆ°äº† **8**ï¼Œä½ å¯ä»¥çœ‹åˆ°æ¨¡å‹çš„å‚æ•°é‡å¢åŠ äº† **1 ä¸‡ä¸ª**ï¼Œ
åŸå› æ˜¯ç¬¬ä¸€ä¸ª `Linear` å±‚çš„è¾“å…¥ç»´åº¦å˜å¤§äº†ï¼ˆç°åœ¨æ˜¯ 8 ä¸ªå­—ç¬¦ â†’ ä¸­é—´å±‚ï¼‰ï¼Œæ‰€ä»¥å‚æ•°æ•°é‡è‡ªç„¶å¢å¤šäº†ã€‚

ä¸è¿‡ä»£ç æ˜¯å¯ä»¥ç›´æ¥è·‘çš„ã€‚
ä½œè€…è¿™é‡ŒåŠ äº†ä¸€ä¸ª break pointï¼Œç¡®è®¤ç¬¬ä¸€è½®è®­ç»ƒè·‘å¾—æ­£å¸¸ã€‚
è™½ç„¶è¿™ä¸ªç½‘ç»œâ€œç»“æ„ä¸Šâ€å¹¶ä¸å¤ªåˆç†ï¼ˆè¿˜æ˜¯ä¸€ä¸‹å­æŠŠ 8 ä¸ªå­—ç¬¦å…¨å‹è¿›ä¸€ä¸ªå¤§ hidden layerï¼Œä¿¡æ¯å‹ç¼©å¤ªå¿«ï¼‰ï¼Œ
ä½†å®ƒ**èƒ½è·‘é€š**ï¼Œå¯ä»¥å…ˆçœ‹çœ‹â€œå•çº¯æŠŠä¸Šä¸‹æ–‡å˜å¤§â€å¸¦æ¥çš„æ•ˆæœã€‚

---

ç„¶åä½œè€…å¥½å¥‡ï¼Œå…ˆä¸æ”¹ WaveNet ç»“æ„ï¼Œç›´æ¥â€œæ‡’å¾—åŠ¨è„‘â€è·‘ä¸€è·‘ baselineï¼Œçœ‹çœ‹ **ä»…ä»…æŠŠ context length ä» 3 æå‡åˆ° 8ï¼Œæ•ˆæœå¦‚ä½•**ã€‚

* è·‘å®Œå‘ç° loss æ›²çº¿ï¼ˆloss curveï¼‰è¿˜ä¸é”™ã€‚
* éªŒè¯é›† loss ä»ä¹‹å‰çš„ **2.10** é™åˆ° **2.02**ï¼Œå…‰é å¢åŠ  context length å°±å¸¦æ¥äº†æ˜æ˜¾æå‡ï¼

è€Œä¸”ä»æ¨¡å‹ sample å‡ºæ¥çš„åå­—ï¼Œè´¨é‡ä¹Ÿæ˜æ˜¾æå‡äº†ï¼Œåå­—â€œçœ‹èµ·æ¥â€æ›´åƒçœŸå®åå­—äº†ã€‚

---

å½“ç„¶ï¼Œä½œè€…è¯´ï¼š

* å…¶å®å…‰ç”¨è¿™ä¸ªç®€å•ç½‘ç»œã€è°ƒæ•´ä¸€ä¸‹è¶…å‚æ•°ï¼ˆhyperparametersï¼‰æ¯”å¦‚ learning rateã€ç½‘ç»œå±‚æ•°ã€hidden sizeï¼Œå…¶å®è¿˜èƒ½ç»§ç»­ä¼˜åŒ–ï¼›
* ä¸è¿‡æˆ‘ä»¬é‡ç‚¹æ˜¯è¦**æ”¹é€ æ¨¡å‹ç»“æ„**ï¼ˆå®ç°å±‚æ¬¡ç»“æ„çš„ WaveNet é£æ ¼ï¼‰ï¼Œæ‰€ä»¥å°±æš‚æ—¶æŠŠè¿™ä¸ªç»“æœå½“ baseline è®°å½•ä¸‹æ¥ï¼Œæ¥ä¸‹æ¥ç»§ç»­å¾€ä¸‹å®ç°æ›´å¥½çš„æ¨¡å‹ã€‚

---

æ€»ç»“ä¸€ä¸‹é€»è¾‘å°±æ˜¯ï¼š
1ï¸âƒ£ ä»…ä»…å¢åŠ  context sizeï¼ˆ3 â†’ 8ï¼‰ï¼Œæ¨¡å‹æ•ˆæœå·²ç»æ˜æ˜¾æå‡ï¼›
2ï¸âƒ£ ä½†æ˜¯ MLP ç»“æ„æœ¬èº«â€œå‹ç¼©å¤ªå¿«â€ï¼Œè¿˜æ˜¯æœ‰ç»“æ„é—®é¢˜ï¼›
3ï¸âƒ£ æˆ‘ä»¬ç›®æ ‡æ˜¯å®ç° WaveNet æ ·å¼çš„**é€å±‚èåˆç½‘ç»œ**ï¼Œè¿™æ‰æ˜¯æ›´åˆç†çš„æ¶æ„ï¼›
4ï¸âƒ£ baseline å…ˆè®°å½•ä¸‹æ¥ï¼Œä¹‹åå¯ä»¥å¯¹æ¯”æ•ˆæœã€‚

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ æ•´ç†ä¸€ä»½\*\*â€œWaveNet æ¶æ„ vs MLP baseline æ¶æ„çš„æ•ˆæœå¯¹æ¯”è®°å½•è¡¨â€\*\*ï¼Œè¿™æ ·ä»¥åä½ å­¦ä¹ åšå®éªŒä¹Ÿä¼šæ›´è§„èŒƒï¼Œè¦ä¸è¦ï¼ŸğŸŒŸ


# implementing WaveNet

is I've created a bit of a scratch space for us to just like look at the forward pass of the neural net and inspect the
shape of the tensor along the way as the neural net uh forwards so here I'm just
temporarily for debugging creating a batch of just say four examples so four random integers then I'm plucking out
those rows from our training set and then I'm passing into the model the input xB
now the shape of XB here because we have only four examples is four by eight and this eight is now the current block size
so uh inspecting XP we just see that we have four examples each one of them is a
row of xB and we have eight characters here and this integer tensor just contains the
identities of those characters so the first layer of our neural net is the embedding layer so passing XB this
integer tensor through the embedding layer creates an output that is four by eight by ten
so our embedding table has for each character a 10-dimensional vector that
we are trying to learn and so what the embedding layer does here is it plucks out the embedding
Vector for each one of these integers and organizes it all in a four by eight
by ten tensor now so all of these integers are translated into 10 dimensional vectors inside this
three-dimensional tensor now passing that through the flattened layer as you recall what this does is it views
this tensor as just a 4 by 80 tensor and what that effectively does is that all
these 10 dimensional embeddings for all these eight characters just end up being stretched out into a long row
and that looks kind of like a concatenation operation basically so by viewing the tensor differently we now
have a four by eighty and inside this 80 it's all the 10 dimensional uh vectors just uh concatenate next to each
other and then the linear layer of course takes uh 80 and creates 200 channels
just via matrix multiplication so so far so good now I'd like to show you something surprising
let's look at the insides of the linear layer and remind ourselves how it works
the linear layer here in the forward pass takes the input X multiplies it with a weight and then optionally adds
bias and the weight here is two-dimensional as defined here and the bias is one dimensional here
so effectively in terms of the shapes involved what's happening inside this linear layer looks like this right now
and I'm using random numbers here but I'm just illustrating the shapes and what happens
basically a 4 by 80 input comes into the linear layer that's multiplied by this 80 by 200 weight Matrix inside and
there's a plus 200 bias and the shape of the whole thing that comes out of the linear layer is four by two hundred as
we see here now notice here by the way that this here will create a 4x200 tensor and then
plus 200 there's a broadcasting happening here about 4 by 200 broadcasts with 200 uh so everything works here
so now the surprising thing that I'd like to show you that you may not expect is that this input here that is being
multiplied uh doesn't actually have to be two-dimensional this Matrix multiply
operator in pytorch is quite powerful and in fact you can actually pass in higher dimensional arrays or tensors and
everything works fine so for example this could be four by five by eighty and the result in that case will become four
by five by two hundred you can add as many dimensions as you like on the left here
and so effectively what's happening is that the matrix multiplication only works on the last Dimension and the
dimensions before it in the input tensor are left unchanged
so that is basically these um these dimensions on the left are all treated as just a batch Dimension so we can have
multiple batch dimensions and then in parallel over all those Dimensions we are doing the matrix multiplication on
the last dimension so this is quite convenient because we can use that in our Network now
because remember that we have these eight characters coming in and we don't want to now uh flatten all
of it out into a large eight-dimensional vector because we don't want to Matrix multiply
80. into a weight Matrix multiply immediately instead we want to group
these like this so every consecutive two elements
one two and three and four and five and six and seven and eight all of these should be now basically flattened out and multiplied
by weight Matrix but all of these four groups here we'd like to process in parallel so it's kind of like a batch
Dimension that we can introduce and then we can in parallel basically
process all of these uh bigram groups in the four batch dimensions of an
individual example and also over the actual batch dimension of the you know four examples in our example here so
let's see how that works effectively what we want is right now we take a 4 by 80
and multiply it by 80 by 200 to in the linear layer this is what happens
but instead what we want is we don't want 80 characters or 80 numbers to come in we only want two characters to come
in on the very first layer and those two characters should be fused so in other words we just want 20 to
come in right 20 numbers would come in and here we don't want a 4 by 80 to feed
into the linear layer we actually want these groups of two to feed in so instead of four by eighty we want this
to be a 4 by 4 by 20. so these are the four groups of two and
each one of them is ten dimensional vector so what we want is now is we need to change the flattened layer so it doesn't
output a four by eighty but it outputs a four by four by Twenty where basically these um
every two consecutive characters are uh packed in on the very last Dimension and
then these four is the first batch Dimension and this four is the second batch Dimension referring to the four
groups inside every one of these examples and then this will just multiply like this so this is what we want to get to
so we're going to have to change the linear layer in terms of how many inputs it expects it shouldn't expect 80 it
should just expect 20 numbers and we have to change our flattened layer so it doesn't just fully flatten out this
entire example it needs to create a 4x4 by 20 instead of four by eighty so let's
see how this could be implemented basically right now we have an input that is a four by eight by ten that
feeds into the flattened layer and currently the flattened layer just stretches it out so if you remember the
implementation of flatten it takes RX and it just views it as whatever the batch Dimension is and then
negative one so effectively what it does right now is it does e dot view of 4 negative one and
the shape of this of course is 4 by 80. so that's what currently happens and we
instead want this to be a four by four by Twenty where these consecutive ten-dimensional vectors get concatenated
so you know how in Python you can take a list of range of 10
so we have numbers from zero to nine and we can index like this to get all the
even parts and we can also index like starting at one and going in steps up two to get all
the odd parts so one way to implement this it would be as follows we can take e and we can
index into it for all the batch elements and then just even elements in this
Dimension so at indexes 0 2 4 and 8. and then all the parts here from this
last dimension and this gives us the even characters
and then here this gives us all the odd characters and basically what we want to do is we make
sure we want to make sure that these get concatenated in pi torch and then we want to concatenate these two tensors
along the second dimension so this and the shape of it would be
four by four by Twenty this is definitely the result we want we are explicitly grabbing the even parts and
the odd parts and we're arranging those four by four by ten right next to each other and concatenate
so this works but it turns out that what also works is you can simply use a view again and just request the right shape
and it just so happens that in this case those vectors will again end up being arranged in exactly the way we want so
in particular if we take e and we just view it as a four by four by Twenty which is what we want
we can check that this is exactly equal to but let me call this this is the explicit concatenation I suppose
um so explosives dot shape is 4x4 by 20. if you just view it as 4x4 by 20 you can
check that when you compare to explicit uh you got a big this is element wise
operation so making sure that all of them are true that is the truth so basically long story short we don't need
to make an explicit call to concatenate Etc we can simply take this input tensor
to flatten and we can just view it in whatever way we want and in particular you don't want to
stretch things out with negative one we want to actually create a three-dimensional array and depending on
how many vectors that are consecutive we want to um fuse like for example two then we can
just simply ask for this Dimension to be 20. and um use a negative 1 here and python will
figure out how many groups it needs to pack into this additional batch dimension so let's now go into flatten and
implement this okay so I scroll up here to flatten and what we'd like to do is we'd like to change it now so let me
create a Constructor and take the number of elements that are consecutive that we would like to concatenate now in the
last dimension of the output so here we're just going to remember solve.n equals n
and then I want to be careful here because pipe pytorch actually has a torch to flatten and its keyword
arguments are different and they kind of like function differently so R flatten is going to start to depart from patreon
flatten so let me call it flat flatten consecutive or something like that just to make sure that our apis are about
equal so this uh basically flattens only some n consecutive elements and puts them
into the last dimension now here the shape of X is B by T by C
so let me pop those out into variables and recall that in our example down below B was 4 T
was 8 and C was 10. now instead of doing x dot view of B by
negative one right this is what we had before
we want this to be B by um negative 1 by
and basically here we want c times n that's how many consecutive elements we
want and here instead of negative one I don't super love the use of negative one because I like to be very explicit so
that you get error messages when things don't go according to your expectation so what do we expect here we expect this
to become t divide n using integer division here so that's what I expect to happen
and then one more thing I want to do here is remember previously all the way in the beginning n was three and uh
basically we're concatenating um all the three characters that existed there so we basically are concatenated
everything and so sometimes I can create a spurious dimension of one here so if it is the
case that x dot shape at one is one then it's kind of like a spurious dimension
um so we don't want to return a three-dimensional tensor with a one here we just want to return a two-dimensional
tensor exactly as we did before so in this case basically we will just say x equals x dot squeeze that is a
pytorch function and squeeze takes a dimension that it
either squeezes out all the dimensions of a tensor that are one or you can specify the exact Dimension that you
want to be squeezed and again I like to be as explicit as possible always so I expect to squeeze out the First
Dimension only of this tensor this three-dimensional tensor and if
this Dimension here is one then I just want to return B by c times n and so self dot out will be X and then
we return salt dot out so that's the candidate implementation and of course this should be self.n
instead of just n so let's run and let's come here now
and take it for a spin so flatten consecutive
and in the beginning let's just use eight so this should recover the previous Behavior so flagging
consecutive of eight uh which is the current block size we can do this uh that should recover
the previous Behavior so we should be able to run the model and here we can inspect I have a little
code snippet here where I iterate over all the layers I print the name of this
class and the shape and so we see the shapes as we expect
them after every single layer in the top bit so now let's try to restructure it
using our flattened consecutive and do it hierarchically so in particular
we want to flatten consecutive not just not block size but just two and then we want to process this with
linear now then the number of inputs to this linear will not be an embed times block size it will now only be n embed
times two 20. this goes through the first layer and
now we can in principle just copy paste this now the next linear layer should expect and hidden times two
and the last piece of it should expect and it enters 2 again
so this is sort of like the naive version of it um so running this we now have a much much
bigger model and we should be able to basically just forward the model
and now we can inspect uh the numbers in between so four byte by 20
was Platinum consecutively into four by four by Twenty this was projected into four by four by
two hundred and then bash storm just worked out of the box we have to verify that bastron
does the correct thing even though it takes a three-dimensional impedance that are two dimensional input then we have 10h which is element wise
then we crushed it again so if we flatten consecutively and ended up with a four by two by 400 now
then linear brought it back down to 200 batch room 10h and lastly we get a 4 by
400 and we see that the flattened consecutive for the last flatten here uh it squeezed out that dimension of one so
we only ended up with four by four hundred and then linear Bachelor on 10h and uh the last linear layer to get our
logents and so The Lodges end up in the same shape as they were before but now we actually have a nice three layer
neural nut and it basically corresponds to whoops sorry it basically corresponds
exactly to this network now except only this piece here because we only have three layers whereas here in this
example there's uh four layers with the total receptive field size of 16
characters instead of just eight characters so the block size here is 16. so this piece of it's basically
implemented here um now we just have to kind of figure out some good Channel numbers to use
here now in particular I changed the number of hidden units to be 68 in this architecture because when I use 68 the

å½“ç„¶ï¼è¿™ä¸€æ®µæ˜¯**WaveNet ç»“æ„å®ç°**çš„å…³é”®éƒ¨åˆ†ï¼Œå†…å®¹å¾ˆå¤šï¼Œæˆ‘å¸®ä½ æ•´ç†æˆ**è¯¦ç»†ä¸­æ–‡è®²è§£**ï¼š

---

### **# å®ç° WaveNet**

1ï¸âƒ£ **å»ºç«‹è°ƒè¯•åŒº**
ä½œè€…å…ˆåšäº†ä¸€ä¸ªâ€œscratch spaceâ€ï¼ˆè°ƒè¯•åŒºï¼‰ç”¨æ¥**è§‚å¯Ÿ forward è¿‡ç¨‹ä¸­çš„å¼ é‡ shape**å˜åŒ–ã€‚

* éšæœºé€‰äº† batch size=4 ä¸ªæ ·æœ¬ï¼ˆ4 è¡Œæ•°æ®ï¼‰ï¼Œ
* æ¯ä¸ªæ ·æœ¬é‡Œæœ‰ **8 ä¸ªå­—ç¬¦**ï¼ˆblock\_size=8ï¼‰ï¼Œ
* ä¼ å…¥æ¨¡å‹ `xB`ï¼Œshape æ˜¯ **\[4, 8]**ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å­—ç¬¦ IDï¼ˆint tensorï¼‰ã€‚

---

2ï¸âƒ£ **Embedding å±‚**

* é€šè¿‡ embedding layerï¼ŒæŠŠ int tensor è½¬æˆå‘é‡ï¼š
  `xB` â†’ `embedding(xB)` â†’ shape å˜æˆ **\[4, 8, 10]**

  * æ¯ä¸ªå­—ç¬¦æ˜ å°„æˆ 10 ç»´å‘é‡ï¼ˆ10-dimensional vectorï¼‰

---

3ï¸âƒ£ **Flatten å±‚**ï¼ˆä¼ ç»Ÿ MLP çš„ flattenï¼‰

* ä¹‹å‰ flatten æ˜¯**ç›´æ¥ view æˆ \[4, 80]**ï¼Œ
  å°±æ˜¯ 8 ä¸ª 10 ç»´ embedding å±•å¼€æ‹¼æ¥æˆä¸€è¡Œï¼š
  `4 (batch), 8 (tokens), 10 (embed_dim)` â†’ flatten â†’ `[4, 80]`

---

4ï¸âƒ£ **Linear å±‚**

* ç„¶å Linear å±‚åšçŸ©é˜µä¹˜æ³•ï¼š
  `[4, 80] * [80, 200] â†’ [4, 200]`
  ä¹Ÿå°±æ˜¯æŠŠ 80 ç»´å‘é‡æ˜ å°„æˆ 200 ç»´ã€‚

---

### **å…³é”®çŸ¥è¯†ç‚¹ï¼šé«˜ç»´å¼ é‡çš„çŸ©é˜µä¹˜æ³•**

PyTorch çš„ Linear æ”¯æŒé«˜ç»´è¾“å…¥ï¼Œæ¯”å¦‚ `[4, 5, 80]` \* `[80, 200]` â†’ `[4, 5, 200]`

* å¤šå‡ºæ¥çš„ç»´åº¦ä¼šè¢«å½“ä½œ batch ç»´åº¦ï¼ŒçŸ©é˜µä¹˜æ³•åªä½œç”¨åœ¨æœ€åä¸€ç»´ã€‚
* è¿™ä¸ªç‰¹æ€§å¯ä»¥ç”¨æ¥å®ç°â€œå¹¶è¡Œå¤„ç† bigramâ€ã€‚

---

5ï¸âƒ£ **å®ç° WaveNet æ ·å¼çš„é€æ­¥èåˆ**

* WaveNet ä¸æƒ³æŠŠ 8 ä¸ªå­—ç¬¦ä¸€æ¬¡æ€§ flattenï¼Œ

  * è€Œæ˜¯**æˆå¯¹èåˆ** â†’ å½¢æˆ bigram â†’ ç»§ç»­èåˆ â†’ å½¢æˆ 4-gram â†’ ç»§ç»­èåˆ...

ç›®æ ‡ï¼š

* ç¬¬ä¸€å±‚ï¼š`[4, 4, 20]`

  * æ¯ä¸¤ä¸ªå­—ç¬¦ï¼ˆ10 + 10ï¼‰æ‹¼æ¥ â†’ 20 ç»´ â†’ æ¯ä¸ªæ ·æœ¬æœ‰ 4 ä¸ª bigram
* ç¬¬äºŒå±‚ï¼š`[4, 2, 40]`

  * æ¯ä¸¤ä¸ª bigram æ‹¼æ¥ â†’ 40 ç»´
* ç¬¬ä¸‰å±‚ï¼š`[4, 1, 80]`

  * æœ€ç»ˆæ‹¼æ¥ â†’ 80 ç»´
  * squeeze æ‰å¤šä½™ç»´åº¦ â†’ `[4, 80]`
* ç„¶åè¾“å‡ºå±‚é¢„æµ‹ã€‚

---

6ï¸âƒ£ **Flatten consecutive å±‚å®ç°**

* å†™äº†ä¸€ä¸ªæ–°çš„ `FlattenConsecutive(n)` å±‚

  * æ¯ n ä¸ªè¿ç»­å‘é‡æ‹¼æ¥
  * ä¸å…¨ flattenï¼Œåªæ‹¼æ¥å±€éƒ¨
  * è¾“å‡º shape: `[B, T // n, C * n]`

---

7ï¸âƒ£ **ç½‘ç»œæ”¹å†™**
æ–°çš„ç½‘ç»œç»“æ„å¤§è‡´æ˜¯ï¼š

```
Embedding  
â†’ FlattenConsecutive(2) â†’ Linear â†’ BatchNorm â†’ Tanh  
â†’ FlattenConsecutive(2) â†’ Linear â†’ BatchNorm â†’ Tanh  
â†’ FlattenConsecutive(2) â†’ Linear â†’ BatchNorm â†’ Tanh  
â†’ Linear â†’ Output
```

* æ¯ä¸€å±‚æ˜¯â€œæˆå¯¹èåˆâ€ï¼Œé€æ­¥å‹ç¼© token æ•°ã€‚
* æœ€ç»ˆè¾“å‡ºçš„ logits shape å’Œä»¥å‰ä¸€æ · `[batch_size, vocab_size]`ã€‚

---

8ï¸âƒ£ **æµ‹è¯• Forward è¿‡ç¨‹**

* æ‰“å°å‡ºæ¯å±‚çš„ tensor shapeï¼Œç¡®è®¤ shape å˜åŒ–æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚
* BatchNorm ä¹Ÿå¯ä»¥ç›´æ¥å¤„ç†è¿™ç§ 3D tensorï¼Œæ²¡é—®é¢˜ã€‚
* æœ€ç»ˆ logits å’ŒåŸæ¥ä¸€æ ·ã€‚

---

### **ç»“æœ**

* ç°åœ¨å®ç°äº† 3 å±‚çš„â€œæ ‘çŠ¶èåˆç½‘ç»œâ€ï¼Œå¯¹åº” WaveNet é‡Œçš„å‰åŠæ®µç»“æ„ã€‚
* å½“å‰ block\_size=8ï¼Œç†è®ºä¸Šå¯ä»¥æ‰©å±•åˆ° block\_size=16ï¼Œ receptive field å˜å¤§ã€‚
* ç°åœ¨éœ€è¦å†è°ƒæ•´ hidden channels è¿™äº›è¶…å‚æ•°ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ•ˆæœã€‚

---

### æ€»ç»“ä¸€å¥è¯ï¼š

é€šè¿‡ `FlattenConsecutive + Linear`ï¼Œå°±èƒ½å®ç° WaveNet çš„**é€æ­¥èåˆ**ï¼Œæ¯”ä¼ ç»Ÿ MLP ä¸€ä¸‹å­ flatten å…¨æ‹¼æ¥æ›´åˆç†ï¼Œæ•ˆæœæ›´å¥½ã€‚

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ æ€»ç»“ä¸€ä¸ªâ€œ**WaveNet å±‚çº§å®ç°æ¨¡æ¿ä»£ç **â€ï¼Œä½ å¯ä»¥ç›´æ¥å¥—ç”¨ï¼è¦ä¸è¦ï¼ŸğŸŒŸ

```python
class FlattenConsecutive:

  def __init__(self, n):
    self.n = n

  def __call__(self, x):
    B, T, C = x.shape
    x = x.view(B, T//self.n, C*self.n)
    if x.shape[1] == 1:
      x = x.squeeze(1)
    self.out = x
    return self.out

  def parameters(self):
    return []
```

è§£é‡Šä¸€ä¸‹

å½“ç„¶ï¼è¿™æ®µæ˜¯ `FlattenConsecutive` çš„ç±»å®šä¹‰ï¼Œæˆ‘ä»¬æ¥ä¸€è¡Œä¸€è¡Œè®²è§£ï¼š

---

```python
class FlattenConsecutive:
```

å®šä¹‰ä¸€ä¸ª**è‡ªå®šä¹‰çš„ PyTorch layer**ï¼Œåå­—å« `FlattenConsecutive`ï¼Œ
è¿™ä¸ªå±‚çš„ä½œç”¨æ˜¯ï¼š

âœ… æŠŠ **ç›¸é‚» n ä¸ª token çš„ embedding å‘é‡æ‹¼æ¥**ï¼Œ
âœ… æŠŠ T ä¸ª token â†’ æ‹¼æˆ T//n ä¸ª tokenï¼Œ
âœ… æ¯ä¸ªæ‹¼æ¥åçš„ token ç»´åº¦æ˜¯ `C * n`ã€‚

---

```python
def __init__(self, n):
    self.n = n
```

* åˆå§‹åŒ–æ—¶ä¼ å…¥å‚æ•° `n`ï¼šè¡¨ç¤º**å¤šå°‘ä¸ªè¿ç»­ token æ‹¼æˆ 1 ä¸ª**ã€‚
  æ¯”å¦‚ `n=2`ï¼Œå°±æ˜¯ **2 ä¸ª token æ‹¼æˆ 1 ä¸ª bigram**ã€‚

---

```python
def __call__(self, x):
```

* è¿™é‡Œå®šä¹‰äº† `__call__` æ–¹æ³•ï¼Œ
* ä¹Ÿå°±æ˜¯è¯´ï¼š**è°ƒç”¨è¿™ä¸ªç±»å®ä¾‹çš„æ—¶å€™ï¼Œå°±ç›´æ¥è§¦å‘è¿™ä¸ª forward æ“ä½œ**ï¼Œ
  å¾ˆåƒ PyTorch çš„ forward æ–¹æ³•ã€‚

---

```python
B, T, C = x.shape
```

* æŠŠè¾“å…¥å¼ é‡ `x` çš„ shape è§£åŒ…æˆï¼š

  * `B`: batch size
  * `T`: sequence lengthï¼ˆtoken æ•°ï¼‰
  * `C`: æ¯ä¸ª token çš„ embedding ç»´åº¦

---

```python
x = x.view(B, T//self.n, C*self.n)
```

* å…³é”®æ“ä½œï¼

* é€šè¿‡ `.view()` é‡å¡‘ shapeï¼š

  * åŸæœ¬æœ‰ `T` ä¸ª tokenï¼Œæ¯ä¸ªæ˜¯ `C` ç»´ï¼Œ
  * ç°åœ¨å˜æˆ `T//n` ä¸ª tokenï¼Œæ¯ä¸ª token æ˜¯ `C*n` ç»´ï¼ˆæ‹¼æ¥äº† n ä¸ªåŸå§‹ tokenï¼‰ï¼

* ä¸¾ä¾‹ï¼š

```
åŸ shape:  (B, 8, 10)    â†’ 8 ä¸ª 10 ç»´ token
æ‹¼æ¥ 2 ä¸ª: â†’ (B, 4, 20)  â†’ 4 ä¸ª 20 ç»´ token
```

---

```python
if x.shape[1] == 1:
    x = x.squeeze(1)
```

* å¦‚æœæ‹¼æ¥å®Œåï¼Œ`T//n == 1`ï¼Œ
  ä¹Ÿå°±æ˜¯è¯´ **åªå‰© 1 ä¸ª token**ï¼Œå°±å¤šå‡ºæ¥ä¸€ä¸ªç»´åº¦æ˜¯ 1ï¼Œ
  ç”¨ `.squeeze(1)` å»æ‰è¿™ä¸€ç»´ï¼Œè®©è¾“å‡º shape æ›´å¹²å‡€ã€‚

---

```python
self.out = x
return self.out
```

* ä¿å­˜ç»“æœ `self.out`ï¼ŒåŒæ—¶è¿”å›è¾“å‡ºã€‚

---

```python
def parameters(self):
    return []
```

* è¿™æ˜¯ä¸ºäº†å’Œå…¶å®ƒ PyTorch layer çš„æ¥å£ä¿æŒä¸€è‡´ï¼š
  è¿™ä¸ª layer **ä¸åŒ…å«å‚æ•°**ï¼Œæ‰€ä»¥ `parameters()` è¿”å›ç©ºåˆ—è¡¨ã€‚

---

### æ€»ç»“ï¼š

`FlattenConsecutive(n)` çš„ä½œç”¨æ˜¯ï¼š

âœ… è¾“å…¥å½¢çŠ¶ `(B, T, C)`
âœ… è¾“å‡ºå½¢çŠ¶ `(B, T//n, C * n)`
âœ… æŠŠ **ç›¸é‚» n ä¸ª token æ‹¼æ¥æˆä¸€ä¸ªæ–°çš„ token**ï¼Œembedding ç»´åº¦å¢åŠ 

---

### ä¸¾ä¾‹ï¼š

```python
e = torch.randn(4, 8, 10)
flatten = FlattenConsecutive(2)
out = flatten(e)

# out.shape == (4, 4, 20)
```

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ æ•´ç†ä¸€ä»½â€œFlattenConsecutive åœ¨ WaveNet ç»“æ„ä¸­æ€ä¹ˆç”¨â€çš„å°ä¾‹å­ä»£ç ï¼Œè¦ä¸è¦ï¼ŸğŸŒŸ



# training the WaveNet: first pass

number of parameters comes out to be 22 000 so that's exactly the same that we had before and we have the same amount
of capacity at this neural net in terms of the number of parameters but the question is whether we are utilizing
those parameters in a more efficient architecture so what I did then is I got rid of a lot of the debugging cells here
and I rerun the optimization and scrolling down to the result we see that we get the identical performance roughly
so our validation loss now is 2.029 and previously it was 2.027 so controlling
for the number of parameters changing from the flat to hierarchical is not giving us anything yet that said there are two things
um to point out number one we didn't really torture the um architecture here very much this is just my first guess
and there's a bunch of hyper parameters search that we could do in order in terms of how we allocate uh our budget
of parameters to what layers number two we still may have a bug inside the
bachelor 1D layer so let's take a look at um uh that because it runs but does it
do the right thing so I pulled up the layer inspector sort of that we have here and printed out the

å½“ç„¶ï¼Œè¿™æ®µæˆ‘æ¥å¸®ä½ ç¿»è¯‘è®²è§£ï¼š

---

**# è®­ç»ƒ WaveNetï¼ˆç¬¬ä¸€è½®ï¼‰**

* ç°åœ¨çš„æ¨¡å‹å‚æ•°æ€»æ•°å¤§æ¦‚ **22,000**ï¼Œ
  è¿™ä¸ªå’Œä¹‹å‰çš„ MLP ç»“æ„æ˜¯**ä¸€æ ·çš„å‚æ•°é‡**ï¼Œæ‰€ä»¥**æ¨¡å‹å®¹é‡ï¼ˆcapacityï¼‰æ˜¯ä¸€æ ·çš„**ï¼Œ
  é—®é¢˜æ˜¯ï¼š

  * é€šè¿‡ WaveNet è¿™ç§ç»“æ„ï¼Œ**è¿™äº›å‚æ•°æ˜¯å¦è¢«ç”¨å¾—æ›´æœ‰æ•ˆ**ï¼Ÿ

---

* ç„¶åï¼Œä½œè€…æŠŠå‰é¢çš„ä¸€äº›è°ƒè¯•ç”¨çš„ cell åˆ æ‰ï¼Œé‡æ–°è·‘äº†è®­ç»ƒè¿‡ç¨‹ã€‚

* è®­ç»ƒç»“æœï¼š

  * ç°åœ¨éªŒè¯é›† loss å¤§æ¦‚æ˜¯ **2.029**ï¼Œ
  * ä¹‹å‰çš„ baseline MLP æ˜¯ **2.027**ï¼Œ
    â†’ ä¹Ÿå°±æ˜¯è¯´ï¼Œ**ç›®å‰è¿™è½® WaveNet å’Œ MLP æ•ˆæœå·®ä¸å¤š**ï¼Œæ²¡æœ‰æ˜æ˜¾æå‡ã€‚

---

ä½œè€…æ€»ç»“ä¸¤ç‚¹åŸå› ï¼š

1ï¸âƒ£ **è¿˜æ²¡æœ‰è°ƒè¶…å‚æ•°**

* ç›®å‰çš„ WaveNet æ¶æ„åªæ˜¯ä¸€ä¸ªåˆç‰ˆâ€œçŒœçš„â€ç‰ˆæœ¬ï¼Œ
* è¿˜æ²¡æœ‰ç»†è°ƒ hidden size / channel æ•°é‡ / å±‚æ•° / learning rate ç­‰ç­‰ï¼Œ
* å…¶å®å¯ä»¥èŠ±ç‚¹æ—¶é—´åšè¶…å‚æ•°æœç´¢ï¼ˆhyperparameter searchï¼‰ï¼Œçœ‹çœ‹å‚æ•° budget å¦‚ä½•æ›´å¥½åœ°åˆ†é…åˆ°å„å±‚ï¼Œå¯èƒ½èƒ½æ˜¾è‘—æå‡æ•ˆæœã€‚

---

2ï¸âƒ£ **BatchNorm1D å¯èƒ½è¿˜æœ‰ bug**

* è™½ç„¶ç½‘ç»œèƒ½è·‘ï¼Œä½†æ˜¯ BatchNorm1D å±‚çš„å®ç°è¦ç¡®è®¤ä¸€ä¸‹ï¼Œ
* æ˜¯å¦åœ¨è¿™ç§ 3D tensor åœºæ™¯ä¸‹æ­£ç¡®å·¥ä½œï¼ˆä¹‹å‰æ”¹äº†å±‚ç»“æ„ï¼ŒBatchNorm ç°åœ¨æ¥çš„è¾“å…¥ tensor shape å˜äº†ï¼‰ï¼Œ
* éœ€è¦ä»”ç»†æ£€æŸ¥ BatchNorm å±‚ï¼Œé¿å…å› ä¸ºå®ç°é—®é¢˜å½±å“äº†ç½‘ç»œæ•ˆæœã€‚

---

æ¥ä¸‹æ¥ï¼Œä½œè€…å°±å‡†å¤‡å»çœ‹ BatchNorm å±‚çš„å®ç°å’Œè¡Œä¸ºäº†ã€‚

---

### æ€»ç»“ä¸€å¥è¯ï¼š

**ç¬¬ä¸€ç‰ˆ WaveNet ç»“æ„å¯ä»¥æ­£å¸¸è®­ç»ƒï¼Œå‚æ•°é‡å’Œ MLP æŒå¹³ï¼Œåˆæ­¥æ•ˆæœæŒå¹³ï¼Œæ¥ä¸‹æ¥éœ€è¦è°ƒå‚ + æ£€æŸ¥ BN å±‚å®ç°ï¼Œæ‰å¯èƒ½é‡Šæ”¾ WaveNet æ¶æ„çš„ä¼˜åŠ¿ã€‚**

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥å¸®ä½ æ•´ç†ä¸€ä»½â€œWaveNet ç»“æ„ vs MLP baseline å¯¹æ¯”è¡¨ + ä¸‹ä¸€æ­¥ä¼˜åŒ–è®¡åˆ’â€ï¼Œ
æ–¹ä¾¿ä½ ä»¥åè‡ªå·±åšå®éªŒæ—¶ä¹Ÿå¯ä»¥å‚è€ƒã€‚è¦ä¸è¦ï¼ŸğŸŒŸ


# fixing batchnorm1d bug

shape along the way and currently it looks like the batch form is receiving an input that is 32 by 4 by 68 right and
here on the right I have the current implementation of Bachelor that we have right now now this bachelor assumed in the way we
wrote it and at the time that X is two-dimensional so it was n by D where n
was the batch size so that's why we only reduced uh the mean and the variance over the zeroth dimension but now X will
basically become three-dimensional so what's happening inside the bachelor right now and how come it's working at all and not giving any errors the reason
for that is basically because everything broadcasts properly but the bachelor is not doing what we need what we wanted to
do so in particular let's basically think through what's happening inside the bathroom uh looking at what's what's do
What's Happening Here I have the code here so we're receiving an input of 32 by 4
by 68 and then we are doing uh here x dot mean here I have e instead of X but
we're doing the mean over zero and that's actually giving us 1 by 4 by 68. so we're doing the mean only over the
very first Dimension and it's giving us a mean and a variance that still maintain this Dimension here
so these means are only taking over 32 numbers in the First Dimension and then when we perform this everything
broadcasts correctly still but basically what ends up happening is
when we also look at the running mean
the shape of it so I'm looking at the model that layers at three which is the first bathroom layer and they're looking at whatever the running mean became and
its shape the shape of this running mean now is 1 by 4 by 68.
right instead of it being um you know just a size of dimension
because we have 68 channels we expect to have 68 means and variances that we're maintaining but actually we have an
array of 4 by 68 and so basically what this is telling us is this bash Norm is only
this bachelor is currently working in parallel over
4 times 68 instead of just 68 channels so basically we are maintaining
statistics for every one of these four positions individually and independently
and instead what we want to do is we want to treat this four as a batch Dimension just like the zeroth dimension
so as far as the bachelor is concerned it doesn't want to average we don't want
to average over 32 numbers we want to now average over 32 times four numbers for every single one of these 68
channels and uh so let me now remove this
it turns out that when you look at the documentation of torch.mean
so let's go to torch.me
in one of its signatures when we specify the dimension we see that the dimension here is not
just it can be in or it can also be a tuple of ins so we can reduce over
multiple integers at the same time over multiple Dimensions at the same time so instead of just reducing over zero we
can pass in a tuple 0 1. and here zero one as well and then what's going to happen is the output of
course is going to be the same but now what's going to happen is because we reduce over 0 and 1 if we
look at immin.shape we see that now we've reduced we took
the mean over both the zeroth and the First Dimension so we're just getting 68 numbers and a
bunch of spurious Dimensions here so now this becomes 1 by 1 by 68 and the
running mean and the running variance analogously will become one by one by 68. so even though there are the
spurious Dimensions uh the current the current the correct thing will happen in that we are only maintaining means and
variances for 64 sorry for 68 channels and we're not calculating the mean
variance across 32 times 4 dimensions so that's exactly what we want and let's
change the implementation of bash term 1D that we have so that it can take in two-dimensional or three-dimensional
inputs and perform accordingly so at the end of the day the fix is relatively straightforward basically the dimension
we want to reduce over is either 0 or the Tuple zero and one depending on the dimensionality of X so if x dot and dim
is two so it's a two dimensional tensor then Dimension we want to reduce over is just the integer zero
L if x dot ending is three so it's a three-dimensional tensor then the dims we're going to assume are zero and one
that we want to reduce over and then here we just pass in dim and if the dimensionality of X is
anything else we'll now get an error which is good um so that should be the fix now I want
to point out one more thing we're actually departing from the API of Pi torch here a little bit because when you
come to batch room 1D and pytorch you can scroll down and you can see that the input to this layer can either be n by C
where n is the batch size and C is the number of features or channels or it actually does accept three-dimensional
inputs but it expects it to be n by C by L where LSA like the sequence length or
something like that so um this is problem because you see how C is
nested here in the middle and so when it gets three-dimensional inputs this bash term layer will reduce over zero and two
instead of zero and one so it basically Pi torch batch number one D layer
assumes that c will always be the First Dimension whereas we'll we assume here
that c is the last Dimension and there are some number of batch Dimensions beforehand
um and so it expects n by C or M by C by all we
expect and by C or n by L by C and so it's a deviation
um I think it's okay I prefer it this way honestly so this is the way that we will
keep it for our purposes so I redefined the layers re-initialize the neural net and did a single forward pass with a break just for one step
looking at the shapes along the way they're of course identical all the shapes are the same but the way we see
that things are actually working as we want them to now is that when we look at the bathroom layer the running mean
shape is now one by one by 68. so we're only maintaining 68 means for every one
of our channels and we're treating both the zeroth and the First Dimension as a batch Dimension which is exactly what we
want so let me retrain the neural lot now okay so I retrained the neural net with the bug fix we get a nice curve and

å½“ç„¶ï¼è¿™ä¸€æ®µæ˜¯è®² **BatchNorm1D å±‚çš„ bug ä¿®å¤**ï¼Œå†…å®¹å¾ˆå¤šï¼Œæˆ‘å¸®ä½ ç»†ç»†ç¿»è¯‘è®²è§£ï¼š

---

### èƒŒæ™¯é—®é¢˜ï¼š

åœ¨æ”¹æˆ WaveNet çš„æ ‘çŠ¶ç½‘ç»œåï¼Œä¸­é—´å±‚çš„å¼ é‡ shape å˜æˆäº† **3 ç»´**ï¼š

```
å½“å‰ BatchNorm æ”¶åˆ°çš„è¾“å…¥æ˜¯ï¼š(32, 4, 68)
```

è§£é‡Šï¼š

* 32ï¼šbatch size
* 4ï¼šä¸€å±‚é‡Œæ¯ä¸ªæ ·æœ¬çš„ token æ•°ï¼ˆæ¯”å¦‚ bigram åå˜æˆ 4 ä¸ª tokenï¼‰
* 68ï¼šæ¯ä¸ª token çš„ channel æ•°ï¼ˆhidden dimï¼‰

---

è€Œæˆ‘ä»¬åŸæ¥çš„ BatchNorm å±‚æ˜¯æ‰‹å†™çš„ï¼Œé»˜è®¤åªå¤„ç† **2D** çš„è¾“å…¥ï¼š

```
(N, D) â€”> (batch_size, channel_dim)
```

åŸä»£ç åªå¯¹ç¬¬ 0 ç»´ï¼ˆbatch ç»´ï¼‰æ±‚ mean å’Œ varï¼š

```python
mean = x.mean(dim=0)
var  = x.var(dim=0)
```

---

ä½†æ˜¯ç°åœ¨ x æ˜¯ 3 ç»´çš„ `(32, 4, 68)`ï¼Œ
**BatchNorm åº”è¯¥å¯¹ batch ç»´å’Œ token ç»´ï¼ˆå‰ä¸¤ç»´ï¼‰ä¸€èµ·æ±‚ mean/var**ï¼

---

### ç›®å‰å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ

ç°åœ¨ä»£ç è¿˜èƒ½â€œè·‘â€ï¼Œæ˜¯å› ä¸ºï¼š

* PyTorch å¹¿æ’­æœºåˆ¶è‡ªåŠ¨å¸®å¿™æ‰©å±•ç»´åº¦ï¼Œä»£ç ä¸æŠ¥é”™ï¼›
* ä½†æ˜¯å®é™…ä¸Šï¼Œ**è®¡ç®—å‡ºæ¥çš„ running\_mean / running\_var æ˜¯é”™è¯¯çš„**ï¼

æ¯”å¦‚ï¼š

```python
running_mean.shape â†’ (1, 4, 68)   âŒ
```

å®é™…ä¸Šæˆ‘ä»¬åªå¸Œæœ›æœ‰ï¼š

```python
running_mean.shape â†’ (1, 1, 68)   âœ…
```

ä¹Ÿå°±æ˜¯è¯´ï¼š

âœ… åªå¯¹æ¯ä¸ª channel ç»Ÿè®¡ 68 ä¸ª mean/varï¼Œ
ä¸åº”è¯¥åŒºåˆ† token ä½ç½®ï¼ˆ4ï¼‰ï¼Œ**ä½ç½®ä¸åº”è¯¥å½±å“ç»Ÿè®¡**ï¼Œ
åº”è¯¥æŠŠ batch ç»´å’Œ token ç»´éƒ½å½“æˆ â€œbatch ç»´åº¦â€ æ±‚å¹³å‡ã€‚

---

### å¦‚ä½•ä¿®ï¼Ÿ

PyTorch `mean()` æ”¯æŒå¤šç»´åŒæ—¶ reduceï¼Œå¯ä»¥ä¼  tupleï¼š

```python
mean = x.mean(dim=(0,1))  # å¯¹ batch ç»´ å’Œ token ç»´ åŒæ—¶æ±‚ mean
```

åŒç†ï¼š

```python
var = x.var(dim=(0,1))
```

è¿™æ · running\_mean / running\_var å°±ä¼šæ˜¯ï¼š

```python
shape â†’ (1, 1, 68)
```

---

### ä»£ç ä¿®æ­£é€»è¾‘

```python
if x.ndim == 2:
    dim = 0
elif x.ndim == 3:
    dim = (0, 1)
else:
    raise Error
```

ç„¶åç»Ÿä¸€ä¼  `dim` ç»™ mean/var è®¡ç®—å³å¯ã€‚

---

### PyTorch å®˜æ–¹ BatchNorm1D å·®å¼‚ï¼š

PyTorch çš„å®˜æ–¹ BatchNorm1D æ¥å—ï¼š

```
(N, C) æˆ– (N, C, L)
```

ä¹Ÿå°±æ˜¯è¯´ï¼Œ**C è¿™ä¸ªç»´åº¦å¿…é¡»æ”¾åœ¨ä¸­é—´**ï¼Œ
æˆ‘ä»¬è‡ªå®šä¹‰çš„ç‰ˆæœ¬ï¼Œæ˜¯ï¼š

```
(N, L, C)
```

ä¹Ÿå°±æ˜¯ **C åœ¨æœ€åä¸€ç»´**ï¼Œè¿™æ˜¯ WaveNet ç½‘ç»œé‡Œæ›´æ–¹ä¾¿çš„æ ¼å¼ï¼Œ
è™½ç„¶å’Œå®˜æ–¹ API ä¸ä¸€æ ·ï¼Œ**ä½†ä¸ºäº†å¥½ç”¨ï¼Œç»§ç»­ç”¨è¿™ä¸ªç‰ˆæœ¬å°±å¥½**ï¼

---

### ç»“æœéªŒè¯ï¼š

é‡æ–°å®šä¹‰ BatchNorm å±‚ â†’ é‡æ–°åˆå§‹åŒ–æ¨¡å‹ â†’ åªè·‘ 1 ä¸ª stepï¼Œæ£€æŸ¥ï¼š

```python
running_mean.shape == (1, 1, 68)  âœ…
```

è¯´æ˜ bug ä¿®å¥½äº†ï¼ŒBatchNorm å±‚ç°åœ¨å¯¹ batch å’Œ token ç»´åšäº†æ­£ç¡®çš„å½’ä¸€åŒ–ï¼Œ
ä¸ä¼šæŠŠ token ä½ç½® (4) ä¹Ÿç»Ÿè®¡è¿›å»ã€‚

---

### æ€»ç»“ä¸€å¥è¯ï¼š

**åŸå…ˆ BatchNorm1D å‡å®šè¾“å…¥ 2Dï¼Œç°åœ¨è¾“å…¥æ˜¯ 3Dï¼Œå¿…é¡»æ”¹æˆåŒæ—¶å¯¹ (batch\_dim, token\_dim) å½’ä¸€åŒ–ï¼Œä¿®å®Œ bug åå°±æ­£å¸¸äº†ã€‚**

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ å†™ä¸€ä»½â€œ**æ‰‹å†™ BatchNorm1D å…¼å®¹ 2D / 3D ç‰ˆæœ¬**â€çš„å®Œæ•´ä»£ç æ¨¡æ¿ï¼Œ
ä½ ä»¥åå¯ä»¥ç›´æ¥å¥—ç”¨ï¼è¦ä¸è¦ï¼ŸğŸŒŸ


# re-training WaveNet with bug fix

when we look at the validation performance we do actually see a slight Improvement so we went from 2.029 to
2.022 so basically the bug inside the bathroom was holding up us back like a
little bit it looks like and we are getting a tiny Improvement now but it's not clear if this is statistical significant
um and the reason we slightly expect an improvement is because we're not maintaining so many different means and
variances that are only estimated using using 32 numbers effectively now we are estimating them using 32 times 4 numbers
so you just have a lot more numbers that go into any one estimate of the mean and variance and it allows things to be a
bit more stable and less Wiggly inside those estimates of those statistics so

å½“ç„¶ï¼Œè¿™ä¸€æ®µæˆ‘å¸®ä½ ç¿»è¯‘è®²è§£ï¼š

---

**# ä¿®å¥½ BatchNorm bug åé‡æ–°è®­ç»ƒ WaveNet**

ä¿®å¤äº† BatchNorm çš„ bug ä¹‹åï¼Œé‡æ–°è®­ç»ƒäº†ä¸€è½® WaveNetï¼Œ
è§‚å¯ŸéªŒè¯é›†çš„æ€§èƒ½ï¼Œç¡®å®çœ‹åˆ°äº†ä¸€ç‚¹ç‚¹æå‡ï¼š

âœ… éªŒè¯ loss ä» **2.029 â†’ 2.022**ï¼Œæœ‰ä¸€ä¸ªå°å¹…åº¦çš„æ”¹è¿›ã€‚

---

è¯´æ˜ï¼š

* ä¹‹å‰çš„ bug åœ¨ BatchNorm å±‚ï¼Œ**mean / var ç»Ÿè®¡ä¸å‡†ç¡®**ï¼Œæœ‰ä¸€ç‚¹å½±å“æ€§èƒ½ï¼›
* ä¿®å¤åï¼Œæ•ˆæœç•¥æœ‰æå‡ï¼Œä½†ç›®å‰è¿˜ä¸ç¡®å®šè¿™ä¸ªæå‡æ˜¯ä¸æ˜¯â€œç»Ÿè®¡æ˜¾è‘—çš„â€â€”â€”ä¹Ÿæœ‰å¯èƒ½æ˜¯éšæœºæ³¢åŠ¨ã€‚

---

ä¸ºä»€ä¹ˆç†è®ºä¸Š**æœŸæœ›ä¼šæœ‰æå‡**ï¼Ÿ

âœ… ä¿®å¤å‰ï¼š

* BatchNorm é‡Œåœ¨ `(32, 4, 68)` è¿™ä¸ªå¼ é‡ä¸Šï¼Œ
* æ¯ä¸ª token ä½ç½®å•ç‹¬ç»´æŠ¤ mean/varï¼Œ
* æ¯ç»„ mean/var åªæœ‰ 32 ä¸ªæ ·æœ¬å‚ä¸ç»Ÿè®¡ï¼ˆæ¯æ¬¡åªæœ‰ batch size ä¸ªæ•°ï¼‰

âœ… ä¿®å¤åï¼š

* ç°åœ¨æŠŠ `(batch ç»´ + token ç»´)` ä¸€èµ·å½“æˆ batchï¼Œ
* æ¯ä¸ª channel çš„ mean/var æ˜¯åœ¨ `32 * 4 = 128` ä¸ªæ ·æœ¬ä¸Šç»Ÿè®¡çš„ï¼Œ
* æ•°æ®é‡æ›´å¤§ï¼Œä¼°è®¡æ›´ç¨³å®šï¼Œä¸å®¹æ˜“æŠ–åŠ¨ï¼ˆless wigglyï¼‰ã€‚

---

### æ€»ç»“ä¸€å¥è¯ï¼š

ä¿®å¥½ BatchNorm åï¼Œloss æœ‰å°å¹…åº¦æ”¹å–„ï¼Œ
å› ä¸ºç°åœ¨ mean/var çš„ä¼°è®¡æ›´ç¨³å®šï¼Œç½‘ç»œå­¦å¾—æ›´æ‰å®ä¸€ç‚¹äº†ï¼

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ ç”»ä¸ªå¯¹æ¯”å°å›¾ï¼Œ**ä¿®å¤å‰å loss æ›²çº¿å¯¹æ¯”**ï¼Œè¦ä¸è¦ï¼ŸğŸŒŸ


# scaling up our WaveNet

pretty nice with this more General architecture in place we are now set up to push the performance further by
increasing the size of the network so for example I bumped up the number of embeddings to 24 instead of 10 and also
increased number of hidden units but using the exact same architecture we now have 76 000 parameters and the training
takes a lot longer but we do get a nice curve and then when you actually evaluate the performance we are now
getting validation performance of 1.993 so we've crossed over the 2.0 sort of
territory and right about 1.99 but we are starting to have to wait quite a bit
longer and we're a little bit in the dark with respect to the correct setting of the hyper parameters here and the
learning rates and so on because the experiments are starting to take longer to train and so we are missing sort of like an experimental harness on which we
could run a number of experiments and really tune this architecture very well so I'd like to conclude now with a few

å½“ç„¶ï¼Œè¿™æ®µæˆ‘æ¥å¸®ä½ ç¿»è¯‘è®²è§£ï¼š

---

**# æ‰©å¤§ WaveNet è§„æ¨¡**

ç°åœ¨æˆ‘ä»¬å·²ç»å®ç°äº†è¿™ä¸ª**æ›´é€šç”¨çš„ WaveNet æ¶æ„**ï¼Œ
æ¥ä¸‹æ¥å°±å¯ä»¥é€šè¿‡**æ‰©å¤§ç½‘ç»œè§„æ¨¡**ï¼Œç»§ç»­æå‡æ¨¡å‹æ€§èƒ½ã€‚

---

ä¸¾ä¾‹æ¥è¯´ï¼š

* æˆ‘æŠŠ embedding ç»´åº¦ä» 10 æé«˜åˆ°äº† **24**ï¼Œ
* åŒæ—¶å¢åŠ äº† hidden å±‚çš„é€šé“æ•°ï¼ˆhidden unitsï¼‰ï¼Œ
* ä½†æ˜¯æ•´ä½“æ¶æ„**ä¿æŒä¸å˜**ã€‚

---

è°ƒæ•´åï¼š

* æ¨¡å‹å‚æ•°æ€»æ•°å˜æˆäº† **76,000** ä¸ªï¼ˆåŸæ¥åªæœ‰ 22,000ï¼‰ã€‚
* è®­ç»ƒæ—¶é—´æ˜æ˜¾å˜é•¿äº†ï¼Œ
* ä½†å¾—åˆ°äº†ä¸€æ¡ä¸é”™çš„ loss æ›²çº¿ã€‚

---

æ€§èƒ½æå‡ï¼š

âœ… éªŒè¯é›† loss ç°åœ¨è¾¾åˆ°äº† **1.993**ï¼Œ
âœ… ä¹Ÿå°±æ˜¯è¯´ loss å·²ç»çªç ´äº† â€œ2.0â€ è¿™ä¸ªåŒºé—´ï¼Œè¾¾åˆ° **1.99**ï¼Œ
âœ… ç½‘ç»œè§„æ¨¡å˜å¤§ç¡®å®èƒ½å¸¦æ¥æ•ˆæœæå‡ã€‚

---

ä¸è¿‡ï¼š

* è®­ç»ƒæ—¶é—´è¶Šæ¥è¶Šé•¿ï¼Œ
* è¶…å‚æ•°ï¼ˆhyperparametersï¼‰ã€learning rate ç­‰**è¿˜æ²¡æœ‰ç»†è°ƒ**ï¼Œ
* è®­ç»ƒä¸€ä¸ªå®éªŒç»“æœéœ€è¦æ—¶é—´ï¼Œè°ƒå‚æ•ˆç‡ä½ï¼Œ
* ç°åœ¨**ç¼ºä¹ä¸€ä¸ªæ›´å¥½çš„å®éªŒæ¡†æ¶**ï¼ˆexperimental harnessï¼‰æ¥ç®¡ç†å¤šä¸ªå®éªŒï¼Œè‡ªåŠ¨è°ƒå‚ä¼šæ›´é«˜æ•ˆã€‚

---

### æ€»ç»“ä¸€å¥è¯ï¼š

**æ‰©å¤§ WaveNet è§„æ¨¡ â†’ loss æ˜æ˜¾ä¸‹é™ â†’ ä½†æ˜¯è®­ç»ƒæ…¢ + è°ƒå‚å›°éš¾ï¼Œéœ€è¦æ›´å¥½çš„å®éªŒæ¡†æ¶æ¥ä¼˜åŒ–æ•´ä½“æ•ˆæœ**ã€‚

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ æ•´ç†ä¸€ä»½**WaveNet æ‰©å¤§è§„æ¨¡æ—¶ï¼Œè¶…å‚æ•°è°ƒæ•´å»ºè®®è¡¨**ï¼Œ
æ–¹ä¾¿ä½ ä»¥åç»ƒä¹ æ—¶çŸ¥é“è¯¥è°ƒå“ªäº›å‚æ•°ï¼Œè¦ä¸è¦ï¼ŸğŸŒŸ


# experimental harness

notes we basically improved our performance from a starting of 2.1 down to 1.9 but I don't want that to be the
focus because honestly we're kind of in the dark we have no experimental harness we're just guessing and checking and
this whole thing is terrible we're just looking at the training loss normally you want to look at both the training and the validation loss together and the
whole thing looks different if you're actually trying to squeeze out numbers that said we did implement this
architecture from the wavenet paper but we did not implement this specific uh
forward pass of it where you have a more complicated a linear layer sort of that is this gated linear layer kind of and
there's residual connections and Skip connections and so on so we did not Implement that we just implemented this

å½“ç„¶ï¼Œè¿™ä¸€æ®µæˆ‘æ¥å¸®ä½ ç¿»è¯‘è®²è§£ï¼š

---

**# å®éªŒæ¡†æ¶ï¼ˆexperimental harnessï¼‰**

ç›®å‰æˆ‘ä»¬å…¶å®æŠŠæ¨¡å‹çš„éªŒè¯ loss ä»æœ€åˆçš„ **2.1** æå‡åˆ°äº† **1.9**ï¼Œ
âœ… çœ‹èµ·æ¥æ•ˆæœæå‡äº†ï¼Œ
â—ï¸ ä½†ä½œè€…è¯´ä¸å¸Œæœ›å¤§å®¶å¤ªå…³æ³¨è¿™ä¸ªæ•°å­—ï¼Œä¸ºä»€ä¹ˆå‘¢ï¼Ÿ

---

åŸå› ï¼š

* **æˆ‘ä»¬ç°åœ¨è¿˜æ²¡æœ‰ä¸€ä¸ªå¥½çš„å®éªŒæ¡†æ¶**ï¼Œ
* æ•´ä¸ªè¿‡ç¨‹åŸºæœ¬æ˜¯â€œçŒœ + è¯•â€ï¼Œæ²¡æœ‰ç³»ç»Ÿæ€§ï¼Œ
* ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™ç§æ”¹è¿›**ç¼ºä¹ç§‘å­¦æ€§**ï¼Œæ˜¯é è¿æ°”çš„æˆåˆ†å¤§ï¼Œ
* ç›®å‰è®­ç»ƒè¿‡ç¨‹ä¸­æˆ‘ä»¬åªçœ‹äº† **training loss**ï¼Œ

  * æ­£ç¡®åšæ³•åº”è¯¥æ˜¯åŒæ—¶çœ‹ **training loss + validation loss**ï¼Œ
  * ä¸¤è€…ä¸€èµ·çœ‹ï¼Œæ‰èƒ½åˆ¤æ–­æ˜¯å¦è¿‡æ‹Ÿåˆã€æ˜¯å¦çœŸçš„æ³›åŒ–æå‡ã€‚

---

* å¦‚æœçœŸçš„æƒ³è¦â€œæ¦¨å¹²æ€§èƒ½ï¼ˆsqueeze out numbersï¼‰â€ï¼Œ
  â†’ ä¸€å®šéœ€è¦æ›´ç³»ç»Ÿçš„å®éªŒç®¡ç†æ¡†æ¶ï¼ˆharnessï¼‰ï¼Œ
  â†’ è‡ªåŠ¨åŒ–è·‘å„ç§è¶…å‚æ•°ç»„åˆã€è‡ªåŠ¨è®°å½•ç»“æœã€‚

---

å¦å¤–ï¼š

* è™½ç„¶æˆ‘ä»¬å®ç°äº† WaveNet è®ºæ–‡é‡Œçš„è¿™ç§â€œæ ‘çŠ¶â€æ¶æ„ï¼Œ
* **ä½†è¿˜æ²¡æœ‰å®ç°è®ºæ–‡ä¸­çš„ gated linear unit (GLU) ç»“æ„**ï¼Œ

  * è®ºæ–‡é‡Œ forward pass æ›´å¤æ‚ï¼ŒåŒ…æ‹¬ï¼š
    âœ… gated linear layersï¼ˆé—¨æ§çº¿æ€§å±‚ï¼‰
    âœ… residual connectionsï¼ˆæ®‹å·®è¿æ¥ï¼‰
    âœ… skip connectionsï¼ˆè·³è·ƒè¿æ¥ï¼‰

---

ä¹Ÿå°±æ˜¯è¯´ï¼š

âœ… æˆ‘ä»¬ç›®å‰å®ç°äº† WaveNet çš„åŸºæœ¬æ€æƒ³ç»“æ„ï¼Œ
âŒ ä½†è¿˜æ²¡æœ‰å®Œæ•´å®ç°è®ºæ–‡é‡Œæ›´å¼ºå¤§çš„ forward ç»“æ„ï¼ˆå¸¦é—¨æ§ + æ®‹å·®çš„ç‰ˆæœ¬ï¼‰ã€‚

---

### æ€»ç»“ä¸€å¥è¯ï¼š

**ç›®å‰æå‡åˆ° 1.9 çš„ lossï¼Œæ˜¯é æ‰‹åŠ¨è°ƒè¯•å¾—åˆ°çš„ï¼Œç¼ºä¹ç³»ç»Ÿçš„å®éªŒæ¡†æ¶ï¼›
å®Œæ•´çš„ WaveNet forward ç»“æ„è¿˜æœ‰å¾ˆå¤šç»†èŠ‚æ²¡å®ç°ï¼Œåç»­å¦‚æœè¡¥ä¸Šï¼Œæ•ˆæœåº”è¯¥è¿˜èƒ½è¿›ä¸€æ­¥æå‡ã€‚**

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ æ•´ç†ä¸€ä»½**å®Œæ•´çš„ WaveNet forward ç»“æ„ç»„ä»¶æ¸…å•**ï¼Œ
æ–¹ä¾¿ä½ ä»¥åæŒ‰è®ºæ–‡ä¸€æ­¥æ­¥å®ç°ï¼Œè¦ä¸è¦ï¼ŸğŸŒŸ


# WaveNet but with â€œdilated causal convolutionsâ€

structure I would like to briefly hint or preview how what we've done here relates to convolutional neural networks
as used in the wavenet paper and basically the use of convolutions is strictly for efficiency it doesn't
actually change the model we've implemented so here for example let me look at a specific name to work
with an example so there's a name in our training set and it's DeAndre and it has
seven letters so that is eight independent examples in our model so all these rows here are independent examples
of the Android now you can forward of course any one of these rows independently so I can take
my model and call call it on any individual index notice by the way here
I'm being a little bit tricky the reason for this is that extra at seven that shape is just
um one dimensional array of eight so you can't actually call the model on it you're going to get an error because
there's no batch dimension so when you do extra at
a list of seven then the shape of this becomes one by eight so I get an extra batch dimension of one and then we can
forward the model so that forwards a single example and you
might imagine that you actually may want to forward all of these eight um at the same time
so pre-allocating some memory and then doing a for Loop eight times and forwarding all of those eight here will
give us all the logits in all these different cases now for us with the model as we've implemented it right now this is eight
independent calls to our model but what convolutions allow you to do is it allow you to basically slide this
model efficiently over the input sequence and so this for Loop can be
done not outside in Python but inside of kernels in Cuda and so this for Loop
gets hidden into the convolution so the convolution basically you can cover this it's a for Loop applying a
little linear filter over space of some input sequence and in our case the space
we're interested in is one dimensional and we're interested in sliding these filters over the input data
so this diagram actually is fairly good as well basically what we've done is here they
are highlighting in Black one individ one single sort of like tree of this calculation so just calculating the
single output example here um and so this is basically what we've
implemented here we've implemented a single this black structure we've implemented that and calculated a single
output like a single example but what collusions allow you to do is it allows you to take this black
structure and kind of like slide it over the input sequence here and calculate
all of these orange outputs at the same time or here that corresponds to
calculating all of these outputs of um at all the positions of DeAndre at
the same time and the reason that this is much more efficient is because number one as I
mentioned the for Loop is inside the Cuda kernels in the sliding so that
makes it efficient but number two notice the variable reuse here for example if we look at this circle this node here
this node here is the right child of this node but is also the left child of
the node here and so basically this node and its value is used twice
and so right now in this naive way we'd have to recalculate it but here we are
allowed to reuse it so in the convolutional neural network you think of these linear layers that we
have up above as filters and we take these filters and they're linear filters
and you slide them over input sequence and we calculate the first layer and then the second layer and then the third
layer and then the output layer of the sandwich and it's all done very efficiently using these convolutions
so we're going to cover that in a future video the second thing I hope you took away from this video is you've seen me

å½“ç„¶ï¼Œè¿™æ®µæˆ‘æ¥å¸®ä½ è¯¦ç»†ç¿»è¯‘è®²è§£ï¼š

---

**# å¸¦â€œæ‰©å¼ å› æœå·ç§¯â€çš„ WaveNet**

ä½œè€…åœ¨è¿™é‡Œç®€å•é¢„å‘Šäº†ä¸€ä¸‹ â€œ**æ‰©å¼ å› æœå·ç§¯ï¼ˆdilated causal convolutionsï¼‰**â€ å’Œæˆ‘ä»¬ç›®å‰å®ç°çš„ WaveNet æœ‰ä»€ä¹ˆå…³ç³»ã€‚

---

**æ ¸å¿ƒè§‚ç‚¹**ï¼š

* è®ºæ–‡é‡Œç”¨å·ç§¯ï¼ˆconvolutional neural networks, CNNï¼‰å…¶å®**ä¸æ˜¯ä¸ºäº†æ”¹æ¨¡å‹ç»“æ„**ï¼Œ
* çº¯ç²¹æ˜¯**ä¸ºäº†æé«˜è®¡ç®—æ•ˆç‡**ï¼Œå¹¶ä¸æ”¹å˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚
* æ¢å¥è¯è¯´ï¼š**æˆ‘ä»¬ç°åœ¨çš„ WaveNet ç»“æ„ï¼Œå’Œç”¨å·ç§¯å®ç°å‡ºæ¥çš„æ˜¯â€œåŠŸèƒ½ç­‰æ•ˆçš„â€**ã€‚

---

### ä¸¾ä¾‹ï¼š

è®­ç»ƒé›†é‡Œæœ‰ä¸ªåå­— `DeAndre`ï¼Œé•¿åº¦ 7 ä¸ªå­—æ¯ï¼Œ
â†’ å¯¹æˆ‘ä»¬æ¥è¯´ï¼Œè¿™ç›¸å½“äº 8 ä¸ªæ ·æœ¬ï¼ˆåŒ…æ‹¬èµ·å§‹ tokenï¼‰ã€‚

ç›®å‰æˆ‘ä»¬å®ç°çš„æ¨¡å‹ï¼š

* ä½ å¯ä»¥æ‰‹åŠ¨ **å•ç‹¬ forward è¿™ 8 ä¸ªæ ·æœ¬**ï¼Œ
* æ¯”å¦‚ï¼š

```python
model(x[0])  
model(x[1])  
...
```

æˆ–è€…ç”¨ for-loop æ‰¹é‡ forward 8 ä¸ªã€‚

---

### ç°åœ¨çš„é—®é¢˜ï¼š

**è¿™ç§æ‰‹åŠ¨ for-loop æ˜¯ä½æ•ˆçš„**ï¼

* æ¯ä¸€ä¸ª forward è°ƒç”¨éƒ½æ˜¯**ç‹¬ç«‹è®¡ç®—**ï¼Œ
* æ²¡æœ‰å¤ç”¨ä¸­é—´ç»“æœï¼Œ
* å¾ˆå¤šåœ°æ–¹ä¼šé‡å¤è®¡ç®—ï¼Œé€Ÿåº¦æ…¢ã€‚

---

### å·ç§¯èƒ½åšä»€ä¹ˆï¼Ÿ

**å·ç§¯ = ç”¨ä¸€ä¸ª sliding window æ»‘åŠ¨è®¡ç®—**ï¼Œ
æœ¬è´¨ä¸Šç›¸å½“äºï¼š

```text
for i in range(len(x)):
    out[i] = linear_filter( x[i:i+window_size] )
```

**å¥½å¤„**ï¼š

1ï¸âƒ£ **for-loop æ˜¯å†™åœ¨ Cuda kernel é‡Œçš„ï¼Œä¸æ˜¯ Python çš„å¾ªç¯ï¼Œé€Ÿåº¦å¿«**ï¼
2ï¸âƒ£ **ä¸­é—´ç»“æœå¯ä»¥å¤ç”¨**ï¼š

ä¸¾ä¾‹ï¼š

```
          parent
         /      \
    left_child  right_child
```

* æŸä¸ªèŠ‚ç‚¹è®¡ç®—å®Œåï¼Œå·¦å³ child èŠ‚ç‚¹å¯ä»¥è¢«å¤šæ¬¡å¤ç”¨ï¼Œ
* è€Œæˆ‘ä»¬ç›®å‰ naive å®ç°æ˜¯â€œæ¯æ¬¡é‡å¤ç®—ä¸€éâ€ï¼Œæµªè´¹æ—¶é—´ã€‚

---

### å›¾è§£ï¼š

è®ºæ–‡é‡Œçš„å›¾ä¹Ÿå¾ˆå¥½è¯´æ˜äº†ï¼š

* ç›®å‰æˆ‘ä»¬å®ç°çš„æ˜¯â€œé»‘è‰²çš„æ ‘â€ï¼šåªè®¡ç®—ä¸€ä¸ª outputï¼Œ
* å·ç§¯å¯ä»¥**å¹³ç§»è¿™æ£µæ ‘**ï¼Œæ»‘åŠ¨æ•´ä¸ªåºåˆ—ï¼Œ
* åŒæ—¶è®¡ç®—å‡ºâ€œæ©™è‰²çš„æ‰€æœ‰ outputâ€ã€‚

ä¸¾ä¾‹ï¼š

```text
DeAndre

ç”¨ç›®å‰ naive å®ç°ï¼šfor å¾ªç¯ 8 æ¬¡ï¼Œæ¯æ¬¡ç®—ä¸€ä¸ª output
ç”¨å·ç§¯å®ç°ï¼šä¸€éæ»‘è¿‡å»ï¼Œ8 ä¸ª output ä¸€æ¬¡ç®—å‡ºæ¥ï¼Œæ•ˆç‡é«˜
```

---

### æ€»ç»“ä¸€å¥è¯ï¼š

âœ… æˆ‘ä»¬ç›®å‰å®ç°çš„ WaveNet æ ‘çŠ¶ç»“æ„ å’Œ è®ºæ–‡çš„ CNN ç‰ˆæ˜¯**åŒä¸€ä¸ªæ¨¡å‹**ï¼Œ
âœ… CNN çš„ä½œç”¨åªæ˜¯ â€œä¼˜åŒ–è®¡ç®—é€Ÿåº¦ + å¤ç”¨ä¸­é—´å˜é‡â€ï¼Œæœ¬è´¨è¡¨è¾¾èƒ½åŠ›æ²¡å˜ï¼Œ
âœ… æœªæ¥ä¼šç”¨ dilated causal convolutions æ¥å®ç°æ›´é«˜æ•ˆç‰ˆã€‚

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ æ•´ç†ä¸€ä»½\*\*â€œç›®å‰å®ç° vs å·ç§¯å®ç°çš„å¯¹æ¯”è¡¨â€\*\*ï¼Œ
å¸®ä½ æ›´å¥½ç†è§£ä¸ºä»€ä¹ˆå·ç§¯ä¼šå¿«å¾ˆå¤šï¼Œè¦ä¸è¦ï¼ŸğŸŒŸ


# torch.nn

basically Implement all of these layer Lego building blocks or module building
blocks and I'm implementing them over here and we've implemented a number of layers together and we've also
implemented these these containers and we've overall pytorchified our code
quite a bit more now basically what we're doing here is we're re-implementing torch.nn which is
the neural networks library on top of torch.tensor and it looks very much like
this except it is much better because because it's in pi torch instead of jingling my Jupiter notebook so I think
going forward I will probably have considered us having unlocked um torch.nn we understand roughly what's
in there how these modules work how they're nested and what they're doing on top of torture tensor so hopefully we'll
just uh we'll just switch over and continue and start using torch.net directly the next thing I hope you got a

# the development process of building deep neural nets

bit of a sense of is what the development process of building deep neural networks looks like which I think
was relatively representative to some extent so number one we are spending a lot of time in the documentation page of
pytorch and we're reading through all the layers looking at documentations where the shapes of the inputs what can
they be what does the layer do and so on unfortunately I have to say the patreon's documentation is not are very
good they spend a ton of time on Hardcore engineering of all kinds of distributed Primitives Etc but as far as
I can tell no one is maintaining any documentation it will lie to you it will be wrong it will be incomplete it will
be unclear so unfortunately it is what it is and you just kind of do your best
um with what they've given us um number two
uh the other thing that I hope you got a sense of is there's a ton of trying to make the shapes work and there's a lot
of gymnastics around these multi-dimensional arrays and are they two-dimensional three-dimensional four-dimensional uh what layers take
what shapes is it NCL or NLC and you're promoting and viewing and it just can
get pretty messy and so that brings me to number three I very often prototype these layers and implementations in
jupyter notebooks and make sure that all the shapes work out and I'm spending a lot of time basically babysitting the
shapes and making sure everything is correct and then once I'm satisfied with the functionality in the Jupiter notebook I will take that code and copy
paste it into my repository of actual code that I'm training with and so then
I'm working with vs code on the side so I usually have jupyter notebook and vs code I develop in Jupiter notebook I
paste into vs code and then I kick off experiments from from the reaper of course from the code repository so
that's roughly some notes on the development process of working with neurons lastly I think this lecture unlocks a lot of potential further

# going forward

lectures because number one we have to convert our neural network to actually use these dilated causal convolutional
layers so implementing the comnet number two potentially starting to get into
what this means whatever residual connections and Skip connections and why are they useful
number three we as I mentioned we don't have any experimental harness so right now I'm just guessing checking
everything this is not representative of typical deep learning workflows you have to set up your evaluation harness you
can kick off experiments you have lots of arguments that your script can take you're you're kicking off a lot of experimentation you're looking at a lot
of plots of training and validation losses and you're looking at what is working and what is not working and you're working on this like population
level and you're doing all these hyper parameter searches and so we've done none of that so far so how to set that
up and how to make it good I think as a whole another topic number three we
should probably cover recurring neural networks RNs lstm's grooves and of course Transformers so many uh places to
go and we'll cover that in the future for now bye sorry I forgot to say that

# improve on my loss! how far can we improve a WaveNet on this data?

if you are interested I think it is kind of interesting to try to beat this number 1.993 because I really haven't
tried a lot of experimentation here and there's quite a bit of fruit potentially to still purchase further so I haven't
tried any other ways of allocating these channels in this neural net maybe the number of dimensions for the embedding
is all wrong maybe it's possible to actually take the original network with just one hidden layer and make it big
enough and actually beat my fancy hierarchical Network it's not obvious that would be kind of embarrassing if
this did not do better even once you torture it a little bit maybe you can read the weight net paper and try to
figure out how some of these layers work and Implement them yourselves using what we have and of course you can always tune some
of the initialization or some of the optimization and see if you can improve it that way so I'd be curious if people
can come up with some ways to beat this and yeah that's it for now bye