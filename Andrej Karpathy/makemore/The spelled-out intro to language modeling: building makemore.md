- [makemore](#makemore)
    - [Usage](#usage)
    - [License](#license)
- [makemore](#makemore-1)
    - [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
    - [è®¸å¯è¯](#è®¸å¯è¯)
- [makemore.py introduction](#makemorepy-introduction)
- [The spelled-out intro to language modeling: building makemore è§†é¢‘ä»‹ç»](#the-spelled-out-intro-to-language-modeling-building-makemore-è§†é¢‘ä»‹ç»)
- [intro](#intro)
- [ä»‹ç»](#ä»‹ç»)
- [reading and exploring the dataset](#reading-and-exploring-the-dataset)
- [è¯»å–å’Œæ¢ç´¢æ•°æ®é›†](#è¯»å–å’Œæ¢ç´¢æ•°æ®é›†)
- [exploring the bigrams in the dataset](#exploring-the-bigrams-in-the-dataset)
- [æ¢ç´¢æ•°æ®é›†ä¸­çš„äºŒå…ƒç»„](#æ¢ç´¢æ•°æ®é›†ä¸­çš„äºŒå…ƒç»„)
- [counting bigrams in a python dictionary](#counting-bigrams-in-a-python-dictionary)
- [åœ¨ Python å­—å…¸ä¸­ç»Ÿè®¡äºŒå…ƒç»„ï¼ˆbigramsï¼‰](#åœ¨-python-å­—å…¸ä¸­ç»Ÿè®¡äºŒå…ƒç»„bigrams)
    - [ä¸€èˆ¬å½¢å¼ï¼š](#ä¸€èˆ¬å½¢å¼)
    - [ä¸¾ä¾‹è¯´æ˜ï¼š](#ä¸¾ä¾‹è¯´æ˜)
      - [âœ… ç¤ºä¾‹ 1ï¼šæ™®é€šå‡½æ•° vs lambda](#-ç¤ºä¾‹-1æ™®é€šå‡½æ•°-vs-lambda)
      - [âœ… ç¤ºä¾‹ 2ï¼šé…åˆ sorted ä½¿ç”¨](#-ç¤ºä¾‹-2é…åˆ-sorted-ä½¿ç”¨)
      - [âœ… ç¤ºä¾‹ 3ï¼šé…åˆ `map()` ä½¿ç”¨](#-ç¤ºä¾‹-3é…åˆ-map-ä½¿ç”¨)
    - [æ€»ç»“ï¼š](#æ€»ç»“)
- [counting bigrams in a 2D torch tensor ("training the model")](#counting-bigrams-in-a-2d-torch-tensor-training-the-model)
- [ç”¨ 2D Torch å¼ é‡ç»Ÿè®¡ bigramï¼ˆâ€œè®­ç»ƒæ¨¡å‹â€ï¼‰](#ç”¨-2d-torch-å¼ é‡ç»Ÿè®¡-bigramè®­ç»ƒæ¨¡å‹)
    - [âœ… æ­¥éª¤ 1ï¼šå¯¼å…¥ PyTorch](#-æ­¥éª¤-1å¯¼å…¥-pytorch)
    - [âœ… æ­¥éª¤ 2ï¼šåˆ›å»ºå¼ é‡ç¤ºä¾‹](#-æ­¥éª¤-2åˆ›å»ºå¼ é‡ç¤ºä¾‹)
    - [âœ… æ­¥éª¤ 3ï¼šæ„å»º 28x28 çš„å¤§å¼ é‡](#-æ­¥éª¤-3æ„å»º-28x28-çš„å¤§å¼ é‡)
    - [âœ… æ­¥éª¤ 4ï¼šå­—ç¬¦è½¬æ•´æ•°çš„æ˜ å°„ï¼ˆlookup è¡¨ï¼‰](#-æ­¥éª¤-4å­—ç¬¦è½¬æ•´æ•°çš„æ˜ å°„lookup-è¡¨)
    - [âœ… æ­¥éª¤ 5ï¼šå¡«å……å¼ é‡ï¼ˆå³ bigram ç»Ÿè®¡ï¼‰](#-æ­¥éª¤-5å¡«å……å¼ é‡å³-bigram-ç»Ÿè®¡)
    - [âœ… æ€»ç»“ï¼š](#-æ€»ç»“)
    - [ğŸ” åˆ†æ­¥è§£é‡Šï¼š](#-åˆ†æ­¥è§£é‡Š)
      - [ç¬¬ä¸€æ­¥ï¼š`''.join(words)`](#ç¬¬ä¸€æ­¥joinwords)
      - [ç¬¬äºŒæ­¥ï¼š`set(...)`](#ç¬¬äºŒæ­¥set)
    - [âœ… ç”¨æ³•åœºæ™¯](#-ç”¨æ³•åœºæ™¯)
    - [ğŸ§  ä¸¾ä¸ªä¾‹å­å†æ€»ç»“ï¼š](#-ä¸¾ä¸ªä¾‹å­å†æ€»ç»“)
    - [âœ… æ€»ç»“ä¸€å¥è¯ï¼š](#-æ€»ç»“ä¸€å¥è¯)
- [visualizing the bigram tensor](#visualizing-the-bigram-tensor)
- [å¯è§†åŒ– bigram å¼ é‡](#å¯è§†åŒ–-bigram-å¼ é‡)
    - [âœ… ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ matplotlib ç®€å•å¯è§†åŒ–](#-ç¬¬ä¸€æ­¥ä½¿ç”¨-matplotlib-ç®€å•å¯è§†åŒ–)
    - [âœ… ç¬¬äºŒæ­¥ï¼šæ„é€ æ›´ç¾è§‚çš„å¯è§†åŒ–](#-ç¬¬äºŒæ­¥æ„é€ æ›´ç¾è§‚çš„å¯è§†åŒ–)
    - [âœ… ç¬¬ä¸‰æ­¥ï¼šå®Œæ•´å¯è§†åŒ–é€»è¾‘](#-ç¬¬ä¸‰æ­¥å®Œæ•´å¯è§†åŒ–é€»è¾‘)
    - [âœ… è¡¥å……è§£é‡Šï¼š](#-è¡¥å……è§£é‡Š)
    - [âœ… æ€»ç»“ï¼š](#-æ€»ç»“-1)
    - [ğŸ”¸ `import matplotlib.pyplot as plt`](#-import-matplotlibpyplot-as-plt)
    - [ğŸ”¸ `%matplotlib inline`](#-matplotlib-inline)
    - [ğŸ”¸ `plt.imshow(N)`](#-pltimshown)
    - [âœ… ä¸¾ä¸ªä¾‹å­ï¼š](#-ä¸¾ä¸ªä¾‹å­)
    - [âœ… æ€»ç»“ï¼š](#-æ€»ç»“-2)
    - [ğŸ”¹ `import matplotlib.pyplot as plt`](#-import-matplotlibpyplot-as-plt-1)
    - [ğŸ”¹ `%matplotlib inline`](#-matplotlib-inline-1)
    - [ğŸ”¹ `plt.figure(figsize=(16, 16))`](#-pltfigurefigsize16-16)
    - [ğŸ”¹ `plt.imshow(N, cmap='Blues')`](#-pltimshown-cmapblues)
    - [ğŸ”¹ åŒå±‚ `for` å¾ªç¯ï¼šé€æ ¼æ ‡æ³¨å­—ç¬¦å’Œè®¡æ•°](#-åŒå±‚-for-å¾ªç¯é€æ ¼æ ‡æ³¨å­—ç¬¦å’Œè®¡æ•°)
      - [ğŸ”¸ `chstr = itos[i] + itos[j]`](#-chstr--itosi--itosj)
      - [ğŸ”¸ `plt.text(j, i, chstr, ha='center', va='bottom', color='gray')`](#-plttextj-i-chstr-hacenter-vabottom-colorgray)
      - [ğŸ”¸ `plt.text(j, i, N[i, j].item(), ha='center', va='top', color='gray')`](#-plttextj-i-ni-jitem-hacenter-vatop-colorgray)
    - [ğŸ”¹ `plt.axis('off')`](#-pltaxisoff)
    - [âœ… æœ€ç»ˆæ•ˆæœï¼š](#-æœ€ç»ˆæ•ˆæœ)
    - [ğŸ¯ æ€»ç»“ä¸€å¥è¯ï¼š](#-æ€»ç»“ä¸€å¥è¯-1)
- [deleting spurious (S) and (E) tokens in favor of a single . token](#deleting-spurious-s-and-e-tokens-in-favor-of-a-single--token)
- [åˆ é™¤å¤šä½™çš„ (S) å’Œ (E) æ ‡è®°ï¼Œæ”¹ç”¨ä¸€ä¸ªç»Ÿä¸€çš„ `.` ç‰¹æ®Šç¬¦å·](#åˆ é™¤å¤šä½™çš„-s-å’Œ-e-æ ‡è®°æ”¹ç”¨ä¸€ä¸ªç»Ÿä¸€çš„--ç‰¹æ®Šç¬¦å·)
    - [âœ… æˆ‘ä»¬å°†åšå‡ºä»¥ä¸‹æ”¹å˜ï¼š](#-æˆ‘ä»¬å°†åšå‡ºä»¥ä¸‹æ”¹å˜)
    - [âœ… é¢å¤–ç¾åŒ–å¤„ç†ï¼š](#-é¢å¤–ç¾åŒ–å¤„ç†)
    - [âœ… æ›´æ–°åçš„è¡Œä¸ºï¼š](#-æ›´æ–°åçš„è¡Œä¸º)
    - [âœ… æ€»ç»“ï¼š](#-æ€»ç»“-3)
- [sampling from the model](#sampling-from-the-model)
- [ä»æ¨¡å‹ä¸­è¿›è¡Œé‡‡æ ·ï¼ˆSampling from the modelï¼‰](#ä»æ¨¡å‹ä¸­è¿›è¡Œé‡‡æ ·sampling-from-the-model)
    - [ğŸ”¹ æ€»ä½“æµç¨‹](#-æ€»ä½“æµç¨‹)
  - [âœ… æ­¥éª¤è¯¦è§£](#-æ­¥éª¤è¯¦è§£)
    - [ğŸ”¸ ç¬¬ä¸€æ­¥ï¼šå–èµ·å§‹è¡Œï¼ˆå³ç‚¹å·å¼€å¤´çš„é¢‘ç‡åˆ†å¸ƒï¼‰](#-ç¬¬ä¸€æ­¥å–èµ·å§‹è¡Œå³ç‚¹å·å¼€å¤´çš„é¢‘ç‡åˆ†å¸ƒ)
    - [ğŸ”¸ ç¬¬äºŒæ­¥ï¼šå°† raw count è½¬ä¸ºæ¦‚ç‡åˆ†å¸ƒ](#-ç¬¬äºŒæ­¥å°†-raw-count-è½¬ä¸ºæ¦‚ç‡åˆ†å¸ƒ)
    - [ğŸ”¸ ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨ `torch.multinomial` æŒ‰æ¦‚ç‡é‡‡æ ·](#-ç¬¬ä¸‰æ­¥ä½¿ç”¨-torchmultinomial-æŒ‰æ¦‚ç‡é‡‡æ ·)
    - [ğŸ”¸ ç¬¬å››æ­¥ï¼šå¾ªç¯é‡‡æ ·å®Œæ•´å•è¯](#-ç¬¬å››æ­¥å¾ªç¯é‡‡æ ·å®Œæ•´å•è¯)
    - [ğŸ”¸ ç¤ºä¾‹é‡‡æ ·ä»£ç ç®€åŒ–ç‰ˆï¼š](#-ç¤ºä¾‹é‡‡æ ·ä»£ç ç®€åŒ–ç‰ˆ)
    - [ğŸ”¸ å¤šæ¬¡é‡‡æ ·å¤šä¸ªåå­—ï¼š](#-å¤šæ¬¡é‡‡æ ·å¤šä¸ªåå­—)
  - [ğŸ§  æ¨¡å‹çš„å®é™…æ•ˆæœ](#-æ¨¡å‹çš„å®é™…æ•ˆæœ)
  - [âœ… ä¸å…¶ä»–æƒ…å†µå¯¹æ¯”ï¼š](#-ä¸å…¶ä»–æƒ…å†µå¯¹æ¯”)
    - [ğŸ“‰ ä½¿ç”¨éšæœºå‡åŒ€åˆ†å¸ƒï¼ˆå®Œå…¨æœªè®­ç»ƒçš„æ¨¡å‹ï¼‰ï¼š](#-ä½¿ç”¨éšæœºå‡åŒ€åˆ†å¸ƒå®Œå…¨æœªè®­ç»ƒçš„æ¨¡å‹)
    - [ğŸ“ˆ ä½¿ç”¨è®­ç»ƒè¿‡çš„ bigram æ¨¡å‹ï¼š](#-ä½¿ç”¨è®­ç»ƒè¿‡çš„-bigram-æ¨¡å‹)
  - [âœ… æ€»ç»“](#-æ€»ç»“-4)
  - [ğŸ¯ å…³é”®ç»“è®º](#-å…³é”®ç»“è®º)
  - [âœ… å‡½æ•°åŸå‹ï¼š](#-å‡½æ•°åŸå‹)
  - [âœ… å‚æ•°è§£é‡Šï¼š](#-å‚æ•°è§£é‡Š)
  - [âœ… è¿”å›å€¼ï¼š](#-è¿”å›å€¼)
  - [âœ… ç¤ºä¾‹ 1ï¼šä»æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·](#-ç¤ºä¾‹-1ä»æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·)
  - [âœ… ç¤ºä¾‹ 2ï¼šå¤šæ¬¡é‡‡æ · + æœ‰æ”¾å›](#-ç¤ºä¾‹-2å¤šæ¬¡é‡‡æ ·--æœ‰æ”¾å›)
  - [âœ… ç¤ºä¾‹ 3ï¼šè®¾å®šéšæœºç§å­ï¼ˆä¿è¯å¯å¤ç°ï¼‰](#-ç¤ºä¾‹-3è®¾å®šéšæœºç§å­ä¿è¯å¯å¤ç°)
  - [â— æ³¨æ„äº‹é¡¹ï¼š](#-æ³¨æ„äº‹é¡¹)
  - [âœ… åº”ç”¨åœºæ™¯ï¼ˆè¯­è¨€æ¨¡å‹ä¸­ï¼‰ï¼š](#-åº”ç”¨åœºæ™¯è¯­è¨€æ¨¡å‹ä¸­)
  - [âœ… æ€»ç»“ä¸€å¥è¯ï¼š](#-æ€»ç»“ä¸€å¥è¯-2)
- [efficiency! vectorized normalization of the rows, tensor broadcasting](#efficiency-vectorized-normalization-of-the-rows-tensor-broadcasting)
- [æé«˜æ•ˆç‡ï¼ç”¨å‘é‡åŒ–æ–¹æ³•å½’ä¸€åŒ– bigram å¼ é‡çš„æ¯ä¸€è¡Œï¼ˆTensor Broadcastingï¼‰](#æé«˜æ•ˆç‡ç”¨å‘é‡åŒ–æ–¹æ³•å½’ä¸€åŒ–-bigram-å¼ é‡çš„æ¯ä¸€è¡Œtensor-broadcasting)
  - [ğŸ¯ é—®é¢˜èƒŒæ™¯](#-é—®é¢˜èƒŒæ™¯)
  - [âœ… ä¼˜åŒ–ç›®æ ‡](#-ä¼˜åŒ–ç›®æ ‡)
  - [ğŸ”§ å®ç°æ­¥éª¤](#-å®ç°æ­¥éª¤)
    - [1. è½¬ä¸º float ç±»å‹](#1-è½¬ä¸º-float-ç±»å‹)
    - [2. å¯¹æ¯ä¸€è¡Œåšå½’ä¸€åŒ–ï¼ˆè€Œä¸æ˜¯å¯¹æ•´ä¸ªçŸ©é˜µï¼‰](#2-å¯¹æ¯ä¸€è¡Œåšå½’ä¸€åŒ–è€Œä¸æ˜¯å¯¹æ•´ä¸ªçŸ©é˜µ)
      - [è®¡ç®—æ¯ä¸€è¡Œçš„æ€»å’Œï¼š](#è®¡ç®—æ¯ä¸€è¡Œçš„æ€»å’Œ)
      - [æ‰§è¡Œé™¤æ³•ï¼š](#æ‰§è¡Œé™¤æ³•)
  - [ğŸ§  å…³äº Broadcastingï¼ˆå¹¿æ’­æœºåˆ¶ï¼‰](#-å…³äº-broadcastingå¹¿æ’­æœºåˆ¶)
    - [âœ… ç¤ºä¾‹ï¼š](#-ç¤ºä¾‹)
  - [âš ï¸ Bug è­¦å‘Šï¼šä¸è¦å¿˜è®° `keepdim=True`](#ï¸-bug-è­¦å‘Šä¸è¦å¿˜è®°-keepdimtrue)
  - [âœ… æ­£ç¡® vs é”™è¯¯å¯¹æ¯”ï¼š](#-æ­£ç¡®-vs-é”™è¯¯å¯¹æ¯”)
  - [ğŸ§  ç»“è®ºï¼šRespect Broadcasting](#-ç»“è®ºrespect-broadcasting)
  - [ğŸ›  æ•ˆç‡å»ºè®®](#-æ•ˆç‡å»ºè®®)
  - [âœ… æ€»ç»“ä¸€å¥è¯ï¼š](#-æ€»ç»“ä¸€å¥è¯-3)
- [loss function (the negative log likelihood of the data under our model)](#loss-function-the-negative-log-likelihood-of-the-data-under-our-model)
    - [ğŸ¯ æ¦‚è¿°ï¼šæˆ‘ä»¬å·²ç»è®­ç»ƒäº†ä¸€ä¸ª Bigram è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€šè¿‡ç»Ÿè®¡æ¯å¯¹å­—ç¬¦å‡ºç°çš„é¢‘ç‡æ¥å»ºç«‹ï¼Œç„¶åå½’ä¸€åŒ–å¾—åˆ°ä¸€ä¸ª**æ¦‚ç‡çŸ©é˜µ** `P`ï¼Œè¯¥çŸ©é˜µè¡¨ç¤ºæ¯ä¸ªå­—ç¬¦åæ¥å¦ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡ã€‚](#-æ¦‚è¿°æˆ‘ä»¬å·²ç»è®­ç»ƒäº†ä¸€ä¸ª-bigram-è¯­è¨€æ¨¡å‹å®ƒé€šè¿‡ç»Ÿè®¡æ¯å¯¹å­—ç¬¦å‡ºç°çš„é¢‘ç‡æ¥å»ºç«‹ç„¶åå½’ä¸€åŒ–å¾—åˆ°ä¸€ä¸ªæ¦‚ç‡çŸ©é˜µ-pè¯¥çŸ©é˜µè¡¨ç¤ºæ¯ä¸ªå­—ç¬¦åæ¥å¦ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡)
    - [ğŸ§  ä¸ºä»€ä¹ˆä½¿ç”¨å¯¹æ•°ä¼¼ç„¶ï¼Ÿ](#-ä¸ºä»€ä¹ˆä½¿ç”¨å¯¹æ•°ä¼¼ç„¶)
    - [ğŸš¨ ä½†é—®é¢˜æ¥äº†ï¼š](#-ä½†é—®é¢˜æ¥äº†)
    - [â— æŸå¤±å‡½æ•°çš„è¯­ä¹‰æ˜¯ï¼š**è¶Šå°è¶Šå¥½**ã€‚](#-æŸå¤±å‡½æ•°çš„è¯­ä¹‰æ˜¯è¶Šå°è¶Šå¥½)
    - [ğŸ“ é€šå¸¸æˆ‘ä»¬è¿˜ä¼š**å¹³å‡åŒ–æŸå¤±**ï¼Œä»¥ä¾¿ä¸åŒé•¿åº¦çš„å¥å­/æ ·æœ¬èƒ½å…¬å¹³æ¯”è¾ƒï¼š](#-é€šå¸¸æˆ‘ä»¬è¿˜ä¼šå¹³å‡åŒ–æŸå¤±ä»¥ä¾¿ä¸åŒé•¿åº¦çš„å¥å­æ ·æœ¬èƒ½å…¬å¹³æ¯”è¾ƒ)
    - [âœ… æ€»ç»“é€»è¾‘é“¾ï¼š](#-æ€»ç»“é€»è¾‘é“¾)
    - [ğŸ“Œ ä¸¾ä¾‹ï¼š](#-ä¸¾ä¾‹)
    - [ğŸ’¡ å»¶ä¼¸ï¼š](#-å»¶ä¼¸)
- [model smoothing with fake counts](#model-smoothing-with-fake-counts)
    - [ğŸ§ª ã€èƒŒæ™¯é—®é¢˜ï¼šæ¨¡å‹å¯¹æœªè§è¿‡çš„ bigram ç»™å‡ºé›¶æ¦‚ç‡ã€‘](#-èƒŒæ™¯é—®é¢˜æ¨¡å‹å¯¹æœªè§è¿‡çš„-bigram-ç»™å‡ºé›¶æ¦‚ç‡)
    - [ğŸ˜¬ é—®é¢˜åˆ†æ](#-é—®é¢˜åˆ†æ)
    - [âœ… è§£å†³æ–¹æ¡ˆï¼š**æ¨¡å‹å¹³æ»‘ï¼ˆModel Smoothingï¼‰**](#-è§£å†³æ–¹æ¡ˆæ¨¡å‹å¹³æ»‘model-smoothing)
      - [ğŸ”§ æ“ä½œæ–¹æ³•ï¼š](#-æ“ä½œæ–¹æ³•)
    - [ğŸŒŠ å¹³æ»‘ç¨‹åº¦å¯è°ƒ](#-å¹³æ»‘ç¨‹åº¦å¯è°ƒ)
    - [ğŸ§¾ æ•ˆæœåˆ†æï¼š](#-æ•ˆæœåˆ†æ)
    - [ğŸ“Œ æ€»ç»“ä¸€å¥è¯ï¼š](#-æ€»ç»“ä¸€å¥è¯-4)
- [PART 2: the neural network approach: intro](#part-2-the-neural-network-approach-intro)
  - [ğŸ”¢ ç¬¬äºŒéƒ¨åˆ†ï¼šç¥ç»ç½‘ç»œæ–¹æ³•ç®€ä»‹](#-ç¬¬äºŒéƒ¨åˆ†ç¥ç»ç½‘ç»œæ–¹æ³•ç®€ä»‹)
  - [ğŸ¤– ç°åœ¨ï¼Œæˆ‘ä»¬å°†é‡‡ç”¨ä¸€ä¸ª**å®Œå…¨ä¸åŒçš„æ–¹å¼** â€”â€” ç”¨ç¥ç»ç½‘ç»œæ¥åšï¼](#-ç°åœ¨æˆ‘ä»¬å°†é‡‡ç”¨ä¸€ä¸ªå®Œå…¨ä¸åŒçš„æ–¹å¼--ç”¨ç¥ç»ç½‘ç»œæ¥åš)
    - [ğŸ¯ æ–°ç›®æ ‡ï¼šæŠŠ bigram å­—ç¬¦çº§è¯­è¨€æ¨¡å‹ **è½¬åŒ–ä¸ºç¥ç»ç½‘ç»œä»»åŠ¡**ã€‚](#-æ–°ç›®æ ‡æŠŠ-bigram-å­—ç¬¦çº§è¯­è¨€æ¨¡å‹-è½¬åŒ–ä¸ºç¥ç»ç½‘ç»œä»»åŠ¡)
  - [ğŸ§  è®­ç»ƒæ–¹æ³•ï¼š](#-è®­ç»ƒæ–¹æ³•)
  - [ğŸ” æ€»ç»“ä¸€ä¸‹æµç¨‹ï¼š](#-æ€»ç»“ä¸€ä¸‹æµç¨‹)
- [creating the bigram dataset for the neural net](#creating-the-bigram-dataset-for-the-neural-net)
  - [ğŸ“Š ä¸ºç¥ç»ç½‘ç»œåˆ›å»º bigram æ•°æ®é›†](#-ä¸ºç¥ç»ç½‘ç»œåˆ›å»º-bigram-æ•°æ®é›†)
    - [ğŸ›  æ­¥éª¤è§£æ](#-æ­¥éª¤è§£æ)
    - [ğŸ§© Bigram ç»“æ„ä¸¾ä¾‹ï¼š](#-bigram-ç»“æ„ä¸¾ä¾‹)
    - [ğŸ§® ä»£ç é€»è¾‘ï¼š](#-ä»£ç é€»è¾‘)
    - [ğŸ“¦ ç¤ºä¾‹è¾“å‡ºï¼ˆä»¥ `"emma"` ä¸ºä¾‹ï¼‰ï¼š](#-ç¤ºä¾‹è¾“å‡ºä»¥-emma-ä¸ºä¾‹)
  - [âš ï¸ å°å¿ƒ Tensor çš„æ„å»ºæ–¹å¼ï¼](#ï¸-å°å¿ƒ-tensor-çš„æ„å»ºæ–¹å¼)
    - [âœ… æ€»ç»“å»ºè®®ï¼š](#-æ€»ç»“å»ºè®®)
- [feeding integers into neural nets? one-hot encodings](#feeding-integers-into-neural-nets-one-hot-encodings)
  - [ğŸ¯ å°†æ•´æ•°è¾“å…¥ç¥ç»ç½‘ç»œï¼Ÿä½¿ç”¨ One-hot ç¼–ç ](#-å°†æ•´æ•°è¾“å…¥ç¥ç»ç½‘ç»œä½¿ç”¨-one-hot-ç¼–ç )
    - [âŒ é—®é¢˜ï¼šæ•´æ•°ä¸èƒ½ç›´æ¥ä½œä¸ºç¥ç»ç½‘ç»œè¾“å…¥](#-é—®é¢˜æ•´æ•°ä¸èƒ½ç›´æ¥ä½œä¸ºç¥ç»ç½‘ç»œè¾“å…¥)
    - [âœ… è§£å†³æ–¹æ¡ˆï¼šOne-hot ç¼–ç ](#-è§£å†³æ–¹æ¡ˆone-hot-ç¼–ç )
    - [ğŸ’¡ PyTorch ä¸­çš„ One-hot ç¼–ç ](#-pytorch-ä¸­çš„-one-hot-ç¼–ç )
    - [ğŸ“Š ç¤ºä¾‹ç»“æœï¼š](#-ç¤ºä¾‹ç»“æœ)
    - [ğŸ“ˆ å¯è§†åŒ–ï¼š](#-å¯è§†åŒ–)
    - [âš ï¸ å°å¿ƒæ•°æ®ç±»å‹ï¼](#ï¸-å°å¿ƒæ•°æ®ç±»å‹)
  - [âœ… æ€»ç»“ï¼š](#-æ€»ç»“-5)
- [the "neural net": one linear layer of neurons implemented with matrix multiplication](#the-neural-net-one-linear-layer-of-neurons-implemented-with-matrix-multiplication)
  - [ğŸ§ ã€Œç¥ç»ç½‘ç»œã€çš„ç¬¬ä¸€å±‚ï¼šç”¨çŸ©é˜µä¹˜æ³•å®ç°çš„çº¿æ€§å±‚ï¼ˆLinear Layerï¼‰](#ç¥ç»ç½‘ç»œçš„ç¬¬ä¸€å±‚ç”¨çŸ©é˜µä¹˜æ³•å®ç°çš„çº¿æ€§å±‚linear-layer)
    - [ğŸ¯ ä¸€ä¸ªç¥ç»å…ƒçš„è®¡ç®—è¿‡ç¨‹å›é¡¾ï¼š](#-ä¸€ä¸ªç¥ç»å…ƒçš„è®¡ç®—è¿‡ç¨‹å›é¡¾)
    - [ğŸ›  ç¬¬ä¸€æ­¥ï¼šå®šä¹‰æƒé‡ W](#-ç¬¬ä¸€æ­¥å®šä¹‰æƒé‡-w)
    - [ğŸ§® ç¬¬äºŒæ­¥ï¼šè¿›è¡ŒçŸ©é˜µä¹˜æ³•](#-ç¬¬äºŒæ­¥è¿›è¡ŒçŸ©é˜µä¹˜æ³•)
    - [ğŸ¯ æ‹“å±•ï¼šç”¨ 27 ä¸ªç¥ç»å…ƒä»£æ›¿ 1 ä¸ª](#-æ‹“å±•ç”¨-27-ä¸ªç¥ç»å…ƒä»£æ›¿-1-ä¸ª)
    - [ğŸ§ª éªŒè¯ï¼šç‚¹ç§¯ç¡®å®æ˜¯è¿™ä¹ˆæ¥çš„](#-éªŒè¯ç‚¹ç§¯ç¡®å®æ˜¯è¿™ä¹ˆæ¥çš„)
    - [âœ… æ€»ç»“](#-æ€»ç»“-6)
- [transforming neural net outputs into probabilities: the softmax](#transforming-neural-net-outputs-into-probabilities-the-softmax)
  - [ğŸ” å°†ç¥ç»ç½‘ç»œçš„è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡ï¼šSoftmax å‡½æ•°](#-å°†ç¥ç»ç½‘ç»œçš„è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡softmax-å‡½æ•°)
    - [ğŸ§  æˆ‘ä»¬æƒ³è®©è¾“å‡ºä»£è¡¨ä»€ä¹ˆï¼Ÿ](#-æˆ‘ä»¬æƒ³è®©è¾“å‡ºä»£è¡¨ä»€ä¹ˆ)
    - [â“å¦‚ä½•æŠŠè¿™äº›è¾“å‡ºå˜æˆâ€œæ¦‚ç‡â€ï¼Ÿ](#å¦‚ä½•æŠŠè¿™äº›è¾“å‡ºå˜æˆæ¦‚ç‡)
    - [ğŸ§® Softmax çš„æ“ä½œè¿‡ç¨‹ï¼š](#-softmax-çš„æ“ä½œè¿‡ç¨‹)
    - [âœ… å¾—åˆ°çš„ç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ](#-å¾—åˆ°çš„ç»“æœæ˜¯ä»€ä¹ˆ)
    - [ğŸ”„ ä¸¾ä¸ªä¾‹å­ï¼š](#-ä¸¾ä¸ªä¾‹å­-1)
    - [ğŸ¯ ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåšï¼Ÿ](#-ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåš)
    - [ğŸ”š æ€»ç»“](#-æ€»ç»“-7)
- [summary, preview to next steps, reference to micrograd](#summary-preview-to-next-steps-reference-to-micrograd)
    - [ğŸ§© æ•´ä½“ç»“æ„å’Œæµç¨‹å›é¡¾ï¼š](#-æ•´ä½“ç»“æ„å’Œæµç¨‹å›é¡¾)
    - [ğŸ“‰ æŸå¤±è®¡ç®—ï¼šNegative Log Likelihood Lossï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰](#-æŸå¤±è®¡ç®—negative-log-likelihood-lossè´Ÿå¯¹æ•°ä¼¼ç„¶)
    - [ğŸ² ä¸ºä»€ä¹ˆ loss å¯èƒ½é«˜ï¼Ÿ](#-ä¸ºä»€ä¹ˆ-loss-å¯èƒ½é«˜)
    - [âš ï¸ è¿™ä¸æ˜¯è®­ç»ƒï¼Œè¿™åªæ˜¯ forward è¿‡ç¨‹ï¼](#ï¸-è¿™ä¸æ˜¯è®­ç»ƒè¿™åªæ˜¯-forward-è¿‡ç¨‹)
    - [ğŸ” ä¸ micrograd å¯¹æ¯”ï¼š](#-ä¸-micrograd-å¯¹æ¯”)
    - [âœ… å°ç»“ï¼š](#-å°ç»“)
- [vectorized loss](#vectorized-loss)
    - [ğŸ§  åŸæ–‡ç¿»è¯‘ + è§£é‡Šï¼š](#-åŸæ–‡ç¿»è¯‘--è§£é‡Š)
    - [âœ… PyTorch å®ç°å‘é‡ç´¢å¼•ï¼š](#-pytorch-å®ç°å‘é‡ç´¢å¼•)
    - [ğŸ§® ç„¶åè®¡ç®—æŸå¤±ï¼š](#-ç„¶åè®¡ç®—æŸå¤±)
    - [ğŸ§¾ ç»“æœï¼š](#-ç»“æœ)
    - [ğŸ“Œ æ€»ç»“ï¼š](#-æ€»ç»“-8)
- [backward and update, in PyTorch](#backward-and-update-in-pytorch)
- [putting everything together](#putting-everything-together)
- [note 1: one-hot encoding really just selects a row of the next Linear layer's weight matrix](#note-1-one-hot-encoding-really-just-selects-a-row-of-the-next-linear-layers-weight-matrix)
- [note 2: model smoothing as regularization loss](#note-2-model-smoothing-as-regularization-loss)
- [sampling from the neural net](#sampling-from-the-neural-net)
- [conclusion](#conclusion)

# makemore

makemore takes one text file as input, where each line is assumed to be one training thing, and generates more things like it. Under the hood, it is an autoregressive character-level language model, with a wide choice of models from bigrams all the way to a Transformer (exactly as seen in GPT). For example, we can feed it a database of names, and makemore will generate cool baby name ideas that all sound name-like, but are not already existing names. Or if we feed it a database of company names then we can generate new ideas for a name of a company. Or we can just feed it valid scrabble words and generate english-like babble.

This is not meant to be too heavyweight library with a billion switches and knobs. It is one hackable file, and is mostly intended for educational purposes. [PyTorch](https://pytorch.org) is the only requirement.

Current implementation follows a few key papers:

- Bigram (one character predicts the next one with a lookup table of counts)
- MLP, following [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
- CNN, following [DeepMind WaveNet 2016](https://arxiv.org/abs/1609.03499) (in progress...)
- RNN, following [Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)
- LSTM, following [Graves et al. 2014](https://arxiv.org/abs/1308.0850)
- GRU, following [Kyunghyun Cho et al. 2014](https://arxiv.org/abs/1409.1259)
- Transformer, following [Vaswani et al. 2017](https://arxiv.org/abs/1706.03762)

### Usage

The included `names.txt` dataset, as an example, has the most common 32K names takes from [ssa.gov](https://www.ssa.gov/oact/babynames/) for the year 2018. It looks like:

```
emma
olivia
ava
isabella
sophia
charlotte
...
```

Let's point the script at it:

```bash
$ python makemore.py -i names.txt -o names
```

Training progress and logs and model will all be saved to the working directory `names`. The default model is a super tiny 200K param transformer; Many more training configurations are available - see the argparse and read the code. Training does not require any special hardware, it runs on my Macbook Air and will run on anything else, but if you have a GPU then training will fly faster. As training progresses the script will print some samples throughout. However, if you'd like to sample manually, you can use the `--sample-only` flag, e.g. in a separate terminal do:

```bash
$ python makemore.py -i names.txt -o names --sample-only
```

This will load the best model so far and print more samples on demand. Here are some unique baby names that get eventually generated from current default settings (test logprob of ~1.92, though much lower logprobs are achievable with some hyperparameter tuning):

```
dontell
khylum
camatena
aeriline
najlah
sherrith
ryel
irmi
taislee
mortaz
akarli
maxfelynn
biolett
zendy
laisa
halliliana
goralynn
brodynn
romima
chiyomin
loghlyn
melichae
mahmed
irot
helicha
besdy
ebokun
lucianno
```

Have fun!

### License

MIT

# makemore

makemore æ¥å—ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ä½œä¸ºè¾“å…¥ï¼Œå…¶ä¸­æ¯ä¸€è¡Œè¢«è§†ä¸ºä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œå¹¶ç”Ÿæˆç±»ä¼¼çš„å†…å®¹ã€‚åœ¨åå°ï¼Œå®ƒæ˜¯ä¸€ä¸ªè‡ªå›å½’çš„å­—ç¬¦çº§è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒä»äºŒå…ƒç»„åˆ°Transformerï¼ˆæ­£å¦‚åœ¨GPTä¸­çœ‹åˆ°çš„ï¼‰ç­‰å¤šç§æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ç»™å®ƒä¸€ä¸ªåå­—çš„æ•°æ®åº“ï¼Œmakemore å°†ç”Ÿæˆå¬èµ·æ¥åƒåå­—çš„é…·å©´å„¿åå­—å»ºè®®ï¼Œä½†è¿™äº›åå­—å¹¶ä¸æ˜¯å·²å­˜åœ¨çš„åå­—ã€‚æˆ–è€…ï¼Œå¦‚æœæˆ‘ä»¬ç»™å®ƒä¸€ä¸ªå…¬å¸åç§°çš„æ•°æ®åº“ï¼Œå®ƒå°±èƒ½ç”Ÿæˆæ–°çš„å…¬å¸åç§°åˆ›æ„ã€‚æˆ–è€…æˆ‘ä»¬å¯ä»¥ç»™å®ƒæœ‰æ•ˆçš„æ‹¼å­—æ¸¸æˆå•è¯ï¼Œmakemore å°†ç”Ÿæˆç±»ä¼¼è‹±è¯­çš„èƒ¡è¨€ä¹±è¯­ã€‚

è¿™ä¸æ˜¯ä¸€ä¸ªå¤æ‚çš„åº“ï¼Œæ²¡æœ‰äº¿ä¸‡ä¸ªå¼€å…³å’ŒæŒ‰é’®ã€‚å®ƒåªæ˜¯ä¸€ä¸ªå¯ä»¥ä¿®æ”¹çš„æ–‡ä»¶ï¼Œä¸»è¦ç”¨äºæ•™è‚²ç›®çš„ã€‚å”¯ä¸€çš„ä¾èµ–æ˜¯ PyTorchã€‚

å½“å‰çš„å®ç°å‚è€ƒäº†å‡ ç¯‡å…³é”®è®ºæ–‡ï¼š

* Bigramï¼ˆä¸€ä¸ªå­—ç¬¦é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œé€šè¿‡æŸ¥æ‰¾è®¡æ•°è¡¨ï¼‰
* MLPï¼Œå‚è€ƒ Bengio ç­‰äºº 2003
* CNNï¼Œå‚è€ƒ DeepMind WaveNet 2016ï¼ˆè¿›è¡Œä¸­...ï¼‰
* RNNï¼Œå‚è€ƒ Mikolov ç­‰äºº 2010
* LSTMï¼Œå‚è€ƒ Graves ç­‰äºº 2014
* GRUï¼Œå‚è€ƒ Kyunghyun Cho ç­‰äºº 2014
* Transformerï¼Œå‚è€ƒ Vaswani ç­‰äºº 2017

### ä½¿ç”¨æ–¹æ³•

æ‰€åŒ…å«çš„ `names.txt` æ•°æ®é›†ä½œä¸ºç¤ºä¾‹ï¼ŒåŒ…å«äº†æ¥è‡ª ssa.gov çš„2018å¹´æœ€å¸¸è§çš„32Kä¸ªåå­—ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

```
emma
olivia
ava
isabella
sophia
charlotte
...
```

æˆ‘ä»¬å¯ä»¥è¿™æ ·è¿è¡Œè„šæœ¬ï¼š

```
$ python makemore.py -i names.txt -o names
```

è®­ç»ƒè¿›åº¦ã€æ—¥å¿—å’Œæ¨¡å‹å°†ä¼šä¿å­˜åˆ°å·¥ä½œç›®å½• `names` ä¸­ã€‚é»˜è®¤æ¨¡å‹æ˜¯ä¸€ä¸ªéå¸¸å°çš„200Kå‚æ•°çš„Transformerï¼›æ›´å¤šçš„è®­ç»ƒé…ç½®å¯ç”¨â€”â€”è¯·æŸ¥çœ‹ argparse å¹¶é˜…è¯»ä»£ç ã€‚è®­ç»ƒä¸éœ€è¦ä»»ä½•ç‰¹æ®Šç¡¬ä»¶ï¼Œå®ƒå¯ä»¥åœ¨æˆ‘çš„ Macbook Air ä¸Šè¿è¡Œï¼Œä¹Ÿå¯ä»¥åœ¨å…¶ä»–ä»»ä½•è®¾å¤‡ä¸Šè¿è¡Œï¼Œä½†å¦‚æœæœ‰ GPUï¼Œè®­ç»ƒä¼šæ›´å¿«ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œè„šæœ¬ä¼šå®šæœŸæ‰“å°ä¸€äº›æ ·æœ¬ã€‚å¦‚æœä½ æƒ³æ‰‹åŠ¨é‡‡æ ·ï¼Œå¯ä»¥ä½¿ç”¨ `--sample-only` æ ‡å¿—ï¼Œä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªå•ç‹¬çš„ç»ˆç«¯ä¸­æ‰§è¡Œï¼š

```
$ python makemore.py -i names.txt -o names --sample-only
```

è¿™å°†åŠ è½½åˆ°ç›®å‰ä¸ºæ­¢è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œå¹¶æŒ‰éœ€æ‰“å°æ›´å¤šæ ·æœ¬ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›åœ¨å½“å‰é»˜è®¤è®¾ç½®ä¸‹æœ€ç»ˆç”Ÿæˆçš„ç‹¬ç‰¹å©´å„¿åå­—ï¼ˆæµ‹è¯•å¯¹æ•°æ¦‚ç‡çº¦ä¸º1.92ï¼Œå°½ç®¡é€šè¿‡è°ƒæ•´è¶…å‚æ•°å¯ä»¥è¾¾åˆ°æ›´ä½çš„å¯¹æ•°æ¦‚ç‡ï¼‰ï¼š

```
dontell
khylum
camatena
aeriline
najlah
sherrith
ryel
irmi
taislee
mortaz
akarli
maxfelynn
biolett
zendy
laisa
halliliana
goralynn
brodynn
romima
chiyomin
loghlyn
melichae
mahmed
irot
helicha
besdy
ebokun
lucianno
```

ç©å¾—å¼€å¿ƒï¼

### è®¸å¯è¯

MIT

# makemore.py introduction

you give this script some words (one per line) and it will generate more things like it.
uses super state of the art Transformer AI tech
this code is intended to be super hackable. tune it to your needs.

Changes from minGPT:
- I removed the from_pretrained function where we init with GPT2 weights
- I removed dropout layers because the models we train here are small,
  it's not necessary to understand at this stage and at this scale.
- I removed weight decay and all of the complexity around what parameters are
  and are not weight decayed. I don't believe this should make a massive
  difference at the scale that we operate on here.

ä½ ç»™è¿™ä¸ªè„šæœ¬ä¸€äº›å•è¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰ï¼Œå®ƒå°†ç”Ÿæˆæ›´å¤šç±»ä¼¼çš„å†…å®¹ã€‚
ä½¿ç”¨æœ€å…ˆè¿›çš„ Transformer AI æŠ€æœ¯ã€‚
è¿™æ®µä»£ç æ—¨åœ¨éå¸¸æ˜“äºä¿®æ”¹ã€‚æ ¹æ®ä½ çš„éœ€æ±‚è¿›è¡Œè°ƒæ•´ã€‚

ä¸ minGPT çš„å˜åŒ–ï¼š

* æˆ‘ç§»é™¤äº† `from_pretrained` å‡½æ•°ï¼ŒåŸæœ¬ç”¨äºåˆå§‹åŒ– GPT2 æƒé‡ã€‚
* æˆ‘ç§»é™¤äº† dropout å±‚ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è¿™é‡Œè®­ç»ƒçš„æ¨¡å‹å¾ˆå°ï¼Œåœ¨è¿™ä¸ªé˜¶æ®µå’Œè§„æ¨¡ä¸‹ä¸éœ€è¦ã€‚
* æˆ‘ç§»é™¤äº†æƒé‡è¡°å‡ä»¥åŠå…³äºå“ªäº›å‚æ•°éœ€è¦è¡°å‡ã€å“ªäº›ä¸éœ€è¦çš„æ‰€æœ‰å¤æ‚æ€§ã€‚æˆ‘è®¤ä¸ºåœ¨æˆ‘ä»¬æ“ä½œçš„è§„æ¨¡ä¸‹ï¼Œè¿™ä¸ä¼šäº§ç”Ÿå·¨å¤§å·®å¼‚ã€‚

# The spelled-out intro to language modeling: building makemore è§†é¢‘ä»‹ç»

We implement a bigram character-level language model, which we will further complexify in followup videos into a modern Transformer language model, like GPT. In this video, the focus is on (1) introducing torch.Tensor and its subtleties and use in efficiently evaluating neural networks and (2) the overall framework of language modeling that includes model training, sampling, and the evaluation of a loss (e.g. the negative log likelihood for classification).

Links:
- makemore on github: https://github.com/karpathy/makemore
- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-t...
- my website: https://karpathy.ai
- my twitter:   / karpathy  
- (new) Neural Networks: Zero to Hero series Discord channel:   / discord   , for people who'd like to chat more and go beyond youtube comments

Useful links for practice:
- Python + Numpy tutorial from CS231n https://cs231n.github.io/python-numpy... . We use torch.tensor instead of numpy.- array in this video. Their design (e.g. broadcasting, data types, etc.) is so similar that practicing one is basically practicing the other, just be careful with some of the APIs - how various functions are named, what arguments they take, etc. - these details can vary.
- PyTorch tutorial on Tensor https://pytorch.org/tutorials/beginne...
- Another PyTorch intro to Tensor https://pytorch.org/tutorials/beginne...

Exercises:
- E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?
- E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?
- E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?
- E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?
- E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?
- E06: meta-exercise! Think of a fun/interesting exercise and complete it.

Chapters:
- 00:00:00 intro
- 00:03:03 reading and exploring the dataset
- 00:06:24 exploring the bigrams in the dataset
- 00:09:24 counting bigrams in a python dictionary
- 00:12:45 counting bigrams in a 2D torch tensor ("training the model")
- 00:18:19 visualizing the bigram tensor
- 00:20:54 deleting spurious (S) and (E) tokens in favor of a single . token
- 00:24:02 sampling from the model
- 00:36:17 efficiency! vectorized normalization of the rows, tensor broadcasting 
- 00:50:14 loss function (the negative log likelihood of the data under our model)
- 01:00:50 model smoothing with fake counts
- 01:02:57 PART 2: the neural network approach: intro
- 01:05:26 creating the bigram dataset for the neural net
- 01:10:01 feeding integers into neural nets? one-hot encodings
- 01:13:53 the "neural net": one linear layer of neurons implemented with matrix multiplication
- 01:18:46 transforming neural net outputs into probabilities: the softmax
- 01:26:17 summary, preview to next steps, reference to micrograd
- 01:35:49 vectorized loss
- 01:38:36 backward and update, in PyTorch
- 01:42:55 putting everything together
- 01:47:49 note 1: one-hot encoding really just selects a row of the next Linear layer's weight matrix
- 01:50:18 note 2: model smoothing as regularization loss
- 01:54:31 sampling from the neural net
- 01:56:16 conclusion

æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªäºŒå…ƒç»„ï¼ˆbigramï¼‰å­—ç¬¦çº§è¯­è¨€æ¨¡å‹ï¼Œä¹‹åæˆ‘ä»¬å°†åœ¨åç»­è§†é¢‘ä¸­å°†å…¶é€æ­¥å¤æ‚åŒ–ï¼Œæœ€ç»ˆå‘å±•æˆåƒ GPT é‚£æ ·çš„ç°ä»£ Transformer è¯­è¨€æ¨¡å‹ã€‚
æœ¬è§†é¢‘çš„é‡ç‚¹åœ¨äºï¼š

1. ä»‹ç» `torch.Tensor`ï¼ŒåŒ…æ‹¬å…¶ç»†èŠ‚å’Œåœ¨é«˜æ•ˆè¯„ä¼°ç¥ç»ç½‘ç»œä¸­çš„ä½¿ç”¨æ–¹å¼ï¼›
2. è®²è§£è¯­è¨€å»ºæ¨¡çš„æ•´ä½“æ¡†æ¶ï¼ŒåŒ…æ‹¬æ¨¡å‹è®­ç»ƒã€é‡‡æ ·ï¼Œä»¥åŠæŸå¤±çš„è¯„ä¼°ï¼ˆä¾‹å¦‚åˆ†ç±»ä»»åŠ¡ä¸­çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰ã€‚

---

ğŸ”— **ç›¸å…³é“¾æ¥ï¼š**

* **makemore é¡¹ç›® GitHub**ï¼š[https://github.com/karpathy/makemore](https://github.com/karpathy/makemore)
* **æœ¬è§†é¢‘ä¸­æ„å»ºçš„ Jupyter Notebook**ï¼š[https://github.com/karpathy/nn-zero-t](https://github.com/karpathy/nn-zero-t)...
* **æˆ‘çš„ç½‘ç«™**ï¼š[https://karpathy.ai](https://karpathy.ai)
* **æˆ‘çš„æ¨ç‰¹**ï¼š[@karpathy](https://twitter.com/karpathy)
* **æ–°å»ºçš„ç¥ç»ç½‘ç»œã€Œä»é›¶åˆ°ç²¾é€šã€ç³»åˆ— Discord é¢‘é“**ï¼šç”¨äºæ›´æ·±å…¥äº¤æµï¼Œé€‚åˆä¸æ»¡è¶³äºåªçœ‹ YouTube è¯„è®ºçš„æœ‹å‹ã€‚

---

ğŸ›  **ç»ƒä¹ æ¨èï¼š**

* **E01**ï¼šè®­ç»ƒä¸€ä¸ªä¸‰å…ƒç»„ï¼ˆtrigramï¼‰è¯­è¨€æ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¾“å…¥ä¸¤ä¸ªå­—ç¬¦æ¥é¢„æµ‹ç¬¬ä¸‰ä¸ªå­—ç¬¦ã€‚ä½ å¯ä»¥ä½¿ç”¨è®¡æ•°æ–¹å¼æˆ–ç¥ç»ç½‘ç»œã€‚è¯„ä¼°æŸå¤±ï¼Œçœ‹çœ‹æ˜¯å¦ä¼˜äº bigram æ¨¡å‹ï¼Ÿ
* **E02**ï¼šå°†æ•°æ®é›†éšæœºåˆ’åˆ†ä¸º 80% è®­ç»ƒé›†ã€10% éªŒè¯é›†ã€10% æµ‹è¯•é›†ã€‚åˆ†åˆ«åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒ bigram å’Œ trigram æ¨¡å‹ï¼Œå¹¶åœ¨éªŒè¯é›†ä¸æµ‹è¯•é›†ä¸Šè¯„ä¼°æ€§èƒ½ã€‚ä½ è§‚å¯Ÿåˆ°äº†ä»€ä¹ˆï¼Ÿ
* **E03**ï¼šä½¿ç”¨éªŒè¯é›†è°ƒèŠ‚ trigram æ¨¡å‹ä¸­çš„å¹³æ»‘ï¼ˆæˆ–æ­£åˆ™åŒ–ï¼‰å¼ºåº¦â€”â€”å°è¯•å¤šä¸ªè®¾ç½®ï¼Œå¹¶è§‚å¯Ÿå“ªä¸ªåœ¨éªŒè¯é›†ä¸Šçš„æŸå¤±æœ€å°ã€‚åœ¨è®­ç»ƒé›†ä¸éªŒè¯é›†æŸå¤±éšå¹³æ»‘å¼ºåº¦å˜åŒ–æ—¶ä½ è§‚å¯Ÿåˆ°ä»€ä¹ˆè§„å¾‹ï¼Ÿç”¨æœ€ä¼˜è®¾ç½®åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ä¸€æ¬¡æœ€ç»ˆæŸå¤±ã€‚
* **E04**ï¼šæˆ‘ä»¬çœ‹åˆ° one-hot å‘é‡åªæ˜¯ç”¨äºé€‰æ‹©çŸ©é˜µ W çš„æŸä¸€è¡Œï¼Œå› æ­¤æ˜¾å¼ç”Ÿæˆ one-hot å‘é‡æœ‰äº›æµªè´¹ã€‚ä½ èƒ½å¦å»æ‰ `F.one_hot` çš„ç”¨æ³•ï¼Œæ”¹ä¸ºç›´æ¥ç´¢å¼•çŸ©é˜µçš„è¡Œï¼Ÿ
* **E05**ï¼šæŸ¥é˜…å¹¶ä½¿ç”¨ `F.cross_entropy`ï¼Œå®ƒåº”è¯¥èƒ½å¾—åˆ°ç›¸åŒçš„ç»“æœã€‚ä½ èƒ½æƒ³åˆ°ä¸ºä»€ä¹ˆæˆ‘ä»¬æ›´æ„¿æ„ç”¨ `F.cross_entropy` å—ï¼Ÿ
* **E06**ï¼šå…ƒç»ƒä¹ ï¼è‡ªå·±è®¾è®¡ä¸€ä¸ªæœ‰è¶£/æœ‰åˆ›æ„çš„ç»ƒä¹ ï¼Œå¹¶å®Œæˆå®ƒã€‚

---

ğŸ“š **å®ç”¨å­¦ä¹ é“¾æ¥ï¼š**

* Python + Numpy æ•™ç¨‹ï¼ˆæ¥è‡ª CS231nï¼‰ï¼š[https://cs231n.github.io/python-numpy/](https://cs231n.github.io/python-numpy/)

  > æœ¬è§†é¢‘ä¸­æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ `torch.tensor` è€Œä¸æ˜¯ `numpy.array`ï¼Œä½†ä¸¤è€…è®¾è®¡éå¸¸ç›¸ä¼¼ï¼ˆå¦‚å¹¿æ’­ã€æ•°æ®ç±»å‹ç­‰ï¼‰ï¼Œç»ƒä¹ ä¸€ä¸ªå‡ ä¹ç­‰äºç»ƒä¹ å¦ä¸€ä¸ªã€‚æ³¨æ„ç»†èŠ‚ï¼šå‡½æ•°å‘½åã€å‚æ•°ç­‰ API å·®å¼‚ã€‚

* PyTorch Tensor æ•™ç¨‹ï¼š[https://pytorch.org/tutorials/beginner/introyt/tensors\_deeper\_tutorial.html](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)

* å¦ä¸€ä¸ª PyTorch å…¥é—¨ Tensor æ•™ç¨‹ï¼š[https://pytorch.org/tutorials/beginner/basics/tensorqs\_tutorial.html](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)

---

ğŸ“º **ç« èŠ‚ç›®å½•ï¼š**

```
00:00:00 ä»‹ç»
00:03:03 è¯»å–å¹¶æ¢ç´¢æ•°æ®é›†
00:06:24 æ¢ç´¢æ•°æ®é›†ä¸­çš„ bigram
00:09:24 ä½¿ç”¨ Python å­—å…¸ç»Ÿè®¡ bigram
00:12:45 ä½¿ç”¨ 2D torch å¼ é‡ç»Ÿè®¡ bigramï¼ˆâ€œè®­ç»ƒæ¨¡å‹â€ï¼‰
00:18:19 å¯è§†åŒ– bigram å¼ é‡
00:20:54 ç”¨å•ä¸ª â€œ.â€ æ›¿ä»£èµ·å§‹ (S) å’Œç»“æŸ (E) æ ‡è®°
00:24:02 ä»æ¨¡å‹ä¸­è¿›è¡Œé‡‡æ ·
00:36:17 æå‡æ•ˆç‡ï¼å¯¹è¡Œè¿›è¡Œå‘é‡åŒ–å½’ä¸€åŒ–ï¼Œå¼ é‡å¹¿æ’­
00:50:14 æŸå¤±å‡½æ•°ï¼ˆæ¨¡å‹ä¸‹æ•°æ®çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰
01:00:50 ä½¿ç”¨è™šå‡è®¡æ•°è¿›è¡Œæ¨¡å‹å¹³æ»‘
01:02:57 ç¬¬2éƒ¨åˆ†ï¼šç¥ç»ç½‘ç»œæ–¹æ³•ä»‹ç»
01:05:26 ä¸ºç¥ç»ç½‘ç»œåˆ›å»º bigram æ•°æ®é›†
01:10:01 å°†æ•´æ•°è¾“å…¥ç¥ç»ç½‘ç»œï¼Ÿä½¿ç”¨ one-hot ç¼–ç 
01:13:53 æ„å»ºç¥ç»ç½‘ç»œï¼šä¸€å±‚çº¿æ€§ç¥ç»å…ƒï¼ˆçŸ©é˜µä¹˜æ³•å®ç°ï¼‰
01:18:46 å°†ç¥ç»ç½‘ç»œè¾“å‡ºè½¬åŒ–ä¸ºæ¦‚ç‡ï¼šsoftmax
01:26:17 æ€»ç»“ + ä¸‹ä¸€æ­¥é¢„å‘Š + æåˆ° micrograd
01:35:49 å‘é‡åŒ–æŸå¤±è®¡ç®—
01:38:36 PyTorch å®ç°åå‘ä¼ æ’­ä¸æ›´æ–°
01:42:55 æ¨¡å‹æ•´åˆ
01:47:49 é™„æ³¨1ï¼šone-hot ç¼–ç å®è´¨æ˜¯é€‰çº¿æ€§å±‚æƒé‡çŸ©é˜µçš„ä¸€è¡Œ
01:50:18 é™„æ³¨2ï¼šæ¨¡å‹å¹³æ»‘ä½œä¸ºæ­£åˆ™åŒ–æŸå¤±
01:54:31 ä»ç¥ç»ç½‘ç»œä¸­é‡‡æ ·
01:56:16 æ€»ç»“
```


å­—å¹•ç¿»è¯‘

# intro

hi everyone hope you're well and next up what i'd like to do is i'd like to build out make more
like micrograd before it make more is a repository that i have on my github webpage
you can look at it but just like with micrograd i'm going to build it out step by step and i'm going to spell everything out so we're
going to build it out slowly and together now what is make more make more as the name suggests
makes more of things that you give it so here's an example names.txt is an example dataset to make
more and when you look at names.txt you'll find that it's a very large data set of
names so here's lots of different types of names in fact i believe there are 32 000 names
that i've sort of found randomly on the government website and if you train make more on this data
set it will learn to make more of things like this
and in particular in this case that will mean more things that sound name-like
but are actually unique names and maybe if you have a baby and you're trying to assign name maybe you're looking for a cool new sounding unique
name make more might help you so here are some example generations from the neural network
once we train it on our data set so here's some example unique names that it will generate
dontel irot zhendi and so on and so all these are sound
name like but they're not of course names so under the hood make more is a
character level language model so what that means is that it is treating every single line here as an example and
within each example it's treating them all as sequences of individual characters so r e e s e is this example
and that's the sequence of characters and that's the level on which we are building out make more and what it means
to be a character level language model then is that it's just uh sort of modeling those sequences of characters
and it knows how to predict the next character in the sequence now we're actually going to implement a
large number of character level language models in terms of the neural networks that are involved in predicting the next
character in a sequence so very simple bi-gram and back of work models multilingual perceptrons recurrent
neural networks all the way to modern transformers in fact the transformer that we will build will be basically the
equivalent transformer to gpt2 if you have heard of gpt uh so that's kind of a big deal it's a modern network and by
the end of the series you will actually understand how that works um on the level of characters now to give you a
sense of the extensions here uh after characters we will probably spend some time on the word level so that we can
generate documents of words not just little you know segments of characters but we can generate entire large much
larger documents and then we're probably going to go into images and image text
networks such as dolly stable diffusion and so on but for now we have to start
here character level language modeling let's go so like before we are starting with a completely blank jupiter notebook page

# ä»‹ç»

å¤§å®¶å¥½ï¼Œå¸Œæœ›ä½ ä»¬ä¸€åˆ‡éƒ½å¥½ã€‚æ¥ä¸‹æ¥æˆ‘æƒ³åšçš„æ˜¯æ„å»º make moreï¼Œå°±åƒä¹‹å‰çš„ micrograd ä¸€æ ·ï¼Œmake more æ˜¯æˆ‘åœ¨ GitHub ä¸Šçš„ä¸€ä¸ªä»“åº“ï¼Œä½ å¯ä»¥å»çœ‹çœ‹ã€‚ä½†å°±åƒ micrograd ä¸€æ ·ï¼Œæˆ‘ä¼šä¸€æ­¥æ­¥åœ°æ„å»ºå®ƒï¼Œå¹¶ä¸”è¯¦ç»†è§£é‡Šæ¯ä¸€ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬å°†ä¸€èµ·æ…¢æ…¢æ„å»ºå®ƒã€‚é‚£ä¹ˆï¼Œä»€ä¹ˆæ˜¯ make more å‘¢ï¼Ÿé¡¾åæ€ä¹‰ï¼Œmake more æ˜¯è®©ä½ ç»™å®ƒçš„ä¸œè¥¿ç”Ÿæˆæ›´å¤šç±»ä¼¼çš„ä¸œè¥¿ã€‚æ¯”å¦‚è¯´ï¼Œ`names.txt` å°±æ˜¯ä¸€ä¸ªç”¨äº make more çš„ç¤ºä¾‹æ•°æ®é›†ã€‚å½“ä½ æŸ¥çœ‹ `names.txt` æ—¶ï¼Œä½ ä¼šå‘ç°å®ƒæ˜¯ä¸€ä¸ªéå¸¸å¤§çš„åå­—æ•°æ®é›†ã€‚é‡Œé¢æœ‰å¾ˆå¤šä¸åŒç±»å‹çš„åå­—ï¼Œå®é™…ä¸Šï¼Œæˆ‘ç›¸ä¿¡è¿™äº›åå­—æœ‰å¤§çº¦ 32000 ä¸ªï¼Œæ˜¯æˆ‘ä»æ”¿åºœç½‘ç«™éšæœºæ‰¾åˆ°çš„ã€‚å¦‚æœä½ ç”¨è¿™ä¸ªæ•°æ®é›†è®­ç»ƒ make moreï¼Œå®ƒä¼šå­¦ä¹ ç”Ÿæˆæ›´å¤šç±»ä¼¼çš„ä¸œè¥¿ï¼Œç‰¹åˆ«æ˜¯åƒè¿™æ ·å¬èµ·æ¥åƒåå­—çš„ä¸œè¥¿ï¼Œä½†å®é™…ä¸Šæ˜¯ç‹¬ä¸€æ— äºŒçš„åå­—ã€‚ä¹Ÿè®¸ä½ æœ‰ä¸€ä¸ªå®å®ï¼Œæ­£åœ¨ä¸ºä»–/å¥¹é€‰æ‹©åå­—ï¼Œå¯èƒ½ä½ åœ¨å¯»æ‰¾ä¸€ä¸ªç‹¬ç‰¹ä¸”å¬èµ·æ¥é…·çš„æ–°åå­—ï¼Œmake more å¯èƒ½ä¼šå¸®åˆ°ä½ ã€‚é‚£ä¹ˆï¼Œè¿™é‡Œæœ‰ä¸€äº›ç¥ç»ç½‘ç»œåœ¨æˆ‘ä»¬ç”¨æ•°æ®é›†è®­ç»ƒåç”Ÿæˆçš„ç¤ºä¾‹åå­—ã€‚è¿™é‡Œæ˜¯ä¸€äº›å®ƒä¼šç”Ÿæˆçš„ç‹¬ç‰¹åå­—ï¼šdontel, irot, zhendi ç­‰ç­‰ã€‚æ‰€æœ‰è¿™äº›åå­—å¬èµ·æ¥åƒåå­—ï¼Œä½†å®ƒä»¬å½“ç„¶å¹¶ä¸æ˜¯å·²å­˜åœ¨çš„åå­—ã€‚

åœ¨åå°ï¼Œmake more æ˜¯ä¸€ä¸ªå­—ç¬¦çº§è¯­è¨€æ¨¡å‹ã€‚è¿™æ„å‘³ç€å®ƒå°†æ¯ä¸€è¡Œéƒ½å½“ä½œä¸€ä¸ªç¤ºä¾‹ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªç¤ºä¾‹ä¸­ï¼Œå®ƒå°†è¿™äº›ç¤ºä¾‹çœ‹ä½œæ˜¯å•ç‹¬å­—ç¬¦çš„åºåˆ—ã€‚æ¯”å¦‚è¯´ï¼Œ`r e e s e` å°±æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œå®ƒæ˜¯ä¸€ä¸ªå­—ç¬¦åºåˆ—ï¼Œæˆ‘ä»¬å°±æ˜¯åœ¨è¿™ä¸ªçº§åˆ«ä¸Šæ„å»º make moreã€‚ä½œä¸ºä¸€ä¸ªå­—ç¬¦çº§è¯­è¨€æ¨¡å‹ï¼Œå®ƒçš„æ„æ€å°±æ˜¯ï¼Œå®ƒå»ºæ¨¡è¿™äº›å­—ç¬¦åºåˆ—ï¼Œå¹¶ä¸”èƒ½å¤Ÿé¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚

æˆ‘ä»¬å®é™…ä¸Šå°†å®ç°å¤§é‡çš„å­—ç¬¦çº§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ç®€å•çš„äºŒå…ƒç»„æ¨¡å‹å’Œå·¥ä½œæ¨¡å‹ã€å¤šå±‚æ„ŸçŸ¥å™¨ã€é€’å½’ç¥ç»ç½‘ç»œï¼Œä¸€ç›´åˆ°ç°ä»£çš„ Transformerã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬å°†æ„å»ºçš„ Transformer åŸºæœ¬ä¸Šæ˜¯ GPT-2 çš„ç­‰æ•ˆæ¨¡å‹ï¼Œå¦‚æœä½ å¬è¯´è¿‡ GPTï¼Œé‚£ä¹ˆè¿™å°±æ˜¯ä¸€ä¸ªå¤§äº‹ï¼Œå®ƒæ˜¯ä¸€ä¸ªç°ä»£çš„ç½‘ç»œï¼Œåˆ°ç³»åˆ—çš„æœ€åï¼Œä½ å°†çœŸæ­£ç†è§£å®ƒå¦‚ä½•åœ¨å­—ç¬¦çº§åˆ«ä¸Šè¿ä½œã€‚

ä¸ºäº†è®©ä½ ä»¬æ›´æ¸…æ¥šæ¥ä¸‹æ¥çš„æ‰©å±•ï¼Œé™¤äº†å­—ç¬¦çº§åˆ«å¤–ï¼Œæˆ‘ä»¬å¯èƒ½è¿˜ä¼šèŠ±ä¸€äº›æ—¶é—´åœ¨å•è¯çº§åˆ«ä¸Šï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ç”Ÿæˆå®Œæ•´çš„å•è¯æ–‡æ¡£ï¼Œè€Œä¸ä»…ä»…æ˜¯å­—ç¬¦çš„å°ç‰‡æ®µã€‚æˆ‘ä»¬å¯ä»¥ç”Ÿæˆæ›´å¤§è§„æ¨¡çš„æ–‡æ¡£ï¼Œç„¶åæˆ‘ä»¬å¯èƒ½è¿˜ä¼šè¿›å…¥å›¾åƒå’Œå›¾åƒæ–‡æœ¬ç½‘ç»œï¼Œæ¯”å¦‚ Dollyã€Stable Diffusion ç­‰ç­‰ã€‚ä½†ç›®å‰æˆ‘ä»¬å¿…é¡»ä»è¿™é‡Œå¼€å§‹â€”â€”å­—ç¬¦çº§è¯­è¨€å»ºæ¨¡ã€‚é‚£ä¹ˆï¼Œå¼€å§‹å§ï¼å°±åƒä¹‹å‰ä¸€æ ·ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªå®Œå…¨ç©ºç™½çš„ Jupyter Notebook é¡µé¢å¼€å§‹ã€‚

# reading and exploring the dataset

the first thing is i would like to basically load up the dataset names.txt so we're going to open up names.txt for
reading and we're going to read in everything into a massive string
and then because it's a massive string we'd only like the individual words and put them in the list so let's call split lines
on that string to get all of our words as a python list of strings
so basically we can look at for example the first 10 words and we have that it's a list of emma
olivia eva and so on and if we look at the top of the page here that is indeed
what we see um so that's good this list actually makes me feel that
this is probably sorted by frequency but okay so
these are the words now we'd like to actually like learn a little bit more about this data set let's look at the
total number of words we expect this to be roughly 32 000 and then what is the for example
shortest word so min of length of each word for w inwards
so the shortest word will be length two and max of one w for w in words so the
longest word will be 15 characters so let's now think through our very first language model
as i mentioned a character level language model is predicting the next character in a sequence given already
some concrete sequence of characters before it now we have to realize here is that every single word here like isabella is
actually quite a few examples packed in to that single word because what is an existence of a word
like isabella in the data set telling us really it's saying that the character i is a very likely
character to come first in the sequence of a name the character s is likely to come
after i the character a is likely to come after is
the character b is very likely to come after isa and so on all the way to a following isabel
and then there's one more example actually packed in here and that is that after there's isabella
the word is very likely to end so that's one more sort of explicit piece of information that we have here
that we have to be careful with and so there's a lot backed into a single individual word in terms of the
statistical structure of what's likely to follow in these character sequences and then of course we don't have just an
individual word we actually have 32 000 of these and so there's a lot of structure here to model
now in the beginning what i'd like to start with is i'd like to start with building a bi-gram language model
now in the bigram language model we're always working with just two characters at a time
so we're only looking at one character that we are given and we're trying to predict the next character in the
sequence so um what characters are likely to follow are what characters are likely to
follow a and so on and we're just modeling that kind of a little local structure and we're forgetting the fact that we
may have a lot more information we're always just looking at the previous character to predict the next one so
it's a very simple and weak language model but i think it's a great place to start so now let's begin by looking at these

# è¯»å–å’Œæ¢ç´¢æ•°æ®é›†

é¦–å…ˆï¼Œæˆ‘æƒ³åšçš„åŸºæœ¬æ­¥éª¤æ˜¯åŠ è½½æ•°æ®é›† `names.txt`ã€‚æˆ‘ä»¬å°†æ‰“å¼€ `names.txt` ä»¥è¯»å–å†…å®¹ï¼Œå¹¶æŠŠæ‰€æœ‰å†…å®¹è¯»å–ä¸ºä¸€ä¸ªå¤§çš„å­—ç¬¦ä¸²ã€‚
ç„¶åï¼Œç”±äºè¿™æ˜¯ä¸€ä¸ªé•¿å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬å¸Œæœ›è·å–å…¶ä¸­çš„æ¯ä¸€ä¸ªå•è¯ï¼Œå¹¶æŠŠå®ƒä»¬æ”¾å…¥ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šå¯¹è¿™ä¸ªå­—ç¬¦ä¸²ä½¿ç”¨ `splitlines()`ï¼ŒæŠŠå®ƒæ‹†åˆ†æˆä¸€ä¸ª Python çš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
æ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥æŸ¥çœ‹å‰åä¸ªå•è¯ï¼Œä¾‹å¦‚å‰10ä¸ªæ˜¯ `emma`ã€`olivia`ã€`eva` ç­‰ç­‰ã€‚
å¦‚æœæˆ‘ä»¬çœ‹é¡µé¢çš„é¡¶éƒ¨ï¼Œé‚£ç¡®å®æ˜¯æˆ‘ä»¬çœ‹åˆ°çš„å†…å®¹ï¼Œè¯´æ˜è¯»å–æˆåŠŸäº†ã€‚

è¿™ä¸ªåˆ—è¡¨çœ‹èµ·æ¥è®©æˆ‘æ„Ÿè§‰è¿™äº›åå­—å¯èƒ½æ˜¯æŒ‰å‡ºç°é¢‘ç‡æ’åºçš„ï¼Œä½†æ²¡å…³ç³»ã€‚
è¿™äº›å°±æ˜¯æˆ‘ä»¬è¦å¤„ç†çš„â€œå•è¯â€ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬æƒ³è¿›ä¸€æ­¥äº†è§£ä¸€ä¸‹è¿™ä¸ªæ•°æ®é›†ã€‚æ¯”å¦‚è¯´ï¼š

* æ€»å…±æœ‰å¤šå°‘ä¸ªåå­—ï¼Ÿæˆ‘ä»¬é¢„è®¡å¤§æ¦‚æ˜¯ 32000 ä¸ªã€‚
* æœ€çŸ­çš„åå­—æœ‰å¤šé•¿ï¼Ÿæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªåå­—çš„é•¿åº¦ï¼Œå–æœ€å°å€¼ï¼Œç»“æœæ˜¯ 2ã€‚
* æœ€é•¿çš„åå­—æœ‰å¤šé•¿ï¼ŸåŒæ ·çš„æ–¹æ³•ï¼Œæœ€å¤§é•¿åº¦æ˜¯ 15 ä¸ªå­—ç¬¦ã€‚

ç°åœ¨æˆ‘ä»¬å¼€å§‹æ€è€ƒæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªè¯­è¨€æ¨¡å‹ã€‚

æ­£å¦‚æˆ‘ä¹‹å‰æåˆ°çš„ï¼Œå­—ç¬¦çº§è¯­è¨€æ¨¡å‹çš„ç›®æ ‡æ˜¯ï¼š**åœ¨ç»™å®šå‰é¢ä¸€æ®µå…·ä½“å­—ç¬¦åºåˆ—çš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦**ã€‚
ç°åœ¨æˆ‘ä»¬è¦æ„è¯†åˆ°çš„ä¸€ç‚¹æ˜¯ï¼šæ•°æ®é›†ä¸­æ¯ä¸€ä¸ªå•è¯ï¼Œæ¯”å¦‚ "isabella"ï¼Œå®é™…ä¸Šåœ¨ç»Ÿè®¡ç»“æ„ä¸ŠåŒ…å«äº†è®¸å¤šé¢„æµ‹ä¿¡æ¯ã€‚

æ¯”å¦‚è¯´ï¼Œ"isabella" è¿™ä¸ªåå­—çš„å­˜åœ¨å‘Šè¯‰æˆ‘ä»¬å¾ˆå¤šäº‹æƒ…ï¼š

* å­—ç¬¦ `i` å¾ˆå¯èƒ½æ˜¯åå­—å¼€å¤´çš„å­—ç¬¦ï¼Œ
* å­—ç¬¦ `s` å¾ˆå¯èƒ½å‡ºç°åœ¨ `i` ä¹‹åï¼Œ
* å­—ç¬¦ `a` å¾ˆå¯èƒ½å‡ºç°åœ¨ `is` ä¹‹åï¼Œ
* å­—ç¬¦ `b` å¾ˆå¯èƒ½å‡ºç°åœ¨ `isa` ä¹‹åï¼Œ
* ä¸€ç›´åˆ°æœ€å `a` å¯èƒ½å‡ºç°åœ¨ `isabell` åé¢ã€‚

å¹¶ä¸”è¿˜æœ‰ä¸€ç‚¹éšå«çš„ä¿¡æ¯æ˜¯ï¼šå½“æˆ‘ä»¬çœ‹åˆ°å®Œæ•´çš„ "isabella" åï¼Œåå­—å¾ˆå¯èƒ½å°±ç»“æŸäº†ã€‚å› æ­¤ï¼Œ"ç»“æŸ" ä¹Ÿæ˜¯ä¸€ä¸ªæ˜ç¡®çš„é¢„æµ‹ç›®æ ‡ã€‚

æ‰€ä»¥åœ¨ä¸€ä¸ªå•è¯ä¸­ï¼Œå®é™…ä¸ŠåŒ…å«äº†ä¸°å¯Œçš„ç»“æ„å’Œä¿¡æ¯ï¼Œç”¨æ¥è®­ç»ƒå­—ç¬¦çº§è¯­è¨€æ¨¡å‹ã€‚è€Œæˆ‘ä»¬ä¸åªæœ‰ä¸€ä¸ªå•è¯ï¼Œæ€»å…±æœ‰çº¦ 32000 ä¸ªè¿™æ ·çš„å•è¯ï¼Œå› æ­¤å¯ä»¥æå–å‡ºå¤§é‡æœ‰ç»“æ„çš„è®­ç»ƒæ•°æ®ã€‚

åœ¨ä¸€å¼€å§‹ï¼Œæˆ‘ä»¬æƒ³ä»æœ€ç®€å•çš„æ¨¡å‹åšèµ·â€”â€”**bigram è¯­è¨€æ¨¡å‹**ã€‚

åœ¨ bigram æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å§‹ç»ˆåªå…³æ³¨ä¸¤ä¸ªå­—ç¬¦ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªå­—ç¬¦ä½œä¸ºè¾“å…¥ï¼Œè¦é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯è°ã€‚

æ¯”å¦‚ï¼š

* å“ªäº›å­—ç¬¦å¸¸å¸¸å‡ºç°åœ¨ `a` åé¢ï¼Ÿ
* å“ªäº›å­—ç¬¦å¸¸å¸¸å‡ºç°åœ¨ `b` åé¢ï¼Ÿ

æˆ‘ä»¬åªå»ºæ¨¡è¿™ç§**å±€éƒ¨ç»“æ„**ï¼Œä¸è€ƒè™‘æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œä¹Ÿå°±æ˜¯åªçœ‹å‰ä¸€ä¸ªå­—ç¬¦æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚

è¿™æ˜¯ä¸€ç§éå¸¸ç®€å•ä¸”è¾ƒå¼±çš„è¯­è¨€æ¨¡å‹ï¼Œä½†å®ƒæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚
æ¥ä¸‹æ¥æˆ‘ä»¬å°±è¦å¼€å§‹å®ç°å®ƒã€‚

å¦‚ä½•ä¸Šä¼ æœ¬åœ°æ–‡ä»¶ names.txt

ç‚¹å‡»å·¦ä¾§çš„æ–‡ä»¶å¤¹ï¼Œä¸Šä¼ ä¹‹åå’Œsample_dataå¹¶åˆ—ï¼Œå‚è€ƒ https://blog.csdn.net/lcnana/article/details/122409044

# exploring the bigrams in the dataset

bi-grams in our data set and what they look like and these bi-grams again are just two characters in a row
so for w in words each w here is an individual word a string
we want to iterate uh for we're going to iterate this word
with consecutive characters so two characters at a time sliding it through the word now a interesting nice way cute
way to do this in python by the way is doing something like this for character one character two in zip off
w and w at one one column
print character one character two and let's not do all the words let's just do the first three words and i'm
going to show you in a second how this works but for now basically as an example let's just do the very first word alone
emma you see how we have a emma and this will just print e m m m a
and the reason this works is because w is the string emma w at one column is
the string mma and zip takes two iterators and it pairs them up
and then creates an iterator over the tuples of their consecutive entries and if any one of these lists is shorter
than the other then it will just halt and return so basically that's why we return em mmm
ma but then because this iterator second one here runs out of elements zip just
ends and that's why we only get these tuples so pretty cute so these are the consecutive elements in
the first word now we have to be careful because we actually have more information here than just these three
examples as i mentioned we know that e is the is very likely to come first and
we know that a in this case is coming last so one way to do this is basically we're
going to create a special array here all characters
and um we're going to hallucinate a special start token here
i'm going to call it like special start so this is a list of one element
plus w and then plus a special end character
and the reason i'm wrapping the list of w here is because w is a string emma list of w will just have the individual
characters in the list and then doing this again now but not iterating
over w's but over the characters will give us something like this
so e is likely so this is a bigram of the start character and e and this is a bigram of the
a and the special end character and now we can look at for example what this looks like for
olivia or eva and indeed we can actually potentially do this for the entire data
set but we won't print that that's going to be too much but these are the individual character diagrams and we can print them

# æ¢ç´¢æ•°æ®é›†ä¸­çš„äºŒå…ƒç»„

æˆ‘ä»¬æ¥çœ‹çœ‹æ•°æ®é›†ä¸­çš„äºŒå…ƒç»„æ˜¯ä»€ä¹ˆæ ·çš„ã€‚äºŒå…ƒç»„å°±æ˜¯ç›¸é‚»çš„ä¸¤ä¸ªå­—ç¬¦ã€‚
å¯¹äºæ¯ä¸ªå•è¯ `w`ï¼Œæ¯ä¸ª `w` æ˜¯ä¸€ä¸ªå•ç‹¬çš„å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬æƒ³è¦è¿­ä»£è¿™ä¸ªå•è¯ï¼Œå°†å…¶æŒ‰é¡ºåºæ‹†åˆ†æˆä¸¤ä¸ªå­—ç¬¦ä¸€ç»„ï¼Œæ»‘åŠ¨éå†æ•´ä¸ªå•è¯ã€‚
é¡ºä¾¿æä¸€ä¸‹ï¼ŒPython ä¸­æœ‰ä¸€ä¸ªå¾ˆæœ‰è¶£ä¸”ç®€æ´çš„æ–¹å¼æ¥åšè¿™ä¸ªï¼šä½ å¯ä»¥ä½¿ç”¨åƒè¿™æ ·çš„ä»£ç ï¼š

```python
for character1, character2 in zip(w, w[1:]):
    print(character1, character2)
```

ä¸è¿‡æˆ‘ä»¬å…ˆä¸å¯¹æ‰€æœ‰å•è¯è¿›è¡Œæ“ä½œï¼Œåªå¤„ç†å‰ä¸‰ä¸ªå•è¯ã€‚æˆ‘é©¬ä¸Šç»™ä½ æ¼”ç¤ºè¿™ä¸ªæ–¹æ³•æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä½†ç°åœ¨å…ˆä¸¾ä¸ªç®€å•çš„ä¾‹å­ï¼Œåªå¤„ç†ç¬¬ä¸€ä¸ªå•è¯ `emma`ã€‚
ä½ ä¼šçœ‹åˆ°è¾“å‡ºæ˜¯è¿™æ ·çš„ï¼š
`e m`, `m m`, `m a`ã€‚
ä¹‹æ‰€ä»¥ä¼šè¿™æ ·è¾“å‡ºï¼Œæ˜¯å› ä¸º `w` æ˜¯å­—ç¬¦ä¸² "emma"ï¼Œ`w[1:]` æ˜¯å­—ç¬¦ä¸² "mma"ã€‚
`zip` ä¼šå°†è¿™ä¸¤ä¸ªè¿­ä»£å™¨é…å¯¹ï¼Œç„¶åç”Ÿæˆä¸€ç³»åˆ—è¿ç»­å­—ç¬¦çš„å…ƒç»„ã€‚å¦‚æœå…¶ä¸­ä¸€ä¸ªåˆ—è¡¨çš„é•¿åº¦æ¯”å¦ä¸€ä¸ªçŸ­ï¼Œ`zip` ä¼šåœæ­¢å¹¶è¿”å›ç»“æœã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬å¾—åˆ°çš„æ˜¯ `e m`, `m m`, `m a`ï¼Œç„¶åç”±äºç¬¬äºŒä¸ªè¿­ä»£å™¨æ²¡æœ‰æ›´å¤šå…ƒç´ ï¼Œ`zip` å°±ç»“æŸäº†ï¼Œå› æ­¤æˆ‘ä»¬åªå¾—åˆ°è¿™å‡ ä¸ªå…ƒç»„ã€‚è¿™å¾ˆæœ‰è¶£å§ï¼

è¿™äº›å°±æ˜¯ç¬¬ä¸€ä¸ªå•è¯ä¸­çš„è¿ç»­å­—ç¬¦å¯¹ã€‚

ä¸è¿‡æˆ‘ä»¬éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ‹¥æœ‰æ¯”è¿™ä¸‰ä¸ªä¾‹å­æ›´å¤šçš„ä¿¡æ¯ã€‚æ­£å¦‚æˆ‘ä¹‹å‰æåˆ°çš„ï¼Œæˆ‘ä»¬çŸ¥é“å­—ç¬¦ `e` å¾ˆå¯èƒ½å‡ºç°åœ¨åå­—çš„å¼€å¤´ï¼Œè€Œå­—ç¬¦ `a` å¾ˆå¯èƒ½å‡ºç°åœ¨ç»“å°¾ã€‚

ä¸€ç§æ–¹æ³•æ˜¯ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªå•è¯åˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„æ•°ç»„ï¼ŒåŒ…æ‹¬æ¯ä¸ªå­—ç¬¦ï¼Œå¹¶ä¸ºæ¯ä¸ªå•è¯åŠ ä¸Šä¸€ä¸ªç‰¹æ®Šçš„å¼€å§‹ç¬¦å·å’Œç»“æŸç¬¦å·ã€‚
æˆ‘ä»¬å¯ä»¥æŠŠå®ƒç§°ä½œ `special_start`ï¼Œæ‰€ä»¥æ•°ç»„ä¼šå˜æˆè¿™æ ·çš„æ ¼å¼ï¼š`[special_start] + w + [special_end]`ã€‚
è¿™æ ·åšçš„åŸå› æ˜¯ï¼Œ`w` æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸² "emma"ï¼Œè€Œ `list(w)` å°±ä¼šæŠŠå®ƒè½¬åŒ–ä¸ºä¸€ä¸ªå­—ç¬¦åˆ—è¡¨ã€‚ç„¶åï¼Œå½“æˆ‘ä»¬ç”¨è¿™ç§æ–¹å¼å†æ¬¡è¿›è¡Œè¿­ä»£æ—¶ï¼Œä¼šå¾—åˆ°ä»¥ä¸‹çš„äºŒå…ƒç»„ï¼š
`special_start` å’Œ `e`ï¼Œ`e` å’Œ `m`ï¼Œ`m` å’Œ `m`ï¼Œ`m` å’Œ `a`ï¼Œ`a` å’Œ `special_end`ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹ç±»ä¼¼çš„æƒ…å†µï¼Œæ¯”å¦‚ `olivia` æˆ– `eva`ï¼Œå¹¶ä¸”æˆ‘ä»¬å®é™…ä¸Šå¯ä»¥å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ“ä½œï¼Œä¸è¿‡æˆ‘ä»¬ä¸ä¼šæ‰“å°å‡ºæ¥ï¼Œå› ä¸ºé‚£ä¼šå¤ªå¤šäº†ã€‚ä½†è¿™äº›å°±æ˜¯æˆ‘ä»¬å¾—åˆ°çš„æ¯ä¸ªå•è¯çš„äºŒå…ƒç»„ã€‚æˆ‘ä»¬å¯ä»¥æŠŠå®ƒä»¬æ‰“å°å‡ºæ¥æŸ¥çœ‹ã€‚

# counting bigrams in a python dictionary

now in order to learn the statistics about which characters are likely to follow other characters the simplest way
in the bigram language models is to simply do it by counting so we're basically just going to count
how often any one of these combinations occurs in the training set in these words
so we're going to need some kind of a dictionary that's going to maintain some counts for every one of these diagrams
so let's use a dictionary b and this will map these bi-grams so
bi-gram is a tuple of character one character two and then b at bi-gram
will be b dot get of bi-gram which is basically the same as b at bigram
but in the case that bigram is not in the dictionary b we would like to by default return to zero
plus one so this will basically add up all the bigrams and count how often they occur
let's get rid of printing or rather let's keep the printing and let's just
inspect what b is in this case and we see that many bi-grams occur just
a single time this one allegedly occurred three times so a was an ending character three times
and that's true for all of these words all of emma olivia and eva and with a
so that's why this occurred three times now let's do it for all the words
oops i should not have printed i'm going to erase that
let's kill this let's just run and now b will have the statistics of
the entire data set so these are the counts across all the words of the individual pie grams
and we could for example look at some of the most common ones and least common ones
this kind of grows in python but the way to do this the simplest way i like is we just use b dot items
b dot items returns the tuples of key value in this case the keys are
the character diagrams and the values are the counts and so then what we want to do is we
want to do sorted of this
but by default sort is on the first
on the first item of a tuple but we want to sort by the values which are the second element of a tuple that is the
key value so we want to use the key equals lambda
that takes the key value and returns the key value at the one not at zero but
at one which is the count so we want to sort by the count of these elements
and actually we wanted to go backwards so here we have is the bi-gram q and r occurs only a single
time dz occurred only a single time and when we sort this the other way around
we're going to see the most likely bigrams so we see that n was very often an ending character
many many times and apparently n almost always follows an a and that's a very likely combination as
well so this is kind of the individual counts
that we achieve over the entire data set now it's actually going to be significantly more convenient for us to

# åœ¨ Python å­—å…¸ä¸­ç»Ÿè®¡äºŒå…ƒç»„ï¼ˆbigramsï¼‰

ç°åœ¨ï¼Œä¸ºäº†äº†è§£å“ªäº›å­—ç¬¦æ›´å¯èƒ½å‡ºç°åœ¨å…¶ä»–å­—ç¬¦ä¹‹åï¼ˆå³å­—ç¬¦ä¹‹é—´çš„ç»Ÿè®¡å…³ç³»ï¼‰ï¼Œåœ¨ bigram è¯­è¨€æ¨¡å‹ä¸­ï¼Œæœ€ç®€å•çš„æ–¹å¼å°±æ˜¯ç›´æ¥**æ•°æ•°**ã€‚
æˆ‘ä»¬åªéœ€è¦ç»Ÿè®¡æ¯ç§å­—ç¬¦å¯¹ï¼ˆbigramï¼‰åœ¨è®­ç»ƒé›†ï¼ˆè¿™äº›å•è¯ï¼‰ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚

æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå­—å…¸ `b`ï¼Œå®ƒç”¨æ¥è®°å½•æ¯ä¸€ä¸ª bigram å‡ºç°çš„æ¬¡æ•°ã€‚
æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå­—å…¸ `b`ï¼Œå®ƒçš„é”®ï¼ˆkeyï¼‰æ˜¯å­—ç¬¦å¯¹çš„å…ƒç»„ `(character1, character2)`ï¼Œå€¼ï¼ˆvalueï¼‰æ˜¯å®ƒä»¬å‡ºç°çš„æ¬¡æ•°ã€‚

```python
bigram = (ch1, ch2)
b[bigram] = b.get(bigram, 0) + 1
```

è¿™è¡Œä»£ç çš„å«ä¹‰æ˜¯ï¼š

* `b.get(bigram, 0)` ä¼šåœ¨å­—å…¸ä¸­æŸ¥æ‰¾è¿™ä¸ª bigramï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°å°±è¿”å›é»˜è®¤å€¼ 0ï¼›
* ç„¶ååŠ  1ï¼Œç›¸å½“äºæŠŠè¿™ä¸ª bigram çš„å‡ºç°æ¬¡æ•°åŠ ä¸€ã€‚

è¿™æ ·æˆ‘ä»¬å°±èƒ½éå†æ•´ä¸ªæ•°æ®é›†ï¼Œå¹¶æŠŠæ‰€æœ‰çš„ bigram å‡ºç°æ¬¡æ•°ç´¯åŠ åˆ°å­—å…¸é‡Œã€‚

æˆ‘ä»¬å¯ä»¥æŠŠæ‰“å°ç»“æœä¿ç•™ä¸€ä¸‹ï¼Œæ£€æŸ¥ `b` çš„å†…å®¹ã€‚æˆ‘ä»¬çœ‹åˆ°å¾ˆå¤š bigram åªå‡ºç°äº†ä¸€æ¬¡ã€‚
æ¯”å¦‚æŸä¸ª bigram å‡ºç°äº†ä¸‰æ¬¡ï¼Œè¡¨ç¤ºå­—ç¬¦ `a` æ˜¯ç»“å°¾å­—ç¬¦çš„æƒ…å†µå‘ç”Ÿäº†ä¸‰æ¬¡ï¼Œè¿™ç¡®å®å‡ºç°åœ¨äº† â€œemmaâ€ã€â€œoliviaâ€ å’Œ â€œevaâ€ ä¸­ã€‚

æ¥ç€æˆ‘ä»¬å¯¹ **æ‰€æœ‰å•è¯** è¿è¡Œè¿™æ®µç»Ÿè®¡ä»£ç ï¼Œæ„å»ºæ•´ä¸ªæ•°æ®é›†çš„ bigram ç»Ÿè®¡è¡¨ã€‚
ä¸è¿‡æˆ‘ä»¬ä¸å†æ‰“å°è¿™äº›å†…å®¹ï¼Œå› ä¸ºå¤ªå¤šäº†ã€‚è¿è¡Œå®Œåï¼Œ`b` ä¸­å°±åŒ…å«äº†æ‰€æœ‰å•è¯ä¸­æ¯ä¸€ä¸ªå­—ç¬¦ bigram çš„å‡ºç°é¢‘æ¬¡ã€‚

ç°åœ¨æˆ‘ä»¬å¯ä»¥åˆ†ææœ€å¸¸è§çš„å’Œæœ€ç½•è§çš„ bigramã€‚Python ä¸­æœ€ç®€å•çš„åšæ³•å°±æ˜¯ä½¿ç”¨ `b.items()`ï¼š

```python
b.items()
```

å®ƒè¿”å›çš„æ˜¯å­—å…¸ä¸­çš„ `(key, value)` å…ƒç»„åˆ—è¡¨ï¼Œå³ `(bigram, count)`ã€‚

æ¥ç€æˆ‘ä»¬å¯ä»¥å¯¹è¿™äº›é¡¹è¿›è¡Œæ’åºï¼š

```python
sorted(b.items(), key=lambda kv: kv[1])
```

è¿™é‡Œçš„ `lambda kv: kv[1]` è¡¨ç¤ºæˆ‘ä»¬æŒ‰ç¬¬äºŒé¡¹ï¼ˆå‡ºç°æ¬¡æ•°ï¼‰æ’åºï¼Œè€Œä¸æ˜¯é»˜è®¤çš„æŒ‰é”®ï¼ˆbigramï¼‰æ’åºã€‚

æˆ‘ä»¬è¿˜å¯ä»¥åŠ ä¸Š `reverse=True` å®ç°**ä»é«˜åˆ°ä½æ’åº**ï¼Œæ¥æŸ¥çœ‹æœ€å¸¸å‡ºç°çš„ bigramï¼š

```python
sorted(b.items(), key=lambda kv: kv[1], reverse=True)
```

æ¯”å¦‚æˆ‘ä»¬ä¼šå‘ç°å­—ç¬¦ `n` ä½œä¸ºç»“å°¾çš„ bigram å‡ºç°äº†å¾ˆå¤šæ¬¡ï¼Œæˆ–è€…å­—ç¬¦ `a` åé¢è·Ÿç€ `n` æ˜¯éå¸¸å¸¸è§çš„ç»„åˆã€‚

è¿™äº›å°±æ˜¯æˆ‘ä»¬é€šè¿‡éå†æ•´ä¸ªæ•°æ®é›†å¾—åˆ°çš„æ¯ä¸ªå­—ç¬¦å¯¹çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œåç»­æˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨è¿™äº›ç»Ÿè®¡ç»“æœæ¥å»ºæ¨¡æˆ–é‡‡æ ·æ–°æ•°æ®äº†ã€‚

---

python lambda è§£é‡Šä¸€ä¸‹

åœ¨ Python ä¸­ï¼Œ`lambda` æ˜¯ç”¨æ¥åˆ›å»º\*\*åŒ¿åå‡½æ•°ï¼ˆæ²¡æœ‰åå­—çš„å‡½æ•°ï¼‰\*\*çš„ä¸€ç§ç®€æ´è¯­æ³•ã€‚

---

### ä¸€èˆ¬å½¢å¼ï¼š

```python
lambda å‚æ•°: è¡¨è¾¾å¼
```

è¿™å®šä¹‰äº†ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒæ¥å—ä¸€äº›å‚æ•°ï¼Œå¹¶è¿”å›ä¸€ä¸ªè¡¨è¾¾å¼çš„ç»“æœã€‚

ç›¸å½“äºä¸‹é¢è¿™ç§æ™®é€šå‡½æ•°çš„ç®€å†™å½¢å¼ï¼š

```python
def å‡½æ•°å(å‚æ•°):
    return è¡¨è¾¾å¼
```

---

### ä¸¾ä¾‹è¯´æ˜ï¼š

#### âœ… ç¤ºä¾‹ 1ï¼šæ™®é€šå‡½æ•° vs lambda

```python
# æ™®é€šå‡½æ•°
def add(x, y):
    return x + y

# lambdaå‡½æ•°
add_lambda = lambda x, y: x + y

print(add(2, 3))         # è¾“å‡º 5
print(add_lambda(2, 3))  # è¾“å‡º 5
```

---

#### âœ… ç¤ºä¾‹ 2ï¼šé…åˆ sorted ä½¿ç”¨

æ¯”å¦‚ä½ æœ‰ä¸€ä¸ªåˆ—è¡¨ï¼Œæƒ³æŒ‰ç¬¬äºŒä¸ªå…ƒç´ æ’åºï¼š

```python
items = [('a', 3), ('b', 1), ('c', 2)]
sorted_items = sorted(items, key=lambda x: x[1])
print(sorted_items)  # è¾“å‡º [('b', 1), ('c', 2), ('a', 3)]
```

è§£é‡Šï¼š

* `lambda x: x[1]` åˆ›å»ºäº†ä¸€ä¸ªåŒ¿åå‡½æ•°ï¼Œè¡¨ç¤ºâ€œå–æ¯ä¸ªå…ƒç´ çš„ç¬¬1ä¸ªç´¢å¼•ï¼ˆå³ç¬¬2ä¸ªå€¼ï¼‰â€
* `sorted(..., key=...)` å°±ä¼šç”¨è¿™ä¸ªå‡½æ•°æ¥è¿›è¡Œæ’åº

---

#### âœ… ç¤ºä¾‹ 3ï¼šé…åˆ `map()` ä½¿ç”¨

```python
nums = [1, 2, 3, 4]
squared = list(map(lambda x: x**2, nums))
print(squared)  # è¾“å‡º [1, 4, 9, 16]
```

---

### æ€»ç»“ï¼š

| ç‰¹ç‚¹       | è¯´æ˜                                    |
| -------- | ------------------------------------- |
| åŒ¿å       | æ²¡æœ‰åå­—ï¼Œé€‚åˆä¸´æ—¶ç”¨ä¸€æ¬¡çš„å°å‡½æ•°                      |
| è¡¨è¾¾å¼åªèƒ½æœ‰ä¸€è¡Œ | ä¸èƒ½åŒ…å«å¤šæ¡è¯­å¥ã€èµ‹å€¼ã€å¾ªç¯ç­‰                       |
| å¸¸ç”¨äºå‡½æ•°å¼æ“ä½œ | å¦‚ `sorted`, `map`, `filter`, `reduce` |

---

å¦‚æœä½ çœ‹åˆ° `lambda`ï¼Œå¯ä»¥ç†è§£æˆï¼š**ä¸´æ—¶é€ äº†ä¸ªå°å‡½æ•°ï¼Œå¹²ç‚¹äº‹å°±æ‰”æ‰**ã€‚å¸¸ç”¨æ¥å†™ç®€æ´çš„ä»£ç ï¼Œä½†ä¸è¦æ»¥ç”¨ï¼Œé€»è¾‘å¤æ‚æ—¶æœ€å¥½ç”¨ `def` æ˜ç¡®å†™å‡ºå‡½æ•°ã€‚


# counting bigrams in a 2D torch tensor ("training the model")

keep this information in a two-dimensional array instead of a python dictionary
so we're going to store this information in a 2d array and
the rows are going to be the first character of the bigram and the columns are going to be the second character and
each entry in this two-dimensional array will tell us how often that first character files the second character in
the data set so in particular the array representation that we're going to use or the library is that of pytorch
and pytorch is a deep learning neural network framework but part of it is also this torch.tensor
which allows us to create multi-dimensional arrays and manipulate them very efficiently so
let's import pytorch which you can do by import torch and then we can create
arrays so let's create a array of zeros and we give it a
size of this array let's create a three by five array as an example and
this is a three by five array of zeros and by default you'll notice a.d type
which is short for data type is float32 so these are single precision floating point numbers
because we are going to represent counts let's actually use d type as torch dot and 32
so these are 32-bit integers so now you see that we have integer data
inside this tensor now tensors allow us to really manipulate all the individual entries
and do it very efficiently so for example if we want to change this bit we have to index into the tensor and in
particular here this is the first row and the
because it's zero indexed so this is row index one and column index zero one two
three so a at one comma three we can set that to one
and then a we'll have a 1 over there we can of course also do things like
this so now a will be 2 over there or 3.
and also we can for example say a 0 0 is 5 and then a will have a 5 over here
so that's how we can index into the arrays now of course the array that we are interested in is much much bigger so
for our purposes we have 26 letters of the alphabet and then we have two special characters
s and e so uh we want 26 plus 2 or 28 by 28
array and let's call it the capital n because it's going to represent sort of the counts
let me erase this stuff so that's the array that starts at zeros 28 by 28
and now let's copy paste this here but instead of having a dictionary b
which we're going to erase we now have an n now the problem here is that we have
these characters which are strings but we have to now um basically index into a
um array and we have to index using integers so we need some kind of a lookup table from characters to integers
so let's construct such a character array and the way we're going to do this is we're going to take all the words which
is a list of strings we're going to concatenate all of it into a massive string so this is just simply the entire data set as a single
string we're going to pass this to the set constructor which takes this massive
string and throws out duplicates because sets do not allow duplicates
so set of this will just be the set of all the lowercase characters
and there should be a total of 26 of them and now we actually don't want a set we
want a list but we don't want a list sorted in some weird arbitrary way we want it to be
sorted from a to z so sorted list
so those are our characters now what we want is this lookup table as
i mentioned so let's create a special s2i i will call it
um s is string or character and this will be an s2i mapping
for is in enumerate of these characters
so enumerate basically gives us this iterator over the integer index and the
actual element of the list and then we are mapping the character to the integer
so s2i is a mapping from a to 0 b to 1 etc all the way from z to 25
and that's going to be useful here but we actually also have to specifically set that s will be 26
and s to i at e will be 27 right because z was 25.
so those are the lookups and now we can come here and we can map both character 1 and character 2 to
their integers so this will be s2i at character 1 and ix2 will be s2i of character 2.
and now we should be able to do this line but using our array so n at
x1 ix2 this is the two-dimensional array indexing i've shown you before and honestly just plus equals one
because everything starts at zero so this should work
and give us a large 28 by 28 array of all these counts so
if we print n this is the array but of course it looks ugly so let's erase this ugly mess and

# ç”¨ 2D Torch å¼ é‡ç»Ÿè®¡ bigramï¼ˆâ€œè®­ç»ƒæ¨¡å‹â€ï¼‰

æˆ‘ä»¬ç°åœ¨æƒ³æŠŠç»Ÿè®¡ä¿¡æ¯å­˜å‚¨åœ¨ä¸€ä¸ª **äºŒç»´æ•°ç»„ï¼ˆ2D arrayï¼‰** ä¸­ï¼Œè€Œä¸æ˜¯ç”¨ Python å­—å…¸ã€‚
åœ¨è¿™ä¸ªæ•°ç»„ä¸­ï¼š

* **è¡Œï¼ˆrowï¼‰ä»£è¡¨ bigram çš„ç¬¬ä¸€ä¸ªå­—ç¬¦**ï¼Œ
* **åˆ—ï¼ˆcolumnï¼‰ä»£è¡¨ç¬¬äºŒä¸ªå­—ç¬¦**ï¼Œ
* æ¯ä¸ªæ•°ç»„å…ƒç´ ï¼ˆå³äºŒç»´åæ ‡ï¼‰è®°å½•äº†è¿™ä¸ªå­—ç¬¦å¯¹åœ¨æ•°æ®é›†ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ **PyTorch** æ¥æ„å»ºè¿™ä¸ªæ•°ç»„ã€‚PyTorch æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå…¶ä¸­çš„ `torch.tensor` æä¾›äº†åˆ›å»ºå’Œé«˜æ•ˆæ“ä½œå¤šç»´æ•°ç»„çš„åŠŸèƒ½ã€‚

---

### âœ… æ­¥éª¤ 1ï¼šå¯¼å…¥ PyTorch

```python
import torch
```

---

### âœ… æ­¥éª¤ 2ï¼šåˆ›å»ºå¼ é‡ç¤ºä¾‹

```python
a = torch.zeros((3, 5), dtype=torch.int32)
```

* åˆ›å»ºä¸€ä¸ª 3 è¡Œ 5 åˆ—çš„å…¨ 0 æ•´æ•°å¼ é‡ã€‚
* é»˜è®¤æ•°æ®ç±»å‹æ˜¯ `float32`ï¼Œæˆ‘ä»¬æ”¹æˆ `int32` æ˜¯å› ä¸ºè¦ç»Ÿè®¡æ¬¡æ•°ã€‚

ä½ å¯ä»¥åƒè¿™æ ·æ“ä½œå¼ é‡çš„æŸä¸ªå…ƒç´ ï¼š

```python
a[1, 3] = 1     # ç¬¬2è¡Œç¬¬4åˆ—è®¾ä¸º1
a[1, 3] += 1    # ç´¯åŠ 
a[0, 0] = 5     # ç¬¬1è¡Œç¬¬1åˆ—è®¾ä¸º5
```

---

### âœ… æ­¥éª¤ 3ï¼šæ„å»º 28x28 çš„å¤§å¼ é‡

å› ä¸ºæˆ‘ä»¬æœ‰ï¼š

* 26 ä¸ªè‹±æ–‡å­—æ¯ï¼ˆa\~zï¼‰
* åŠ ä¸Šä¸¤ä¸ªç‰¹æ®Šå­—ç¬¦ï¼š`S`ï¼ˆå¼€å§‹ï¼‰å’Œ `E`ï¼ˆç»“æŸï¼‰

æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€ä¸ª **28Ã—28** çš„å¼ é‡æ¥è¡¨ç¤ºæ‰€æœ‰å¯èƒ½çš„ bigramï¼š

```python
N = torch.zeros((28, 28), dtype=torch.int32)
```

---

### âœ… æ­¥éª¤ 4ï¼šå­—ç¬¦è½¬æ•´æ•°çš„æ˜ å°„ï¼ˆlookup è¡¨ï¼‰

å¼ é‡ç´¢å¼•åªèƒ½ç”¨æ•´æ•°ï¼Œä½†å­—ç¬¦æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æŠŠæ¯ä¸ªå­—ç¬¦æ˜ å°„æˆä¸€ä¸ªæ•´æ•°ã€‚

æˆ‘ä»¬å¯ä»¥ä»æ•°æ®é›†ä¸­æ”¶é›†æ‰€æœ‰å­—ç¬¦ï¼š

```python
chars = sorted(list(set(''.join(words))))  # æŠŠæ‰€æœ‰å•è¯æ‹¼æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå†å–å”¯ä¸€å­—ç¬¦ï¼Œå†æ’åº
```

ç„¶åæ„å»ºå­—ç¬¦ â†’ ç´¢å¼•çš„æ˜ å°„å­—å…¸ï¼š

```python
s2i = { ch:i for i, ch in enumerate(chars) }
s2i['S'] = 26   # å¼€å§‹å­—ç¬¦
s2i['E'] = 27   # ç»“æŸå­—ç¬¦
```

---

### âœ… æ­¥éª¤ 5ï¼šå¡«å……å¼ é‡ï¼ˆå³ bigram ç»Ÿè®¡ï¼‰

ç°åœ¨æˆ‘ä»¬ç”¨å­—ç¬¦å¯¹ç´¢å¼•è¿™ä¸ªäºŒç»´å¼ é‡ï¼Œå¹¶å°†å¯¹åº”ä½ç½®çš„å€¼åŠ  1ï¼š

```python
for w in words:
    chs = ['S'] + list(w) + ['E']  # ç»™æ¯ä¸ªå•è¯åŠ ä¸Šèµ·å§‹å’Œç»“æŸç¬¦
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1 = s2i[ch1]
        ix2 = s2i[ch2]
        N[ix1, ix2] += 1
```

è¿™æ ·å°±å®Œæˆäº†æ•´ä¸ªæ•°æ®é›†ä¸­æ‰€æœ‰ bigram çš„ç»Ÿè®¡ï¼Œä¿å­˜åœ¨ä¸€ä¸ªé«˜æ•ˆçš„ PyTorch å¼ é‡ä¸­ã€‚

ä½ å¯ä»¥æ‰“å° `N` çœ‹ç»“æœï¼Œä½†å®ƒæ˜¯ä¸€ä¸ª 28Ã—28 çš„å¤§æ•°ç»„ï¼Œå¯èƒ½ä¼šå¾ˆâ€œä¸‘â€ã€‚æ¥ä¸‹æ¥å¯ä»¥è¿›ä¸€æ­¥åˆ†ææˆ–å¯è§†åŒ–å®ƒã€‚

---

### âœ… æ€»ç»“ï¼š

æˆ‘ä»¬åšäº†è¿™äº›äº‹ï¼š

* ç”¨ `torch.zeros()` å»ºç«‹ 2D ç»Ÿè®¡å¼ é‡
* ç”¨ `set + sorted` æå–æ‰€æœ‰å­—ç¬¦ï¼Œæ„å»ºå­—ç¬¦â†’æ•´æ•°çš„æ˜ å°„
* ç”¨ä¸¤ä¸ªå­—ç¬¦ç»„æˆ bigramï¼Œå¹¶åœ¨å¼ é‡ä¸­ç´¢å¼•å¯¹åº”ä½ç½® +1
* ç”¨ PyTorch é«˜æ•ˆå­˜å‚¨å’Œæ“ä½œå­—ç¬¦é¢‘ç‡ä¿¡æ¯ï¼Œä¸ºåç»­å»ºæ¨¡åšå‡†å¤‡

è¿™ç›¸å½“äºæ˜¯æˆ‘ä»¬ bigram æ¨¡å‹çš„â€œè®­ç»ƒâ€è¿‡ç¨‹ï¼Œåªä¸è¿‡ç”¨çš„æ˜¯æ•°æ•°è€Œä¸æ˜¯ç¥ç»ç½‘ç»œã€‚

---

è¿™è¡Œä»£ç ï¼š

```python
set(''.join(words))
```

çš„ä½œç”¨æ˜¯ï¼š**æå–åˆ—è¡¨ `words` ä¸­æ‰€æœ‰å•è¯ä¸­å‡ºç°è¿‡çš„**â€œ**å”¯ä¸€å­—ç¬¦é›†åˆ**â€ã€‚

---

### ğŸ” åˆ†æ­¥è§£é‡Šï¼š

å‡è®¾ä½ æœ‰ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼š

```python
words = ["emma", "olivia", "ava"]
```

#### ç¬¬ä¸€æ­¥ï¼š`''.join(words)`

æŠŠæ‰€æœ‰å•è¯æ‹¼æ¥æˆä¸€ä¸ªé•¿å­—ç¬¦ä¸²ï¼Œæ²¡æœ‰ç©ºæ ¼ï¼š

```python
''.join(words)  â†’  "emmaoliviaava"
```

#### ç¬¬äºŒæ­¥ï¼š`set(...)`

æŠŠè¿™ä¸ªå­—ç¬¦ä¸²è½¬æ¢æˆä¸€ä¸ªé›†åˆï¼Œè‡ªåŠ¨å»é‡ï¼Œåªä¿ç•™æ¯ä¸ªå­—ç¬¦å‡ºç°ä¸€æ¬¡ï¼š

```python
set("emmaoliviaava") â†’ {'e', 'm', 'a', 'o', 'l', 'i', 'v'}
```

é›†åˆçš„ç‰¹ç‚¹æ˜¯ï¼š**ä¸é‡å¤ã€æ— åº**ã€‚

---

### âœ… ç”¨æ³•åœºæ™¯

è¿™ä¸ªæ“ä½œå¸¸ç”¨æ¥ï¼š

* è·å–è®­ç»ƒæ•°æ®ä¸­ç”¨åˆ°çš„æ‰€æœ‰å­—ç¬¦ï¼ˆå»é‡åï¼‰
* æ„å»ºå­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„ï¼ˆå¦‚ `{'a':0, 'b':1, ...}`ï¼‰

---

### ğŸ§  ä¸¾ä¸ªä¾‹å­å†æ€»ç»“ï¼š

```python
words = ["cat", "car", "dog"]
set(''.join(words))  â†’ {'c', 'a', 't', 'r', 'd', 'o', 'g'}
```

å³ï¼šæŠŠæ‰€æœ‰å•è¯è¿èµ·æ¥ â†’ æ‹†æˆå­—ç¬¦ â†’ å»é‡ â†’ å¾—åˆ°æ‰€æœ‰**å”¯ä¸€å‡ºç°è¿‡çš„å­—ç¬¦**

---

### âœ… æ€»ç»“ä¸€å¥è¯ï¼š

```python
set(''.join(words))
```

è¿™æ˜¯ä¸€ç§å¿«é€Ÿè·å–è®­ç»ƒè¯­æ–™ä¸­**å…¨éƒ¨ä¸é‡å¤å­—ç¬¦**çš„å†™æ³•ã€‚

---

# visualizing the bigram tensor

let's try to visualize it a bit more nicer so for that we're going to use a library
called matplotlib so matplotlib allows us to create figures so we can do things like plt
item show of the counter array so this is the 28x28 array
and this is structure but even this i would say is still pretty ugly so we're going to try to create a much
nicer visualization of it and i wrote a bunch of code for that the first thing we're going to need is
we're going to need to invert this array here this dictionary so s2i
is mapping from s to i and in i2s we're going to reverse this dictionary so iterator of all the items
and just reverse that array so i2s maps inversely from 0 to a 1 to b etc
so we'll need that and then here's the code that i came up with to try to make this a little bit
nicer we create a figure we plot
n and then we do and then we visualize a bunch of things later let me just run it so you get a sense of what this is
okay so you see here that we have the array spaced out
and every one of these is basically like b follows g zero times
b follows h 41 times um so a follows j 175 times
and so what you can see that i'm doing here is first i show that entire array
and then i iterate over all the individual little cells here and i create a character string here
which is the inverse mapping i2s of the integer i and the integer j so those are
the bi-grams in a character representation and then i plot just the diagram text
and then i plot the number of times that this bigram occurs now the reason that there's a dot item
here is because when you index into these arrays these are torch tensors
you see that we still get a tensor back so the type of this thing you'd think it
would be just an integer 149 but it's actually a torch.tensor and so if you do dot item then it will pop out
that in individual integer so it will just be 149.
so that's what's happening there and these are just some options to make it look nice so what is the structure of this array
we have all these counts and we see that some of them occur often and some of them do not occur often now if you scrutinize this carefully you

# å¯è§†åŒ– bigram å¼ é‡

ç°åœ¨æˆ‘ä»¬æƒ³æŠŠç»Ÿè®¡å¥½çš„ bigram å¼ é‡ï¼ˆ28Ã—28ï¼‰**æ›´æ¼‚äº®åœ°å¯è§†åŒ–**å‡ºæ¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå«åš **matplotlib** çš„å¯è§†åŒ–åº“ï¼Œå®ƒå¯ä»¥åˆ›å»ºå›¾è¡¨å’Œå›¾å½¢ã€‚

---

### âœ… ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ matplotlib ç®€å•å¯è§†åŒ–

æˆ‘ä»¬å¯ä»¥ç”¨ `matplotlib.pyplot` ä¸­çš„ `imshow` æ–¹æ³•å±•ç¤ºäºŒç»´æ•°ç»„ï¼š

```python
import matplotlib.pyplot as plt

plt.imshow(N)  # N æ˜¯ 28Ã—28 çš„ bigram è®¡æ•°å¼ é‡
plt.show()
```

è¿™ä¼šç”»å‡ºä¸€ä¸ªçŸ©é˜µçƒ­å›¾ï¼Œä½†æ ·å­å¯èƒ½è¿˜æ˜¯æ¯”è¾ƒâ€œä¸‘â€ï¼Œä¸å¤ªç›´è§‚ã€‚

---

### âœ… ç¬¬äºŒæ­¥ï¼šæ„é€ æ›´ç¾è§‚çš„å¯è§†åŒ–

æˆ‘ä»¬å¯ä»¥æ”¹è¿›å¯è§†åŒ–æ•ˆæœã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦æŠŠåŸæ¥çš„å­—ç¬¦â†’ç´¢å¼•å­—å…¸ `s2i` åè½¬ï¼Œå¾—åˆ°ä¸€ä¸ªä»ç´¢å¼•â†’å­—ç¬¦çš„å­—å…¸ `i2s`ï¼š

```python
i2s = {i: s for s, i in s2i.items()}
```

è¿™æ ·æˆ‘ä»¬å°±èƒ½åœ¨å›¾é‡Œæ˜¾ç¤ºå­—ç¬¦è€Œä¸æ˜¯æ•°å­—ï¼Œä¾‹å¦‚æ¨ªè½´æ˜¯å­—ç¬¦ bï¼Œçºµè½´æ˜¯å­—ç¬¦ gï¼Œè¡¨ç¤ºçš„æ˜¯ â€œg åé¢æ¥ b å‡ºç°äº†å‡ æ¬¡â€ã€‚

---

### âœ… ç¬¬ä¸‰æ­¥ï¼šå®Œæ•´å¯è§†åŒ–é€»è¾‘

ä½ å¯ä»¥ä½¿ç”¨å¦‚ä¸‹ç»“æ„è¿›è¡Œæ›´è¯¦ç»†çš„ç»˜åˆ¶ï¼ˆä¼ªä»£ç ç»“æ„å¦‚ä¸‹ï¼‰ï¼š

```python
plt.figure(figsize=(16, 16))  # åˆ›å»ºå¤§ç”»å¸ƒ
plt.imshow(N, cmap='Blues')   # ç”¨è“è‰²è‰²å¸¦æ˜¾ç¤ºè®¡æ•°çŸ©é˜µ
for i in range(28):
    for j in range(28):
        chstr = i2s[i] + i2s[j]             # æ¯”å¦‚ "th"ã€"ar" ç­‰ bigram
        count = N[i, j].item()              # è·å–å…·ä½“æ•°å­—ï¼ˆtensor â†’ intï¼‰
        plt.text(j, i, chstr, ha='center', va='bottom', color='gray')
        plt.text(j, i, count, ha='center', va='top', color='black')
plt.axis('off')
plt.show()
```

---

### âœ… è¡¥å……è§£é‡Šï¼š

* `N[i, j]` æ˜¯ torch tensorï¼Œå¦‚æœä½ ç›´æ¥æ‰“å°æ˜¯ `tensor(41)` ä¹‹ç±»çš„å¯¹è±¡ã€‚
  ç”¨ `.item()` å¯ä»¥å°†å…¶å˜ä¸ºæ™®é€šæ•´æ•°ï¼Œå¦‚ `41`ã€‚
* `plt.text(...)` æ˜¯åœ¨å›¾çš„æŸä¸ªä½ç½®å†™å­—ï¼Œæ¯”å¦‚ bigram çš„å­—ç¬¦å¯¹å’Œå‡ºç°æ¬¡æ•°ã€‚
* å…³é—­åæ ‡è½´ (`axis('off')`) è®©å›¾æ›´æ¸…çˆ½ã€‚
* è¿™æ ·ä½ ä¼šçœ‹åˆ°ä¸€ä¸ª 28Ã—28 çš„å­—ç¬¦çŸ©é˜µï¼Œæ¯ä¸ªæ ¼å­æ ‡æ³¨äº†ï¼š

  * å“ªä¸ªå­—ç¬¦ç»„åˆï¼ˆbigramï¼‰
  * å‡ºç°äº†å¤šå°‘æ¬¡

---

### âœ… æ€»ç»“ï¼š

æˆ‘ä»¬åšçš„äº‹æ˜¯ï¼š

1. ç”¨ PyTorch æ„å»ºäº†ä¸€ä¸ª 28Ã—28 çš„ bigram è®¡æ•°å¼ é‡ï¼›
2. ç”¨ matplotlib å±•ç¤ºäº†è¿™ä¸ªå¼ é‡ï¼Œè§†è§‰åŒ–ç»Ÿè®¡ç»“æ„ï¼›
3. ç”¨å­—ç¬¦æ ‡æ³¨æ¯ä¸ª bigram åŠå…¶é¢‘æ¬¡ï¼Œæå‡å¯è¯»æ€§ï¼›
4. å­¦ä¼šäº† `.item()` æŠŠ tensor è½¬æ¢ä¸º Python æ•´æ•°ã€‚

è¿™ä¸ªå›¾èƒ½ç›´è§‚åæ˜ å“ªäº› bigram å¾ˆå¸¸è§ï¼Œå“ªäº›å‡ ä¹æ²¡å‡ºç°ã€‚åˆ†æè¿™äº›ç»“æ„å¯¹æ„å»ºè¯­è¨€æ¨¡å‹éå¸¸æœ‰å¸®åŠ©ã€‚

---

è¿™æ®µä»£ç ç”¨äº**åœ¨ Jupyter Notebook ä¸­å¯è§†åŒ–ä¸€ä¸ªäºŒç»´å¼ é‡ï¼ˆä¾‹å¦‚ bigram ç»Ÿè®¡çŸ©é˜µï¼‰**ã€‚
ä¸‹é¢é€è¡Œè§£é‡Šï¼š

---

### ğŸ”¸ `import matplotlib.pyplot as plt`

* å¯¼å…¥ Python ä¸­æœ€å¸¸ç”¨çš„å¯è§†åŒ–åº“ **Matplotlib** çš„å­æ¨¡å— `pyplot`ã€‚
* å¹¶ç»™å®ƒèµ·äº†ä¸€ä¸ªç®€å†™å `plt`ï¼Œæ–¹ä¾¿åé¢ä½¿ç”¨ã€‚
* `pyplot` æä¾›äº†ç±»ä¼¼ MATLAB çš„ç»˜å›¾ APIï¼Œæ¯”å¦‚ç”»å›¾ã€æ˜¾ç¤ºå›¾åƒã€æ·»åŠ æ–‡å­—ç­‰ã€‚

---

### ğŸ”¸ `%matplotlib inline`

* è¿™æ˜¯ **Jupyter Notebook** ç‰¹æœ‰çš„â€œé­”æ³•å‘½ä»¤â€ï¼ˆmagic commandï¼‰ã€‚
* å®ƒçš„ä½œç”¨æ˜¯ï¼š**è®©å›¾åƒç›´æ¥æ˜¾ç¤ºåœ¨ notebook å•å…ƒæ ¼ä¸­ï¼Œè€Œä¸æ˜¯å¼¹å‡ºä¸€ä¸ªæ–°çª—å£**ã€‚
* ä½ åªéœ€è¦åœ¨ notebook çš„å¼€å¤´è¿è¡Œä¸€æ¬¡è¿™ä¸ªå‘½ä»¤ã€‚

---

### ğŸ”¸ `plt.imshow(N)`

* `imshow` æ˜¯ `pyplot` æä¾›çš„å‡½æ•°ï¼Œç”¨æ¥**ä»¥å›¾åƒå½¢å¼å±•ç¤ºä¸€ä¸ªäºŒç»´æ•°ç»„**ã€‚
* `N` æ˜¯ä¸€ä¸ª **28Ã—28 çš„ PyTorch å¼ é‡**ï¼Œè®°å½•äº† bigram å‡ºç°çš„é¢‘ç‡ã€‚
* `imshow` ä¼šæŠŠå®ƒå½“æˆâ€œåƒç´ çŸ©é˜µâ€ç”»å‡ºæ¥ï¼Œæ•°å€¼è¶Šå¤§ï¼Œé¢œè‰²è¶Šæ·±ï¼ˆé»˜è®¤æ˜¯å½©è‰²çƒ­å›¾ï¼‰ã€‚

ç­‰ä»·äºå¯è§†åŒ–ä¸€ä¸ªâ€œç»Ÿè®¡çƒ­åŠ›å›¾â€æˆ–â€œç°åº¦å›¾â€ã€‚

---

### âœ… ä¸¾ä¸ªä¾‹å­ï¼š

å‡è®¾ï¼š

```python
N = torch.tensor([[1, 2], [3, 4]])
plt.imshow(N)
```

ä¼šæ˜¾ç¤ºä¸€ä¸ª 2Ã—2 çš„å½©è‰²æ–¹æ ¼å›¾ï¼Œå…¶ä¸­ï¼š

* é¢œè‰²æ·±æµ…è¡¨ç¤ºæ•°å­—çš„å¤§å°ï¼Œ
* å·¦ä¸Šè§’æ˜¯ `1`ï¼Œå³ä¸‹è§’æ˜¯ `4`ï¼Œé¢œè‰²æœ€æ·±ã€‚

---

### âœ… æ€»ç»“ï¼š

| è¡Œå·                                | å«ä¹‰                             |
| --------------------------------- | ------------------------------ |
| `import matplotlib.pyplot as plt` | å¯¼å…¥ç»˜å›¾åº“                          |
| `%matplotlib inline`              | è®©å›¾åµŒå…¥åœ¨ Jupyter å•å…ƒæ ¼ä¸­æ˜¾ç¤º           |
| `plt.imshow(N)`                   | æŠŠå¼ é‡ N ä½œä¸ºå›¾åƒæ˜¾ç¤ºå‡ºæ¥ï¼ˆæ¯”å¦‚ bigram ç»Ÿè®¡çƒ­å›¾ï¼‰ |

è¿™åœ¨åˆ†æç¥ç»ç½‘ç»œä¸­çš„çŸ©é˜µç»“æ„ã€æ³¨æ„åŠ›å›¾ã€ç»Ÿè®¡é¢‘æ¬¡ç­‰åœºæ™¯ä¸­éå¸¸å¸¸ç”¨ã€‚

---

è¿™æ®µä»£ç æ˜¯åœ¨ **Jupyter Notebook ä¸­å¯è§†åŒ– bigram ç»Ÿè®¡çŸ©é˜µ** `N`ï¼Œå¹¶åœ¨å›¾ä¸­**æ ‡æ³¨å­—ç¬¦ç»„åˆåŠå…¶å‡ºç°æ¬¡æ•°**ã€‚ä¸‹é¢é€è¡Œè¯¦ç»†è§£é‡Šï¼š

---

### ğŸ”¹ `import matplotlib.pyplot as plt`

å¯¼å…¥å¯è§†åŒ–åº“ Matplotlib çš„ `pyplot` æ¨¡å—ï¼Œå¹¶ç®€å†™ä¸º `plt`ï¼Œç”¨äºç»˜å›¾ã€‚

---

### ğŸ”¹ `%matplotlib inline`

è¿™æ˜¯ Jupyter çš„â€œé­”æ³•å‘½ä»¤â€ï¼Œç¡®ä¿å›¾åƒåœ¨ notebook å•å…ƒæ ¼å†…ç›´æ¥æ˜¾ç¤ºï¼Œè€Œä¸æ˜¯å¼¹å‡ºçª—å£ã€‚

---

### ğŸ”¹ `plt.figure(figsize=(16, 16))`

åˆ›å»ºä¸€ä¸ªæ–°çš„ç»˜å›¾çª—å£ï¼ˆç”»å¸ƒï¼‰ï¼Œå¤§å°ä¸º 16Ã—16 è‹±å¯¸ã€‚

* å›¾åƒè¶Šå¤§ï¼Œè¶Šæ¸…æ™°ï¼Œé€‚åˆå±•ç¤ºå¯†é›†çš„æ–‡æœ¬ä¿¡æ¯ï¼ˆæ¯”å¦‚ 28Ã—28 çš„ bigram çŸ©é˜µï¼‰ã€‚

---

### ğŸ”¹ `plt.imshow(N, cmap='Blues')`

å°†å¼ é‡ `N` å¯è§†åŒ–ä¸ºä¸€å¹…å›¾åƒï¼ˆçƒ­åŠ›å›¾ï¼‰ï¼š

* `N` æ˜¯ä¸€ä¸ª 28Ã—28 çš„äºŒç»´å¼ é‡ï¼Œè¡¨ç¤ºå­—ç¬¦ bigram çš„å‡ºç°é¢‘ç‡ã€‚
* `cmap='Blues'` è¡¨ç¤ºä½¿ç”¨â€œè“è‰²â€è‰²å¸¦ï¼Œå€¼è¶Šå¤§é¢œè‰²è¶Šæ·±ã€‚

---

### ğŸ”¹ åŒå±‚ `for` å¾ªç¯ï¼šé€æ ¼æ ‡æ³¨å­—ç¬¦å’Œè®¡æ•°

```python
for i in range(28):
  for j in range(28):
```

éå†çŸ©é˜µä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œi è¡¨ç¤ºè¡Œç´¢å¼•ï¼ˆå‰ä¸€ä¸ªå­—ç¬¦ï¼‰ï¼Œj è¡¨ç¤ºåˆ—ç´¢å¼•ï¼ˆåä¸€ä¸ªå­—ç¬¦ï¼‰ã€‚

#### ğŸ”¸ `chstr = itos[i] + itos[j]`

* `itos` æ˜¯ç´¢å¼•è½¬å­—ç¬¦çš„å­—å…¸ï¼ˆä¾‹å¦‚ï¼š`{0:'a', 1:'b', ..., 26:'S', 27:'E'}`ï¼‰
* æŠŠæ•´æ•°ç´¢å¼• `i` å’Œ `j` è½¬æ¢ä¸ºå­—ç¬¦åæ‹¼æ¥æˆ bigram ç»„åˆï¼Œæ¯”å¦‚ `"th"`ã€`"an"` ç­‰ã€‚

#### ğŸ”¸ `plt.text(j, i, chstr, ha='center', va='bottom', color='gray')`

* åœ¨å›¾åƒä¸­ (j, i) ä½ç½®æ ‡æ³¨ bigram å­—ç¬¦ä¸² `chstr`ï¼Œæ¯”å¦‚ `"th"`
* `ha='center'`ï¼šæ°´å¹³å±…ä¸­ï¼›`va='bottom'`ï¼šå‚ç›´åº•éƒ¨å¯¹é½
* `color='gray'`ï¼šä½¿ç”¨ç°è‰²å­—ä½“

#### ğŸ”¸ `plt.text(j, i, N[i, j].item(), ha='center', va='top', color='gray')`

* åœ¨ç›¸åŒä½ç½®å†æ ‡æ³¨è¯¥ bigram å‡ºç°çš„æ¬¡æ•°
* `.item()` å°† `torch.tensor` è½¬æ¢ä¸º Python æ•´æ•°ï¼ˆå¦åˆ™æ‰“å°ä¸å‡ºæ¥ï¼‰

---

### ğŸ”¹ `plt.axis('off')`

éšè—å›¾åƒçš„åæ ‡è½´ï¼Œè®©å›¾è¡¨æ›´æ¸…çˆ½ã€‚

---

### âœ… æœ€ç»ˆæ•ˆæœï¼š

* å›¾åƒä¸­æ˜¯ä¸€ä¸ª 28Ã—28 çš„æ ¼å­çŸ©é˜µ
* æ¯æ ¼ä¸Šæ–¹æ˜¾ç¤ºå­—ç¬¦ç»„åˆï¼ˆbigramï¼‰ï¼Œä¸‹æ–¹æ˜¾ç¤ºå®ƒçš„å‡ºç°æ¬¡æ•°
* è“è‰²æ·±æµ…ä»£è¡¨å‡ºç°é¢‘ç‡ï¼šé¢œè‰²è¶Šæ·±ï¼Œå‡ºç°è¶Šå¤š
* éå¸¸é€‚åˆè§‚å¯Ÿå“ªäº›å­—ç¬¦ç»„åˆåœ¨æ•°æ®ä¸­æœ€å¸¸è§ï¼Œå“ªäº›ä»æœªå‡ºç°

---

### ğŸ¯ æ€»ç»“ä¸€å¥è¯ï¼š

è¿™æ®µä»£ç æŠŠå­—ç¬¦çº§ bigram çš„ç»Ÿè®¡ç»“æœ `N` è½¬æ¢æˆä¸€ä¸ª **ç¾è§‚çš„å¯è§†åŒ–çƒ­åŠ›å›¾**ï¼Œæ¯ä¸€æ ¼æ¸…æ™°åœ°æ ‡å‡ºå­—ç¬¦å¯¹å’Œå®ƒçš„å‡ºç°æ¬¡æ•°ï¼Œéå¸¸é€‚åˆæ•™å­¦æˆ–æ¨¡å‹åˆ†æä½¿ç”¨ã€‚

---

# deleting spurious (S) and (E) tokens in favor of a single . token

will notice that we're not actually being very clever that's because when you come over here you'll notice that for example we have
an entire row of completely zeros and that's because the end character is never possibly going to be the first
character of a bi-gram because we're always placing these end tokens all at the end of the diagram
similarly we have entire columns zeros here because the s
character will never possibly be the second element of a bigram because we always start with s and we end with e
and we only have the words in between so we have an entire column of zeros an entire row of zeros and in this little
two by two matrix here as well the only one that can possibly happen is if s directly follows e
that can be non-zero if we have a word that has no letters so in that case
there's no letters in the word it's an empty word and we just have s follows e but the other ones are just not possible
and so we're basically wasting space and not only that but the s and the e are getting very crowded here
i was using these brackets because there's convention and natural language processing to use these kinds of brackets to denote special tokens
but we're going to use something else so let's fix all this and make it prettier
we're not actually going to have two special tokens we're only going to have one special token so
we're going to have n by n array of 27 by 27 instead
instead of having two we will just have one and i will call it a dot
okay let me swing this over here
now one more thing that i would like to do is i would actually like to make this special character half position zero
and i would like to offset all the other letters off i find that a little bit more pleasing
so we need a plus one here so that the first character which is a will start at
one so s2i will now be a starts at one and dot is 0
and i2s of course we're not changing this because i2s just creates a reverse mapping and this will work fine so 1 is
a 2 is b 0 is dot so we've reversed that here
we have a dot and a dot this should work fine
make sure i start at zeros count and then here we don't go up to 28 we go
up to 27 and this should just work
okay so we see that dot never happened it's at zero because we don't have empty words
then this row here now is just uh very simply the um counts for all the first letters so
uh j starts a word h starts a word i starts a word etc and then these are all
the ending characters and in between we have the structure of what characters follow each other
so this is the counts array of our entire data set so this array actually has all

# åˆ é™¤å¤šä½™çš„ (S) å’Œ (E) æ ‡è®°ï¼Œæ”¹ç”¨ä¸€ä¸ªç»Ÿä¸€çš„ `.` ç‰¹æ®Šç¬¦å·

æˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°ï¼Œä¹‹å‰çš„å¤„ç†æ–¹å¼å…¶å®ä¸æ˜¯å¾ˆèªæ˜ã€‚æ¯”å¦‚ä½ çœ‹å¯è§†åŒ–å›¾æ—¶ï¼Œä¼šå‘ç°ï¼š

* æœ‰ä¸€æ•´è¡Œæ˜¯å…¨ 0 çš„ï¼Œé‚£æ˜¯å› ä¸ºç»“å°¾ç¬¦å· `E` ä¸å¯èƒ½å‡ºç°åœ¨ bigram çš„ç¬¬ä¸€ä¸ªä½ç½®ï¼ˆå³ä¸ä¼šä½œä¸ºèµ·å§‹å­—ç¬¦ï¼‰ã€‚

  * å› ä¸ºæˆ‘ä»¬åªåœ¨æ¯ä¸ªå•è¯æœ«å°¾æ”¾ `E`ï¼Œæ‰€ä»¥å®ƒæ°¸è¿œä¸ä¼šä½œä¸ºå‰ä¸€ä¸ªå­—ç¬¦ã€‚

* åŒæ ·ï¼Œä¹Ÿæœ‰ä¸€æ•´åˆ—æ˜¯å…¨ 0 çš„ï¼Œé‚£æ˜¯å› ä¸ºå¼€å§‹ç¬¦å· `S` ä¸ä¼šå‡ºç°åœ¨ bigram çš„ç¬¬äºŒä¸ªä½ç½®ï¼ˆå³ä¸ä¼šä½œä¸ºç»“æŸå­—ç¬¦ï¼‰ã€‚

  * å› ä¸ºæˆ‘ä»¬åªåœ¨å•è¯å‰é¢åŠ  `S`ï¼Œä»æ¥ä¸ä¼šè®©å®ƒå‡ºç°åœ¨ bigram çš„ç»“å°¾ã€‚

æ­¤å¤–ï¼Œåœ¨å·¦ä¸Šè§’çš„è¿™ä¸ªå° 2Ã—2 çŸ©é˜µï¼ˆ`S` å’Œ `E` äº¤å‰ï¼‰ä¸­ï¼Œ**å”¯ä¸€å¯èƒ½çš„ç»„åˆæ˜¯ `S` ç›´æ¥è·Ÿç€ `E`**ï¼ˆå³ç©ºå•è¯çš„æƒ…å†µï¼‰ï¼Œå…¶ä»–ç»„åˆæ ¹æœ¬ä¸å¯èƒ½å‡ºç°ã€‚

æ‰€ä»¥æˆ‘ä»¬æµªè´¹äº†ä¸å°‘ç©ºé—´ï¼Œå¹¶ä¸” `S` å’Œ `E` çš„ä½ç½®åœ¨å›¾ä¸­å¾ˆæ‹¥æŒ¤ã€‚

è™½ç„¶ä½¿ç”¨ `S` å’Œ `E` ç¬¦å·æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å¸¸è§çš„åšæ³•ï¼ˆç”¨ç±»ä¼¼æ–¹æ‹¬å·çš„æ ‡è®°è¡¨ç¤ºç‰¹æ®Šç¬¦å·ï¼‰ï¼Œä½†æˆ‘ä»¬ç°åœ¨å†³å®šç”¨æ›´ç®€æ´çš„æ–¹å¼æ¥å¤„ç†ï¼š

---

### âœ… æˆ‘ä»¬å°†åšå‡ºä»¥ä¸‹æ”¹å˜ï¼š

1. **ä¸å†ä½¿ç”¨ä¸¤ä¸ªç‰¹æ®Šå­—ç¬¦ `S` å’Œ `E`**ï¼Œç»Ÿä¸€æ”¹ä¸ºä¸€ä¸ªç‰¹æ®Šå­—ç¬¦ `.`ï¼ˆç‚¹å·ï¼‰ã€‚
2. **æŠŠæ•°ç»„å¤§å°ä» 28Ã—28 æ”¹ä¸º 27Ã—27**ï¼š

   * 26 ä¸ªå­—æ¯ + 1 ä¸ªç‚¹å·ï¼Œæ€»å…± 27 ä¸ªå­—ç¬¦ã€‚

---

### âœ… é¢å¤–ç¾åŒ–å¤„ç†ï¼š

æˆ‘ä»¬è¿˜å¸Œæœ›è¿™ä¸ªç‰¹æ®Šç¬¦å· `.` æ’åœ¨å­—ç¬¦è¡¨çš„æœ€å‰é¢ï¼Œå¯¹åº”ç´¢å¼• `0`ï¼Œ
ç„¶å `'a'` ä»ç´¢å¼• `1` å¼€å§‹ï¼Œ`'b'` æ˜¯ `2`ï¼Œä»¥æ­¤ç±»æ¨ã€‚

å› æ­¤ï¼š

```python
s2i = {
    '.': 0,
    'a': 1,
    'b': 2,
    ...
    'z': 26
}
```

`i2s`ï¼ˆç´¢å¼•è½¬å­—ç¬¦ï¼‰æ˜¯ `s2i` çš„åè½¬å­—å…¸ï¼Œæ‰€ä»¥æ²¡é—®é¢˜ï¼Œå®ƒä¼šæ­£ç¡®æ˜ å°„å›å»ã€‚

---

### âœ… æ›´æ–°åçš„è¡Œä¸ºï¼š

* ç©ºå•è¯ä¸ä¼šå‡ºç°ï¼Œæ‰€ä»¥ `.` â†’ `.` çš„ bigramï¼ˆå³ `N[0][0]`ï¼‰è®¡æ•°ä¸º 0ã€‚
* ç¬¬ä¸€è¡Œè®°å½•äº†ä»¥ `.` å¼€å¤´çš„ bigramï¼Œè¡¨ç¤ºæŸå­—ç¬¦æ˜¯å¦æ˜¯å•è¯èµ·å§‹å­—ç¬¦ã€‚
* ç¬¬ä¸€åˆ—è®°å½•äº†ä»¥æŸå­—ç¬¦ç»“å°¾çš„ bigramï¼ˆâ†’ `.`ï¼‰ï¼Œè¡¨ç¤ºè¿™äº›å­—ç¬¦ä½œä¸ºå•è¯ç»“å°¾ã€‚
* ä¸­é—´çš„åŒºåŸŸè®°å½•äº†æ™®é€šå­—ç¬¦ä¹‹é—´çš„ bigram å…³ç³»ã€‚

è¿™æ ·å¤„ç†åï¼Œæ•°æ®æ›´ç´§å‡‘ã€ç»“æ„æ›´æ¸…æ™°ã€å¯è§†åŒ–ä¹Ÿæ›´ç¾è§‚ã€‚

---

### âœ… æ€»ç»“ï¼š

æˆ‘ä»¬å°†ï¼š

* ç”¨ä¸€ä¸ªç»Ÿä¸€çš„ç‰¹æ®Šå­—ç¬¦ `.` ä»£æ›¿ä¹‹å‰çš„ `S`ï¼ˆstartï¼‰å’Œ `E`ï¼ˆendï¼‰ï¼›
* æŠŠç»Ÿè®¡çŸ©é˜µä» 28Ã—28 ç¼©å°ä¸º 27Ã—27ï¼›
* å¹¶è°ƒæ•´å­—ç¬¦ç´¢å¼•ï¼Œä½¿ `.` å¯¹åº”ä½ç½® 0ï¼Œå…¶å®ƒå­—ç¬¦ä» 1 å¼€å§‹ï¼›
* è¿™æ ·å¯ä»¥å‡å°‘å†—ä½™ã€èŠ‚çœç©ºé—´ã€æé«˜é€»è¾‘æ¸…æ™°åº¦ã€‚

# sampling from the model

the information necessary for us to actually sample from this bigram uh character level language model
and um roughly speaking what we're going to do is we're just going to start following these probabilities and these
counts and we're going to start sampling from the from the model so in the beginning of course
we start with the dot the start token dot so to sample the first character of a
name we're looking at this row here so we see that we have the counts and
those concepts terminally are telling us how often any one of these characters is to start a word
so if we take this n and we grab the first row
we can do that by using just indexing as zero and then using this notation column for
the rest of that row so n zero colon is indexing into the zeroth
row and then it's grabbing all the columns and so this will give us a one-dimensional array
of the first row so zero four four ten you know zero four four ten one three oh
six one five four two etc it's just the first row the shape of this is 27 it's just the row of 27
and the other way that you can do this also is you just you don't need to actually give this you just grab the zeroth row like this
this is equivalent now these are the counts and now what we'd like to do is we'd
like to basically um sample from this since these are the raw counts we actually have to convert this to
probabilities so we create a probability vector
so we'll take n of zero and we'll actually convert this to float
first okay so these integers are converted to float floating point numbers and the reason
we're creating floats is because we're about to normalize these counts so to create a probability distribution
here we want to divide we basically want to do p p p divide p
that sum and now we get a vector of smaller
numbers and these are now probabilities so of course because we divided by the sum the sum of p now is 1.
so this is a nice proper probability distribution it sums to 1 and this is giving us the probability for any single
character to be the first character of a word so now we can try to sample from this
distribution to sample from these distributions we're going to use storch.multinomial which i've pulled up
here so torch.multinomial returns uh
samples from the multinomial probability distribution which is a complicated way of saying you give me probabilities and
i will give you integers which are sampled according to the property distribution
so this is the signature of the method and to make everything deterministic we're going to use a generator object in
pytorch so this makes everything deterministic so when you run this on your computer
you're going to the exact get the exact same results that i'm getting here on my computer so let me show you how this works
here's the deterministic way of creating a torch generator object
seeding it with some number that we can agree on so that seeds a generator gets gives us
an object g and then we can pass that g to a function that creates um
here random numbers twerk.rand creates random numbers three of them
and it's using this generator object to as a source of randomness
so without normalizing it i can just print
this is sort of like numbers between 0 and 1 that are random according to this thing and whenever i run it again
i'm always going to get the same result because i keep using the same generator object which i'm seeing here
and then if i divide to normalize i'm going to get a nice
probability distribution of just three elements and then we can use torsion multinomial
to draw samples from it so this is what that looks like tertiary multinomial we'll take the
torch tensor of probability distributions then we can ask for a number of samples
let's say 20. replacement equals true means that when we draw an element
we will uh we can draw it and then we can put it back into the list of eligible indices to draw again
and we have to specify replacement as true because by default uh for some reason it's false
and i think you know it's just something to be careful with and the generator is passed in here so
we're going to always get deterministic results the same results so if i run these two
we're going to get a bunch of samples from this distribution now you'll notice here that the
probability for the first element in this tensor is 60
so in these 20 samples we'd expect 60 of them to be zero
we'd expect thirty percent of them to be one and because the the element index two
has only ten percent probability very few of these samples should be two and indeed we only have a small number of
twos and we can sample as many as we'd like and the more we sample the more
these numbers should um roughly have the distribution here so we should have lots of zeros
half as many um ones and we should have um three times
as few oh sorry s few ones and three times as few uh
twos so you see that we have very few twos we have some ones and most of them are zero
so that's what torsion multinomial is doing for us here
we are interested in this row we've created this p here
and now we can sample from it so if we use the same seed
and then we sample from this distribution let's just get one sample
then we see that the sample is say 13. so this will be the index
and let's you see how it's a tensor that wraps 13 we again have to use that item
to pop out that integer and now index would be just the number 13.
and of course the um we can do we can map the i2s of ix to figure out
exactly which character we're sampling here we're sampling m so we're saying that the first character
is in our generation and just looking at the road here
m was drawn and you we can see that m actually starts a large number of words uh m
started 2 500 words out of 32 000 words so almost
a bit less than 10 percent of the words start with them so this was actually a fairly likely character to draw
um so that would be the first character of our work and now we can continue to sample more characters because now we
know that m started m is already sampled so now to draw the next character we
will come back here and we will look for the row that starts with m
so you see m and we have a row here so we see that m dot is
516 m a is this many and b is this many etc so these are the counts for the next
row and that's the next character that we are going to now generate so i think we are ready to actually just
write out the loop because i think you're starting to get a sense of how this is going to go the um
we always begin at index 0 because that's the start token
and then while true we're going to grab the row corresponding to index
that we're currently on so that's p so that's n array at ix
converted to float is rp then we normalize
this p to sum to one i accidentally ran the infinite loop we
normalize p to something one then we need this generator object
now we're going to initialize up here and we're going to draw a single sample from this distribution
and then this is going to tell us what index is going to be next
if the index sampled is 0 then that's now the end token
so we will break otherwise we are going to print
s2i of ix i2s
and uh that's pretty much it we're just uh this should work okay more
so that's that's the name that we've sampled we started with m the next step was o then r and then dot
and this dot we it here as well so
let's now do this a few times so let's actually create an
out list here and instead of printing we're going to
append so out that append this character
and then here let's just print it at the end so let's just join up all the outs and we're just going to print more okay
now we're always getting the same result because of the generator so if we want to do this a few times we
can go for i in range 10 we can sample 10 names
and we can just do that 10 times and these are the names that we're getting out
let's do 20.
i'll be honest with you this doesn't look right so i started a few minutes to convince myself that it actually is right
the reason these samples are so terrible is that bigram language model is actually look just like really
terrible we can generate a few more here and you can see that they're kind of like their name like a little bit like
yanu o'reilly etc but they're just like totally messed up um
and i mean the reason that this is so bad like we're generating h as a name but you have to think through
it from the model's eyes it doesn't know that this h is the very first h all it
knows is that h was previously and now how likely is h the last character well
it's somewhat likely and so it just makes it last character it doesn't know that there were other things before it or there
were not other things before it and so that's why it's generating all these like nonsense names
another way to do this is to convince yourself that this is actually doing something reasonable even
though it's so terrible is these little piece here are 27 right
like 27. so how about if we did something like this
instead of p having any structure whatsoever how about if p was just torch dot once
of 27 by default this is a float 32 so this is fine divide 27
so what i'm doing here is this is the uniform distribution which will make everything equally likely
and we can sample from that so let's see if that does any better okay so it's
this is what you have from a model that is completely untrained where everything is equally likely so it's obviously
garbage and then if we have a trained model which is trained on just bi-grams
this is what we get so you can see that it is more name-like it is actually working it's just um
my gram is so terrible and we have to do better now next i would like to fix an inefficiency that we have going on here

# ä»æ¨¡å‹ä¸­è¿›è¡Œé‡‡æ ·ï¼ˆSampling from the modelï¼‰

è¿™ä¸€éƒ¨åˆ†è®²è§£äº†å¦‚ä½•**æ ¹æ® bigram å­—ç¬¦çº§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–°å•è¯ï¼ˆæ¯”å¦‚åå­—ï¼‰**ï¼Œå³â€œä»æ¨¡å‹ä¸­é‡‡æ ·â€ã€‚

---

### ğŸ”¹ æ€»ä½“æµç¨‹

1. æ¨¡å‹çš„è¾“å…¥æ˜¯å­—ç¬¦å¯¹ï¼ˆbigramï¼‰ç»Ÿè®¡çŸ©é˜µ `N`ï¼ˆ27Ã—27ï¼‰
2. æˆ‘ä»¬ä»ç‰¹æ®Šèµ·å§‹ç¬¦å· `.`ï¼ˆç´¢å¼• 0ï¼‰å¼€å§‹
3. æ¯ä¸€æ­¥æ ¹æ®å½“å‰å­—ç¬¦å¯¹åº”çš„è¡Œï¼ˆå³å½“å‰å­—ç¬¦åé¢å¯èƒ½æ¥ä»€ä¹ˆï¼‰ï¼Œ**æŒ‰æ¦‚ç‡é‡‡æ ·ä¸‹ä¸€ä¸ªå­—ç¬¦**
4. å¦‚æœé‡‡åˆ° `.`ï¼Œè¡¨ç¤ºç»“æŸï¼Œé‡‡æ ·ç»ˆæ­¢
5. å¦åˆ™ç»§ç»­é‡‡æ ·ä¸‹ä¸€ä¸ªå­—ç¬¦

---

## âœ… æ­¥éª¤è¯¦è§£

---

### ğŸ”¸ ç¬¬ä¸€æ­¥ï¼šå–èµ·å§‹è¡Œï¼ˆå³ç‚¹å·å¼€å¤´çš„é¢‘ç‡åˆ†å¸ƒï¼‰

```python
N[0]  # å–å¼ é‡ç¬¬0è¡Œï¼Œå¯¹åº”ä»â€œèµ·å§‹â€å­—ç¬¦å‡ºå‘çš„ bigram ç»Ÿè®¡
```

å¾—åˆ°ä¸€ä¸ªå¤§å°ä¸º 27 çš„ä¸€ç»´æ•°ç»„ï¼Œå¯¹åº”äºä»â€œ.â€å¼€å§‹ï¼Œåˆ†åˆ«æ¥ aã€bã€c... çš„æ¬¡æ•°ã€‚

---

### ğŸ”¸ ç¬¬äºŒæ­¥ï¼šå°† raw count è½¬ä¸ºæ¦‚ç‡åˆ†å¸ƒ

```python
p = N[0].float()      # å°†æ•´æ•°å¼ é‡è½¬ä¸º float
p = p / p.sum()       # å½’ä¸€åŒ–ï¼šæ¯ä¸ªå…ƒç´ é™¤ä»¥æ€»å’Œï¼Œä½¿å…¶å˜æˆåˆæ³•çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆå’Œä¸º 1ï¼‰
```

---

### ğŸ”¸ ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨ `torch.multinomial` æŒ‰æ¦‚ç‡é‡‡æ ·

```python
g = torch.Generator().manual_seed(2147483647)   # åˆ›å»ºéšæœºç”Ÿæˆå™¨ï¼Œè®¾å®šéšæœºç§å­ï¼ˆä¿è¯ç»“æœå¯å¤ç°ï¼‰
ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
```

* `torch.multinomial(p, 1)` ä»æ¦‚ç‡åˆ†å¸ƒ `p` ä¸­é‡‡æ ·ä¸€ä¸ªç´¢å¼•
* `.item()` æŠŠç»“æœä» tensor è½¬ä¸ºæ™®é€šæ•´æ•°

---

### ğŸ”¸ ç¬¬å››æ­¥ï¼šå¾ªç¯é‡‡æ ·å®Œæ•´å•è¯

æˆ‘ä»¬ä¸æ–­åœ°é‡å¤ä»¥ä¸Šè¿‡ç¨‹ï¼š

1. æ¯æ¬¡æ ¹æ®å½“å‰å­—ç¬¦çš„ç´¢å¼• `ix` é€‰å– `N[ix]` è¿™ä¸€è¡Œ
2. å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ `p`
3. ç”¨ `torch.multinomial` é‡‡æ ·ä¸‹ä¸€ä¸ªå­—ç¬¦çš„ç´¢å¼•
4. å¦‚æœé‡‡å‡ºçš„æ˜¯ç´¢å¼• 0ï¼ˆå³ç‚¹å·ï¼‰ï¼Œè¡¨ç¤ºå•è¯ç»“æŸï¼Œé€€å‡ºå¾ªç¯
5. å¦åˆ™ï¼Œå°†å­—ç¬¦åŠ å…¥è¾“å‡ºåˆ—è¡¨ï¼Œç»§ç»­å¾ªç¯

---

### ğŸ”¸ ç¤ºä¾‹é‡‡æ ·ä»£ç ç®€åŒ–ç‰ˆï¼š

```python
out = []
ix = 0  # ä» . å¼€å§‹
while True:
    p = N[ix].float()
    p = p / p.sum()
    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
    if ix == 0:
        break
    out.append(itos[ix])  # å°†ç´¢å¼•è½¬å›å­—ç¬¦
print(''.join(out))
```

è¿™æ ·å¯ä»¥ç”Ÿæˆä¸€ä¸ªâ€œçœ‹èµ·æ¥åƒåå­—â€çš„å­—ç¬¦ä¸²ã€‚

---

### ğŸ”¸ å¤šæ¬¡é‡‡æ ·å¤šä¸ªåå­—ï¼š

```python
for i in range(20):
    out = []
    ix = 0
    while True:
        ...
    print(''.join(out))
```

---

## ğŸ§  æ¨¡å‹çš„å®é™…æ•ˆæœ

ä½ å¯èƒ½æ³¨æ„åˆ°ï¼Œç”Ÿæˆçš„åå­—çœ‹èµ·æ¥å¾ˆå¥‡æ€ªï¼ˆä¾‹å¦‚ "yanu", "chm", "eet"ï¼‰ã€‚åŸå› æ˜¯ï¼š

* **Bigram æ¨¡å‹æœ¬è´¨ä¸Šå¾ˆå¼±**ï¼šå®ƒåªæ ¹æ®å‰ä¸€ä¸ªå­—ç¬¦é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œæ— æ³•ç†è§£æ•´ä½“ç»“æ„ã€‚
* å®ƒä¸ç†è§£â€œå•è¯é•¿åº¦â€ã€â€œå‘éŸ³ç»“æ„â€ã€â€œå…ƒéŸ³è¾…éŸ³äº¤æ›¿â€ç­‰è§„å¾‹ã€‚
* å®ƒä¸çŸ¥é“è‡ªå·±ç”Ÿæˆçš„æ˜¯åå­—ï¼Œä¹Ÿæ— æ³•è€ƒè™‘â€œå†å²ä¸Šä¸‹æ–‡â€ã€‚

---

## âœ… ä¸å…¶ä»–æƒ…å†µå¯¹æ¯”ï¼š

### ğŸ“‰ ä½¿ç”¨éšæœºå‡åŒ€åˆ†å¸ƒï¼ˆå®Œå…¨æœªè®­ç»ƒçš„æ¨¡å‹ï¼‰ï¼š

```python
p = torch.ones(27) / 27
```

æ¯ä¸ªå­—ç¬¦éƒ½æœ‰ç›¸åŒæ¦‚ç‡ï¼Œä¼šç”Ÿæˆå®Œå…¨æ²¡æœ‰è§„å¾‹çš„ä¹±ç ã€‚

### ğŸ“ˆ ä½¿ç”¨è®­ç»ƒè¿‡çš„ bigram æ¨¡å‹ï¼š

è™½ç„¶å¼±ï¼Œä½†ç»“æœçœ‹èµ·æ¥æ›´åƒâ€œåå­—â€ï¼Œæ¯”å¦‚ï¼š

* "mory"
* "jorlen"
* "avie"
* "ronna"

è¯´æ˜æ¨¡å‹ç¡®å®å­¦ä¹ åˆ°äº†æŸäº›å­—ç¬¦ä¹‹é—´çš„ç»„åˆæ¨¡å¼ï¼Œè™½ç„¶è¿˜è¿œä¸å¤Ÿå¥½ã€‚

---

## âœ… æ€»ç»“

| æ­¥éª¤         | å†…å®¹                                |
| ---------- | --------------------------------- |
| 1. åˆå§‹åŒ–     | ä»ç‰¹æ®Šå­—ç¬¦ `.` å¼€å§‹ï¼ˆç´¢å¼• 0ï¼‰                |
| 2. è·å–æ¦‚ç‡    | ä» bigram è®¡æ•°çŸ©é˜µä¸­å–å‡ºå¯¹åº”è¡Œï¼Œå¹¶å½’ä¸€åŒ–ä¸ºæ¦‚ç‡       |
| 3. é‡‡æ ·ä¸‹ä¸€ä¸ªå­—ç¬¦ | ä½¿ç”¨ `torch.multinomial()` ä»æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ · |
| 4. åˆ¤æ–­æ˜¯å¦ç»“æŸ  | å¦‚æœé‡‡æ ·ç»“æœæ˜¯ `.`ï¼Œå°±ç»ˆæ­¢ï¼Œå¦åˆ™ç»§ç»­              |
| 5. é‡å¤      | ä¸æ–­æ„å»ºå­—ç¬¦ï¼Œç›´åˆ°é‡åˆ° `.`                   |

---

## ğŸ¯ å…³é”®ç»“è®º

* Bigram æ¨¡å‹èƒ½å­¦åˆ°ä¸€äº›å­—ç¬¦ç»„åˆæ¨¡å¼ï¼Œä½†éå¸¸æœ‰é™ï¼Œæ•ˆæœä¸ç†æƒ³ï¼›
* è¿™æ˜¯æ„å»ºè¯­è¨€æ¨¡å‹çš„**ç¬¬ä¸€æ­¥**ï¼Œåç»­æˆ‘ä»¬å¯ä»¥ç”¨ MLPã€RNNã€Transformer æ¥å¢å¼ºï¼›
* å½“å‰çš„é‡‡æ ·é€»è¾‘å¯¹åç»­æ„å»ºæ›´å¤æ‚æ¨¡å‹ä»ç„¶é€‚ç”¨ï¼Œæ¡†æ¶å·²ç»æ­å¥½äº†ã€‚

---

`torch.multinomial` æ˜¯ PyTorch ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äº**ä»ç»™å®šçš„æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·**æ•´æ•°ç´¢å¼•ï¼ˆå³é€‰æ‹©ä¸‹ä¸€ä¸ªåŠ¨ä½œã€å­—ç¬¦ã€token ç­‰ï¼‰ï¼Œéå¸¸é€‚ç”¨äºè¯­è¨€æ¨¡å‹ä¸­çš„ç”Ÿæˆä»»åŠ¡ã€‚

---

## âœ… å‡½æ•°åŸå‹ï¼š

```python
torch.multinomial(input, num_samples, replacement=False, *, generator=None)
```

---

## âœ… å‚æ•°è§£é‡Šï¼š

| å‚æ•°            | å«ä¹‰                        |
| ------------- | ------------------------- |
| `input`       | ä¸€ç»´æˆ–äºŒç»´å¼ é‡ï¼Œè¡¨ç¤ºæ¦‚ç‡æˆ–æƒé‡ï¼ˆéè´Ÿæ•°ï¼‰      |
| `num_samples` | è¦é‡‡æ ·å¤šå°‘ä¸ªç»“æœï¼ˆæ•´æ•°ï¼‰              |
| `replacement` | æ˜¯å¦æœ‰æ”¾å›é‡‡æ ·ï¼ˆTrue è¡¨ç¤ºå¯ä»¥é‡å¤æŠ½ä¸­åŒä¸€ä¸ªï¼‰ |
| `generator`   | å¯é€‰ï¼Œç”¨äºæ§åˆ¶éšæœºç§å­çš„ç”Ÿæˆå™¨ï¼ˆä¿è¯ç»“æœå¯å¤ç°ï¼‰  |

---

## âœ… è¿”å›å€¼ï¼š

è¿”å›ä¸€ä¸ªå¼ é‡ï¼Œè¡¨ç¤ºé‡‡æ ·å‡ºçš„ç´¢å¼•ï¼ˆç´¢å¼•ä½ç½®ï¼Œè€Œä¸æ˜¯æ¦‚ç‡æœ¬èº«ï¼‰ã€‚

---

## âœ… ç¤ºä¾‹ 1ï¼šä»æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·

```python
import torch

p = torch.tensor([0.6, 0.3, 0.1])  # æ¦‚ç‡åˆ†å¸ƒ
sample = torch.multinomial(p, num_samples=1)
print(sample)  # å¯èƒ½æ˜¯ tensor([0])
```

è¡¨ç¤ºï¼š60% æ¦‚ç‡é‡‡æ ·å‡ºç´¢å¼• 0ï¼Œ30% æ˜¯ç´¢å¼• 1ï¼Œ10% æ˜¯ç´¢å¼• 2ã€‚

---

## âœ… ç¤ºä¾‹ 2ï¼šå¤šæ¬¡é‡‡æ · + æœ‰æ”¾å›

```python
torch.multinomial(p, num_samples=10, replacement=True)
```

* è¡¨ç¤ºä» `p` ä¸­é‡‡æ · 10 æ¬¡ï¼Œå…è®¸é‡å¤ã€‚
* è¿™ç±»ä¼¼äºæ ¹æ®æ¦‚ç‡åˆ†å¸ƒâ€œæŠ• 10 æ¬¡éª°å­â€ã€‚

---

## âœ… ç¤ºä¾‹ 3ï¼šè®¾å®šéšæœºç§å­ï¼ˆä¿è¯å¯å¤ç°ï¼‰

```python
g = torch.Generator().manual_seed(42)
sample = torch.multinomial(p, 1, generator=g)
```

ä½¿ç”¨å›ºå®šçš„éšæœºæ•°ç”Ÿæˆå™¨ `g`ï¼Œç¡®ä¿ä½ å’Œåˆ«äººè·‘å‡ºç›¸åŒçš„ç»“æœï¼ˆåœ¨æ•™å­¦ã€è°ƒè¯•ä¸­å¾ˆæœ‰ç”¨ï¼‰ã€‚

---

## â— æ³¨æ„äº‹é¡¹ï¼š

* `input` ä¸éœ€è¦æ˜¯æ­£è§„åŒ–çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆå³æ€»å’Œä¸å¿…æ˜¯ 1ï¼‰ï¼Œä½†å®ƒä¸èƒ½åŒ…å«è´Ÿæ•°ã€‚
* å¦‚æœæ˜¯æœªå½’ä¸€åŒ–çš„â€œæƒé‡â€ï¼Œå‡½æ•°ä¼šè‡ªåŠ¨åœ¨å†…éƒ¨è½¬æ¢ä¸ºæ¦‚ç‡ã€‚
* `replacement=False` æ—¶ä¸èƒ½é‡‡æ ·æ•°é‡è¶…è¿‡éé›¶å…ƒç´ æ•°é‡ã€‚

---

## âœ… åº”ç”¨åœºæ™¯ï¼ˆè¯­è¨€æ¨¡å‹ä¸­ï¼‰ï¼š

åœ¨å­—ç¬¦çº§è¯­è¨€æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¸¸å¸¸ç”¨ï¼š

```python
p = next_char_probs  # ä¸€ä¸ª 1D çš„æ¦‚ç‡åˆ†å¸ƒå¼ é‡ï¼Œé•¿åº¦ä¸ºå­—ç¬¦è¡¨å¤§å°ï¼ˆå¦‚ 27ï¼‰
ix = torch.multinomial(p, num_samples=1).item()
```

è¿™ä¸€æ­¥çš„æ„æ€æ˜¯ï¼š**æ ¹æ®æ¨¡å‹å½“å‰é¢„æµ‹å‡ºçš„æ¦‚ç‡ï¼Œéšæœºâ€œé€‰ä¸­â€ä¸€ä¸ªä¸‹ä¸€ä¸ªå­—ç¬¦çš„ç´¢å¼•**ï¼Œè€Œä¸æ˜¯æ€»æ˜¯é€‰æœ€å¤§æ¦‚ç‡é‚£ä¸ªã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯ï¼š

> `torch.multinomial` æ˜¯ç”¨æ¥**ä»ä¸€ç»„æ¦‚ç‡æˆ–æƒé‡ä¸­æŒ‰æ¦‚ç‡éšæœºæŠ½æ ·ç´¢å¼•**çš„å·¥å…·ï¼Œå¹¿æ³›ç”¨äºè¯­è¨€æ¨¡å‹ã€å¼ºåŒ–å­¦ä¹ ç­‰ä»»åŠ¡ä¸­çš„â€œå†³ç­–é‡‡æ ·â€æ“ä½œã€‚

---

# efficiency! vectorized normalization of the rows, tensor broadcasting

because what we're doing here is we're always fetching a row of n from the counts matrix up ahead
and then we're always doing the same things we're converting to float and we're dividing and we're doing this every single iteration of this loop and
we just keep renormalizing these rows over and over again and it's extremely inefficient and wasteful so what i'd like to do is i'd like to actually
prepare a matrix capital p that will just have the probabilities in it so in
other words it's going to be the same as the capital n matrix here of counts but every single row will have the row of
probabilities uh that is normalized to 1 indicating the probability distribution for the next character given the
character before it um as defined by which row we're in so basically what we'd like to do is
we'd like to just do it up front here and then we would like to just use that row here so here we would like to just
do p equals p of ix instead okay
the other reason i want to do this is not just for efficiency but also i would like us to practice these n-dimensional tensors and i'd like
us to practice their manipulation and especially something that's called broadcasting that we'll go into in a second
we're actually going to have to become very good at these tensor manipulations because if we're going to build out all
the way to transformers we're going to be doing some pretty complicated um array operations for efficiency and we
need to really understand that and be very good at it so intuitively what we want to do is we
first want to grab the floating point copy of n and i'm mimicking the line here
basically and then we want to divide all the rows so that they sum to 1.
so we'd like to do something like this p divide p dot sum but
now we have to be careful because p dot sum actually produces a sum
sorry equals and that float copy p dot sum produces a um
sums up all of the counts of this entire matrix n and gives us a single number of just the summation of everything so
that's not the way we want to define divide we want to simultaneously and in parallel divide all the rows
by their respective sums so what we have to do now is we have to
go into documentation for torch.sum and we can scroll down here to a definition that is relevant to us which
is where we don't only provide an input array that we want to sum but we also provide the dimension along which we
want to sum and in particular we want to sum up over rows
right now one more argument that i want you to pay attention to here is the keep them
is false if keep them is true then the output tensor is of the same size as input
except of course the dimension along which is summed which will become just one but if you pass in keep them as false
then this dimension is squeezed out and so torch.sum not only does the sum and collapses dimension to be of size one
but in addition it does what's called a squeeze where it squeezes out it squeezes out that dimension
so basically what we want here is we instead want to do p dot sum of some axis
and in particular notice that p dot shape is 27 by 27 so when we sum up across axis zero then
we would be taking the zeroth dimension and we would be summing across it so when keep them as true
then this thing will not only give us the counts across um
along the columns but notice that basically the shape of this is 1 by 27 we just get a row vector
and the reason we get a row vector here again is because we passed in zero dimension so this zero dimension becomes
one and we've done a sum and we get a row and so basically we've done the sum
this way vertically and arrived at just a single 1 by 27 vector of counts
what happens when you take out keep them is that we just get 27. so it squeezes
out that dimension and we just get a one-dimensional vector of size 27.
now we don't actually want one by 27 row vector because that gives
us the counts or the sums across the columns
we actually want to sum the other way along dimension one and you'll see that the shape of this is 27 by one so it's a
column vector it's a 27 by one vector of counts
okay and that's because what's happened here is that we're going horizontally and this 27 by 27 matrix becomes a 27 by 1
array now you'll notice by the way that um the actual numbers
of these counts are identical and that's because this special array of counts here comes from bi-gram
statistics and actually it just so happens by chance or because of the way this array is
constructed that the sums along the columns or along the rows horizontally or vertically is identical
but actually what we want to do in this case is we want to sum across the rows
horizontally so what we want here is p that sum of one with keep in true
27 by one column vector and now what we want to do is we want to divide by that
now we have to be careful here again is it possible to take what's a um p dot shape you see here 27
by 27 is it possible to take a 27 by 27 array and divide it by what is a 27 by 1
array is that an operation that you can do and whether or not you can perform this
operation is determined by what's called broadcasting rules so if you just search broadcasting semantics in torch
you'll notice that there's a special definition for what's called broadcasting that uh for whether or not um these two uh arrays
can be combined in a binary operation like division so the first condition is each tensor
has at least one dimension which is the case for us and then when iterating over the dimension sizes starting at the trailing
dimension the dimension sizes must either be equal one of them is one or one of them does not exist
okay so let's do that we need to align the two arrays and their shapes which is
very easy because both of these shapes have two elements so they're aligned then we iterate over from the from the
right and going to the left each dimension must be either equal one
of them is a one or one of them does not exist so in this case they're not equal but one of them is a one so this is fine
and then this dimension they're both equal so uh this is fine so all the dimensions are fine and
therefore the this operation is broadcastable so that means that this operation is allowed
and what is it that these arrays do when you divide 27 by 27 by 27 by one
what it does is that it takes this dimension one and it stretches it out it copies it to match
27 here in this case so in our case it takes this column vector which is 27 by 1
and it copies it 27 times to make these both be 27 by 27 internally you
can think of it that way and so it copies those counts and then it does an element-wise division
which is what we want because these counts we want to divide by them on every single one of these columns in
this matrix so this actually we expect will normalize every single row
and we can check that this is true by taking the first row for example and taking its sum we expect this to be
1. because it's not normalized and then we expect this now because if
we actually correctly normalize all the rows we expect to get the exact same result here so let's run this
it's the exact same result this is correct so now i would like to scare you a little bit
uh you actually have to like i basically encourage you very strongly to read through broadcasting semantics
and i encourage you to treat this with respect and it's not something to play fast and loose with it's something to
really respect really understand and look up maybe some tutorials for broadcasting and practice it and be careful with it because you can very
quickly run into books let me show you what i mean you see how here we have p dot sum of
one keep them as true the shape of this is 27 by one let me take out this line just so we have the n
and then we can see the counts we can see that this is a all the counts across all the
rows and it's a 27 by one column vector right now suppose that i tried to do the
following but i erase keep them just true here what does that do if keep them is not
true it's false then remember according to documentation it gets rid of this dimension one it squeezes it out so
basically we just get all the same counts the same result except the shape of it is not 27 by 1 it is just 27 the
one disappears but all the counts are the same so you'd think that this divide that
would uh would work first of all can we even uh write this and will it is it even is it even
expected to run is it broadcastable let's determine if this result is broadcastable p.summit one is shape
is 27. this is 27 by 27. so 27 by 27
broadcasting into 27. so now rules of broadcasting number one align
all the dimensions on the right done now iteration over all the dimensions starting from the right going to the
left all the dimensions must either be equal one of them must be one or one that does
not exist so here they are all equal here the dimension does not exist so internally what broadcasting will do
is it will create a one here and then we see that one of them is a one and
this will get copied and this will run this will broadcast okay so you'd expect this
to work because we we are
this broadcast and this we can divide this now if i run this you'd expect it to work but
it doesn't uh you actually get garbage you get a wrong dissolve because this is actually a bug
this keep them equals true makes it work
this is a bug in both cases we are doing the correct counts we are summing up
across the rows but keep them is saving us and making it work so in this case
i'd like to encourage you to potentially like pause this video at this point and try to think about why this is buggy and
why the keep dim was necessary here okay so the reason to do
for this is i'm trying to hint it here when i was sort of giving you a bit of a hint on how this works
this 27 vector internally inside the broadcasting this becomes a 1 by 27
and 1 by 27 is a row vector right and now we are dividing 27 by 27 by 1 by
27 and torch will replicate this dimension so basically
uh it will take it will take this row vector and it will copy it
vertically now 27 times so the 27 by 27 lies exactly and element wise divides
and so basically what's happening here is we're actually normalizing the columns
instead of normalizing the rows so you can check that what's happening
here is that p at zero which is the first row of p dot sum is not one it's seven
it is the first column as an example that sums to one
so to summarize where does the issue come from the issue comes from the silent adding of a dimension here because in
broadcasting rules you align on the right and go from right to left and if dimension doesn't exist you create it
so that's where the problem happens we still did the counts correctly we did the counts across the rows and we got
the the counts on the right here as a column vector but because the keep things was true this this uh this
dimension was discarded and now we just have a vector of 27. and because of broadcasting the way it works this
vector of 27 suddenly becomes a row vector and then this row vector gets replicated vertically and that every single point
we are dividing by the by the count in the opposite direction
so uh so this thing just uh doesn't work this needs to be keep things equal true in
this case so then then we have that p at zero is normalized
and conversely the first column you'd expect to potentially not be normalized and this is what makes it work
so pretty subtle and uh hopefully this helps to scare you that you should have
a respect for broadcasting be careful check your work uh and uh understand how it works under the hood and make sure
that it's broadcasting in the direction that you like otherwise you're going to introduce very subtle bugs very hard to
find bugs and uh just be careful one more note on efficiency we don't want to be doing this here because this creates
a completely new tensor that we store into p we prefer to use in place operations if possible
so this would be an in-place operation it has the potential to be faster it doesn't create new memory
under the hood and then let's erase this we don't need it and let's
also um just do fewer just so i'm not wasting space

# æé«˜æ•ˆç‡ï¼ç”¨å‘é‡åŒ–æ–¹æ³•å½’ä¸€åŒ– bigram å¼ é‡çš„æ¯ä¸€è¡Œï¼ˆTensor Broadcastingï¼‰

---

## ğŸ¯ é—®é¢˜èƒŒæ™¯

åœ¨å‰é¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬æ¯æ¬¡ç”Ÿæˆä¸‹ä¸€ä¸ªå­—ç¬¦æ—¶ï¼š

1. éƒ½è¦ä» bigram è®¡æ•°çŸ©é˜µ `N` ä¸­å–å‡ºä¸€è¡Œï¼›
2. è½¬æ¢æˆ floatï¼›
3. ç„¶åå†é™¤ä»¥è¯¥è¡Œå…ƒç´ ä¹‹å’Œï¼Œå½’ä¸€åŒ–æˆæ¦‚ç‡åˆ†å¸ƒã€‚

è¿™ä¸ªæ“ä½œæˆ‘ä»¬æ¯ç”Ÿæˆä¸€ä¸ªå­—ç¬¦å°±é‡å¤ä¸€éï¼Œ**éå¸¸ä½æ•ˆ**ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨å¾ªç¯ä¸­ä¸åœåœ°é‡å¤ç›¸åŒçš„è®¡ç®—ã€‚

---

## âœ… ä¼˜åŒ–ç›®æ ‡

æå‰è®¡ç®—å¥½ä¸€ä¸ªæ–°çš„çŸ©é˜µ `P`ï¼š

* `P` ä¸ `N` çš„å½¢çŠ¶ä¸€è‡´ï¼ˆ27 Ã— 27ï¼‰
* `P[i]` æ˜¯ `N[i]` çš„å½’ä¸€åŒ–ç‰ˆæœ¬ï¼Œå³ç¬¬ `i` è¡Œè¢«å½’ä¸€åŒ–æˆä¸€ä¸ª**æ¦‚ç‡åˆ†å¸ƒ**
* è¿™æ ·ï¼Œæˆ‘ä»¬ä»¥åé‡‡æ ·åªéœ€è¦ä» `P[ix]` ä¸­å–æ¦‚ç‡åˆ†å¸ƒï¼Œæ— éœ€æ¯æ¬¡éƒ½æ‰‹åŠ¨å½’ä¸€åŒ–

---

## ğŸ”§ å®ç°æ­¥éª¤

### 1. è½¬ä¸º float ç±»å‹

```python
P = N.float()
```

### 2. å¯¹æ¯ä¸€è¡Œåšå½’ä¸€åŒ–ï¼ˆè€Œä¸æ˜¯å¯¹æ•´ä¸ªçŸ©é˜µï¼‰

æˆ‘ä»¬éœ€è¦å°† `P` ä¸­çš„æ¯ä¸€è¡Œé™¤ä»¥è¿™ä¸€è¡Œçš„æ€»å’Œã€‚è¿™éœ€è¦ç”¨åˆ° PyTorch çš„ **å¹¿æ’­æœºåˆ¶ï¼ˆbroadcastingï¼‰**ã€‚

#### è®¡ç®—æ¯ä¸€è¡Œçš„æ€»å’Œï¼š

```python
P.sum(1, keepdim=True)
```

è§£é‡Šï¼š

* `dim=1`ï¼šåœ¨â€œåˆ—â€æ–¹å‘æ±‚å’Œï¼Œç›¸å½“äºæ¨ªå‘åœ°æ±‚æ¯ä¸€è¡Œçš„å’Œï¼›
* `keepdim=True`ï¼šä¿ç•™ç»´åº¦ï¼Œè¿™æ ·è¿”å›çš„æ˜¯ `(27, 1)` çš„åˆ—å‘é‡ï¼›

  * è¿™å¾ˆé‡è¦ï¼Œå¦åˆ™ç»´åº¦ä¸å¯¹ä¼šå¯¼è‡´å¹¿æ’­é”™è¯¯ï¼

#### æ‰§è¡Œé™¤æ³•ï¼š

```python
P = P / P.sum(1, keepdim=True)
```

è¿™æ ·æ¯ä¸€è¡Œéƒ½ä¼šè¢«è¯¥è¡Œçš„å’Œé™¤ï¼Œç»“æœä»æ˜¯ `(27, 27)`ï¼Œä½†ç°åœ¨æ¯ä¸€è¡Œçš„å…ƒç´ åŠ èµ·æ¥æ˜¯ 1ï¼Œå³ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚

---

## ğŸ§  å…³äº Broadcastingï¼ˆå¹¿æ’­æœºåˆ¶ï¼‰

PyTorch çš„å¹¿æ’­è§„åˆ™å…è®¸å½¢çŠ¶ä¸åŒçš„å¼ é‡è¿›è¡Œè¿ç®—ï¼Œæ¡ä»¶å¦‚ä¸‹ï¼š

* ä¸¤ä¸ªå¼ é‡ä»æœ€åä¸€ä¸ªç»´åº¦å¼€å§‹å¯¹é½ï¼›
* æ¯ä¸ªç»´åº¦è¦ä¹ˆç›¸ç­‰ï¼Œè¦ä¹ˆå…¶ä¸­ä¸€ä¸ªæ˜¯ `1`ï¼Œæˆ–è€…å…¶ä¸­ä¸€ä¸ªä¸å­˜åœ¨ï¼›
* å¦‚æœç»´åº¦ä¸º `1`ï¼Œä¼šè‡ªåŠ¨æ‰©å±•ï¼ˆå¤åˆ¶ï¼‰ä»¥åŒ¹é…å¦ä¸€ä¸ªç»´åº¦ï¼›

### âœ… ç¤ºä¾‹ï¼š

```python
P      : shape (27, 27)
row_sums: shape (27, 1)
```

å¹¿æ’­è¿‡ç¨‹ä¼šè‡ªåŠ¨å°† `(27, 1)` **å¤åˆ¶ä¸º** `(27, 27)`ï¼Œä»è€Œä¸ `P` å…¼å®¹ï¼Œå®ç°**é€è¡Œé™¤æ³•**ã€‚

---

## âš ï¸ Bug è­¦å‘Šï¼šä¸è¦å¿˜è®° `keepdim=True`

å¦‚æœä½ å†™æˆï¼š

```python
P = P / P.sum(1)
```

é‚£ `P.sum(1)` ä¼šè¿”å› `(27,)`ï¼Œæ˜¯ä¸€ä¸ªä¸€ç»´å‘é‡ã€‚

å¹¿æ’­æ—¶ï¼Œå®ƒä¼šè¢«**å½“ä½œè¡Œå‘é‡** `(1, 27)` å¤åˆ¶åˆ° 27 è¡Œï¼Œç»“æœæ˜¯ **æŒ‰åˆ—å½’ä¸€åŒ–** è€Œä¸æ˜¯æŒ‰è¡Œå½’ä¸€åŒ–ã€‚

è¿™ä¼šæ‚„æ‚„åœ°è®©ä½ å¾—åˆ°é”™è¯¯ç»“æœï¼Œè€Œä¸æŠ¥é”™ï¼Œ**éå¸¸éšè”½ï¼**

---

## âœ… æ­£ç¡® vs é”™è¯¯å¯¹æ¯”ï¼š

| å†™æ³•                           | ä½œç”¨        | æ˜¯å¦æ­£ç¡® |
| ---------------------------- | --------- | ---- |
| `P / P.sum(1, keepdim=True)` | æŒ‰è¡Œå½’ä¸€åŒ– âœ…   | âœ… æ­£ç¡® |
| `P / P.sum(1)`               | å®é™…æŒ‰åˆ—å½’ä¸€åŒ– âŒ | âŒ é”™è¯¯ |

---

## ğŸ§  ç»“è®ºï¼šRespect Broadcasting

ä½œè€…å¼ºè°ƒäº†ä¸¤ä¸ªé‡è¦å»ºè®®ï¼š

1. **ä¸è¦è½»è§† broadcasting**ï¼šå®ƒåŠŸèƒ½å¼ºå¤§ï¼Œä½†å®¹æ˜“äº§ç”Ÿéšæ€§ bugï¼›
2. **è®°å¾—æŸ¥æ–‡æ¡£å¹¶æµ‹è¯•æ¯ä¸€æ­¥**ï¼šçœ‹ç»´åº¦ã€æ‰“å°å½¢çŠ¶ã€æ ¸å¯¹é€»è¾‘ï¼›

---

## ğŸ›  æ•ˆç‡å»ºè®®

* ç”¨ **å°±åœ°æ“ä½œï¼ˆin-place operationï¼‰** æ¯”å¦‚ `P.div_()` ä»£æ›¿ `P = P / ...` å¯å‡å°‘å†…å­˜å¼€é”€ï¼Œæé«˜é€Ÿåº¦ï¼›
* å°½é‡æŠŠä¸€æ¬¡æ€§å¯é¢„è®¡ç®—çš„ä¸œè¥¿**æå‰ç®—å¥½**ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é‡å¤è®¡ç®—ï¼›

---

## âœ… æ€»ç»“ä¸€å¥è¯ï¼š

> **ç”¨ `P = N.float(); P = P / P.sum(1, keepdim=True)` å¯ä¸€æ¬¡æ€§é«˜æ•ˆæ„å»ºå‡º bigram æ¦‚ç‡çŸ©é˜µï¼Œå¹¶ç¡®ä¿å¹¿æ’­è¡Œä¸ºæ­£ç¡®ã€‚ç‰¢è®° broadcasting çš„è§„åˆ™å’Œé™·é˜±ï¼Œé¿å…éšè”½ bugï¼**

# loss function (the negative log likelihood of the data under our model)

okay so we're actually in a pretty good spot now we trained a bigram language model and we trained it really just by counting uh
how frequently any pairing occurs and then normalizing so that we get a nice property distribution
so really these elements of this array p are really the parameters of our biogram language model giving us and summarizing
the statistics of these bigrams so we train the model and then we know how to sample from a model we just
iteratively uh sample the next character and feed it in each time and get a next
character now what i'd like to do is i'd like to somehow evaluate the quality of this model we'd like to somehow summarize the
quality of this model into a single number how good is it at predicting the training set
and as an example so in the training set we can evaluate now the training loss
and this training loss is telling us about sort of the quality of this model in a single number just like we saw in
micrograd so let's try to think through the quality of the model and how we would evaluate it
basically what we're going to do is we're going to copy paste this code that we previously used for counting
okay and let me just print these diagrams first we're gonna use f strings and i'm gonna print character one
followed by character two these are the diagrams and then i don't wanna do it for all the words just do the first three words so here we have emma olivia
and ava bigrams now what we'd like to do is we'd like to basically look at the probability that
the model assigns to every one of these diagrams so in other words we can look at the probability which is
summarized in the matrix b of i x 1 x 2 and then we can print it here
as probability and because these properties are way too large let me present
or call in 0.4 f to like truncate it a bit so what do we have here right we're
looking at the probabilities that the model assigns to every one of these bigrams in the dataset and so we can see some of them are four
percent three percent etc just to have a measuring stick in our mind by the way um we have 27 possible
characters or tokens and if everything was equally likely then you'd expect all these probabilities
to be four percent roughly so anything above four percent means
that we've learned something useful from these bigram statistics and you see that roughly some of these are four percent
but some of them are as high as 40 percent 35 percent and so on so you see that the model actually assigned a pretty high
probability to whatever's in the training set and so that's a good thing um basically if you have a very good
model you'd expect that these probabilities should be near one because that means that your model is correctly
predicting what's going to come next especially on the training set where you where you trained your model
so now we'd like to think about how can we summarize these probabilities into a single number that measures the quality
of this model now when you look at the literature into maximum likelihood estimation and
statistical modeling and so on you'll see that what's typically used here is something called the likelihood
and the likelihood is the product of all of these probabilities and so the product of all these
probabilities is the likelihood and it's really telling us about the probability of the entire data set assigned uh
assigned by the model that we've trained and that is a measure of quality so the product of these
should be as high as possible when you are training the model and when you have a good model your pro your
product of these probabilities should be very high um now because the product of these probabilities is an unwieldy thing to
work with you can see that all of them are between zero and one so your product of these probabilities will be a very tiny number
um so for convenience what people work with usually is not the likelihood but they work with what's called the log
likelihood so the product of these is the likelihood to get the log likelihood we just have
to take the log of the probability and so the log of the probability here i have the log of x from zero to one
the log is a you see here monotonic transformation of the probability where if you pass in one
you get zero so probability one gets your log probability of zero and then as you go lower and lower
probability the log will grow more and more negative until all the way to negative infinity at zero
so here we have a log prob which is really just a torch.log of probability let's print it out to get a sense of
what that looks like log prob also 0.4 f
okay so as you can see when we plug in numbers that are very close some of our
higher numbers we get closer and closer to zero and then if we plug in very bad probabilities we get more and more
negative number that's bad so and the reason we work with this is for
a large extent convenience right because we have mathematically that if you have some product a times b times c
of all these probabilities right the likelihood is the product of all these probabilities
then the log of these is just log of a plus
log of b plus log of c if you remember your logs
from your high school or undergrad and so on so we have that basically
the likelihood of the product probabilities the log likelihood is just the sum of the logs of the individual
probabilities so log likelihood
starts at zero and then log likelihood here we can just accumulate simply
and in the end we can print this
print the log likelihood f strings
maybe you're familiar with this so log likelihood is negative 38.
okay now we actually want um
so how high can log likelihood get it can go to zero so when all the
probabilities are one log likelihood will be zero and then when all the probabilities are lower this will grow
more and more negative now we don't actually like this because what we'd like is a loss function and a
loss function has the semantics that low is good because we're trying to minimize the
loss so we actually need to invert this and that's what gives us something called the negative log likelihood
negative log likelihood is just negative of the log likelihood
these are f strings by the way if you'd like to look this up negative log likelihood equals
so negative log likelihood now is just negative of it and so the negative log block load is a very nice loss function
because um the lowest it can get is zero and the higher it is the worse off the
predictions are that you're making and then one more modification to this that sometimes people do is that for
convenience uh they actually like to normalize by they like to make it an average instead of a sum
and so uh here let's just keep some counts as well so n plus equals one
starts at zero and then here um we can have sort of like a normalized log likelihood
um if we just normalize it by the count then we will sort of get the average
log likelihood so this would be usually our loss function here is what this we would this is what we would use
uh so our loss function for the training set assigned by the model is 2.4 that's the quality of this model
and the lower it is the better off we are and the higher it is the worse off we are and
the job of our you know training is to find the parameters that minimize the
negative log likelihood loss and that would be like a high quality
model okay so to summarize i actually wrote it out here so our goal is to maximize likelihood
which is the product of all the probabilities assigned by the model and we want to maximize this likelihood
with respect to the model parameters and in our case the model parameters here are defined in the table these numbers
the probabilities are the model parameters sort of in our program language models so far but you
have to keep in mind that here we are storing everything in a table format the probabilities but what's coming up as a
brief preview is that these numbers will not be kept explicitly but these numbers will be calculated by a neural network
so that's coming up and we want to change and tune the parameters of these neural networks we
want to change these parameters to maximize the likelihood the product of the probabilities
now maximizing the likelihood is equivalent to maximizing the log likelihood because log is a monotonic
function here's the graph of log and basically all it is doing is it's
just scaling your um you can look at it as just a scaling of the loss function
and so the optimization problem here and here are actually equivalent because
this is just scaling you can look at it that way and so these are two identical optimization problems
um maximizing the log-likelihood is equivalent to minimizing the negative log likelihood and then in practice
people actually minimize the average negative log likelihood to get numbers like 2.4
and then this summarizes the quality of your model and we'd like to minimize it and make it as small as possible
and the lowest it can get is zero and the lower it is the better off your model is because
it's signing it's assigning high probabilities to your data now let's estimate the probability over the entire
training set just to make sure that we get something around 2.4 let's run this over the entire oops
let's take out the print segment as well okay 2.45 or the entire training set
now what i'd like to show you is that you can actually evaluate the probability for any word that you want like for example
if we just test a single word andre and bring back the print statement
then you see that andre is actually kind of like an unlikely word like on average we take
three log probability to represent it and roughly that's because ej apparently is very uncommon as an example

ä»¥ä¸‹æ˜¯è¯¥æ®µè‹±æ–‡å…³äº\*\*è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼ˆNegative Log Likelihood Loss, NLLï¼‰\*\*çš„å®Œæ•´ä¸­æ–‡ç¿»è¯‘ä¸è®²è§£ï¼š

---

### ğŸ¯ æ¦‚è¿°ï¼šæˆ‘ä»¬å·²ç»è®­ç»ƒäº†ä¸€ä¸ª Bigram è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€šè¿‡ç»Ÿè®¡æ¯å¯¹å­—ç¬¦å‡ºç°çš„é¢‘ç‡æ¥å»ºç«‹ï¼Œç„¶åå½’ä¸€åŒ–å¾—åˆ°ä¸€ä¸ª**æ¦‚ç‡çŸ©é˜µ** `P`ï¼Œè¯¥çŸ©é˜µè¡¨ç¤ºæ¯ä¸ªå­—ç¬¦åæ¥å¦ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¦è¯„ä¼°è¿™ä¸ªæ¨¡å‹çš„å¥½åã€‚æœ€å¸¸è§çš„è¯„ä¼°æ–¹å¼æ˜¯ä½¿ç”¨**æŸå¤±å‡½æ•°ï¼ˆloss functionï¼‰**ï¼Œç‰¹åˆ«æ˜¯**è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼ˆNLLï¼‰**ï¼Œå®ƒæ˜¯è¯­è¨€æ¨¡å‹ä¸­éå¸¸æ ¸å¿ƒçš„ä¸€ä¸ªæ¦‚å¿µã€‚

---

### ğŸ§  ä¸ºä»€ä¹ˆä½¿ç”¨å¯¹æ•°ä¼¼ç„¶ï¼Ÿ

æˆ‘ä»¬å¸Œæœ›æ¨¡å‹èƒ½ç»™è®­ç»ƒé›†ä¸­çš„æ¯ä¸ªå­—å¯¹ï¼ˆbigramï¼‰åˆ†é…å°½å¯èƒ½**é«˜çš„æ¦‚ç‡**ã€‚ä¸€ç§è¡¡é‡æ–¹æ³•æ˜¯å°†æ‰€æœ‰è¿™äº›æ¦‚ç‡**ç›¸ä¹˜**ï¼Œä¹Ÿå°±æ˜¯è®¡ç®—**æ•´ä¸ªæ•°æ®çš„ä¼¼ç„¶ï¼ˆlikelihoodï¼‰**ï¼š

```
Likelihood = P(xâ‚) * P(xâ‚‚ | xâ‚) * P(xâ‚ƒ | xâ‚‚) * ...
```

è¿™ä¸ªä¹˜ç§¯è¶Šå¤§ï¼Œè¯´æ˜æ¨¡å‹å¯¹æ•°æ®æ‹Ÿåˆå¾—è¶Šå¥½ã€‚

---

### ğŸš¨ ä½†é—®é¢˜æ¥äº†ï¼š

æ‰€æœ‰è¿™äº›æ¦‚ç‡éƒ½æ˜¯å°äº 1 çš„ï¼Œæ‰€ä»¥ä¹˜èµ·æ¥ä¼šå˜å¾—éå¸¸å°ï¼ˆæ¥è¿‘äº 0ï¼‰ï¼Œéš¾ä»¥å¤„ç†ã€‚å› æ­¤æˆ‘ä»¬é‡‡ç”¨å¯¹æ•°æ“ä½œï¼š

```
Log-Likelihood = log(Pâ‚) + log(Pâ‚‚) + log(Pâ‚ƒ) + ...
```

* å¯¹æ•°æ“ä½œå°†ä¹˜æ³•å˜æˆåŠ æ³•ï¼Œæ•°å€¼ä¸Šæ›´ç¨³å®š
* å¦‚æœæŸä¸ª bigram æ¦‚ç‡é«˜ï¼ˆæ¥è¿‘ 1ï¼‰ï¼Œlog å€¼æ¥è¿‘ 0
* å¦‚æœæŸä¸ªæ¦‚ç‡ä½ï¼ˆæ¥è¿‘ 0ï¼‰ï¼Œlog å€¼æ˜¯è´Ÿæ•°ï¼Œæ‹‰ä½æ€»å’Œ

---

### â— æŸå¤±å‡½æ•°çš„è¯­ä¹‰æ˜¯ï¼š**è¶Šå°è¶Šå¥½**ã€‚

è€Œå¯¹æ•°ä¼¼ç„¶è¶Šå¤§è¶Šå¥½ï¼ˆè¶Šæ¥è¿‘ 0ï¼‰ã€‚æ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ **è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNegative Log Likelihoodï¼‰**ï¼š

```python
NLL = - (log(Pâ‚) + log(Pâ‚‚) + log(Pâ‚ƒ) + ...)
```

è¿™æ ·æŸå¤±å°±å˜æˆäº†â€œè¶Šå°è¶Šå¥½â€ã€‚

---

### ğŸ“ é€šå¸¸æˆ‘ä»¬è¿˜ä¼š**å¹³å‡åŒ–æŸå¤±**ï¼Œä»¥ä¾¿ä¸åŒé•¿åº¦çš„å¥å­/æ ·æœ¬èƒ½å…¬å¹³æ¯”è¾ƒï¼š

```python
avg_NLL = NLL / æ€»bigramæ•°
```

---

### âœ… æ€»ç»“é€»è¾‘é“¾ï¼š

| æ­¥éª¤            | å«ä¹‰               |
| ------------- | ---------------- |
| æ¨¡å‹è¾“å‡ºæ¦‚ç‡çŸ©é˜µ `P`  | æ¯ä¸ªå­—ç¬¦è·Ÿéšå¦ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡   |
| æŸ¥æ‰¾è®­ç»ƒé›†ä¸­æ¯å¯¹å­—ç¬¦çš„æ¦‚ç‡ | æ¥è‡ª `P[ix1, ix2]` |
| å–å¯¹æ•° log(P)    | ä¾¿äºåŠ æ³•æ“ä½œå’Œç¨³å®šè®¡ç®—      |
| å–è´Ÿæ•° -log(P)   | è½¬æ¢ä¸ºæŸå¤±ï¼ˆè¶Šå°è¶Šå¥½ï¼‰      |
| å¹³å‡åŒ–           | å¾—åˆ°ç»Ÿä¸€æ ‡å‡†çš„ loss åˆ†å€¼  |

---

### ğŸ“Œ ä¸¾ä¾‹ï¼š

å‡è®¾ `emma` è¿™ä¸ªè¯äº§ç”Ÿäº†ä»¥ä¸‹ bigramï¼š

* `.e`, `e.m`, `m.m`, `m.a`, `a.`

å¯¹äºæ¯ä¸€å¯¹å­—ç¬¦ï¼Œæˆ‘ä»¬æŸ¥æ‰¾æ¦‚ç‡ â†’ å–å¯¹æ•° â†’ å–è´Ÿæ•° â†’ æ±‚å’Œ â†’ å¹³å‡

æœ€ç»ˆï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªå¹³å‡æŸå¤±å€¼ï¼ˆå¦‚ 2.4ï¼‰ï¼Œè¯´æ˜è¯¥æ¨¡å‹å¹³å‡æ¯ä¸ªå­—ç¬¦éœ€è¦çº¦ 2.4 bits çš„ä¿¡æ¯é‡å»ç¼–ç è®­ç»ƒæ•°æ®ã€‚

---

### ğŸ’¡ å»¶ä¼¸ï¼š

å°†æ¥æˆ‘ä»¬ä¼šç”¨ç¥ç»ç½‘ç»œï¼ˆè€Œä¸æ˜¯æ‰‹åŠ¨ç»Ÿè®¡ï¼‰æ¥é¢„æµ‹ `P`ï¼Œå³æ¯ä¸€ä¸ª bigram çš„æ¦‚ç‡å°†é€šè¿‡ç½‘ç»œå‚æ•°è®¡ç®—è€Œæ¥ï¼Œè®­ç»ƒç›®æ ‡ä»ç„¶æ˜¯æœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚

---

å¦‚æœä½ è¿˜æƒ³è¿›ä¸€æ­¥å¯è§†åŒ–è¿™ä¸ª loss æ˜¯æ€ä¹ˆè®¡ç®—å‡ºæ¥çš„ï¼Œæˆ–ç”¨ä»£ç ä¾‹å­è·‘ä¸€éï¼Œæˆ‘å¯ä»¥ç»™ä½ å†™ä¸€ä¸ªç®€å•çš„å®ç°ã€‚éœ€è¦å—ï¼Ÿ

# model smoothing with fake counts

now think through this um when i take andre and i append q and i
test the probability of it under q we actually get
infinity and that's because jq has a zero percent probability according to our model so
the log likelihood so the log of zero will be negative infinity we get infinite loss
so this is kind of undesirable right because we plugged in a string that could be like a somewhat reasonable name but basically what this is saying is
that this model is exactly zero percent likely to uh to predict this name
and our loss is infinity on this example and really what the reason for that is
that j is followed by q uh zero times
uh where's q jq is zero and so jq is uh zero percent likely
so it's actually kind of gross and people don't like this too much to fix this there's a very simple fix that people like to do to sort of like smooth
out your model a little bit and it's called model smoothing and roughly what's happening is that we will eight we will add some fake counts
so imagine adding a count of one to everything so we add a count of one
like this and then we recalculate the probabilities and that's model smoothing and you can
add as much as you like you can add five and it will give you a smoother model and the more you add here
the more uniform model you're going to have and the less you add the more peaked model you are going to
have of course so one is like a pretty decent count to add and that will ensure that there will be
no zeros in our probability matrix p and so this will of course change the generations a little bit in this case it
didn't but in principle it could but what that's going to do now is that nothing will be infinity unlikely
so now our model will predict some other probability and we see that jq now has a very small probability so the model
still finds it very surprising that this was a word or a bigram but we don't get negative infinity so it's kind of like a
nice fix that people like to apply sometimes and it's called model smoothing okay so we've now trained a respectable bi-gram character level

ä»¥ä¸‹æ˜¯è¿™æ®µå…³äºâ€œ**æ¨¡å‹å¹³æ»‘ï¼ˆModel Smoothingï¼‰**â€çš„å®Œæ•´ä¸­æ–‡ç¿»è¯‘å’Œè®²è§£ï¼š

---

### ğŸ§ª ã€èƒŒæ™¯é—®é¢˜ï¼šæ¨¡å‹å¯¹æœªè§è¿‡çš„ bigram ç»™å‡ºé›¶æ¦‚ç‡ã€‘

ä½œè€…ä¸¾äº†ä¸€ä¸ªä¾‹å­ï¼š

> æˆ‘ä»¬æŠŠ "andre" è¿™ä¸ªåå­—åé¢æ¥ä¸€ä¸ª "q" å­—æ¯ï¼Œç„¶åè®¡ç®—å®ƒçš„æ¦‚ç‡ï¼Œç»“æœæŸå¤±å˜æˆäº† **æ— ç©·å¤§ï¼ˆinfinityï¼‰**ã€‚

ä¸ºä»€ä¹ˆï¼Ÿ

* å› ä¸º bigram `"jq"` åœ¨è®­ç»ƒé›†ä¸­**ä»æœªå‡ºç°è¿‡**ã€‚
* æ‰€ä»¥æ¨¡å‹ä¸­ `P[j][q] = 0`ï¼Œå³è¿™ä¸ªç»„åˆçš„æ¦‚ç‡ä¸º 0ã€‚
* è€Œ `log(0)` åœ¨æ•°å­¦ä¸Šæ˜¯è´Ÿæ— ç©·ï¼ˆ`-âˆ`ï¼‰ï¼Œ
* æ‰€ä»¥æœ€ç»ˆè®¡ç®—çš„ **log likelihood = -âˆ**ï¼Œä¹Ÿå°±æ˜¯æŸå¤± = âˆã€‚

---

### ğŸ˜¬ é—®é¢˜åˆ†æ

è¿™æ„å‘³ç€ï¼š

* æ¨¡å‹è®¤ä¸º "jq" è¿™ä¸ªç»„åˆ**ç»å¯¹ä¸å¯èƒ½**å‡ºç°ï¼ˆæ¦‚ç‡ä¸º 0ï¼‰ï¼Œ
* å¯¼è‡´ä»»ä½•åŒ…å«è¿™ä¸ªç»„åˆçš„è¯éƒ½ä¼šè¢«æ¨¡å‹è§†ä¸ºâ€œå®Œå…¨é”™è¯¯â€â€”â€”å³æŸå¤±ä¸ºæ— ç©·å¤§ã€‚

ä½†è¿™åœ¨å®é™…ä¸­å¾ˆ**ä¸åˆç†**ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®æ€»æ˜¯æœ‰é™çš„ï¼Œ**æ²¡å‡ºç°è¿‡ â‰  å®Œå…¨ä¸å¯èƒ½**ã€‚

---

### âœ… è§£å†³æ–¹æ¡ˆï¼š**æ¨¡å‹å¹³æ»‘ï¼ˆModel Smoothingï¼‰**

è§£å†³æ–¹å¼å°±æ˜¯ï¼š**ç»™æ‰€æœ‰ç»„åˆéƒ½åŠ ä¸Šä¸€ä¸ªè™šæ‹Ÿçš„â€œä¼ªè®¡æ•°ï¼ˆfake countï¼‰â€**ã€‚

#### ğŸ”§ æ“ä½œæ–¹æ³•ï¼š

åŸæ¥æˆ‘ä»¬ç»Ÿè®¡ bigram çš„é¢‘æ•°æ˜¯ï¼š

```python
N[i][j] += 1   # çœŸå®å‡ºç°è¿‡å°±+1
```

æ”¹ä¸ºï¼š

```python
N[i][j] += 1   # çœŸå®æ•°æ®
N += 1         # ç»™æ¯ä¸€ä¸ªä½ç½®éƒ½åŠ ä¸Šä¸€ä¸ª "1" çš„ä¼ªè®¡æ•°
```

> è¿™ç›¸å½“äºæˆ‘ä»¬å‡è®¾æ¯ä¸ªå­—ç¬¦ç»„åˆ**éƒ½å‡ºç°è¿‡ä¸€æ¬¡**ã€‚

ç„¶åå†å½’ä¸€åŒ–ä¸ºæ¦‚ç‡ï¼ˆé™¤ä»¥è¡Œå’Œï¼‰é‡æ–°å¾—åˆ° `P`ã€‚

---

### ğŸŒŠ å¹³æ»‘ç¨‹åº¦å¯è°ƒ

ä½ å¯ä»¥åŠ çš„ä¸æ˜¯ 1ï¼Œè€Œæ˜¯ä»»æ„ä¸€ä¸ªæ•°ï¼ˆä¾‹å¦‚ 0.1, 5, 10ï¼‰ï¼š

* **åŠ å¾—è¶Šå¤š**ï¼šæ¨¡å‹è¶Šâ€œå¹³æ»‘â€ï¼Œè¶Šæ¥è¿‘äºå‡åŒ€åˆ†å¸ƒï¼ˆè¶Šä¸è‡ªä¿¡ï¼‰
* **åŠ å¾—è¶Šå°‘**ï¼šæ¨¡å‹è¶Šâ€œå°–é”â€ï¼Œè¶Šè´´è¿‘è®­ç»ƒæ•°æ®ï¼ˆè¶Šè‡ªä¿¡ï¼‰

---

### ğŸ§¾ æ•ˆæœåˆ†æï¼š

* åŸæ¥ jq = 0ï¼Œå¯¼è‡´ `log(0)` = -âˆï¼Œloss æ— æ³•ä½¿ç”¨
* åŠ ä¸Šå¹³æ»‘åï¼Œjq çš„æ¦‚ç‡ä¸å†ä¸º 0ï¼Œè™½ç„¶å¾ˆå°ï¼Œä½† `log(prob)` æ˜¯ä¸€ä¸ªæœ‰é™çš„è´Ÿæ•°
* æœ€ç»ˆæˆ‘ä»¬é¿å…äº†æ— ç©·å¤§çš„æŸå¤±ï¼Œè®©æ¨¡å‹æ›´åŠ å¥å£®

---

### ğŸ“Œ æ€»ç»“ä¸€å¥è¯ï¼š

> **æ¨¡å‹å¹³æ»‘å°±æ˜¯ç»™æ‰€æœ‰ bigram åŠ ä¸Šä¸€ä¸ªâ€œä¼ªé€ çš„â€å‡ºç°æ¬¡æ•°ï¼Œé¿å…å‡ºç°é›¶æ¦‚ç‡ï¼Œä»è€Œè®©æ¨¡å‹æ›´å¥å£®ã€æ›´æ³›åŒ–ã€‚**

è¿™ç§æ–¹æ³•ä¹Ÿå«åš **Laplace smoothing** æˆ– **Add-one smoothing**ï¼Œæ˜¯è¯­è¨€å»ºæ¨¡é‡Œéå¸¸ç»å…¸çš„ä¸€ç§æŠ€å·§ã€‚

å¦‚éœ€ï¼Œæˆ‘ä¹Ÿå¯ä»¥ç»™ä½ å†™ä¸€æ®µä»£ç å®ç°è¿™ä¸ªå¹³æ»‘è¿‡ç¨‹ã€‚æ˜¯å¦éœ€è¦ï¼Ÿ

# PART 2: the neural network approach: intro

language model and we saw that we both sort of trained the model by looking at
the counts of all the bigrams and normalizing the rows to get probability distributions
we saw that we can also then use those parameters of this model to perform
sampling of new words so we sample new names according to those distributions and we also saw that
we can evaluate the quality of this model and the quality of this model is summarized in a single number which is
the negative log likelihood and the lower this number is the better the model is because it is giving high probabilities
to the actual next characters in all the bi-grams in our training set so that's all well and good but we've
arrived at this model explicitly by doing something that felt sensible we were just performing counts and then we
were normalizing those counts now what i would like to do is i would like to take an alternative approach we
will end up in a very very similar position but the approach will look very different because i would like to cast
the problem of bi-gram character level language modeling into the neural network framework in the neural network framework we're
going to approach things slightly differently but again end up in a very similar spot i'll go into that later now
our neural network is going to be a still a background character level language model so it receives a single
character as an input then there's neural network with some weights or some parameters w
and it's going to output the probability distribution over the next character in a sequence it's going to make guesses as
to what is likely to follow this character that was input to the model
and then in addition to that we're going to be able to evaluate any setting of the parameters of the neural net because
we have the loss function the negative log likelihood so we're going to take a look at its probability distributions and we're going to use the
labels which are basically just the identity of the next character in that diagram the
second character so knowing what second character actually comes next in the bigram allows
us to then look at what how high of probability the model assigns to that character
and then we of course want the probability to be very high and that is another way of saying that the loss is low
so we're going to use gradient-based optimization then to tune the parameters of this network because we have the loss
function and we're going to minimize it so we're going to tune the weights so that the neural net is correctly
predicting the probabilities for the next character so let's get started the first thing i want to do is i want to compile the

ä»¥ä¸‹æ˜¯è¿™æ®µå…³äº **ç¥ç»ç½‘ç»œæ–¹æ³•å®ç° bigram è¯­è¨€æ¨¡å‹ï¼ˆç¬¬äºŒéƒ¨åˆ†ï¼‰** çš„ä¸­æ–‡ç¿»è¯‘å’Œè®²è§£ï¼š

---

## ğŸ”¢ ç¬¬äºŒéƒ¨åˆ†ï¼šç¥ç»ç½‘ç»œæ–¹æ³•ç®€ä»‹

æˆ‘ä»¬ä¹‹å‰å·²ç»ç”¨ç»Ÿè®¡æ–¹æ³•åšäº†è¯­è¨€æ¨¡å‹ï¼š

* é€šè¿‡ç»Ÿè®¡ **æ‰€æœ‰ bigramï¼ˆå­—å¯¹ï¼‰** çš„å‡ºç°æ¬¡æ•°ï¼›
* ç„¶åå¯¹æ¯ä¸€è¡Œè¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°æ¯ä¸ªå­—ç¬¦åé¢æ¥å“ªä¸ªå­—ç¬¦çš„**æ¦‚ç‡åˆ†å¸ƒ**ã€‚

æˆ‘ä»¬ä¹Ÿåšäº†ä¸‰ä»¶äº‹ï¼š

1. **è®­ç»ƒæ¨¡å‹**ï¼šé€šè¿‡è®¡æ•° & å½’ä¸€åŒ–ï¼›
2. **é‡‡æ ·ç”Ÿæˆæ–°è¯**ï¼šæ ¹æ® bigram æ¦‚ç‡ç”Ÿæˆåå­—ï¼›
3. **è¯„ä¼°æ¨¡å‹è´¨é‡**ï¼šç”¨â€œ**è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNegative Log-Likelihood, NLLï¼‰**â€æ¥è¡¡é‡æ¨¡å‹æ€§èƒ½ â€”â€” è¶Šä½è¶Šå¥½ï¼Œè¡¨ç¤ºæ¨¡å‹è¶Šæ“…é•¿é¢„æµ‹çœŸå®è®­ç»ƒé›†ä¸­å­—ç¬¦çš„ç»„åˆã€‚

---

## ğŸ¤– ç°åœ¨ï¼Œæˆ‘ä»¬å°†é‡‡ç”¨ä¸€ä¸ª**å®Œå…¨ä¸åŒçš„æ–¹å¼** â€”â€” ç”¨ç¥ç»ç½‘ç»œæ¥åšï¼

è™½ç„¶æœ€ç»ˆå¾—åˆ°çš„ä¸œè¥¿ä¼šå¾ˆç›¸ä¼¼ï¼Œä½†**å®ç°æ–¹æ³•å®Œå…¨ä¸åŒ**ï¼š

### ğŸ¯ æ–°ç›®æ ‡ï¼šæŠŠ bigram å­—ç¬¦çº§è¯­è¨€æ¨¡å‹ **è½¬åŒ–ä¸ºç¥ç»ç½‘ç»œä»»åŠ¡**ã€‚

* è¾“å…¥ï¼šä¸€ä¸ªå­—ç¬¦ï¼ˆæ¯”å¦‚ `'a'`ï¼‰
* ç½‘ç»œï¼šæœ‰ä¸€å¥—å¯å­¦ä¹ å‚æ•° `W`
* è¾“å‡ºï¼š**ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒ**

ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä¼šæ ¹æ®è¾“å…¥å­—ç¬¦ï¼Œ**é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯å“ªä¸ªçš„æ¦‚ç‡**ã€‚

---

## ğŸ§  è®­ç»ƒæ–¹æ³•ï¼š

1. **è¾“å…¥ä¸€ä¸ªå­—ç¬¦**ï¼›
2. ç¥ç»ç½‘ç»œç»™å‡ºä¸€ä¸ªâ€œä¸‹ä¸€ä¸ªå­—ç¬¦â€çš„æ¦‚ç‡åˆ†å¸ƒï¼›
3. æˆ‘ä»¬çŸ¥é“è®­ç»ƒé›†ä¸­çœŸå®çš„ä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯ä»€ä¹ˆï¼ˆä¹Ÿå°±æ˜¯ bigram çš„ç¬¬äºŒä¸ªå­—ç¬¦ï¼‰ï¼›
4. çœ‹çœ‹ç½‘ç»œå¯¹çœŸå®å­—ç¬¦é¢„æµ‹çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼š

   * **è¶Šé«˜è¶Šå¥½**ï¼›
   * **æ¦‚ç‡è¶Šä½ï¼ŒæŸå¤±è¶Šå¤§ï¼ˆNegative Log Likelihood è¶Šå¤§ï¼‰**ï¼›
5. æ‰€ä»¥æˆ‘ä»¬å°±æœ‰äº†ä¸€ä¸ªæ˜ç¡®çš„â€œç›®æ ‡å‡½æ•°â€ï¼šè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰ï¼›
6. æˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨\*\*åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ï¼ˆåå‘ä¼ æ’­ + æ¢¯åº¦ä¸‹é™ï¼‰\*\*æ¥æ›´æ–°ç½‘ç»œå‚æ•°ï¼Œä½¿å¾—æ¨¡å‹æ›´æ“…é•¿é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚

---

## ğŸ” æ€»ç»“ä¸€ä¸‹æµç¨‹ï¼š

| æ­¥éª¤ | å†…å®¹                         |
| -- | -------------------------- |
| è¾“å…¥ | å•ä¸ªå­—ç¬¦ï¼ˆä¾‹å¦‚ `'a'`ï¼‰             |
| è¾“å‡º | æ‰€æœ‰å¯èƒ½çš„â€œä¸‹ä¸€ä¸ªå­—ç¬¦â€çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆ27ä¸ªï¼‰     |
| çœŸå€¼ | è®­ç»ƒé›†é‡ŒçœŸå®çš„ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼ˆæ¯”å¦‚ `'b'`ï¼‰     |
| æŸå¤± | ç½‘ç»œé¢„æµ‹ `'b'` çš„æ¦‚ç‡è¶Šé«˜ï¼ŒæŸå¤±è¶Šå°ï¼›åä¹‹è¶Šå¤§ |
| ä¼˜åŒ– | ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥æœ€å°åŒ–è¿™ä¸ªæŸå¤±ï¼ˆNLLï¼‰        |

---

ä¸‹ä¸€æ­¥ï¼Œä½œè€…å°†å¼€å§‹å®ç°ç¥ç»ç½‘ç»œæ¨¡å‹çš„ä»£ç ã€‚

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥ä¸ºä½ è¯¦ç»†å†™å‡ºä» bigram åˆ°ç¥ç»ç½‘ç»œæ¨¡å‹çš„ PyTorch å®ç°æ­¥éª¤ã€‚æ˜¯å¦éœ€è¦ç»§ç»­ï¼Ÿ

# creating the bigram dataset for the neural net

training set of this neural network right so create the training set
of all the bigrams okay and
here i'm going to copy paste this code because this code iterates over all the
programs so here we start with the words we iterate over all the bygrams and
previously as you recall we did the counts but now we're not going to do counts we're just creating a training
set now this training set will be made up of two lists
we have the inputs and the targets
the the labels and these bi-grams will denote x y those are the characters right
and so we're given the first character of the bi-gram and then we're trying to predict the next one both of these are going to be integers
so here we'll take x's that append is just x1 ystat append ix2
and then here we actually don't want lists of integers we will create tensors out of these so
axis is torch.tensor of axis and wise a storage.tensor of ys
and then we don't actually want to take all the words just yet because i want everything to be manageable
so let's just do the first word which is emma and then it's clear what these x's and
y's would be here let me print character 1 character 2 just so you see
what's going on here so the bigrams of these characters is
dot e e m m m a a dot so this single word as i mentioned has one two three
four five examples for our neural network there are five separate examples in emma
and those examples are summarized here when the input to the neural network is integer 0
the desired label is integer 5 which corresponds to e when the input to the
neural network is 5 we want its weights to be arranged so that 13 gets a very high probability
when 13 is put in we want 13 to have a high probability when 13 is put in we also want 1 to have
a high probability when one is input we want zero to have a very high probability so there are five
separate input examples to a neural nut in this data set
i wanted to add a tangent of a node of caution to be careful with a lot of the apis of some of these frameworks
you saw me silently use torch.tensor with a lowercase t and the output looked right
but you should be aware that there's actually two ways of constructing a tensor there's a torch.lowercase tensor
and there's also a torch.capital tensor class which you can also construct so you can actually call both you can
also do torch.capital tensor and you get a nexus and wise as well
so that's not confusing at all um there are threads on what is the difference between these two
and um unfortunately the docs are just like not clear on the difference and when you look at the the docs of lower case
tensor construct tensor with no autograd history by copying data it's just like it doesn't
it doesn't make sense so the actual difference as far as i can tell is explained eventually in this random thread that you can google
and really it comes down to i believe that um
what is this torch.tensor in first d-type the data type automatically while torch.tensor
just returns a float tensor i would recommend stick to torch.lowercase tensor
so um indeed we see that when i construct this with a capital t the data
type here of xs is float32 but towards that lowercase tensor
you see how it's now x dot d type is now integer
so um it's advised that you use lowercase t and you can read more about it if you
like in some of these threads but basically um i'm pointing out some of these things
because i want to caution you and i want you to re get used to reading a lot of documentation and reading through a lot
of q and a's and threads like this and you know some of the stuff is
unfortunately not easy and not very well documented and you have to be careful out there what we want here is integers
because that's what makes uh sense um and so lowercase tensor is what we are using

ä»¥ä¸‹æ˜¯è¿™æ®µå…³äº**ä¸ºç¥ç»ç½‘ç»œåˆ›å»º bigram æ•°æ®é›†**çš„å†…å®¹ç¿»è¯‘å’Œè¯¦ç»†è§£é‡Šï¼š

---

## ğŸ“Š ä¸ºç¥ç»ç½‘ç»œåˆ›å»º bigram æ•°æ®é›†

æˆ‘ä»¬ç°åœ¨è¦ä¸ºç¥ç»ç½‘ç»œåˆ›å»ºè®­ç»ƒé›†ï¼Œä½¿ç”¨çš„ä»ç„¶æ˜¯å­—ç¬¦çº§çš„ bigram è¯­è¨€æ¨¡å‹ã€‚

---

### ğŸ›  æ­¥éª¤è§£æ

> **ç›®æ ‡ï¼š** æŠŠæ‰€æœ‰çš„ bigram å˜æˆç¥ç»ç½‘ç»œçš„è®­ç»ƒæ ·æœ¬ï¼ˆè¾“å…¥å’Œç›®æ ‡è¾“å‡ºï¼‰ã€‚

---

### ğŸ§© Bigram ç»“æ„ä¸¾ä¾‹ï¼š

ä»¥å•è¯ `"emma"` ä¸ºä¾‹ï¼š

æˆ‘ä»¬åœ¨æ¯ä¸ªè¯å‰ååŠ ä¸Šç‰¹æ®Šå­—ç¬¦ `'.'` è¡¨ç¤ºèµ·å§‹å’Œç»“æŸï¼š

* åŸå§‹è¯å˜æˆï¼š`.emma.`
* å¯¹åº”çš„ bigram æ˜¯ï¼š`.e`ã€`e.m`ã€`m.m`ã€`m.a`ã€`a.`

---

### ğŸ§® ä»£ç é€»è¾‘ï¼š

æˆ‘ä»¬éå†è¿™äº› bigramï¼Œä¸å†**ç»Ÿè®¡å‡ºç°æ¬¡æ•°**ï¼Œè€Œæ˜¯ï¼š

* æŠŠæ¯å¯¹å­—ç¬¦ï¼ˆx, yï¼‰è½¬ä¸ºå¯¹åº”çš„æ•´æ•°ï¼ˆç´¢å¼•ï¼‰
* å­˜å…¥ä¸¤ä¸ªåˆ—è¡¨ï¼š

  * `xs`ï¼šè¾“å…¥å­—ç¬¦çš„ç´¢å¼•
  * `ys`ï¼šç›®æ ‡å­—ç¬¦çš„ç´¢å¼•

ä¾‹å¦‚ï¼š

```python
xs.append(ix1)  # è¾“å…¥å­—ç¬¦ç´¢å¼•
ys.append(ix2)  # ç›®æ ‡å­—ç¬¦ç´¢å¼•
```

æœ€åï¼Œæˆ‘ä»¬å°†è¿™ä¸¤ä¸ªåˆ—è¡¨è½¬æ¢ä¸º PyTorch çš„å¼ é‡ï¼š

```python
xs = torch.tensor(xs)  
ys = torch.tensor(ys)
```

---

### ğŸ“¦ ç¤ºä¾‹è¾“å‡ºï¼ˆä»¥ `"emma"` ä¸ºä¾‹ï¼‰ï¼š

| bigram | è¾“å…¥å­—ç¬¦ï¼ˆxï¼‰  | ç›®æ ‡å­—ç¬¦ï¼ˆyï¼‰  |
| ------ | -------- | -------- |
| `.e`   | `.` â†’ 0  | `e` â†’ 5  |
| `e.m`  | `e` â†’ 5  | `m` â†’ 13 |
| `m.m`  | `m` â†’ 13 | `m` â†’ 13 |
| `m.a`  | `m` â†’ 13 | `a` â†’ 1  |
| `a.`   | `a` â†’ 1  | `.` â†’ 0  |

æ‰€ä»¥è¿™ä¸ª wordï¼ˆemmaï¼‰å°±è´¡çŒ®äº† 5 ä¸ªè®­ç»ƒæ ·æœ¬ã€‚

---

## âš ï¸ å°å¿ƒ Tensor çš„æ„å»ºæ–¹å¼ï¼

ä½œè€…ç‰¹åˆ«æé†’ï¼š

* PyTorch æœ‰ä¸¤ç§æ–¹å¼å¯ä»¥æ„å»º tensorï¼š

  * `torch.tensor(...)`ï¼ˆå°å†™ï¼‰âœ… æ¨è
  * `torch.Tensor(...)`ï¼ˆå¤§å†™ï¼‰âš ï¸ ä¸æ¨è

> ä¸¤è€…è¡Œä¸º**ä¸åŒ**ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ•´æ•°æ—¶ï¼

* å°å†™ `.tensor()` ä¼šæ ¹æ®ä½ æä¾›çš„å†…å®¹è‡ªåŠ¨æ¨æ–­æ•°æ®ç±»å‹ï¼ˆä¾‹å¦‚æ•´æ•° -> `int64`ï¼‰
* å¤§å†™ `.Tensor()` ä¼šé»˜è®¤ç”Ÿæˆ float32 ç±»å‹ï¼ˆå³ä½¿ä½ ä¼ çš„æ˜¯æ•´æ•°ï¼ï¼‰

ä¾‹å¦‚ï¼š

```python
xs = torch.Tensor([1, 2, 3])  # âŒ å¾—åˆ° float32
xs = torch.tensor([1, 2, 3])  # âœ… å¾—åˆ° int64
```

---

### âœ… æ€»ç»“å»ºè®®ï¼š

* å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚å­—ç¬¦ç´¢å¼•é¢„æµ‹ï¼‰ï¼Œä½ å¸Œæœ›æ˜¯ **æ•´æ•°ç±»å‹ï¼ˆint64ï¼‰**
* æ‰€ä»¥åº”è¯¥ä½¿ç”¨å°å†™çš„ `torch.tensor(...)`

---

éœ€è¦æˆ‘å¸®ä½ æŠŠè¿™ä¸ªæ•°æ®å¤„ç†éƒ¨åˆ†çš„ä»£ç å†™å‡ºæ¥å—ï¼Ÿæˆ–è€…ä½ æƒ³ç»§ç»­ç¿»è¯‘åç»­å†…å®¹ï¼Ÿ

# feeding integers into neural nets? one-hot encodings

okay now we want to think through how we're going to feed in these examples into a neural network
now it's not quite as straightforward as plugging it in because these examples right now are integers so there's like a
0 5 or 13 it gives us the index of the character and you can't just plug an integer index into a neural net
these neural nets right are sort of made up of these neurons and
these neurons have weights and as you saw in micrograd these weights act multiplicatively on the inputs w x plus
b there's 10 h's and so on and so it doesn't really make sense to make an input neuron take on integer values that
you feed in and then multiply on with weights so instead a common way of encoding integers is
what's called one hot encoding in one hot encoding we take an integer like 13 and we create
a vector that is all zeros except for the 13th dimension which we turn to a
one and then that vector can feed into a neural net now conveniently
uh pi torch actually has something called the one hot function inside torching and functional
it takes a tensor made up of integers um long is a is a as an integer
um and it also takes a number of classes um which is how large you want your uh
tensor uh your vector to be so here let's import
torch.n.functional sf this is a common way of importing it and then let's do f.1 hot
and we feed in the integers that we want to encode so we can actually feed in the entire array of x's
and we can tell it that num classes is 27. so it doesn't have to try to guess it it
may have guessed that it's only 13 and would give us an incorrect result
so this is the one hot let's call this x inc for x encoded
and then we see that x encoded that shape is 5 by 27
and uh we can also visualize it plt.i am show of x inc
to make it a little bit more clear because this is a little messy so we see that we've encoded all the five examples uh into vectors we have
five examples so we have five rows and each row here is now an example into a neural nut
and we see that the appropriate bit is turned on as a one and everything else is zero
so um here for example the zeroth bit is turned on the fifth bit is turned on
13th bits are turned on for both of these examples and then the first bit here is turned on
so that's how we can encode integers into vectors and then these
vectors can feed in to neural nets one more issue to be careful with here by the way is
let's look at the data type of encoding we always want to be careful with data types what would you expect x encoding's data
type to be when we're plugging numbers into neural nuts we don't want them to be integers we want them to be floating point
numbers that can take on various values but the d type here is actually 64-bit
integer and the reason for that i suspect is that one hot received a 64-bit integer
here and it returned the same data type and when you look at the signature of one hot it doesn't even take a d type a
desired data type of the output tensor and so we can't in a lot of functions in
torch we'd be able to do something like d type equal storage.float32 which is what we want but one heart does
not support that so instead we're going to want to cast this to float like this
so that these everything is the same everything looks the same but the d-type
is float32 and floats can feed into neural nets so now let's construct our

ä»¥ä¸‹æ˜¯è¿™æ®µå…³äº **å°†æ•´æ•°è¾“å…¥ç¥ç»ç½‘ç»œï¼šOne-hot ç¼–ç ** çš„å®Œæ•´ç¿»è¯‘ä¸è®²è§£ï¼š

---

## ğŸ¯ å°†æ•´æ•°è¾“å…¥ç¥ç»ç½‘ç»œï¼Ÿä½¿ç”¨ One-hot ç¼–ç 

---

ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº†è®­ç»ƒæ•°æ®ï¼ˆä¾‹å¦‚å­—ç¬¦ç´¢å¼•ï¼š0ã€5ã€13 ç­‰ï¼‰ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬æƒ³è¦æŠŠå®ƒä»¬è¾“å…¥ç¥ç»ç½‘ç»œä¸­ã€‚

ä½†äº‹æƒ…**ä¸æ˜¯é‚£ä¹ˆç®€å•** â€”â€” å› ä¸ºï¼š

### âŒ é—®é¢˜ï¼šæ•´æ•°ä¸èƒ½ç›´æ¥ä½œä¸ºç¥ç»ç½‘ç»œè¾“å…¥

ç¥ç»ç½‘ç»œçš„è¾“å…¥æ˜¯è¦å’Œæƒé‡è¿›è¡Œ**ä¹˜æ³•è®¡ç®—**çš„ï¼Œä¾‹å¦‚ `wÂ·x + b`ã€‚
å¦‚æœç›´æ¥è¾“å…¥æ•´æ•°ï¼ˆä¾‹å¦‚ 13ï¼‰ï¼Œç¥ç»å…ƒä¼šæŠŠå®ƒå½“ä½œæ˜¯ä¸€ä¸ªå®æ•°å€¼æ¥ä¹˜ï¼Œè¿™**æ²¡æœ‰æ„ä¹‰**ã€‚

---

### âœ… è§£å†³æ–¹æ¡ˆï¼šOne-hot ç¼–ç 

æ‰€è°“ **One-hot encodingï¼ˆç‹¬çƒ­ç¼–ç ï¼‰** æ˜¯æŠŠæ¯ä¸ªæ•´æ•°è½¬æ¢æˆä¸€ä¸ªå‘é‡ï¼Œè¿™ä¸ªå‘é‡é™¤äº†æŸä¸€ä¸ªä½ç½®æ˜¯ 1 ä»¥å¤–ï¼Œå…¶ä½™éƒ½æ˜¯ 0ã€‚

ä¾‹å¦‚ï¼š

```text
ç´¢å¼• 13 â†’ [0, 0, ..., 0, 1, 0, ..., 0] ï¼ˆåªæœ‰ç¬¬13ä½æ˜¯1ï¼Œå…¶ä»–éƒ½æ˜¯0ï¼‰
```

è¿™ç§ç¼–ç æ–¹å¼**ä¸æºå¸¦ä»»ä½•æ•°å€¼å¤§å°ä¿¡æ¯**ï¼Œçº¯ç²¹åªæ˜¯ä¸€ä¸ªâ€œä½ç½®â€çš„æ ‡è®°ï¼Œéå¸¸é€‚åˆç”¨æ¥è¡¨è¾¾â€œåˆ†ç±»ç´¢å¼•â€ã€‚

---

### ğŸ’¡ PyTorch ä¸­çš„ One-hot ç¼–ç 

PyTorch æä¾›äº†å†…ç½®å‡½æ•°ï¼š

```python
torch.nn.functional.one_hot()
```

ç”¨æ³•ï¼š

```python
import torch.nn.functional as F
F.one_hot(xs, num_classes=27)
```

å…¶ä¸­ï¼š

* `xs` æ˜¯åŸå§‹æ•´æ•°å¼ é‡ï¼ˆå¦‚ `[0, 5, 13, 13, 1]`ï¼‰
* `num_classes=27` è¡¨ç¤ºæˆ‘ä»¬å…±æœ‰ 27 ä¸ªç±»åˆ«ï¼ˆå­—ç¬¦ï¼‰

è¿™å°†è¿”å›ä¸€ä¸ªå¼ é‡ï¼Œå…¶å½¢çŠ¶ä¸º `[æ ·æœ¬æ•°, ç±»åˆ«æ•°]`ã€‚

---

### ğŸ“Š ç¤ºä¾‹ç»“æœï¼š

ä»¥ `"emma"` ä¸ºä¾‹ï¼š

```python
xs = tensor([0, 5, 13, 13, 1])
F.one_hot(xs, num_classes=27)
```

ç»“æœæ˜¯ä¸€ä¸ª 5Ã—27 çš„çŸ©é˜µï¼Œæ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯è¡Œåªæœ‰ä¸€ä¸ªä½ç½®ä¸º1ï¼Œå…¶ä»–ä¸º0ã€‚

---

### ğŸ“ˆ å¯è§†åŒ–ï¼š

å¯ä»¥ä½¿ç”¨ `matplotlib` æ¥å¯è§†åŒ–è¿™ä¸ª one-hot ç¼–ç ï¼š

```python
plt.imshow(x_enc)
```

ä¼šçœ‹åˆ°ä¸€ä¸ªåªæœ‰å°‘æ•°ç™½ç‚¹ï¼ˆä»£è¡¨1ï¼‰çš„å›¾ï¼Œå…¶ä»–éƒ½æ˜¯é»‘è‰²ï¼ˆä»£è¡¨0ï¼‰ã€‚

---

### âš ï¸ å°å¿ƒæ•°æ®ç±»å‹ï¼

é»˜è®¤æƒ…å†µä¸‹ï¼Œ`F.one_hot()` ä¼šè¿”å› **æ•´æ•°å‹ï¼ˆint64ï¼‰** çš„å¼ é‡ï¼Œä½†ç¥ç»ç½‘ç»œè¦æ±‚è¾“å…¥æ˜¯**æµ®ç‚¹å‹ï¼ˆfloat32ï¼‰**ã€‚

å› æ­¤ä½ éœ€è¦æ‰‹åŠ¨è½¬æ¢ï¼š

```python
x_enc = x_enc.float()  # è½¬æ¢ä¸º float32 ç±»å‹
```

è¿™æ ·æ‰èƒ½è¾“å…¥åˆ°ç¥ç»ç½‘ç»œä¸­ã€‚

---

## âœ… æ€»ç»“ï¼š

| æ­¥éª¤    | æè¿°                              |
| ----- | ------------------------------- |
| âŒ ä¸å¯è¡Œ | ç›´æ¥æŠŠæ•´æ•°ç´¢å¼•ä½œä¸ºè¾“å…¥ï¼Œä¸èƒ½å‚ä¸ç¥ç»ç½‘ç»œçš„è®¡ç®—         |
| âœ… å¯è¡Œ  | æŠŠæ•´æ•°ç´¢å¼•è½¬æ¢ä¸º One-hot å‘é‡             |
| ğŸ“Œ å·¥å…· | `torch.nn.functional.one_hot()` |
| ğŸ§½ æ³¨æ„ | æœ€ç»ˆç»“æœéœ€ `.float()` è½¬ä¸ºæµ®ç‚¹æ•°å¼ é‡        |

---

éœ€è¦æˆ‘å¸®ä½ å†™ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ä»£ç æ¥å±•ç¤ºä» `xs` åˆ° `one-hot` çš„å…¨è¿‡ç¨‹å—ï¼Ÿ

# the "neural net": one linear layer of neurons implemented with matrix multiplication

first neuron this neuron will look at these input vectors
and as you remember from micrograd these neurons basically perform a very simple function w x plus b where w x is a dot
product right so we can achieve the same thing here let's first define the weights of this
neuron basically what are the initial weights at initialization for this neuron let's initialize them with torch.rendin
torch.rendin is um fills a tensor with random numbers
drawn from a normal distribution and a normal distribution has a probability density function like this
and so most of the numbers drawn from this distribution will be around 0 but some of them will be as high as
almost three and so on and very few numbers will be above three in magnitude
so we need to take a size as an input here and i'm going to use size as to be 27 by
one so 27 by one and then let's visualize w so
w is a column vector of 27 numbers and
these weights are then multiplied by the inputs so now to perform this multiplication we
can take x encoding and we can multiply it with w this is a matrix multiplication operator
in pi torch and the output of this operation is five by one
the reason is five by five is the following we took x encoding which is five by twenty seven and we multiplied it by
twenty seven by one and in matrix multiplication
you see that the output will become five by one because these 27
will multiply and add so basically what we're seeing here outs
out of this operation is we are seeing the five activations
of this neuron on these five inputs and we've evaluated all of them in
parallel we didn't feed in just a single input to the single neuron we fed in simultaneously all the five inputs into
the same neuron and in parallel patrol has evaluated the wx plus b but here is just the wx
there's no bias it has value w times x for all of them
independently now instead of a single neuron though i would like to have 27 neurons and i'll show you in a second
why i want 27 neurons so instead of having just a 1 here which is indicating this presence of one
single neuron we can use 27 and then when w is 27 by 27
this will in parallel evaluate all the 27 neurons on all the 5 inputs
giving us a much better much much bigger result so now what we've done is 5 by 27 multiplied 27 by 27
and the output of this is now 5 by 27 so we can see that the shape of this
is 5 by 27. so what is every element here telling us right
it's telling us for every one of 27 neurons that we created
what is the firing rate of those neurons on every one of those five examples
so the element for example 3 comma 13
is giving us the firing rate of the 13th neuron looking at the third input
and the way this was achieved is by a dot product between the third
input and the 13th column of this w matrix here
okay so using matrix multiplication we can very efficiently evaluate
the dot product between lots of input examples in a batch and lots of neurons where all those
neurons have weights in the columns of those w's and in matrix multiplication we're just doing those dot products and
in parallel just to show you that this is the case we can take x and we can take the third
row and we can take the w and take its 13th column
and then we can do x and get three elementwise multiply with w at 13.
and sum that up that's wx plus b well there's no plus b it's just wx dot
product and that's this number so you see that this is just being done efficiently by the matrix multiplication
operation for all the input examples and for all the output neurons of this first layer

ä»¥ä¸‹æ˜¯è¿™æ®µå†…å®¹çš„è¯¦ç»†ä¸­æ–‡ç¿»è¯‘ä¸è§£é‡Šï¼š

---

## ğŸ§ ã€Œç¥ç»ç½‘ç»œã€çš„ç¬¬ä¸€å±‚ï¼šç”¨çŸ©é˜µä¹˜æ³•å®ç°çš„çº¿æ€§å±‚ï¼ˆLinear Layerï¼‰

---

æˆ‘ä»¬ç°åœ¨è¦å®ç°ä¸€ä¸ª**çº¿æ€§ç¥ç»å…ƒå±‚**ï¼Œè¿™ä¸ªç¥ç»å…ƒå±‚ä¼šæ¥æ”¶å‰é¢æ„é€ çš„ one-hot å‘é‡ä½œä¸ºè¾“å…¥ã€‚

---

### ğŸ¯ ä¸€ä¸ªç¥ç»å…ƒçš„è®¡ç®—è¿‡ç¨‹å›é¡¾ï¼š

è¿˜è®°å¾—åœ¨ micrograd ä¸­ï¼Œæˆ‘ä»¬çš„ç¥ç»å…ƒçš„è®¡ç®—å…¬å¼æ˜¯ï¼š

```
output = w Â· x + b
```

å…¶ä¸­ `wÂ·x` æ˜¯ç‚¹ç§¯ï¼ˆdot productï¼‰ï¼Œç„¶ååŠ ä¸Šåç½® bã€‚

---

### ğŸ›  ç¬¬ä¸€æ­¥ï¼šå®šä¹‰æƒé‡ W

æˆ‘ä»¬ç”¨ `torch.randn()` åˆå§‹åŒ–ç¥ç»å…ƒçš„æƒé‡ï¼š

```python
W = torch.randn(27, 1)
```

è¿™é‡Œæ˜¯ä¸€ä¸ª **27Ã—1 çš„åˆ—å‘é‡**ï¼Œè¡¨ç¤ºæœ‰ä¸€ä¸ªç¥ç»å…ƒï¼Œå®ƒéœ€è¦ä¹˜ä»¥ä¸€ä¸ª 27 ç»´çš„è¾“å…¥å‘é‡ï¼ˆå› ä¸ºæˆ‘ä»¬ç”¨çš„æ˜¯ one-hot ç¼–ç ï¼Œæœ‰ 27 ä¸ªå¯èƒ½å­—ç¬¦ï¼‰ã€‚

`torch.randn` ç”Ÿæˆçš„æ˜¯æœä»**æ­£æ€åˆ†å¸ƒ**ï¼ˆå‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1ï¼‰çš„éšæœºæ•°ï¼Œå¤§å¤šæ•°å€¼åœ¨ \[-3, 3] åŒºé—´å†…ã€‚

---

### ğŸ§® ç¬¬äºŒæ­¥ï¼šè¿›è¡ŒçŸ©é˜µä¹˜æ³•

å‡è®¾æˆ‘ä»¬æœ‰äº”ä¸ªè¾“å…¥æ ·æœ¬ï¼ˆæ¯”å¦‚ "emma" çš„ 5 ä¸ª bigramï¼‰ï¼Œå®ƒä»¬çš„ one-hot ç¼–ç æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸ºï¼š

```python
x_enc.shape = (5, 27)
```

æˆ‘ä»¬è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼š

```python
out = x_enc @ W
```

* `x_enc` æ˜¯ 5Ã—27ï¼ˆ5 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ª 27 ç»´å‘é‡ï¼‰
* `W` æ˜¯ 27Ã—1ï¼ˆä¸€ä¸ªç¥ç»å…ƒçš„æƒé‡ï¼‰
* ç»“æœ `out` æ˜¯ 5Ã—1ï¼ˆ5 ä¸ªæ ·æœ¬åˆ†åˆ«è¢«ç¥ç»å…ƒå¤„ç†åçš„è¾“å‡ºï¼‰

è¿™è¡¨ç¤ºæˆ‘ä»¬**ä¸€æ¬¡æ€§å¹¶è¡Œè®¡ç®—äº†**è¿™ä¸ªç¥ç»å…ƒå¯¹ 5 ä¸ªæ ·æœ¬çš„å“åº”ï¼ˆfiring rateï¼‰ã€‚

---

### ğŸ¯ æ‹“å±•ï¼šç”¨ 27 ä¸ªç¥ç»å…ƒä»£æ›¿ 1 ä¸ª

æˆ‘ä»¬ä¸åªéœ€è¦ä¸€ä¸ªç¥ç»å…ƒï¼Œè€Œæ˜¯è¦æœ‰ **27 ä¸ªç¥ç»å…ƒ** â€”â€” æ¯ä¸ªç¥ç»å…ƒé¢„æµ‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡ï¼ˆa-z + .ï¼‰ã€‚

å› æ­¤æˆ‘ä»¬æŠŠ `W` æ‰©å±•ä¸ºï¼š

```python
W = torch.randn(27, 27)
```

ç°åœ¨è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼š

```python
out = x_enc @ W
```

* `x_enc`: 5Ã—27
* `W`: 27Ã—27
* `out`: 5Ã—27

ç°åœ¨çš„è¾“å‡º `out[i][j]` è¡¨ç¤ºï¼š

> ç¬¬ `i` ä¸ªæ ·æœ¬è¾“å…¥ï¼Œåœ¨ç¬¬ `j` ä¸ªç¥ç»å…ƒä¸Šçš„æ¿€æ´»å€¼ã€‚

æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªç¥ç»å…ƒã€‚

---

### ğŸ§ª éªŒè¯ï¼šç‚¹ç§¯ç¡®å®æ˜¯è¿™ä¹ˆæ¥çš„

ä¾‹å¦‚ï¼š

```python
x_enc[3]         # ç¬¬4ä¸ªè¾“å…¥æ ·æœ¬
W[:,13]          # ç¬¬14ä¸ªç¥ç»å…ƒçš„æƒé‡ï¼ˆW çš„ç¬¬13åˆ—ï¼‰
(x_enc[3] * W[:,13]).sum()  # è¿™å°±æ˜¯ dot productï¼ˆæ‰‹åŠ¨è®¡ç®—ï¼‰
```

è¿™ä¼šç»™å‡ºå’Œ `out[3,13]` ç›¸åŒçš„ç»“æœï¼ŒéªŒè¯çŸ©é˜µä¹˜æ³•çš„ç¡®æ˜¯é«˜æ•ˆå®Œæˆäº†æ‰€æœ‰ dot product çš„å¹¶è¡Œè®¡ç®—ã€‚

---

### âœ… æ€»ç»“

| æ­¥éª¤   | å†…å®¹                                    |
| ---- | ------------------------------------- |
| å®šä¹‰æƒé‡ | ä½¿ç”¨ `torch.randn(27, N)`ï¼ŒN æ˜¯ç¥ç»å…ƒæ•°ç›®      |
| è¾“å…¥æ•°æ® | One-hot ç¼–ç ï¼Œå½¢çŠ¶ä¸º `[æ ·æœ¬æ•°, 27]`            |
| å‰å‘ä¼ æ’­ | ä½¿ç”¨çŸ©é˜µä¹˜æ³• `@` æ‰¹é‡è®¡ç®—æ‰€æœ‰è¾“å…¥çš„å“åº”                |
| è¾“å‡ºç»“æœ | ä¸€ä¸ª `[æ ·æœ¬æ•°, ç¥ç»å…ƒæ•°]` çš„çŸ©é˜µï¼Œæ¯ä¸ªå€¼è¡¨ç¤ºå¯¹åº”ç¥ç»å…ƒå¯¹è¾“å…¥çš„å“åº” |

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€ä¸ªç¤ºæ„å›¾æ¥è¯´æ˜è¿™äº›çŸ©é˜µæ“ä½œçš„å«ä¹‰ï¼Œæ˜¯å¦éœ€è¦ï¼Ÿ

# transforming neural net outputs into probabilities: the softmax

okay so we fed our 27-dimensional inputs into a first layer of a neural net that
has 27 neurons right so we have 27 inputs and now we have 27 neurons these
neurons perform w times x they don't have a bias and they don't have a non-linearity like 10 h we're going to
leave them to be a linear layer in addition to that we're not going to have any other layers this is going to
be it it's just going to be the dumbest smallest simplest neural net which is just a single linear layer
and now i'd like to explain what i want those 27 outputs to be intuitively what we're trying to produce
here for every single input example is we're trying to produce some kind of a probability distribution for the next
character in a sequence and there's 27 of them but we have to come up with like precise
semantics for exactly how we're going to interpret these 27 numbers that these neurons take on
now intuitively you see here that these numbers are negative and some of them are positive etc
and that's because these are coming out of a neural net layer initialized with these
normal distribution parameters but what we want is we want something like we had here
like each row here told us the counts and then we normalized the counts to get probabilities and we want something
similar to come out of the neural net but what we just have right now is just some negative and positive numbers
now we want those numbers to somehow represent the probabilities for the next character but you see that probabilities they they
have a special structure they um they're positive numbers and they sum to one
and so that doesn't just come out of a neural net and then they can't be counts because these counts are positive and
counts are integers so counts are also not really a good thing to output from a neural net
so instead what the neural net is going to output and how we are going to interpret the um
the 27 numbers is that these 27 numbers are giving us log counts
basically um so instead of giving us counts directly like in this table they're giving us log
counts and to get the counts we're going to take the log counts and we're going to exponentiate them
now exponentiation takes the following form
it takes numbers that are negative or they are positive it takes the entire real line
and then if you plug in negative numbers you're going to get e to the x which is uh always below one
so you're getting numbers lower than one and if you plug in numbers greater than zero you're getting numbers greater than
one all the way growing to the infinity and this here grows to zero
so basically we're going to take these numbers here
and instead of them being positive and negative and all over the place we're
going to interpret them as log counts and then we're going to element wise exponentiate these numbers
exponentiating them now gives us something like this and you see that these numbers now because they went through an exponent
all the negative numbers turned into numbers below 1 like 0.338 and all the
positive numbers originally turned into even more positive numbers sort of greater than one
so like for example seven is some positive number over here
that is greater than zero but exponentiated outputs here
basically give us something that we can use and interpret as the equivalent of counts originally so you see these
counts here 112 7 51 1 etc the neural net is kind of now predicting
uh counts and these counts are positive numbers
they can never be below zero so that makes sense and uh they can now take on various values
depending on the settings of w so let me break this down
we're going to interpret these to be the log counts
in other words for this that is often used is so-called logits these are logits log counts
then these will be sort of the counts largest exponentiated and this is equivalent to the n matrix
sort of the n array that we used previously remember this was the n
this is the the array of counts and each row here are the counts for the
for the um next character sort of so those are the counts and now the
probabilities are just the counts um normalized and so um
i'm not going to find the same but basically i'm not going to scroll all over the place we've already done this we want to
counts that sum along the first dimension and we want to keep them as true
we've went over this and this is how we normalize the rows of our counts matrix
to get our probabilities props so now these are the probabilities
and these are the counts that we ask currently and now when i show the probabilities
you see that um every row here of course
will sum to 1 because they're normalized and the shape of this
is 5 by 27 and so really what we've achieved is for every one of our five examples
we now have a row that came out of a neural net and because of the transformations here
we made sure that this output of this neural net now are probabilities or we can interpret to be probabilities
so our wx here gave us logits and then we interpret those to be log
counts we exponentiate to get something that looks like counts and then we normalize those counts to
get a probability distribution and all of these are differentiable operations so what we've done now is we're taking
inputs we have differentiable operations that we can back propagate through and we're getting out probability
distributions so for example for the zeroth example that
fed in right which was um the zeroth example here was a one-half
vector of zero and um it basically corresponded to feeding in
this example here so we're feeding in a dot into a neural net and the way we fed the dot into a neural net is that we
first got its index then we one hot encoded it then it went into the neural net and out
came this distribution of probabilities and its shape
is 27 there's 27 numbers and we're going to interpret this as the neural nets assignment for how likely every one of
these characters um the 27 characters are to come next
and as we tune the weights w we're going to be of course getting different probabilities out for any
character that you input and so now the question is just can we optimize and find a good w
such that the probabilities coming out are pretty good and the way we measure pretty good is by the loss function okay

ä»¥ä¸‹æ˜¯ä½ æä¾›å†…å®¹çš„å®Œæ•´ä¸­æ–‡ç¿»è¯‘å’Œè®²è§£ï¼š

---

## ğŸ” å°†ç¥ç»ç½‘ç»œçš„è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡ï¼šSoftmax å‡½æ•°

---

æˆ‘ä»¬å‰é¢å·²ç»åšäº†ä»€ä¹ˆï¼Ÿ

æˆ‘ä»¬å°† 27 ç»´çš„ one-hot ç¼–ç è¾“å…¥åˆ°äº†ä¸€ä¸ª**åŒ…å« 27 ä¸ªç¥ç»å…ƒçš„ç¥ç»ç½‘ç»œå±‚**ä¸­ï¼Œè¿™äº›ç¥ç»å…ƒæ‰§è¡Œçš„æ˜¯ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼š
**`output = WÂ·x`**ï¼ˆæ²¡æœ‰åç½®ï¼Œæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼‰ã€‚

è¿™ä¸ªç¥ç»ç½‘ç»œéå¸¸ç®€å•ï¼Œåªæœ‰ä¸€ä¸ªçº¿æ€§å±‚ï¼Œæ˜¯æœ€â€œç¬¨â€çš„ç½‘ç»œä¹‹ä¸€ã€‚

---

### ğŸ§  æˆ‘ä»¬æƒ³è®©è¾“å‡ºä»£è¡¨ä»€ä¹ˆï¼Ÿ

æˆ‘ä»¬å¸Œæœ›æ¯ä¸€ä¸ªè¾“å…¥æ ·æœ¬ï¼Œæœ€ç»ˆè¾“å‡ºçš„æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼ˆç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼‰ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³è®­ç»ƒçš„æ˜¯ä¸€ä¸ª**è¯­è¨€æ¨¡å‹**ã€‚

* ä¸€å…±æœ‰ 27 ä¸ªå¯èƒ½çš„å­—ç¬¦ï¼ˆa-z å’Œ `.`ï¼‰ï¼Œæ‰€ä»¥æ¯ä¸ªè¾“å‡ºåº”è¯¥æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º 27 çš„å‘é‡ã€‚
* è¿™ä¸ªå‘é‡çš„æ¯ä¸€é¡¹è¡¨ç¤ºå½“å‰è¾“å…¥å­—ç¬¦åé¢æŸä¸ªå­—ç¬¦å‡ºç°çš„â€œæ¦‚ç‡â€ã€‚

ä½†é—®é¢˜æ˜¯ï¼š

> ç°åœ¨ç¥ç»ç½‘ç»œçš„è¾“å‡ºåªæ˜¯ä¸€äº›æ­£æ•°æˆ–è´Ÿæ•°ï¼Œåˆ†å¸ƒéšæ„ã€‚å®ƒ**ä¸æ˜¯æ¦‚ç‡åˆ†å¸ƒ**ã€‚

---

### â“å¦‚ä½•æŠŠè¿™äº›è¾“å‡ºå˜æˆâ€œæ¦‚ç‡â€ï¼Ÿ

æˆ‘ä»¬éœ€è¦æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š

1. æ‰€æœ‰å€¼éƒ½ä¸ºæ­£æ•°
2. æ‰€æœ‰å€¼åŠ èµ·æ¥ç­‰äº 1

è¿™å°±æ˜¯æˆ‘ä»¬éœ€è¦ç”¨ **Softmax** å‡½æ•°çš„åœ°æ–¹ã€‚

---

### ğŸ§® Softmax çš„æ“ä½œè¿‡ç¨‹ï¼š

1. **è§£é‡Šè¾“å‡ºä¸º Log-countï¼ˆlogitsï¼‰**

   * è¾“å‡ºå€¼ä¸æ˜¯æ¦‚ç‡ï¼Œè€Œæ˜¯â€œå¯¹æ•°çš„è®¡æ•°â€å€¼ï¼ˆlog-countï¼‰
   * ç¥ç»ç½‘ç»œç›´æ¥è¾“å‡º logitsï¼ˆé€šå¸¸ç§°ä¸ºâ€œæœªå½’ä¸€åŒ–å¾—åˆ†â€ï¼‰

2. **å¯¹è¿™äº› log-count æ‰§è¡Œ `exp()` æ“ä½œ**

   * æ‰€æœ‰è´Ÿæ•°ä¼šå˜æˆ (0,1) ä¹‹é—´çš„å°æ•°
   * æ‰€æœ‰æ­£æ•°å˜æˆå¤§äº 1 çš„æ•°
   * è¿™æ ·å¾—åˆ°çš„å°±æ˜¯â€œä¼ªè®¡æ•°â€

3. **å°†è¿™äº›â€œè®¡æ•°â€å½’ä¸€åŒ–ï¼ˆnormalizeï¼‰**

   * æ¯ä¸€è¡Œé™¤ä»¥å…¶æ€»å’Œï¼Œå°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªçœŸæ­£çš„æ¦‚ç‡åˆ†å¸ƒ

è¿™ä¸ªè¿‡ç¨‹å°±æ˜¯ Softmaxï¼š

```python
logits = x_enc @ W           # çº¿æ€§è¾“å‡º (logits)
counts = logits.exp()        # ä¼ªè®¡æ•°ï¼ˆå…¨ä¸ºæ­£ï¼‰
probs = counts / counts.sum(dim=1, keepdim=True)  # æŒ‰è¡Œå½’ä¸€åŒ–ä¸ºæ¦‚ç‡
```

---

### âœ… å¾—åˆ°çš„ç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ

* `probs` çš„å½¢çŠ¶æ˜¯ `[5, 27]`ï¼Œè¡¨ç¤º 5 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ
* æ¯ä¸€è¡Œéƒ½ä»£è¡¨ä¸€ä¸ªæ ·æœ¬ä¸‹æ‰€æœ‰å¯èƒ½å­—ç¬¦çš„æ¦‚ç‡å’Œä¸º 1

---

### ğŸ”„ ä¸¾ä¸ªä¾‹å­ï¼š

å‡è®¾è¾“å…¥æ˜¯å­—ç¬¦ `.`ï¼Œå®ƒçš„ one-hot æ˜¯ `[1, 0, 0, ..., 0]`
å®ƒç»è¿‡ç¥ç»ç½‘ç»œåï¼Œè¾“å‡ºä¸€ä¸ª logits å‘é‡ï¼Œæ¯”å¦‚ï¼š

```
[ 0.1, -1.3, 0.5, ..., -0.7 ]
```

ç„¶åæˆ‘ä»¬è¿›è¡Œ `exp()` å¾—åˆ°ï¼š

```
[1.11, 0.27, 1.65, ..., 0.49]
```

æ¥ç€å†é™¤ä»¥æ€»å’Œï¼Œå¾—åˆ°ï¼š

```
[0.05, 0.01, 0.07, ..., 0.02]
```

ç°åœ¨å®ƒå°±æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒäº†ï¼Œæ¯ä¸€é¡¹è¡¨ç¤º `.` åé¢æ¥å“ªä¸ªå­—ç¬¦çš„æ¦‚ç‡ã€‚

---

### ğŸ¯ ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåšï¼Ÿ

* è¾“å‡º logits æ˜¯ç¥ç»ç½‘ç»œæ›´å®¹æ˜“å­¦ä¹ çš„å½¢å¼ï¼ˆä¸ç”¨å¼ºåˆ¶æ»¡è¶³å½’ä¸€åŒ–ï¼‰
* Softmax ä¿ç•™äº†è¿™äº› logits çš„â€œç›¸å¯¹å¤§å°â€ä¿¡æ¯
* æ‰€æœ‰æ­¥éª¤æ˜¯å¯å¾®çš„ï¼ˆå¯ä»¥åå‘ä¼ æ’­ï¼‰

---

### ğŸ”š æ€»ç»“

| é˜¶æ®µ            | è¯´æ˜                      |
| ------------- | ----------------------- |
| `WÂ·x`         | å¾—åˆ° logitsï¼Œæœªå½’ä¸€åŒ–çš„åˆ†æ•°ï¼ˆå¯æ­£å¯è´Ÿï¼‰ |
| `exp(logits)` | è½¬æ¢ä¸ºæ­£æ•°ï¼ˆä¼ªè®¡æ•°ï¼‰              |
| `softmax`     | æŒ‰è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°åˆæ³•çš„æ¦‚ç‡åˆ†å¸ƒ         |
| ç»“æœ            | æ¯ä¸ªè¾“å…¥å­—ç¬¦ï¼Œå¾—åˆ° 27 ä¸ªè¾“å‡ºå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒ |

è¿™æ ·ï¼Œæˆ‘ä»¬å°±èƒ½è®©ç¥ç»ç½‘ç»œè¾“å‡ºä¸€ä¸ªå¯ä»¥ç”¨äºé‡‡æ ·å’Œè®¡ç®—æŸå¤±çš„åˆæ³•åˆ†å¸ƒäº†ã€‚

---

éœ€è¦æˆ‘å¸®ä½ ç”»å‡ºæ•´ä¸ªæµç¨‹å›¾æˆ–ä»£ç ç¤ºæ„å›¾å—ï¼Ÿè¿™èƒ½å¸®åŠ©ä½ æ›´ç›´è§‚ç†è§£æ¯ä¸€æ­¥ã€‚

# summary, preview to next steps, reference to micrograd

so i organized everything into a single summary so that hopefully it's a bit more clear so it starts here
with an input data set we have some inputs to the neural net and we have some labels for the correct
next character in a sequence these are integers here i'm using uh torch generators now
so that you see the same numbers that i see and i'm generating um
27 neurons weights and each neuron here receives 27 inputs
then here we're going to plug in all the input examples x's into a neural net so here this is a forward pass
first we have to encode all of the inputs into one hot representations so we have 27 classes we pass in these
integers and x inc becomes a array that is 5 by 27
zeros except for a few ones we then multiply this in the first layer of a neural net to get logits
exponentiate the logits to get fake counts sort of and normalize these counts to get
probabilities so we lock these last two lines by the way here are called the softmax
which i pulled up here soft max is a very often used layer in a neural net
that takes these z's which are logics exponentiates them
and divides and normalizes it's a way of taking outputs of a neural net layer and these
these outputs can be positive or negative and it outputs probability distributions
it outputs something that is always sums to one and are positive numbers just like probabilities
um so it's kind of like a normalization function if you want to think of it that way and you can put it on top of any other linear layer inside a neural net
and it basically makes a neural net output probabilities that's very often used and we used it as well here
so this is the forward pass and that's how we made a neural net output probability now
you'll notice that um all of these this entire forward pass is made up of
differentiable layers everything here we can back propagate through and we saw some of the
back propagation in micrograd this is just multiplication and addition all that's
happening here is just multiply and then add and we know how to backpropagate through them exponentiation we know how to
backpropagate through and then here we are summing and sum is is easily backpropagable as
well and division as well so everything here is differentiable operation
and we can back propagate through now we achieve these probabilities which
are 5 by 27 for every single example we have a vector of probabilities that's into one
and then here i wrote a bunch of stuff to sort of like break down uh the examples
so we have five examples making up emma right and there are five bigrams inside emma
so bigram example a bigram example1 is that e is the beginning character right
after dot and the indexes for these are zero and five so then we feed in a zero
that's the input of the neural net we get probabilities from the neural net that are 27 numbers
and then the label is 5 because e actually comes after dot so that's the label
and then we use this label 5 to index into the probability distribution here
so this index 5 here is 0 1 2 3 4 5. it's this
number here which is here so that's basically the probability
assigned by the neural net to the actual correct character you see that the network currently thinks that this next character that e
following dot is only one percent likely which is of course not very good right because this actually is a training
example and the network thinks this is currently very very unlikely but that's just because we didn't get very lucky in
generating a good setting of w so right now this network things it says unlikely and 0.01 is not a good outcome
so the log likelihood then is very negative and the negative log likelihood is very
positive and so four is a very high negative log likelihood and that means we're going to
have a high loss because what is the loss the loss is just the average negative log likelihood
so the second character is em and you see here that also the network thought that m following e is very
unlikely one percent the for m following m i thought it was
two percent and for a following m it actually thought it was seven percent likely so
just by chance this one actually has a pretty good probability and therefore pretty low negative log likelihood
and finally here it thought this was one percent likely so overall our average negative log
likelihood which is the loss the total loss that summarizes basically the how well this network
currently works at least on this one word not on the full data suggested one word is 3.76 which is actually very
fairly high loss this is not a very good setting of w's now here's what we can do
we're currently getting 3.76 we can actually come here and we can change our w we can resample it so let
me just add one to have a different seed and then we get a different w and then we can rerun this
and with this different c with this different setting of w's we now get 3.37
so this is a much better w right and that and it's better because the probabilities just happen to come out
higher for the for the characters that actually are next and so you can imagine actually just
resampling this you know we can try two so
okay this was not very good let's try one more we can try three
okay this was terrible setting because we have a very high loss so anyway i'm going to erase this
what i'm doing here which is just guess and check of randomly assigning parameters and seeing if the network is good that is uh amateur hour that's not
how you optimize a neural net the way you optimize your neural net is you start with some random guess and we're
going to commit to this one even though it's not very good but now the big deal is we have a loss function
so this loss is made up only of differentiable operations and we can minimize the loss
by tuning ws by computing the gradients of the loss with respect to
these w matrices and so then we can tune w to minimize the loss and find a good setting of w
using gradient based optimization so let's see how that will work now things are actually going to look almost
identical to what we had with micrograd so here i pulled up the lecture from micrograd
the notebook it's from this repository and when i scroll all the way to the end where we left off with micrograd we had
something very very similar we had a number of input examples in this case we had four input examples inside axis
and we had their targets these are targets just like here we have our axes now but
we have five of them and they're now integers instead of vectors but we're going to convert our integers
to vectors except our vectors will be 27 large instead of three large
and then here what we did is first we did a forward pass where we ran a neural net on all of the inputs
to get predictions our neural net at the time this nfx was a multi-layer perceptron
our neural net is going to look different because our neural net is just a single layer
single linear layer followed by a soft max so that's our neural net
and the loss here was the mean squared error so we simply subtracted the prediction from the ground truth and
squared it and summed it all up and that was the loss and loss was the single number that summarized the quality of
the neural net and when loss is low like almost zero that means the neural net is
predicting correctly so we had a single number that uh that summarized the uh the performance of the
neural net and everything here was differentiable and was stored in massive compute graph
and then we iterated over all the parameters we made sure that the gradients are set to zero and we called
lost up backward and lasted backward initiated back propagation at the final output node of
loss right so yeah remember these expressions we had loss all the way at the end we start
back propagation and we went all the way back and we made sure that we populated all the parameters dot grad
so that graph started at zero but back propagation filled it in and then in the update we iterated over
all the parameters and we simply did a parameter update where every single
element of our parameters was nudged in the opposite direction of the gradient
and so we're going to do the exact same thing here so i'm going to pull this up
on the side here so that we have it available and we're
actually going to do the exact same thing so this was the forward pass so where we did this
and probs is our wipe red so now we have to evaluate the loss but we're not using the mean squared error we're using the

è¿™ä¸€æ®µå†…å®¹æ˜¯ä¸€ä¸ª **ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹**æ•™å­¦çš„**æ€»ç»“ä¸è¿‡æ¸¡éƒ¨åˆ†**ï¼Œç”¨æ¥å›é¡¾æˆ‘ä»¬åˆšåˆšåšäº†ä»€ä¹ˆï¼Œå¹¶å¼•å‡ºä¸‹ä¸€æ­¥ï¼š**ä½¿ç”¨åå‘ä¼ æ’­ï¼ˆbackpropagationï¼‰æ¥è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹çš„å‚æ•°**ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†è§£é‡Šï¼š

---

### ğŸ§© æ•´ä½“ç»“æ„å’Œæµç¨‹å›é¡¾ï¼š

1. **æ•°æ®å‡†å¤‡ï¼ˆè¾“å…¥å’Œæ ‡ç­¾ï¼‰**ï¼š

   * ä½ æœ‰ä¸€ä¸ªå•è¯ï¼Œæ¯”å¦‚ `emma`ã€‚
   * æ‹†æˆ bigramsï¼ˆåŒå­—ç¬¦å¯¹ï¼‰åå¾—åˆ°ï¼š`.e`, `em`, `mm`, `ma`, `a.`ã€‚
   * æ¯å¯¹å­—ç¬¦ç”¨ä¸€ä¸ªæ•´æ•°è¡¨ç¤ºï¼Œè¾“å…¥æ˜¯å‰ä¸€ä¸ªå­—ç¬¦ï¼ˆå¦‚ `e`ï¼‰ï¼Œç›®æ ‡æ˜¯åä¸€ä¸ªå­—ç¬¦ï¼ˆå¦‚ `m`ï¼‰ã€‚

2. **One-hot ç¼–ç **ï¼š

   * è¾“å…¥å­—ç¬¦ç”¨ one-hot å‘é‡è¡¨ç¤ºï¼Œæ¯”å¦‚å­—ç¬¦æ€»æ•°æ˜¯ 27ï¼Œé‚£ä¹ˆæ¯ä¸ªå­—ç¬¦å°±å˜æˆä¸€ä¸ª 27ç»´å‘é‡ï¼Œå…¶ä¸­åªæœ‰ä¸€ä¸ªä½ç½®æ˜¯ 1ã€‚

3. **å‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰**ï¼š

   * è¾“å…¥ one-hot å‘é‡ä¹˜ä»¥æƒé‡çŸ©é˜µ Wï¼Œå¾—åˆ° 27 ç»´çš„è¾“å‡ºï¼ˆlogitsï¼‰ã€‚
   * logits è¢«è§£é‡Šä¸ºâ€œlog-countâ€ï¼ˆå¯¹æ•°è®¡æ•°ï¼‰ï¼Œé€šè¿‡ softmax è½¬æˆæ¦‚ç‡åˆ†å¸ƒã€‚

4. **softmax**ï¼š

   * å°†ç¥ç»ç½‘ç»œè¾“å‡ºè½¬ä¸ºæ¦‚ç‡ï¼Œæ»¡è¶³æ€»å’Œä¸º1ï¼Œæ•°å€¼ä¸ºæ­£ã€‚
   * å®ç°æ–¹å¼æ˜¯ï¼šå¯¹ logits åš `exp()`ï¼Œå†é™¤ä»¥æ€»å’Œè¿›è¡Œå½’ä¸€åŒ–ã€‚

---

### ğŸ“‰ æŸå¤±è®¡ç®—ï¼šNegative Log Likelihood Lossï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰

* æ¨¡å‹é¢„æµ‹ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œä½ æœ‰å®é™…æ ‡ç­¾ï¼ˆæ­£ç¡®çš„ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼‰ã€‚
* å–æ¨¡å‹é¢„æµ‹ä¸­ **è¯¥æ ‡ç­¾å¯¹åº”ä½ç½®çš„æ¦‚ç‡**ï¼Œç„¶åå¯¹å…¶å–å¯¹æ•°å†å–è´Ÿå·ï¼Œå°±æ˜¯è¿™ä¸ªæ ·æœ¬çš„æŸå¤±ï¼š

  $$
  \text{Loss} = -\log P(\text{æ­£ç¡®å­—ç¬¦})
  $$
* å¤šä¸ªæ ·æœ¬å–å¹³å‡ï¼Œå°±æ˜¯æ€»çš„ lossã€‚

ä¾‹å¦‚ï¼š

| è¾“å…¥  | æ­£ç¡®è¾“å‡º | æ¨¡å‹é¢„æµ‹æ¦‚ç‡ | è´Ÿ log æ¦‚ç‡         |
| --- | ---- | ------ | ---------------- |
| `.` | `e`  | 0.01   | -log(0.01) â‰ˆ 4.6 |
| `e` | `m`  | 0.01   | â‰ˆ 4.6            |
| `m` | `m`  | 0.02   | â‰ˆ 3.9            |
| `m` | `a`  | 0.07   | â‰ˆ 2.6            |
| `a` | `.`  | 0.01   | â‰ˆ 4.6            |

å¹³å‡åå¾—åˆ°æ€» lossã€‚

---

### ğŸ² ä¸ºä»€ä¹ˆ loss å¯èƒ½é«˜ï¼Ÿ

* ç½‘ç»œåˆšåˆå§‹åŒ–ï¼Œæƒé‡æ˜¯éšæœºçš„ã€‚
* å¯¹æ­£ç¡®å­—ç¬¦åˆ†é…çš„æ¦‚ç‡å¯èƒ½éå¸¸å° â†’ loss é«˜ã€‚
* æ‰€ä»¥åˆšå¼€å§‹æ¨¡å‹â€œå¾ˆç¬¨â€ã€‚

å¯ä»¥æ‰‹åŠ¨å¤šè¯•å‡ æ¬¡éšæœºæƒé‡åˆå§‹åŒ–ï¼ˆseed 0, 1, 2, ...ï¼‰ï¼Œçœ‹å“ªä¸ªåˆå§‹åŒ–çš„ loss æ›´ä½ï¼ˆä½†è¿™æ˜¯ççŒœï¼‰ã€‚

---

### âš ï¸ è¿™ä¸æ˜¯è®­ç»ƒï¼Œè¿™åªæ˜¯ forward è¿‡ç¨‹ï¼

çœŸæ­£çš„è®­ç»ƒæ˜¯ï¼š

* è®¡ç®— loss
* **åå‘ä¼ æ’­ï¼ˆBackward Passï¼‰**ï¼šè‡ªåŠ¨è®¡ç®—æŸå¤±å‡½æ•°å¯¹æƒé‡çš„å¯¼æ•°ï¼ˆgradientsï¼‰
* **å‚æ•°æ›´æ–°ï¼ˆParameter Updateï¼‰**ï¼šç”¨æ¢¯åº¦ä¸‹é™æ¥æ›´æ–° W

---

### ğŸ” ä¸ micrograd å¯¹æ¯”ï¼š

`micrograd` æ˜¯ä½œè€…è‡ªå·±å†™çš„ä¸€ä¸ªå¾®å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ•™å­¦ã€‚ä¹‹å‰æˆ‘ä»¬ä¹Ÿå­¦è¿‡ï¼š

* å®šä¹‰ç½‘ç»œ
* å‰å‘ä¼ æ’­è®¡ç®— loss
* åå‘ä¼ æ’­ `.backward()`
* ç”¨ `.grad` æ›´æ–°å‚æ•°

ç°åœ¨ PyTorch ä¹Ÿä¼šåšç±»ä¼¼çš„äº‹æƒ…ï¼Œåªä¸è¿‡ç”¨æ›´å¼ºå¤§çš„åº•å±‚å¼•æ“è‡ªåŠ¨å¤„ç†äº†å¾®åˆ†å’Œå›¾æ„å»ºã€‚

---

### âœ… å°ç»“ï¼š

| æ­¥éª¤      | å†…å®¹                                       |
| ------- | ---------------------------------------- |
| 1. æ•°æ®   | æ‹†æˆ bigramsï¼Œæ˜ å°„ä¸ºæ•´æ•°ç´¢å¼•                       |
| 2. ç¼–ç    | ç”¨ one-hot å‘é‡è¡¨ç¤º                           |
| 3. å‰å‘ä¼ æ’­ | `one_hot @ W â†’ logits â†’ softmax â†’ probs` |
| 4. è®¡ç®—æŸå¤± | ä½¿ç”¨ NLL Loss                              |
| 5. æœªæ¥æ­¥éª¤ | ä½¿ç”¨åå‘ä¼ æ’­ + æ¢¯åº¦ä¸‹é™è®­ç»ƒæ¨¡å‹ï¼ˆè°ƒæ•´ Wï¼‰                  |

---

æ˜¯å¦éœ€è¦æˆ‘å¸®ä½ å†™å‡º PyTorch çš„å®Œæ•´è®­ç»ƒä»£ç ç‰‡æ®µï¼Œæˆ–æ˜¯ç”»å‡ºè¿™ä¸ªæµç¨‹å›¾æ¥è¾…åŠ©ç†è§£ï¼Ÿ

# vectorized loss

negative log likelihood because we are doing classification we're not doing regression as it's called
so here we want to calculate loss now the way we calculate it is it's just
this average negative log likelihood now this probs here
has a shape of 5 by 27 and so to get all the we basically want
to pluck out the probabilities at the correct indices here so in particular because the labels are
stored here in array wise basically what we're after is for the first example we're looking at
probability of five right at index five for the second example
at the the second row or row index one we are interested in the probability assigned to index 13.
at the second example we also have 13. at the third row we want one
and then the last row which is four we want zero so these are the probabilities we're interested in right
and you can see that they're not amazing as we saw above so these are the probabilities we want
but we want like a more efficient way to access these probabilities not just listing them out in a tuple
like this so it turns out that the way to do this in pytorch uh one of the ways at least is we can basically pass in
all of these sorry about that all of these um
integers in the vectors so the these ones you see how they're just 0 1
2 3 4 we can actually create that using mp not mp sorry torch dot range of 5
0 1 2 3 4. so we can index here with torch.range of 5
and here we index with ys and you see that that gives us exactly these numbers
so that plucks out the probabilities of that the neural network assigns to the correct next character
now we take those probabilities and we don't we actually look at the log probability so we want to dot log
and then we want to just average that up so take the mean of all of that
and then it's the negative average log likelihood that is the loss
so the loss here is 3.7 something and you see that this loss 3.76 3.76 is
exactly as we've obtained before but this is a vectorized form of that expression so
we get the same loss and the same loss we can consider service part of this forward pass
and we've achieved here now loss okay so we made our way all the way to loss we've defined the forward pass

è¿™æ®µå†…å®¹æ˜¯è®²è§£å¦‚ä½•**å‘é‡åŒ–åœ°è®¡ç®—ç¥ç»ç½‘ç»œçš„æŸå¤±å‡½æ•°**ï¼ˆlossï¼‰ï¼Œå¹¶ç”¨ PyTorch å®ç°ä¸€ä¸ªé«˜æ•ˆçš„å†™æ³•ã€‚ä¸‹é¢æ˜¯é€æ­¥ç¿»è¯‘ä¸è§£é‡Šï¼š

---

### ğŸ§  åŸæ–‡ç¿»è¯‘ + è§£é‡Šï¼š

> **negative log likelihood** because we are doing **classification**, we're not doing **regression** as it's called
> å› ä¸ºæˆ‘ä»¬åœ¨åš**åˆ†ç±»ä»»åŠ¡**ï¼Œæ‰€ä»¥ä½¿ç”¨\*\*è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNegative Log Likelihood, NLLï¼‰\*\*ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œè€Œä¸æ˜¯å›å½’ä»»åŠ¡é‡Œå¸¸ç”¨çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€‚

---

> so here we want to calculate loss now the way we calculate it is it's just this **average negative log likelihood**
> ç°åœ¨æˆ‘ä»¬è¦è®¡ç®— lossï¼Œæˆ‘ä»¬çš„åšæ³•å°±æ˜¯å–æ‰€æœ‰æ ·æœ¬çš„ **è´Ÿå¯¹æ•°æ¦‚ç‡çš„å¹³å‡å€¼**ã€‚

---

> `probs` here has shape (5, 27)
> å½“å‰å˜é‡ `probs` æ˜¯ä¸€ä¸ª 5 Ã— 27 çš„çŸ©é˜µï¼š

* æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼ˆæ¯”å¦‚æ¥è‡ªå•è¯ "emma" ä¸­çš„ bigramï¼‰
* æ¯ä¸€åˆ—å¯¹åº”æŸä¸€ä¸ªå¯èƒ½çš„è¾“å‡ºå­—ç¬¦ï¼ˆå…± 27 ä¸ªå­—ç¬¦ï¼‰

---

> to get all the probabilities at the correct indices...
> æˆ‘ä»¬è¦ä» `probs` é‡Œ**æå–å‡ºç¥ç»ç½‘ç»œå¯¹æ¯ä¸ªæ ·æœ¬ä¸­â€œæ­£ç¡®å­—ç¬¦â€çš„æ¦‚ç‡**ã€‚ä¸¾ä¾‹ï¼š

* ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ­£ç¡®æ ‡ç­¾æ˜¯ `5`ï¼Œæˆ‘ä»¬æƒ³å–ç¬¬0è¡Œçš„ç¬¬5åˆ—æ¦‚ç‡ï¼ˆ`probs[0, 5]`ï¼‰
* ç¬¬äºŒä¸ªæ ·æœ¬æ ‡ç­¾æ˜¯ `13`ï¼Œå– `probs[1, 13]`
  â€¦â€¦

---

> but we want a more efficient way to access these probabilities
> æˆ‘ä»¬å¸Œæœ›ç”¨æ›´**å‘é‡åŒ–çš„æ–¹å¼**ä¸€æ¬¡æ€§å–å‡ºæ‰€æœ‰æ ·æœ¬çš„å¯¹åº”æ¦‚ç‡ã€‚

---

### âœ… PyTorch å®ç°å‘é‡ç´¢å¼•ï¼š

```python
probs[torch.arange(5), ys]
```

è§£é‡Šï¼š

* `torch.arange(5)` â†’ `[0, 1, 2, 3, 4]` â†’ æ ·æœ¬çš„è¡Œç´¢å¼•
* `ys` æ˜¯çœŸå®æ ‡ç­¾ï¼ˆä¾‹å¦‚ `[5, 13, 13, 1, 0]`ï¼‰â†’ åˆ—ç´¢å¼•
* `probs[è¡Œ, åˆ—]` å°±å¯ä»¥ä¸€æ¬¡æ€§å–å‡ºæ¯ä¸ªæ ·æœ¬ä¸­ï¼Œç¥ç»ç½‘ç»œå¯¹æ­£ç¡®å­—ç¬¦çš„æ¦‚ç‡å€¼ã€‚

---

### ğŸ§® ç„¶åè®¡ç®—æŸå¤±ï¼š

```python
loss = -torch.log(probs[torch.arange(5), ys]).mean()
```

* `torch.log(...)` â†’ å–å¯¹æ•°
* `-log(...)` â†’ è´Ÿå¯¹æ•°ï¼ˆNegative Log-Likelihoodï¼‰
* `.mean()` â†’ æ‰€æœ‰æ ·æœ¬çš„å¹³å‡ï¼Œå¾—åˆ° lossã€‚

---

### ğŸ§¾ ç»“æœï¼š

> æ‰€ä»¥è¿™ä¸ª `loss` æ˜¯ `3.76`ï¼Œå’Œä¹‹å‰è®¡ç®—çš„ä¸€æ¨¡ä¸€æ ·ï¼Œåªæ˜¯å†™æ³•æ›´é«˜æ•ˆã€ç´§å‡‘ã€‚
> è¿™éƒ¨åˆ†å®Œæˆäº†æˆ‘ä»¬ç¥ç»ç½‘ç»œçš„ **å‰å‘ä¼ æ’­ï¼ˆforward passï¼‰**ï¼ŒåŒ…æ‹¬è¾“å‡ºé¢„æµ‹ â†’ è½¬ä¸ºæ¦‚ç‡ â†’ è®¡ç®—æŸå¤±ã€‚

---

### ğŸ“Œ æ€»ç»“ï¼š

è¿™æ®µä»£ç å®Œæˆçš„äº‹æƒ…ï¼š

1. ç½‘ç»œè¾“å‡º logits â†’ softmax â†’ probsï¼ˆæ¦‚ç‡ï¼‰
2. ä» probs ä¸­å–å‡ºæ¯ä¸ªæ ·æœ¬â€œæ­£ç¡®æ ‡ç­¾â€å¯¹åº”çš„æ¦‚ç‡å€¼
3. å¯¹è¿™äº›æ¦‚ç‡å– logï¼Œå–è´Ÿï¼Œæ±‚å¹³å‡ â†’ å¾—åˆ° loss

è¿™å°±æ˜¯åˆ†ç±»é—®é¢˜ä¸­å¸¸è§çš„ **äº¤å‰ç†µæŸå¤±ï¼ˆCross Entropy Lossï¼‰** çš„æ ¸å¿ƒè®¡ç®—é€»è¾‘ã€‚

---

éœ€è¦æˆ‘å¸®ä½ å†™å‡ºå®Œæ•´çš„è¿™éƒ¨åˆ† PyTorch ä»£ç å—ï¼Ÿæˆ–è€…å¯è§†åŒ–è¿™ä¸ªæŸå¤±æµç¨‹å›¾ä¹Ÿå¯ä»¥ã€‚

# backward and update, in PyTorch

we forwarded the network and the loss now we're ready to do the backward pass so backward pass
we want to first make sure that all the gradients are reset so they're at zero now in pytorch you can set the gradients
to be zero but you can also just set it to none and setting it to none is more efficient and pi torch will interpret
none as like a lack of a gradient and is the same as zeros so this is a way to set to zero the
gradient and now we do lost it backward
before we do lost that backward we need one more thing if you remember from micrograd pytorch actually requires
that we pass in requires grad is true so that when we tell
pythorge that we are interested in calculating gradients for this leaf tensor by default this is false
so let me recalculate with that and then set to none and lost that backward
now something magical happened when lasted backward was run because pytorch just like micrograd when
we did the forward pass here it keeps track of all the operations under the hood it builds a full
computational graph just like the graphs we've produced in micrograd those graphs exist
inside pi torch and so it knows all the dependencies and all the mathematical operations of
everything and when you then calculate the loss we can call a dot backward on it
and that backward then fills in the gradients of all the intermediates
all the way back to w's which are the parameters of our neural net so now we
can do w grad and we see that it has structure there's stuff inside it
and these gradients every single element here so w dot shape is 27 by 27
w grad shape is the same 27 by 27 and every element of w that grad
is telling us the influence of that weight on the loss function
so for example this number all the way here if this element the zero zero element of
w because the gradient is positive is telling us that this has a positive
influence in the loss slightly nudging w slightly taking w 0 0
and adding a small h to it would increase the loss
mildly because this gradient is positive some of these gradients are also negative
so that's telling us about the gradient information and we can use this gradient information to update the weights of
this neural network so let's now do the update it's going to be very similar to what we had in micrograd we need no loop
over all the parameters because we only have one parameter uh tensor and that is w so we simply do w dot data plus equals
uh the we can actually copy this almost exactly negative 0.1 times w dot grad
and that would be the update to the tensor
so that updates the tensor and
because the tensor is updated we would expect that now the loss should decrease so
here if i print loss that item
it was 3.76 right so we've updated the w here so if i
recalculate forward pass loss now should be slightly lower so
3.76 goes to 3.74 and then
we can again set to set grad to none and backward update
and now the parameters changed again so if we recalculate the forward pass we expect a lower loss again 3.72
okay and this is again doing the we're now doing gradient descent
and when we achieve a low loss that will mean that the network is assigning high probabilities to the correctness
characters okay so i rearranged everything and i put it all together from scratch

# putting everything together

so here is where we construct our data set of bigrams you see that we are still iterating only
on the first word emma i'm going to change that in a second i added a number that counts the number of
elements in x's so that we explicitly see that number of examples is five
because currently we're just working with emma and there's five backgrounds there and here i added a loop of exactly what
we had before so we had 10 iterations of grainy descent of forward pass backward pass and an update
and so running these two cells initialization and gradient descent gives us some improvement
on the loss function but now i want to use all the words
and there's not 5 but 228 000 bigrams now however this should require no
modification whatsoever everything should just run because all the code we wrote doesn't care if there's five migrants or 228 000 bigrams and with
everything we should just work so you see that this will just run but now we are optimizing over the
entire training set of all the bigrams and you see now that we are decreasing very slightly so actually we can
probably afford a larger learning rate and probably for even larger learning
rate
even 50 seems to work on this very very simple example right so let me re-initialize and let's run 100
iterations see what happens
okay we seem to be
coming up to some pretty good losses here 2.47 let me run 100 more
what is the number that we expect by the way in the loss we expect to get something around what we had originally
actually so all the way back if you remember in the beginning of this video when we
optimized uh just by counting our loss was roughly 2.47
after we had it smoothing but before smoothing we had roughly 2.45
likelihood sorry loss and so that's actually roughly the vicinity of what we expect to achieve
but before we achieved it by counting and here we are achieving the roughly the same result but with gradient based
optimization so we come to about 2.4 6 2.45 etc
and that makes sense because fundamentally we're not taking any additional information we're still just taking in the previous character and
trying to predict the next one but instead of doing it explicitly by counting and normalizing
we are doing it with gradient-based learning and it just so happens that the explicit approach happens to very well
optimize the loss function without any need for a gradient based optimization because the setup for bigram language
models are is so straightforward that's so simple we can just afford to estimate those probabilities directly and
maintain them in a table but the gradient-based approach is significantly more flexible
so we've actually gained a lot because what we can do now is
we can expand this approach and complexify the neural net so currently we're just taking a single character and
feeding into a neural net and the neural that's extremely simple but we're about to iterate on this substantially we're
going to be taking multiple previous characters and we're going to be feeding feeding them into increasingly more
complex neural nets but fundamentally out the output of the neural net will always just be logics
and those logits will go through the exact same transformation we are going to take them through a soft max
calculate the loss function and the negative log likelihood and do gradient based optimization and so actually
as we complexify the neural nets and work all the way up to transformers none of this will really fundamentally
change none of this will fundamentally change the only thing that will change is the way we do the forward pass where we
take in some previous characters and calculate logits for the next character in the sequence that will become more
complex and uh but we'll use the same machinery to optimize it and um
it's not obvious how we would have extended this bigram approach into the case where there are many more
characters at the input because eventually these tables would get way too large because there's way too many
combinations of what previous characters could be if you only have one previous character
we can just keep everything in a table that counts but if you have the last 10 characters that are input we can't
actually keep everything in the table anymore so this is fundamentally an unscalable approach and the neural network approach is significantly more
scalable and it's something that actually we can improve on over time so that's where we will be digging next i
wanted to point out two more things number one i want you to notice that

# note 1: one-hot encoding really just selects a row of the next Linear layer's weight matrix

this x ink here this is made up of one hot vectors and
then those one hot vectors are multiplied by this w matrix and we think of this as multiple neurons
being forwarded in a fully connected manner but actually what's happening here is that for example
if you have a one hot vector here that has a one at say the fifth dimension
then because of the way the matrix multiplication works multiplying that one-half vector with w
actually ends up plucking out the fifth row of w log logits would become just the fifth
row of w and that's because of the way the matrix multiplication works
um so that's actually what ends up happening so but that's actually exactly what
happened before because remember all the way up here we have a bigram we took the first
character and then that first character indexed into a row of this array here
and that row gave us the probability distribution for the next character so the first character was used as a lookup
into a matrix here to get the probability distribution
well that's actually exactly what's happening here because we're taking the index we're encoding it as one hot and
multiplying it by w so logics literally becomes the
the appropriate row of w and that gets just as before exponentiated to create the counts
and then normalized and becomes probability so this w here is literally
the same as this array here but w remember is the log counts not the
counts so it's more precise to say that w exponentiated w dot x is this array
but this array was filled in by counting and by basically
populating the counts of bi-grams whereas in the gradient-based framework we initialize it randomly and then we
let the loss guide us to arrive at the exact same array
so this array exactly here is basically the array w at the end of
optimization except we arrived at it piece by piece by following the loss
and that's why we also obtain the same loss function at the end and the second note is if i come here

# note 2: model smoothing as regularization loss

remember the smoothing where we added fake counts to our counts in order to
smooth out and make more uniform the distributions of these probabilities and that prevented us from assigning
zero probability to to any one bigram now if i increase the count here
what's happening to the probability as i increase the count probability
becomes more and more uniform right because these counts go only up to
like 900 or whatever so if i'm adding plus a million to every single number here you can see how
the row and its probability then when we divide is just going to become more and more close to exactly even probability
uniform distribution it turns out that the gradient based framework has an equivalent to smoothing
in particular think through these w's here
which we initialized randomly we could also think about initializing w's to be zero
if all the entries of w are zero then you'll see that logits will become
all zero and then exponentiating those logics becomes all one and then the probabilities turned out to
be exactly uniform so basically when w's are all equal to each other or say especially zero
then the probabilities come out completely uniform so trying to incentivize w to be near zero
is basically equivalent to label smoothing and the more you incentivize that in the loss function
the more smooth distribution you're going to achieve so this brings us to something that's called
regularization where we can actually augment the loss function to have a small component that we call a
regularization loss in particular what we're going to do is we can take w and we can for example
square all of its entries and then we can um whoops
sorry about that we can take all the entries of w and we can sum them
and because we're squaring uh there will be no signs anymore um negatives and positives all get squashed
to be positive numbers and then the way this works is you achieve zero loss if w is exactly or
zero but if w has non-zero numbers you accumulate loss and so we can actually take this and we
can add it on here so we can do something like loss plus
w square dot sum or let's actually instead of sum let's take a mean because otherwise the sum
gets too large so mean is like a little bit more manageable
and then we have a regularization loss here say 0.01 times or something like that you can choose
the regularization strength and then we can just optimize this and
now this optimization actually has two components not only is it trying to make all the probabilities work out but in
addition to that there's an additional component that simultaneously tries to make all w's be zero because if w's are
non-zero you feel a loss and so minimizing this the only way to achieve that is for w to be zero
and so you can think of this as adding like a spring force or like a gravity force that that pushes w to be zero so w
wants to be zero and the probabilities want to be uniform but they also simultaneously want to match up your
your probabilities as indicated by the data and so the strength of this regularization is exactly controlling
the amount of counts that you add here adding a lot more counts
here corresponds to increasing this number
because the more you increase it the more this part of the loss function dominates this part and the more these
these weights will un be unable to grow because as they grow they uh accumulate way too much loss
and so if this is strong enough then we are not able to overcome the
force of this loss and we will never and basically everything will be uniform predictions
so i thought that's kind of cool okay and lastly before we wrap up i wanted to show you how you would sample from this neural net model

# sampling from the neural net

and i copy-pasted the sampling code from before where remember that we sampled five
times and all we did we start at zero we grabbed the current ix row of p and that
was our probability row from which we sampled the next index and just accumulated that and
break when zero and running this gave us these
results still have the p in memory so this is fine now
the speed doesn't come from the row of b instead it comes from this neural net
first we take ix and we encode it into a one hot row of x
inc this x inc multiplies rw which really just plucks out the row of
w corresponding to ix really that's what's happening and that gets our logits and then we
normalize those low jets exponentiate to get counts and then normalize to get uh the distribution and
then we can sample from the distribution so if i run this
kind of anticlimactic or climatic depending how you look at it but we get the exact same result
um and that's because this is in the identical model not only does it achieve the same loss
but as i mentioned these are identical models and this w is the log counts of
what we've estimated before but we came to this answer in a very different way and it's got a very different
interpretation but fundamentally this is basically the same model and gives the same samples here and so
that's kind of cool okay so we've actually covered a lot of ground we introduced the bigram character level

# conclusion
language model we saw how we can train the model how we can sample from the model and how we can
evaluate the quality of the model using the negative log likelihood loss and then we actually trained the model
in two completely different ways that actually get the same result and the same model in the first way we just counted up the
frequency of all the bigrams and normalized in a second way we used the
negative log likelihood loss as a guide to optimizing the counts matrix
or the counts array so that the loss is minimized in the in a gradient-based framework and we saw that both of them
give the same result and that's it now the second one of these the
gradient-based framework is much more flexible and right now our neural network is super simple we're taking a
single previous character and we're taking it through a single linear layer to calculate the logits
this is about to complexify so in the follow-up videos we're going to be taking more and more of these characters
and we're going to be feeding them into a neural net but this neural net will still output the exact same thing the neural net will output logits
and these logits will still be normalized in the exact same way and all the loss and everything else and the gradient gradient-based framework
everything stays identical it's just that this neural net will now complexify all the way to transformers
so that's gonna be pretty awesome and i'm looking forward to it for now bye
