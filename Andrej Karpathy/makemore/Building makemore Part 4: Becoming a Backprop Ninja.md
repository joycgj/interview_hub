We take the 2-layer MLP (with BatchNorm) from the previous video and backpropagate through it manually without using PyTorch autograd's loss.backward(): through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table. Along the way, we get a strong intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in micrograd. This helps build competence and intuition around how neural nets are optimized and sets you up to more confidently innovate on and debug modern neural networks.

!!!!!!!!!!!!
I recommend you work through the exercise yourself but work with it in tandem and whenever you are stuck unpause the video and see me give away the answer. This video is not super intended to be simply watched. The exercise is here:
https://colab.research.google.com/dri...
!!!!!!!!!!!!

Links:
- makemore on github: https://github.com/karpathy/makemore
- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-t...
- collab notebook: https://colab.research.google.com/dri...
- my website: https://karpathy.ai
- my twitter:   / karpathy  
- our Discord channel:   / discord  

Supplementary links:
- Yes you should understand backprop:   / yes-you-should-understand-backprop  
- BatchNorm paper: https://arxiv.org/abs/1502.03167
- Besselâ€™s Correction: http://math.oxford.emory.edu/site/mat...
- Bengio et al. 2003 MLP LM https://www.jmlr.org/papers/volume3/b... 

Chapters:
```
00:00:00 intro: why you should care & fun history
00:07:26 starter code
00:13:01 exercise 1: backproping the atomic compute graph
01:05:17 brief digression: besselâ€™s correction in batchnorm
01:26:31 exercise 2: cross entropy loss backward pass
01:36:37 exercise 3: batch norm layer backward pass
01:50:02 exercise 4: putting it all together
01:54:24 outro
```


å½“ç„¶å¯ä»¥ï¼Œä¸‹é¢æ˜¯è¿™æ®µè§†é¢‘è¯´æ˜çš„ **ä¸­æ–‡ç¿»è¯‘ç‰ˆ**ï¼š

---

æˆ‘ä»¬ä¼šç”¨ä¸Šä¸€ä¸ªè§†é¢‘é‡Œè®­ç»ƒå¥½çš„ **2 å±‚ MLPï¼ˆå¸¦ BatchNormï¼‰**ï¼Œè¿™æ¬¡ä¸ç”¨ `PyTorch` çš„è‡ªåŠ¨åå‘ä¼ æ’­ï¼ˆ`loss.backward()`ï¼‰ï¼Œè€Œæ˜¯**æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­**ï¼Œä¸€æ­¥æ­¥é€šè¿‡ï¼š

âœ… äº¤å‰ç†µæŸå¤±ï¼ˆcross entropy lossï¼‰
âœ… ç¬¬äºŒä¸ªçº¿æ€§å±‚ï¼ˆlinear layerï¼‰
âœ… Tanh æ¿€æ´»å‡½æ•°
âœ… BatchNorm
âœ… ç¬¬ä¸€ä¸ªçº¿æ€§å±‚
âœ… å­—ç¬¦ embedding table

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬èƒ½å»ºç«‹ **éå¸¸æ‰å®çš„ç›´è§‰**ï¼Œç†è§£æ¢¯åº¦åˆ°åº•æ˜¯æ€ä¹ˆæ²¿ç€è®¡ç®—å›¾æµåŠ¨çš„ï¼Œé‡ç‚¹æ˜¯ **ä» tensor çº§åˆ«** ç†è§£ï¼ˆä¸æ˜¯å•ç‹¬æ ‡é‡çš„å¾®å°ç½‘ç»œäº† micrograd è¿™ç§ï¼‰ï¼Œè¿™æ ·æœ‰åŠ©äºï¼š

âœ… çœŸæ­£ç†è§£ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•è¢«ä¼˜åŒ–çš„
âœ… æ›´æœ‰ä¿¡å¿ƒ debug æˆ–æ”¹è¿›ç°ä»£ç¥ç»ç½‘ç»œæ¶æ„

---

ğŸš¨ğŸš¨ğŸš¨
**å¼ºçƒˆå»ºè®®ä½ åŠ¨æ‰‹åšè¿™ä¸ªç»ƒä¹ ï¼**
å¯ä»¥å’Œè§†é¢‘ä¸€èµ·å¯¹ç€åšï¼Œå¡ä½çš„æ—¶å€™æš‚åœè§†é¢‘çœ‹è®²è§£ã€‚
è¿™ä¸ªè§†é¢‘ **ä¸æ˜¯ç»™ä½ çº¯â€œçœ‹ç€â€å°±è¡Œçš„å†…å®¹**ã€‚
ç»ƒä¹ åœ°å€ ğŸ‘‰
\[[https://colab.research.google.com/drive/](https://colab.research.google.com/drive/)...]

---

ğŸ”— é“¾æ¥ï¼š

* makemore ä»£ç ä»“åº“ï¼š [https://github.com/karpathy/makemore](https://github.com/karpathy/makemore)
* æœ¬è§†é¢‘ç”¨åˆ°çš„ jupyter notebook: [https://github.com/karpathy/nn-zero-to...](https://github.com/karpathy/nn-zero-to...)
* Colab notebook: [https://colab.research.google.com/dri...](https://colab.research.google.com/dri...)
* æˆ‘çš„åšå®¢ç½‘ç«™: [https://karpathy.ai](https://karpathy.ai)
* æˆ‘çš„æ¨ç‰¹: /karpathy
* æˆ‘ä»¬çš„ Discord é¢‘é“: /discord

---

ğŸ“š è¡¥å……èµ„æ–™ï¼š

* ä¸ºä»€ä¹ˆä½ åº”è¯¥ç†è§£åå‘ä¼ æ’­: /yes-you-should-understand-backprop
* BatchNorm åŸè®ºæ–‡: [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)
* Bessel æ ¡æ­£ï¼ˆBatchNorm é‡Œä¼šæåˆ°ï¼‰: [http://math.oxford.emory.edu/site/mat...](http://math.oxford.emory.edu/site/mat...)
* Bengio ç­‰äºº 2003 å¹´ MLP è¯­è¨€æ¨¡å‹è®ºæ–‡: [https://www.jmlr.org/papers/volume3/b...](https://www.jmlr.org/papers/volume3/b...)

---

ğŸ“º è§†é¢‘ç« èŠ‚ï¼š

```
00:00:00 å¼€åœºï¼šä¸ºä»€ä¹ˆè¦å…³å¿ƒåå‘ä¼ æ’­ + æœ‰è¶£çš„å†å²
00:07:26 èµ·å§‹ä»£ç 
00:13:01 ç»ƒä¹  1ï¼šåå‘ä¼ æ’­ â€œæœ€å°è®¡ç®—å›¾â€ (atomic compute graph)
01:05:17 å°æ’æ›²ï¼šBatchNorm é‡Œçš„ Bessel æ ¡æ­£
01:26:31 ç»ƒä¹  2ï¼šäº¤å‰ç†µæŸå¤±çš„ backward pass
01:36:37 ç»ƒä¹  3ï¼šBatchNorm å±‚çš„ backward pass
01:50:02 ç»ƒä¹  4ï¼šæ•´ä½“ä¸²èµ·æ¥ end-to-end backward
01:54:24 æ€»ç»“
```

---

å¦‚æœä½ è¿˜æƒ³è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ æ•´ç†ä¸€ä¸ªï¼š

âœ… æœ¬èŠ‚è¯¾é‡ç‚¹çŸ¥è¯†åˆ—è¡¨
âœ… å­¦åå‘ä¼ æ’­ï¼ˆbackpropï¼‰æ¨èæ€è€ƒé¡ºåº
âœ… å¸¸è§æ–°æ‰‹è¸©å‘æ€»ç»“

è¦ä¸è¦ï¼Ÿ ğŸš€âœ¨


# intro: why you should care & fun history

hi everyone so today we are once again continuing our implementation of make more now so far we've come up to here
montalia perceptrons and our neural net looked like this and we were implementing this over the last few
lectures now I'm sure everyone is very excited to go into recurring neural networks and all of their variants and how they work
and the diagrams look cool and it's very exciting and interesting and we're going to get a better result but unfortunately
I think we have to remain here for one more lecture and the reason for that is
we've already trained this multilio perceptron right and we are getting pretty good loss and I think we have a pretty decent understanding of the
architecture and how it works but the line of code here that I take an issue with is here lost up backward that is we
are taking a pytorch auto grad and using it to calculate all of our gradients along the way and I would like to remove
the use of lost at backward and I would like us to write our backward pass manually on the level of tensors and I
think that this is a very useful exercise for the following reasons I actually have an entire blog post on
this topic but I'd like to call back propagation a leaky abstraction
and what I mean by that is back propagation does doesn't just make your neural networks just work magically it's
not the case they can just Stack Up arbitrary Lego blocks of differentiable functions and just cross your fingers and back propagate and everything is
great things don't just work automatically it is a leaky abstraction in the sense that you can shoot yourself
in the foot if you do not understanding its internals it will magically not work or not work optimally and you will need
to understand how it works under the hood if you're hoping to debug it and if you are hoping to address it in your neural nut
um so this blog post here from a while ago goes into some of those examples so for example we've already covered them
some of them already for example the flat tails of these functions and how
you do not want to saturate them too much because your gradients will die the case of dead neurons which I've already
covered as well the case of exploding or Vanishing gradients in the case of repair neural
networks which we are about to cover and then also you will often come across
some examples in the wild this is a snippet that I found uh in a random code base on the internet where
they actually have like a very subtle but pretty major bug in their implementation and the bug points at the
fact that the author of this code does not actually understand by propagation so they're trying to do here is they're
trying to clip the loss at a certain maximum value but actually what they're trying to do is they're trying to
collect the gradients to have a maximum value instead of trying to clip the loss at a maximum value and
um indirectly they're basically causing some of the outliers to be actually ignored because when you clip a loss of
an outlier you are setting its gradient to zero and so have a look through this
and read through it but there's basically a bunch of subtle issues that you're going to avoid if you actually know what you're doing and that's why I
don't think it's the case that because pytorch or other Frameworks offer autograd it is okay for us to ignore how
it works now we've actually already covered covered autograd and we wrote micrograd
but micrograd was an autograd engine only on the level of individual scalars so the atoms were single individual
numbers and uh you know I don't think it's enough and I'd like us to basically think about back propagation on level of
tensors as well and so in a summary I think it's a good exercise I think it is
very very valuable you're going to become better at debugging neural networks and making sure that you
understand what you're doing it is going to make everything fully explicit so you're not going to be nervous about what is hidden away from you and
basically in general we're going to emerge stronger and so let's get into it a bit of a fun historical note here is
that today writing your backward pass by hand and manually is not recommended and no one does it except for the purposes
of exercise but about 10 years ago in deep learning this was fairly standard and in fact pervasive so at the time
everyone used to write their own backward pass by hand manually including myself and it's just what you would do
so we used to ride backward pass by hand and now everyone just calls lost that backward uh we've lost something I want
to give you a few examples of this so here's a 2006 paper from Jeff Hinton and
Russell selectinov in science that was influential at the time and this was training some architectures called
restricted bolstery machines and basically it's an auto encoder trained here and this is from roughly 2010 I had
a library for training researchable machines and this was at the time written in Matlab so python was not used
for deep learning pervasively it was all Matlab and Matlab was this a scientific Computing package that everyone would
use so we would write Matlab which is barely a programming language as well but I've had a very convenient tensor
class and was this a Computing environment and you would run here it would all run on a CPU of course but you
would have very nice plots to go with it and a built-in debugger and it was pretty nice now the code in this package
in 2010 that I wrote for fitting research multiple machines to a large
extent is recognizable but I wanted to show you how you would well I'm creating the data in the XY batches I'm
initializing the neural nut so it's got weights and biases just like we're used to and then this is the training Loop
where we actually do the forward pass and then here at this time they didn't even necessarily use back propagation to
train neural networks so this in particular implements contrastive Divergence which estimates a gradient
and then here we take that gradient and use it for a parameter update along the lines that we're used to
um yeah here but you can see that basically people are meddling with these gradients uh
directly and inline and themselves uh it wasn't that common to use an auto grad engine here's one more example from a
paper of mine from 2014 um called the fragmented embeddings and here what I was doing is I was
aligning images and text um and so it's kind of like a clip if you're familiar with it but instead of
working on the level of entire images and entire sentences it was working on the level of individual objects and little pieces of sentences and I was
embedding them and then calculating very much like a clip-like loss and I dig up the code from 2014 of how I implemented
this and it was already in numpy and python and here I'm planting the cost function
and it was standard to implement not just the cost but also the backward pass manually so here I'm calculating the
image embeddings sentence embeddings the loss function I calculate this course this is the loss function and then once
I have the loss function I do the backward pass right here so I backward through the loss function and through
the neural nut and I append regularization so everything was done by hand manually and you were just right
out the backward pass and then you would use a gradient Checker to make sure that your numerical estimate of the gradient
agrees with the one you calculated during back propagation so this was very standard for a long time but today of
course it is standard to use an auto grad engine um but it was definitely useful and I
think people sort of understood how these neural networks work on a very intuitive level and so I think it's a good exercise again and this is where we
want to be okay so just as a reminder from our previous lecture this is The jupyter Notebook that we implemented at
the time and we're going to keep everything the same so we're still going to have a two layer
multiplayer perceptron with a batch normalization layer so the forward pass will be basically identical to this
lecture but here we're going to get rid of lost and backward and instead we're going to write the backward pass manually

å½“ç„¶å¯ä»¥ï¼è¿™æ®µè¯æ˜¯ Karpathy çš„è§†é¢‘å¼€å¤´éƒ¨åˆ†çš„è®²è§£å†…å®¹ï¼Œä¸»è¦è®²äº† **ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦æ‰‹åŠ¨å†™åå‘ä¼ æ’­ï¼ˆbackward passï¼‰**ï¼Œæˆ‘ç”¨ä¸­æ–‡å¸®ä½ è¯¦ç»†è§£é‡Šä¸€ä¸‹ï¼š

---

### 1ï¸âƒ£ ä¸ºä»€ä¹ˆè¦å…³å¿ƒï¼Ÿ+ ä¸€ç‚¹å†å²è¶£é—»

å¤§å®¶å¥½ï¼Œä»Šå¤©æˆ‘ä»¬ç»§ç»­å­¦ä¹  `makemore` è¿™ä¸ªé¡¹ç›®ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ª **å¤šå±‚æ„ŸçŸ¥æœº MLP (Multi-Layer Perceptron)**ï¼Œç½‘ç»œå¤§è‡´é•¿è¿™æ ·ï¼Œå‰å‡ èŠ‚è¯¾æˆ‘ä»¬å·²ç»æŠŠå®ƒè®­ç»ƒèµ·æ¥äº†ï¼Œlossï¼ˆæŸå¤±ï¼‰ä¹Ÿé™å¾—ä¸é”™ï¼Œå¯¹æ¶æ„çš„ç†è§£ä¹Ÿæœ‰äº†ä¸€å®šåŸºç¡€ã€‚

å¾ˆå¤šäººè‚¯å®šå¾ˆæœŸå¾…æ¥ä¸‹æ¥å­¦ä¹  **å¾ªç¯ç¥ç»ç½‘ç»œ RNN**ï¼Œå› ä¸ºå®ƒä»¬çš„ç»“æ„å›¾çœ‹èµ·æ¥å¾ˆé…·ï¼Œæ€§èƒ½æ›´å¥½ï¼Œç»“æœä¹Ÿä¼šæ›´æ£’ï¼Œå¤§å®¶éƒ½å¾ˆå…´å¥‹ã€‚ä½†... æˆ‘ä»¬è¿˜éœ€è¦å…ˆç•™åœ¨è¿™ä¸ªå¤šå±‚æ„ŸçŸ¥æœºè¿™èŠ‚è¯¾é‡Œå†å­¦ä¸€ä¼šå„¿ã€‚

åŸå› æ˜¯ï¼š
è™½ç„¶æˆ‘ä»¬å·²ç»è®­ç»ƒå‡ºäº†è¿™ä¸ª MLPï¼Œç°åœ¨ç”¨çš„ä»£ç æ˜¯ `loss.backward()` â€”â€” ä¹Ÿå°±æ˜¯ç”¨ PyTorch è‡ªåŠ¨å¾®åˆ† autograd åŠŸèƒ½ï¼Œè‡ªåŠ¨è®¡ç®—äº†æ¢¯åº¦ã€‚ä½†æ˜¯æˆ‘å¸Œæœ›æˆ‘ä»¬**è‡ªå·±æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­**ï¼Œä¸ä¾èµ– autogradï¼Œè€Œæ˜¯ç›´æ¥åœ¨ Tensor å±‚é¢ä¸Šè‡ªå·±å†™å‡ºæ¥ã€‚

æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰ä»·å€¼çš„ç»ƒä¹ ï¼ŒåŸå› æœ‰ï¼š

* æˆ‘å†™è¿‡ä¸€ç¯‡åšæ–‡ï¼ŒæŠŠ **åå‘ä¼ æ’­ç§°ä¸ºä¸€ä¸ªâ€œæ³„æ¼çš„æŠ½è±¡â€**ã€‚ä»€ä¹ˆæ„æ€ï¼Ÿæ„æ€æ˜¯å¾ˆå¤šäººä»¥ä¸ºï¼Œåªè¦æœ‰ autogradï¼Œç¥ç»ç½‘ç»œå°±èƒ½â€œè‡ªåŠ¨å·¥ä½œâ€ï¼Œä½†å…¶å®ä¸æ˜¯çš„ã€‚
* ä½ ä¸èƒ½åªæ˜¯å †ç§¯èµ·ä¸€äº›å¯å¾®å‡½æ•°çš„ç§¯æœ¨å—ï¼Œç„¶åé  autograd å°±å¸Œæœ› magically èµ·ä½œç”¨ã€‚å¾ˆå¤šæ—¶å€™ï¼Œå¦‚æœä½ ä¸äº†è§£èƒŒåçš„æœºåˆ¶ï¼Œæ¨¡å‹å°±ä¸ä¼šå¥½ç”¨ï¼Œç”šè‡³è®­ç»ƒä¸äº†ã€‚
* ä¸¾ä¸ªä¾‹å­ï¼š

  * å¦‚æœæ¿€æ´»å‡½æ•°å°¾éƒ¨â€œå¹³å¦â€äº†ï¼Œæ¢¯åº¦ä¼šæ¶ˆå¤±ï¼ˆdead neuronsï¼‰
  * RNN é‡Œä¼šæœ‰æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±é—®é¢˜
  * ä»£ç é‡Œå¦‚æœä½ â€œé”™è¯¯åœ°â€è£å‰ªäº† loss è€Œä¸æ˜¯æ¢¯åº¦ï¼Œä¼šå¯¼è‡´ä¸¥é‡çš„è®­ç»ƒé—®é¢˜

æ‰€ä»¥å¦‚æœä¸äº†è§£åå‘ä¼ æ’­çš„å†…éƒ¨æœºåˆ¶ï¼Œå¯èƒ½ä¼šå†™å‡ºæœ‰ bug çš„ä»£ç ï¼ˆæ¯”å¦‚æŸäº› loss è¢«é”™è¯¯è£å‰ªï¼Œç»“æœæ˜¯ outlier æ ·æœ¬å®Œå…¨è¢«å¿½ç•¥ï¼‰ã€‚

æ€»ç»“ï¼šå³ä½¿æ¡†æ¶é‡Œæœ‰ autogradï¼Œç†è§£å…¶åŸç†ä»ç„¶éå¸¸é‡è¦ã€‚

---

### 2ï¸âƒ£ ä¸ºä»€ä¹ˆè¦ Tensor çº§åˆ«å®ç°ï¼Ÿ

ä¹‹å‰æˆ‘ä»¬å®ç°è¿‡ä¸€ä¸ª **micrograd** å°é¡¹ç›®ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¯ä»¥å¯¹â€œå•ä¸ªæ ‡é‡â€è¿›è¡Œè‡ªåŠ¨å¾®åˆ†çš„ç®€å• autograd engineï¼Œä½†è¿˜ä¸å¤Ÿã€‚æˆ‘ä»¬è¿˜è¦ç»ƒä¹ åœ¨ **Tensor å±‚é¢** ç†è§£å’Œå®ç°åå‘ä¼ æ’­ã€‚

å¥½å¤„æ˜¯ï¼š

* æ›´å¥½ç†è§£ç½‘ç»œå·¥ä½œåŸç†
* è®­ç»ƒå‡ºé—®é¢˜æ—¶èƒ½æ›´å¥½è°ƒè¯•
* ä»£ç é€æ˜ã€ä¸ä¼šå®³æ€•â€œé»‘ç®±â€

---

### 3ï¸âƒ£ å†å²ä¸Šçš„æ‰‹å†™ backward

æœ‰è¶£çš„æ˜¯ï¼šä»Šå¤©å†™åå‘ä¼ æ’­æ‰‹å†™å®ç°åªæ˜¯ä¸€ç§â€œç»ƒä¹ â€æ–¹å¼ï¼Œä½† 10 å¹´å‰ï¼Œåœ¨æ·±åº¦å­¦ä¹ æ—©æœŸï¼Œè¿™æ˜¯æ ‡å‡†åšæ³•ï¼Œå¤§å®¶éƒ½æ˜¯æ‰‹åŠ¨å†™åå‘ä¼ æ’­ä»£ç çš„ï¼ŒåŒ…æ‹¬ Karpathy è‡ªå·±ã€‚

ä»–ä¸¾äº†å‡ ä¸ªä¾‹å­ï¼š

* 2006 å¹´ Hinton çš„ paperï¼Œè®­ç»ƒå—é™ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰ï¼Œå½“æ—¶å°±æ‰‹å†™åå‘ä¼ æ’­
* 2010 å¹´ï¼Œä»–ç”¨ Matlab å†™çš„ä»£ç ï¼Œæ‰‹åŠ¨å¤„ç†æ¢¯åº¦æ›´æ–°
* 2014 å¹´ä»–ç”¨ numpy + Python å†™çš„ paperï¼Œæ‰‹åŠ¨å®ç° forward å’Œ backward passï¼Œè¿˜ç”¨ gradient checker éªŒè¯æ¢¯åº¦æ­£ç¡®æ€§

å½“å¹´å¤§å®¶å¯¹ç½‘ç»œå·¥ä½œåŸç†çš„ç†è§£æ˜¯éå¸¸ç›´è§‚ä¸”æ·±å…¥çš„ã€‚

---

### 4ï¸âƒ£ è¿™èŠ‚è¯¾è¦åšä»€ä¹ˆï¼Ÿ

æ‰€ä»¥ï¼Œä»Šå¤©è¿™èŠ‚è¯¾æˆ‘ä»¬å°±è¦ï¼š

* ä¿æŒä¹‹å‰çš„ **2 å±‚ MLP + BatchNorm**
* ä¸ç”¨ loss.backward()
* **æ‰‹åŠ¨å†™å‡º backward pass**

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ æ€»ç»“å‡ºè§†é¢‘ä¸­é‡ç‚¹æåˆ°çš„â€œä¸ºä»€ä¹ˆåå‘ä¼ æ’­æ˜¯ä¸€ä¸ªæ³„æ¼çš„æŠ½è±¡â€ï¼Œè¿™æ ·ä½ ä»¥åçœ‹ä»£ç æˆ–è€…è°ƒ bug æ—¶æ›´å®¹æ˜“ç†è§£é—®é¢˜æ¥æºï¼Œè¦ä¸è¦ï¼Ÿ ğŸŒŸ


# starter code

now here's the starter code for this lecture we are becoming a back prop ninja in this notebook
and the first few cells here are identical to what we are used to so we are doing some imports loading the data
set and processing the data set none of this changed now here I'm introducing a utility
function that we're going to use later to compare the gradients so in particular we are going to have the gradients that we estimate manually
ourselves and we're going to have gradients that Pi torch calculates and we're going to be checking for
correctness assuming of course that pytorch is correct um then here we have the initialization
that we are quite used to so we have our embedding table for the characters the first layer second layer and the batch
normalization in between and here's where we create all the parameters now you will note that I
changed the initialization a little bit uh to be small numbers so normally you would set the biases to be all zero here
I am setting them to be small random numbers and I'm doing this because if your variables are initialized to
exactly zero sometimes what can happen is that can mask an incorrect implementation of a gradient
um because uh when everything is zero it sort of like simplifies and gives you a much simpler expression of the gradient than you would otherwise get and so by
making it small numbers I'm trying to unmask those potential errors in these calculations
you also notice that I'm using uh B1 in the first layer I'm using a bias despite
batch normalization right afterwards um so this would typically not be what you do because we talked about the fact
that you don't need the bias but I'm doing this here just for fun um because we're going to have a gradient with respect to it and we can
check that we are still calculating it correctly even though this bias is asparious so here I'm calculating a single batch
and then here I'm doing a forward pass now you'll notice that the forward pass is significantly expanded from what we
are used to here the forward pass was just um here now the reason that the forward pass is
longer is for two reasons number one here we just had an F dot cross entropy but here I am bringing back a explicit
implementation of the loss function and number two I've broken up the implementation into
manageable chunks so we have a lot a lot more intermediate tensors along the way
in the forward pass and that's because we are about to go backwards and calculate the gradients in this back
propagation from the bottom to the top so we're going to go upwards and just
like we have for example the lock props tensor in a forward pass in the backward pass we're going to have a d-lock probes
which is going to store the derivative of the loss with respect to the lock props tensor and so we're going to be prepending D to every one of these
tensors and calculating it along the way of this back propagation so as an example we have a b and raw
here we're going to be calculating a DB in raw so here I'm telling pytorch that
we want to retain the grad of all these intermediate values because here in exercise one we're going to calculate
the backward pass so we're going to calculate all these D values D variables and use the CNP function I've introduced
above to check our correctness with respect to what pi torch is telling us this is going to be exercise one uh
where we sort of back propagate through this entire graph now just to give you a very quick preview of what's going to happen in
exercise two and below here we have fully broken up the loss and back
propagated through it manually in all the little Atomic pieces that make it up but here we're going to collapse the
laws into a single cross-entropy call and instead we're going to analytically derive using math and paper and pencil
the gradient of the loss with respect to the logits and instead of back propagating through all of its little
chunks one at a time we're just going to analytically derive what that gradient is and we're going to implement that
which is much more efficient as we'll see in the in a bit then we're going to do the exact same
thing for patch normalization so instead of breaking up bass drum into all the old tiny components we're going to use
uh pen and paper and Mathematics and calculus to derive the gradient through the bachelor Bachelor layer so we're
going to calculate the backward passthrough bathroom layer in a much more efficient expression instead of backward propagating through all of its
little pieces independently so there's going to be exercise three and then in exercise four we're going to
put it all together and this is the full code of training this two layer MLP and we're going to basically insert our
manual back prop and we're going to take out lost it backward and you will basically see that you can get all the
same results using fully your own code and the only thing we're using from
pytorch is the torch.tensor to make the calculations efficient but otherwise you
will understand fully what it means to forward and backward and neural net and train it and I think that'll be awesome so let's get to it
okay so I read all the cells of this notebook all the way up to here and I'm
going to erase this and I'm going to start implementing backward pass starting with d lock problems so we want
to understand what should go here to calculate the gradient of the loss with respect to all the elements of the log
props tensor now I'm going to give away the answer here but I wanted to put a quick note here that I think would be most
pedagogically useful for you is to actually go into the description of this video and find the link to this Jupiter
notebook you can find it both on GitHub but you can also find Google collab with it so you don't have to install anything you'll just go to a website on Google
collab and you can try to implement these derivatives or gradients yourself and then if you are not able to come to
my video and see me do it and so work in Tandem and try it first yourself and then see me give away the answer and I
think that'll be most valuable to you and that's how I recommend you go through this lecture so we are starting here with d-log props

å½“ç„¶å¯ä»¥ï¼è¿™æ®µæ˜¯è§†é¢‘çš„ç¬¬äºŒéƒ¨åˆ†ï¼Œè®²è§£äº†æœ¬èŠ‚è¯¾â€œæ‰‹å†™åå‘ä¼ æ’­â€çš„ **starter code èµ·å§‹ä»£ç ** æ˜¯æ€ä¹ˆç»„ç»‡çš„ï¼Œç›®çš„æ˜¯ä»€ä¹ˆã€‚
æˆ‘æ¥å¸®ä½ ç”¨ä¸­æ–‡è¯¦ç»†è§£é‡Šï¼š

---

### 1ï¸âƒ£ è¿™èŠ‚è¯¾çš„ç›®æ ‡

* è¿™èŠ‚è¯¾çš„ç›®æ ‡æ˜¯ï¼šæˆ‘ä»¬è¦ç»ƒä¹ æˆä¸ºä¸€ä¸ª â€œbackprop ninjaâ€ â€”â€” ä¹Ÿå°±æ˜¯è‡ªå·±æ‰‹åŠ¨å†™å®Œæ•´çš„åå‘ä¼ æ’­ï¼
* Notebook çš„å‰é¢å‡ æ ¼ä»£ç æ˜¯æˆ‘ä»¬ä¹‹å‰ä¹ æƒ¯çš„å†…å®¹ï¼Œå’Œä¹‹å‰ä¸€æ ·ï¼Œåšä¸€äº›ï¼š

  * importï¼ˆå¯¼å…¥åŒ…ï¼‰
  * è¯»å–ã€å¤„ç†æ•°æ®é›†
    è¿™äº›éƒ¨åˆ†æ²¡æœ‰å˜åŒ–ã€‚

---

### 2ï¸âƒ£ æ–°å¢ä¸€ä¸ªå°å·¥å…·å‡½æ•°

* è¿™é‡Œæ–°å¢äº†ä¸€ä¸ª **å·¥å…·å‡½æ•°**ï¼Œç”¨æ¥æ¯”è¾ƒæ¢¯åº¦æ˜¯å¦æ­£ç¡®ã€‚
* å› ä¸ºè¿™æ¬¡æˆ‘ä»¬æ˜¯â€œæ‰‹åŠ¨å†™â€åå‘ä¼ æ’­ï¼Œæ‰€ä»¥ä¼šæœ‰ä¸¤ä¸ªç‰ˆæœ¬çš„æ¢¯åº¦ï¼š

  * æˆ‘ä»¬æ‰‹åŠ¨ç®—å‡ºæ¥çš„
  * PyTorch autograd è‡ªåŠ¨ç®—çš„
* å·¥å…·å‡½æ•°ä¼šå¸®æˆ‘ä»¬å¯¹æ¯”è¿™ä¸¤ä¸ªç»“æœï¼Œæ£€æŸ¥æˆ‘ä»¬å†™çš„æ˜¯å¦æ­£ç¡®ã€‚ï¼ˆé»˜è®¤å‡è®¾ pytorch æ˜¯å¯¹çš„ï¼‰

---

### 3ï¸âƒ£ ç½‘ç»œå‚æ•°åˆå§‹åŒ–

* ç½‘ç»œåˆå§‹åŒ–éƒ¨åˆ†å’Œä¹‹å‰ä¸€æ ·ï¼š

  * embedding è¡¨
  * ç¬¬ä¸€å±‚ã€ç¬¬äºŒå±‚ Linear å±‚
  * ä¸­é—´æœ‰ BatchNorm å±‚
* **å°å˜åŒ–**ï¼š

  * è¿™æ¬¡ bias åç½®é¡¹æˆ‘æ²¡æœ‰å…¨è®¾ä¸º 0ï¼Œè€Œæ˜¯è®¾æˆäº†å¾ˆå°çš„éšæœºæ•°ã€‚
  * ä¸ºä»€ä¹ˆï¼Ÿå¦‚æœ bias æ˜¯ 0ï¼Œæœ‰æ—¶å€™ä¼šâ€œæ©ç›–â€æˆ‘ä»¬å®ç°é‡Œçš„ä¸€äº›å°é”™è¯¯ï¼Œè®©æ¢¯åº¦å…¬å¼å˜å¾—å¤ªç®€å•ã€‚
  * è®¾æˆå°éšæœºæ•°ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬æš´éœ²å‡ºå¯èƒ½å­˜åœ¨çš„ bugã€‚
* æ­¤å¤–ï¼Œå³ä½¿ç”¨äº† BatchNormï¼Œç¬¬ä¸€å±‚æˆ‘è¿˜æ˜¯åŠ äº† biasï¼ˆè™½ç„¶å®é™…ä¸Šä¸éœ€è¦ï¼‰ï¼Œç›®çš„æ˜¯è®©æˆ‘ä»¬æœ‰æ›´å¤šå˜é‡å¯ä»¥æ±‚æ¢¯åº¦ï¼Œæµ‹è¯•æˆ‘ä»¬çš„åå‘ä¼ æ’­æ˜¯å¦æ­£ç¡®ã€‚

---

### 4ï¸âƒ£ Forward Passï¼ˆå‰å‘ä¼ æ’­ï¼‰æ”¹äº†å•¥ï¼Ÿ

* è¿™æ¬¡çš„ forward passï¼ˆå‰å‘ä¼ æ’­ï¼‰ä»£ç æ¯”ä¹‹å‰é•¿å¾ˆå¤šã€‚
* æœ‰ä¸¤ä¸ªåŸå› ï¼š

  1. ä¹‹å‰çš„ loss ç”¨çš„æ˜¯ `F.cross_entropy()`ï¼Œç°åœ¨æ”¹æˆ **æ‰‹åŠ¨å®ç°çš„ loss å‡½æ•°**ã€‚
  2. æŠŠ forward pass åˆ†æˆäº†å¾ˆå¤šå°å—ï¼Œæœ‰å¾ˆå¤šä¸­é—´å˜é‡ï¼ˆtensorsï¼‰ä¸€æ­¥ä¸€æ­¥æ¥ã€‚
     è¿™æ ·åšæ˜¯å› ä¸ºæˆ‘ä»¬é©¬ä¸Šè¦åš backward passï¼Œéœ€è¦è¿™äº›ä¸­é—´å˜é‡çš„å€¼ã€‚

ä¸¾ä¾‹ï¼š

* ä¹‹å‰çš„ forward æ˜¯ï¼š

  ```
  logits = model(x)
  loss = F.cross_entropy(logits, y)
  ```

* ç°åœ¨ä¼šæ‹†æˆï¼š

  ```
  emb = ...
  hidden_pre = ...
  hidden_post = ...
  bn = ...
  logits = ...
  log_probs = ...
  loss = ...
  ```

ä¸ºä»€ä¹ˆï¼Ÿ

* å› ä¸º backward pass çš„æ—¶å€™ï¼Œæˆ‘ä»¬è¦ä¸€æ­¥ä¸€æ­¥è®¡ç®—ï¼š

  ```
  d_log_probs -> d_logits -> d_bn -> d_hidden_post -> d_hidden_pre -> d_emb -> d_embedding_table
  ```

---

### 5ï¸âƒ£ Exercise 1 è¦åšçš„äº‹æƒ…

* åœ¨ Exercise 1 é‡Œï¼Œæˆ‘ä»¬è¦æ‰‹åŠ¨å†™ backward passï¼Œè®¡ç®—è¿™äº› D å˜é‡ï¼ˆå³å¯¹ä¸­é—´å˜é‡æ±‚å¯¼ï¼‰ï¼š

  ```
  d_log_probs
  d_logits
  d_bn
  d_hidden_post
  ...
  ```

* æˆ‘ä»¬ä¼šç”¨ä¸Šé¢é‚£ä¸ªå·¥å…·å‡½æ•°ï¼Œå’Œ pytorch autograd ç®—å‡ºçš„æ¢¯åº¦åšå¯¹æ¯”ï¼Œç¡®è®¤è‡ªå·±å†™çš„æ­£ç¡®ã€‚

---

### 6ï¸âƒ£ é¢„å‘Šåé¢å‡ ä¸ª Exercise

* **Exercise 2**:

  * cross entropy loss æ‰‹åŠ¨æ¨å¯¼å…¬å¼ï¼ˆç”¨æ•°å­¦æ¨å¯¼ï¼Œä¸åˆ†å—é€æ­¥ä¼ æ’­ï¼‰ï¼Œç›´æ¥å®ç° gradientï¼Œæ¯”ä¸€æ­¥æ­¥ä¼ æ’­å¿«å¾ˆå¤šã€‚
* **Exercise 3**:

  * BatchNorm ä¹Ÿç”¨æ•°å­¦æ¨å¯¼å‡ºæ›´é«˜æ•ˆçš„åå‘ä¼ æ’­å…¬å¼ï¼Œä¸ç”¨åˆ†æ­¥éª¤æ…¢æ…¢ä¼ ã€‚
* **Exercise 4**:

  * å…¨éƒ¨ä¸²èµ·æ¥ï¼Œè®­ç»ƒæ•´ä¸ª 2 å±‚ MLPï¼Œ**å®Œå…¨ä¸ç”¨ `loss.backward()`**ï¼Œå…¨ç¨‹æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­ã€‚
  * åˆ°æ—¶å€™ä½ ä¼šå‘ç°ï¼Œè‡ªå·±å†™å‡ºæ¥çš„å’Œ pytorch autograd ç®—çš„æ•ˆæœæ˜¯ä¸€æ ·çš„ã€‚

---

### 7ï¸âƒ£ æ¨èçš„å­¦ä¹ æ–¹å¼

Karpathy å»ºè®®å¤§å®¶è¿™æ ·å­¦æ•ˆæœæœ€å¥½ï¼š

1. æ‰“å¼€è§†é¢‘ä¸‹é¢çš„é“¾æ¥ï¼ŒNotebook æœ‰ GitHub ç‰ˆå’Œ Google Colab ç‰ˆï¼Œç›´æ¥ç”¨ Colab æœ€æ–¹ä¾¿ï¼Œå…å®‰è£…ã€‚
2. è‡ªå·±å…ˆå°è¯•å†™ backward passã€‚
3. å¦‚æœå¡ä½äº†ï¼Œå†å›æ¥å¯¹ç…§è§†é¢‘è®²è§£ç­”æ¡ˆã€‚
4. è¿™æ ·æ•ˆæœä¼šæ¯”åªçœ‹è§†é¢‘è¦å¥½å¾ˆå¤šï¼

---

### 8ï¸âƒ£ è¿™èŠ‚è¯¾çš„èµ·ç‚¹

* æˆ‘ä»¬ç°åœ¨ä»£ç å·²ç»å†™åˆ° forward pass ç»“æŸäº†ã€‚
* æ¥ä¸‹æ¥æˆ‘ä»¬å°±è¦ä» **d\_log\_probs** å¼€å§‹ï¼Œæ‰‹åŠ¨å†™ backward passï¼Œä» output åå‘ä¸€æ­¥æ­¥ä¼ å›å»ã€‚

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½  **ç”»ä¸€ä¸ª forward â†’ backward è®¡ç®—å›¾**ï¼Œæ›´æ–¹ä¾¿ä½ ç†è§£è¦æ€ä¹ˆä¸€æ­¥æ­¥â€œä¼ å›å»â€ã€‚éœ€è¦çš„è¯å‘Šè¯‰æˆ‘ï¼ ğŸŒŸ

```python
# utility function we will use later when comparing manual gradients to PyTorch gradients
def cmp(s, dt, t):
  ex = torch.all(dt == t.grad).item()
  app = torch.allclose(dt, t.grad)
  maxdiff = (dt - t.grad).abs().max().item()
  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')
```

è§£é‡Šä¸€ä¸‹

è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ª **æ¯”è¾ƒå‡½æ•°ï¼ˆ`cmp`ï¼‰**ï¼Œç”¨äº **æ¯”è¾ƒæ‰‹åŠ¨è®¡ç®—çš„æ¢¯åº¦ï¼ˆ`dt`ï¼‰** å’Œ **PyTorch è‡ªåŠ¨è®¡ç®—çš„æ¢¯åº¦ï¼ˆ`t.grad`ï¼‰** æ˜¯å¦ä¸€è‡´ã€‚ä¸»è¦ç”¨äºéªŒè¯æ‰‹åŠ¨å®ç°çš„åå‘ä¼ æ’­æ˜¯å¦æ­£ç¡®ã€‚

ä¸‹é¢æˆ‘é€æ­¥è§£é‡Šæ¯ä¸ªéƒ¨åˆ†ï¼š

---

### 1ï¸âƒ£ **å‡½æ•°å®šä¹‰**

```python
def cmp(s, dt, t):
```

* `s`ï¼šä¸€ä¸ª **å­—ç¬¦ä¸²**ï¼Œç”¨äºæ‰“å°è¾“å‡ºæ—¶æ ‡è¯†å½“å‰æ­£åœ¨æ¯”è¾ƒçš„æ¢¯åº¦åç§°ã€‚
* `dt`ï¼š**æ‰‹åŠ¨è®¡ç®—çš„æ¢¯åº¦**ï¼Œå½¢çŠ¶ä¸ `t.grad` ç›¸åŒï¼Œæ˜¯ä¸€ä¸ª tensorã€‚
* `t`ï¼šä¸€ä¸ª PyTorch çš„ **tensor**ï¼ŒåŒ…å«äº†æ¨¡å‹çš„è®¡ç®—å›¾ï¼Œ`t.grad` æ˜¯ **PyTorch è‡ªåŠ¨è®¡ç®—çš„æ¢¯åº¦**ã€‚

---

### 2ï¸âƒ£ **`ex = torch.all(dt == t.grad).item()`**

* **`dt == t.grad`**ï¼šé€å…ƒç´ æ¯”è¾ƒæ‰‹åŠ¨è®¡ç®—çš„æ¢¯åº¦ `dt` å’Œ PyTorch è‡ªåŠ¨è®¡ç®—çš„æ¢¯åº¦ `t.grad` æ˜¯å¦ç›¸ç­‰ã€‚è¿”å›çš„æ˜¯ä¸€ä¸ªå¸ƒå°”å‹çš„ tensorã€‚
* **`torch.all(dt == t.grad)`**ï¼šæ£€æŸ¥ `dt == t.grad` ä¸­æ‰€æœ‰å…ƒç´ æ˜¯å¦éƒ½ä¸º `True`ï¼Œå³æ‰€æœ‰å…ƒç´ æ˜¯å¦éƒ½ç›¸ç­‰ã€‚å¦‚æœå…¨éƒ¨ç›¸ç­‰ï¼Œè¿”å› `True`ï¼›å¦åˆ™ï¼Œè¿”å› `False`ã€‚
* **`.item()`**ï¼šå°†å¸ƒå°”å€¼è½¬æ¢ä¸º Python æ ‡é‡ï¼ˆ`True` ä¸º 1ï¼Œ`False` ä¸º 0ï¼‰ã€‚

**ä¾‹å­ï¼š**

```text
dt = [1.0, 2.0]
t.grad = [1.0, 2.0]
```

`dt == t.grad` ç»“æœæ˜¯ `[True, True]`ï¼Œæ‰€ä»¥ `torch.all(dt == t.grad)` ç»“æœæ˜¯ `True`ï¼Œ`ex = 1`ã€‚

---

### 3ï¸âƒ£ **`app = torch.allclose(dt, t.grad)`**

* **`torch.allclose(dt, t.grad)`**ï¼šåˆ¤æ–­ `dt` å’Œ `t.grad` ä¸­çš„å…ƒç´ æ˜¯å¦ **è¿‘ä¼¼ç›¸ç­‰**ï¼ˆè€ƒè™‘æµ®åŠ¨è¯¯å·®ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸å¸¸ç”¨çš„æ–¹æ³•æ¥æ¯”è¾ƒæµ®ç‚¹æ•°æ˜¯å¦æ¥è¿‘ï¼Œå› ä¸ºåœ¨è®¡ç®—æœºä¸­ï¼Œæµ®ç‚¹æ•°çš„è®¡ç®—å¯èƒ½å­˜åœ¨ç²¾åº¦è¯¯å·®ã€‚

è¿”å›å€¼æ˜¯ä¸€ä¸ªå¸ƒå°”å€¼ï¼ˆ`True` æˆ– `False`ï¼‰ï¼Œè¡¨ç¤ºä¸¤è€…æ˜¯å¦è¶³å¤Ÿæ¥è¿‘ã€‚

---

### 4ï¸âƒ£ **`maxdiff = (dt - t.grad).abs().max().item()`**

* **`(dt - t.grad)`**ï¼šè®¡ç®— `dt` å’Œ `t.grad` ä¹‹é—´çš„ **å…ƒç´ å·®å¼‚**ã€‚
* **`.abs()`**ï¼šå–å·®å¼‚çš„ **ç»å¯¹å€¼**ã€‚
* **`.max()`**ï¼šæ‰¾åˆ°å·®å¼‚çš„æœ€å¤§å€¼ï¼Œè¡¨ç¤ºæœ€å¤§è¯¯å·®ã€‚
* **`.item()`**ï¼šå°†æœ€å¤§è¯¯å·®çš„ tensor è½¬æ¢ä¸º Python æ ‡é‡ï¼Œä¾¿äºæ‰“å°è¾“å‡ºã€‚

**ä¾‹å­ï¼š**

```text
dt = [1.0, 2.0]
t.grad = [1.1, 2.2]
```

å·®å¼‚æ˜¯ `[0.1, 0.2]`ï¼Œæ‰€ä»¥æœ€å¤§å·®å¼‚æ˜¯ `0.2`ã€‚

---

### 5ï¸âƒ£ **`print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')`**

è¿™è¡Œä»£ç çš„ä½œç”¨æ˜¯ **æ ¼å¼åŒ–è¾“å‡º** æ¯”è¾ƒç»“æœã€‚

* `f'{s:15s}'`ï¼šå°† `s` å­—ç¬¦ä¸²æ‰“å°ä¸º 15 ä¸ªå­—ç¬¦å®½ï¼Œä¸è¶³æ—¶å¡«å……ç©ºæ ¼ã€‚
* `exact`ï¼šæ‰“å° `ex`ï¼Œè¡¨ç¤ºæ¢¯åº¦æ˜¯å¦å®Œå…¨ç›¸ç­‰ã€‚
* `approximate`ï¼šæ‰“å° `app`ï¼Œè¡¨ç¤ºæ¢¯åº¦æ˜¯å¦è¿‘ä¼¼ç›¸ç­‰ã€‚
* `maxdiff`ï¼šæ‰“å°ä¸¤è€…ä¹‹é—´çš„æœ€å¤§å·®å¼‚ã€‚

**ä¾‹å­ï¼š**

```text
s = "gradient_w1"
ex = 1
app = True
maxdiff = 0.1
```

è¾“å‡ºï¼š

```text
gradient_w1    | exact: True   | approximate: True   | maxdiff: 0.1
```

---

### 6ï¸âƒ£ **æ€»ç»“**

è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯ï¼š

* æ¯”è¾ƒ **æ‰‹åŠ¨è®¡ç®—çš„æ¢¯åº¦** å’Œ **PyTorch è‡ªåŠ¨è®¡ç®—çš„æ¢¯åº¦**ã€‚
* é€šè¿‡ä¸‰ç§æ–¹å¼éªŒè¯ï¼š

  1. `exact`ï¼šåˆ¤æ–­ä¸¤è€…æ˜¯å¦å®Œå…¨ç›¸ç­‰ã€‚
  2. `approximate`ï¼šåˆ¤æ–­ä¸¤è€…æ˜¯å¦è¿‘ä¼¼ç›¸ç­‰ï¼ˆè€ƒè™‘æµ®åŠ¨è¯¯å·®ï¼‰ã€‚
  3. `maxdiff`ï¼šè®¡ç®—ä¸¤è€…ä¹‹é—´çš„ **æœ€å¤§å·®å¼‚**ã€‚

å®ƒç”¨äº **è°ƒè¯•å’ŒéªŒè¯æ‰‹åŠ¨å®ç°çš„åå‘ä¼ æ’­** æ˜¯å¦ä¸ PyTorch çš„è‡ªåŠ¨æ¢¯åº¦è®¡ç®—ä¸€è‡´ã€‚

---

å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œæˆ–è€…æƒ³è®©æˆ‘ç»™ä½ ä¸¾ä¸ªå®é™…çš„ä¾‹å­ï¼ŒéªŒè¯ä¸€ä¸‹æ¢¯åº¦çš„æ¯”è¾ƒæ–¹æ³•ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼ ğŸŒŸ


```python
# Note: I am initializating many of these parameters in non-standard ways
# because sometimes initializating with e.g. all zeros could mask an incorrect
# implementation of the backward pass.
```

ç¿»è¯‘ä¸€ä¸‹

æ³¨æ„ï¼šæˆ‘å¯¹å¾ˆå¤šå‚æ•°çš„åˆå§‹åŒ–ç”¨äº†éæ ‡å‡†çš„æ–¹å¼ï¼Œ
å› ä¸ºå¦‚æœåƒé€šå¸¸é‚£æ ·ç”¨å…¨ 0 åˆå§‹åŒ–ï¼Œåå‘ä¼ æ’­é‡Œçš„æŸäº›é”™è¯¯å¯èƒ½ä¼šè¢«æ©ç›–æ‰ï¼Œä¸å®¹æ˜“å‘ç°ã€‚

# exercise 1: backproping the atomic compute graph

now d-lock props will hold the derivative of the loss with respect to
all the elements of log props what is inside log blobs the shape of
this is 32 by 27. so it's not going to surprise you that D log props should
also be an array of size 32 by 27 because we want the derivative loss with respect to all of its elements so the
sizes of those are always going to be equal now how how does log props influence the
loss okay loss is negative block probes indexed with range of N and YB and then
the mean of that now just as a reminder YB is just a basically an array of all
the correct indices um so what we're doing here is we're
taking the lock props array of size 32 by 27. right
and then we are going in every single row and in each row we are plugging plucking out the index eight and then 14
and 15 and so on so we're going down the rows that's the iterator range of N and then we are always plucking out the
index of the column specified by this tensor YB so in the zeroth row we are taking the eighth column in the first
row we're taking the 14th column Etc and so log props at this plugs out
all those log probabilities of the correct next character in a sequence
so that's what that does and the shape of this or the size of it is of course 32 because our batch size is 32.
so these elements get plugged out and then their mean and the negative of that
becomes loss so I always like to work with simpler examples to understand the numerical
form of derivative what's going on here is once we've plucked out these examples
um we're taking the mean and then the negative so the loss basically I can write it this way is the negative
of say a plus b plus c and the mean of those three numbers would be say negative would divide three
that would be how we achieve the mean of three numbers ABC although we actually have 32 numbers here
and so what is basically the loss by say like d a right
well if we simplify this expression mathematically this is negative one over three of A and negative plus negative
one over three of B plus negative 1 over 3 of c and so what
is D loss by D A it's just negative one over three and so you can see that if we don't just have a b and c but we have 32 numbers
then D loss by D um you know every one of those numbers is going to be one over N More generally
because n is the um the size of the batch 32 in this case
so D loss by um D Lock probs is negative 1 over n
in all these places now what about the other elements inside lock problems because lock props is
large array you see that lock problems at shape is 32 by 27. but only 32 of
them participate in the loss calculation so what's the derivative of all the other most of the elements that do not
get plucked out here while their loss intuitively is zero sorry they're gradient intuitively is
zero and that's because they did not participate in the loss so most of these numbers inside this
tensor does not feed into the loss and so if we were to change these numbers then the loss doesn't change which is
the equivalent of way of saying that the derivative of the loss with respect to them is zero they don't impact it
so here's a way to implement this derivative then we start out with torch.zeros of shape 32 by 27 or let's
just say instead of doing this because we don't want to hard code numbers let's do torch.zeros like
block probs so basically this is going to create an array of zeros exactly in the shape of log probs
and then we need to set the derivative of negative 1 over n inside exactly these locations so here's what we can do
the lock props indexed in The Identical way will be just set to negative one over
zero divide n right just like we derived here
so now let me erase all this reasoning and then this is the candidate derivative for D log props let's
uncomment the first line and check that this is correct okay so CMP ran and let's go back to CMP
and you see that what it's doing is it's calculating if the calculated value by us which is DT
is exactly equal to T dot grad as calculated by pi torch and then this is making sure that all the elements are
exactly equal and then converting this to a single Boolean value because we don't want the Boolean tensor we just
want to Boolean value and then here we are making sure that okay if they're not exactly equal maybe
they are approximately equal because of some floating Point issues but they're very very close so here we are using torch.allclose
which has a little bit of a wiggle available because sometimes you can get very very close but if you use a
slightly different calculation because a floating Point arithmetic you can get a slightly different result so this is
checking if you get an approximately close result and then here we are checking the maximum uh basically the value that has
the highest difference and what is the difference in the absolute value difference between those two and so we
are printing whether we have an exact equality an approximate equality and what is the largest difference
and so here we see that we actually have exact equality and so therefore of course we
also have an approximate equality and the maximum difference is exactly zero so basically our d-log props is exactly
equal to what pytors calculated to be lockprops.grad in its back propagation
so so far we're working pretty well okay so let's now continue our back propagation we have that lock props depends on
probes through a log so all the elements of probes are being element wise applied log to
now if we want deep props then then remember your micrograph training we have like a log node it takes in
probs and creates log probs and the props will be the local derivative of
that individual Operation Log times the derivative loss with respect to its output which in this case is D log props
so what is the local derivative of this operation well we are taking log element wise and we can come here and we can see
well from alpha is your friend that d by DX of log of x is just simply one of our X
so therefore in this case X is problems so we have d by DX is one over X which
is one of our probes and then this is the local derivative and then times we want to chain it
so this is chain rule times do log props let me uncomment this and let me run the
cell in place and we see that the derivative of props as we calculated here is exactly correct
and so notice here how this works probes that are props is going to be inverted
and then element was multiplied here so if your probes is very very close to one that means you are your network is
currently predicting the character correctly then this will become one over one and D log probes just gets passed
through but if your probabilities are incorrectly assigned so if the correct character here is getting a very low
probability then 1.0 dividing by it will boost this
and then multiply by the log props so basically what this line is doing intuitively is it's taking the examples
that have a very low probability currently assigned and it's boosting their gradient uh you can you can look
at it that way next up is Count some imp so we want the river of this now let me
just pause here and kind of introduce What's Happening Here in general because I know it's a little bit confusing we
have the locusts that come out of the neural nut here what I'm doing is I'm finding the maximum in each row and I'm
subtracting it for the purposes of numerical stability and we talked about how if you do not do this you run
numerical issues if some of the logits take on two large values because we end up exponentiating them
so this is done just for safety numerically then here's the exponentiation of all the sort of like
logits to create our accounts and then we want to take the some of these counts
and normalize so that all of the probes sum to one now here instead of using one over count
sum I use uh raised to the power of negative one mathematically they are identical I just found that there's
something wrong with the pytorch implementation of the backward pass of division um and it gives like a real result but
that doesn't happen for star star native one that's why I'm using this formula instead but basically all that's
happening here is we got the logits we're going to exponentiate all of them and want to normalize the counts to
create our probabilities it's just that it's happening across multiple lines so now
here we want to First Take the derivative we
want to back propagate into account sumiv and then into counts as well so what should be the count sum M now we
actually have to be careful here because we have to scrutinize and be careful with the shapes so counts that shape and
then count some inverse shape are different so in particular counts as 32 by 27 but
this count sum m is 32 by 1. and so in this multiplication here we also have an
implicit broadcasting that pytorch will do because it needs to take this column tensor of 32 numbers and replicate it
horizontally 27 times to align these two tensors so it can do an element twice multiply
so really what this looks like is the following using a toy example again what we really have here is just props
is counts times conservative so it's a C equals a times B but a is 3 by 3 and b is just three by
one a column tensor and so pytorch internally replicated this elements of B and it did that across all the columns
so for example B1 which is the first element of B would be replicated here across all the columns in this
multiplication and now we're trying to back propagate through this operation to count some m
so when we're calculating this derivative it's important to realize that these two
this looks like a single operation but actually is two operations applied sequentially the first operation that
pytorch did is it took this column tensor and replicated it across all the
um across all the columns basically 27 times so that's the first operation it's a replication and then the second
operation is the multiplication so let's first background through the multiplication
if these two arrays are of the same size and we just have a and b of both of them
three by three then how do we mult how do we back propagate through a multiplication so if we just have
scalars and not tensors then if you have C equals a times B then what is uh the
order of the of C with respect to B well it's just a and so that's the local derivative
so here in our case undoing the multiplication and back propagating through just the multiplication itself
which is element wise is going to be the local derivative which in this case is
simply counts because counts is the a so this is the local derivative and then
times because the chain rule D props so this here is the derivative or the
gradient but with respect to replicated B but we don't have a replicated B we just
have a single B column so how do we now back propagate through the replication
and intuitively this B1 is the same variable and it's just reused multiple times
and so you can look at it as being equivalent to a case we've encountered in micrograd
and so here I'm just pulling out a random graph we used in micrograd we had an example where a single node
has its output feeding into two branches of basically the graph until the last function and we're talking about how the
correct thing to do in the backward pass is we need to sum all the gradients that arrive at any one node so across these
different branches the gradients would sum so if a node is used multiple times the
gradients for all of its uses sum during back propagation so here B1 is used multiple times in all
these columns and therefore the right thing to do here is to sum horizontally across all the rows so I'm
going to sum in Dimension one but we want to retain this Dimension so that the uh so that counts
some end and its gradient are going to be exactly the same shape so we want to make sure that we keep them as true so
we don't lose this dimension and this will make the count sum M be exactly shape 32 by 1.
so revealing this comparison as well and running this we see that we get an exact
match so this derivative is exactly correct and let me erase
this now let's also back propagate into counts which is the other variable here
to create probes so from props to count some INF we just did that let's go into counts as well
so decounts will be the chances are a so DC by d a is just B
so therefore it's count summative um and then times chain rule the props
now councilman is three two by One D probs is 32 by 27.
so um those will broadcast fine and will give us decounts there's no additional
summation required here um there will be a broadcasting that happens in this multiply here because
count some M needs to be replicated again to correctly multiply D props but
that's going to give the correct result so as far as the single operation is concerned so we back probably go from
props to counts but we can't actually check the derivative counts uh I have it
much later on and the reason for that is because count sum in depends on counts and so there's a second Branch here that
we have to finish because can't summon back propagates into account sum and count sum will buy properly into counts
and so counts is a node that is being used twice it's used right here in two props and it goes through this other
Branch through count summative so even though we've calculated the first contribution of it we still have
to calculate the second contribution of it later okay so we're continuing with this Branch we have the derivative for count
sum if now we want the derivative of count sum so D count sum equals what is
the local derivative of this operation so this is basically an element wise one over counts sum
so count sum raised to the power of negative one is the same as one over count sum if we go to all from alpha we
see that x to the negative one D by D by D by DX of it is basically Negative X to
the negative 2. right one negative one over squared is the same as Negative X to the negative two
so D count sum here will be local derivative is going to be negative
um counts sum to the negative two that's the local derivative times chain rule
which is D count sum in so that's D count sum
let's uncomment this and check that I am correct okay so we have perfect equality
and there's no sketchiness going on here with any shapes because these are of the same shape okay next up we want to back
propagate through this line we have that count sum it's count.sum along the rows so I wrote out
um some help here we have to keep in mind that counts of course is 32 by 27 and count sum is 32 by 1. so in this
back propagation we need to take this column of derivatives and transform it
into a array of derivatives two-dimensional array so what is this operation doing we're
taking in some kind of an input like say a three by three Matrix a and we are summing up the rows into a column tells
her B1 b2b3 that is basically this so now we have the derivatives of the
loss with respect to B all the elements of B and now we want to derivative loss with
respect to all these little A's so how do the B's depend on the ace is
basically what we're after what is the local derivative of this operation well we can see here that B1 only
depends on these elements here the derivative of B1 with respect to all of
these elements down here is zero but for these elements here like a11 a12 Etc the
local derivative is one right so DB 1 by D A 1 1 for example is one so it's one
one and one so when we have the derivative of loss with respect to B1
did a local derivative of B1 with respect to these inputs is zeros here but it's one on these guys
so in the chain rule we have the local derivative uh times sort of the derivative of B1 and so
because the local derivative is one on these three elements the look of them are multiplying the derivative of B1
will just be the derivative of B1 and so you can look at it as a router basically
an addition is a router of gradient whatever gradient comes from above it just gets routed equally to all the
elements that participate in that addition so in this case the derivative of B1 will just flow equally to the derivative
of a11 a12 and a13 . so if we have a derivative of all the elements of B and in this column tensor
which is D counts sum that we've calculated just now we basically see that what that amounts
to is all of these are now flowing to all these elements of a and they're doing that horizontally
so basically what we want is we want to take the decount sum of size 30 by 1 and
we just want to replicate it 27 times horizontally to create 32 by 27 array
so there's many ways to implement this operation you could of course just replicate the tensor but I think maybe
one clean one is that the counts is simply torch dot once like
so just an two-dimensional arrays of ones in the shape of counts so 32 by 27
times D counts sum so this way we're letting the broadcasting here basically
implement the replication you can look at it that way but then we have to also be careful
because decounts was already calculated we calculated earlier here and that was
just the first branch and we're now finishing the second Branch so we need to make sure that these gradients add so
plus equals and then here um let's comment out the comparison and
let's make sure crossing fingers that we have the correct result so pytorch agrees with us on this gradient as well
okay hopefully we're getting a hang of this now counts as an element-wise X of Norm legits so now we want D Norm logits
and because it's an element price operation everything is very simple what is the local derivative of e to the X it's famously just e to the x so this is
the local derivative that is the local derivative now we
already calculated it and it's inside counts so we may as well potentially just reuse counts that is the local
derivative times uh D counts
funny as that looks constant decount is derivative on the normal objects and now
let's erase this and let's verify and it looks good
so that's uh normal agents okay so we are here on this line now the
normal objects we have that and we're trying to calculate the logits and deloget Maxes
so back propagating through this line now we have to be careful here because the shapes again are not the same and so
there's an implicit broadcasting Happening Here so normal jits has this shape 32 by 27
logist does as well but logit Maxis is only 32 by one so there's a broadcasting
here in the minus now here I try to sort of write out a
two example again we basically have that this is our C equals a minus B and we see that because of the shape
these are three by three but this one is just a column and so for example every element of C we
have to look at how it uh came to be and every element of C is just the corresponding element of a minus uh
basically that associated b so it's very clear now that the
derivatives of every one of these c's with respect to their inputs are one for
the corresponding a and it's a negative one for the corresponding B
and so therefore um the derivatives on the C will flow
equally to the corresponding Ace and then also to the corresponding base but
then in addition to that the B's are broadcast so we'll have to do the additional sum just like we did before
and of course the derivatives for B's will undergo a minus because the local derivative here is uh negative one
so DC three two by D B3 is negative one so let's just Implement that basically
delugits will be uh exactly copying the derivative on normal objects
so delugits equals the norm logits and I'll do a DOT clone for safety so we're just
making a copy and then we have that the loaded Maxis will be the negative of the non-legits
because of the negative sign and then we have to be careful because logic Maxis is a column
and so just like we saw before because we keep replicating the same elements
across all the columns then in the backward pass because we keep reusing this these are all just
like separate branches of use of that one variable and so therefore we have to do a Sum along one would keep them
equals true so that we don't destroy this dimension and then the logic Maxes will be the
same shape now we have to be careful because this deloaches is not the final deloaches and that's because not only do
we get gradient signal into logits through here but the logic Maxes as a function of logits and that's a second
Branch into logits so this is not yet our final derivative for logits we will come back later for the second branch
for now the logic Maxis is the final derivative so let me uncomment this CMP here and let's just run this
and logit Maxes hit by torch agrees with us so that was the derivative into through
this line now before we move on I want to pause here briefly and I want to look at these
logic Maxes and especially their gradients we've talked previously in the previous lecture that the only reason we're doing
this is for the numerical stability of the softmax that we are implementing here and we talked about how if you take
these logents for any one of these examples so one row of this logit's tensor if you add or subtract any value
equally to all the elements then the value of the probes will be unchanged
you're not changing soft Max the only thing that this is doing is it's making sure that X doesn't overflow and the
reason we're using a Max is because then we are guaranteed that each row of logits the highest number is zero and so
this will be safe and so um basically what that has repercussions
if it is the case that changing logit Maxis does not change the props and therefore there's not change the loss
then the gradient on logic masses should be zero right because saying those two things is the same
so indeed we hope that this is very very small numbers so indeed we hope this is zero now because of floating Point uh
sort of wonkiness um this doesn't come out exactly zero only in some of the rows it does but we
get extremely small values like one e negative nine or ten and so this is telling us that the values of loaded
Maxes are not impacting the loss as they shouldn't it feels kind of weird to back propagate
through this branch honestly because if you have any implementation of like f
dot cross entropy and pytorch and you you block together all these elements and you're not doing the back propagation piece by piece then you
would probably assume that the derivative through here is exactly zero uh so you would be sort of
um skipping this branch because it's only done for numerical stability but
it's interesting to see that even if you break up everything into the full atoms and you still do the computation as
you'd like with respect to numerical stability uh the correct thing happens and you still get a very very small
gradients here um basically reflecting the fact that the values of these do not matter with
respect to the final loss okay so let's now continue back propagation through this line here we've
just calculated the logit Maxis and now we want to back prop into logits through this second branch
now here of course we took legits and we took the max along all the rows and then we looked at its values here now the way
this works is that in pytorch this thing here
the max returns both the values and it Returns the indices at which those values to count the maximum value
now in the forward pass we only used values because that's all we needed but in the backward pass it's extremely
useful to know about where those maximum values occurred and we have the indices
at which they occurred and this will of course helps us to help us do the back propagation because what should the
backward pass be here in this case we have the largest tensor which is 32 by 27 and in each row we find the maximum
value and then that value gets plucked out into loaded Maxis and so intuitively
um basically the derivative flowing through here then should be one
times the look of derivatives is 1 for the appropriate entry that was plucked out
and then times the global derivative of the logic axis so really what we're doing here if you
think through it is we need to take the deloachet Maxis and we need to scatter it to the correct positions in these
logits from where the maximum values came and so um
I came up with one line of code sort of that does that let me just erase a bunch of stuff here so the line of uh you
could do it kind of very similar to what we've done here where we create a zeros and then we populate uh the correct
elements uh so we use the indices here and we would set them to be one but you
can also use one hot so F dot one hot and then I'm taking the
lowest of Max over the First Dimension dot indices and I'm telling uh pytorch
that the dimension of every one of these tensors should be um
27 and so what this is going to do is okay I apologize this is crazy filthy
that I am sure of this it's really just a an array of where the Maxes came from in each row and that
element is one and the all the other elements are zero so it's a one-half Vector in each row and these indices are
now populating a single one in the proper place and then what I'm doing here is I'm
multiplying by the logit Maxis and keep in mind that this is a column
of 32 by 1. and so when I'm doing this times the logic Maxis the logic Maxes
will broadcast and that column will you know get replicated and in an element wise multiply will ensure that each of
these just gets routed to whichever one of these bits is turned on and so that's another way to implement
uh this kind of a this kind of a operation and both of these can be used
I just thought I would show an equivalent way to do it and I'm using plus equals because we already calculated the logits here and this is
not the second branch so let's look at logits and make sure that this
is correct and we see that we have exactly the correct answer
next up we want to continue with logits here that is an outcome of a matrix multiplication and a bias offset in this
linear layer so I've printed out the shapes of all these intermediate tensors we see that
logits is of course 32 by 27 as we've just seen then the H here is 32 by 64. so these
are 64 dimensional hidden States and then this W Matrix projects those 64 dimensional vectors into 27 dimensions
and then there's a 27 dimensional offset which is a one-dimensional vector
now we should note that this plus here actually broadcasts because H multiplied by by W2 will give us a 32 by 27. and so
then this plus B2 is a 27 dimensional lecture here now in the rules of broadcasting what's
going to happen with this bias Vector is that this one-dimensional Vector of 27 will get aligned with a padded dimension
of one on the left and it will basically become a row vector and then it will get replicated vertically 32 times to make
it 32 by 27 and then there's an element-wise multiply now
the question is how do we back propagate from logits to the hidden States the weight Matrix W2 and the bias B2
and you might think that we need to go to some Matrix calculus and then we have
to look up the derivative for a matrix multiplication but actually you don't have to do any of that and you can go
back to First principles and derive this yourself on a piece of paper and specifically what I like to do and I
what I find works well for me is you find a specific small example that you then fully write out and then in the
process of analyzing how that individual small example works you will understand the broader pattern and you'll be able
to generalize and write out the full general formula for what how these derivatives flow in an expression like
this so let's try that out so pardon the low budget production here but what I've done here is I'm writing
it out on a piece of paper really what we are interested in is we have a multiply B plus C and that creates a d
and we have the derivative of the loss with respect to D and we'd like to know what the derivative of the losses with
respect to a b and c now these here are little two-dimensional examples of a matrix
multiplication Two by Two Times a two by two plus a 2 a vector of just two elements
C1 and C2 gives me a two by two now notice here that I have a bias
Vector here called C and the bisex vector is C1 and C2 but as I described
over here that bias Vector will become a row Vector in the broadcasting and will replicate vertically so that's what's
happening here as well C1 C2 is replicated vertically and we see how we have two rows of C1 C2 as a result
so now when I say write it out I just mean like this basically break up this matrix multiplication into the actual
thing that that's going on under the hood so as a result of matrix multiplication and how it works d11 is
the result of a DOT product between the first row of a and the First Column of B so a11 b11 plus a12 B21 plus C1
and so on so forth for all the other elements of D and once you actually write it out it becomes obvious this is
just a bunch of multipliers and um adds and we know from micrograd how
to differentiate multiplies and adds and so this is not scary anymore it's not just matrix multiplication it's just uh
tedious unfortunately but this is completely tractable we have DL by D for all of these and we want DL by uh all
these little other variables so how do we achieve that and how do we actually get the gradients okay so the low budget
production continues here so let's for example derive the derivative of the loss with respect to
a11 we see here that a11 occurs twice in our simple expression right here right here
and influences d11 and D12 . so this is so what is DL by d a one
one well it's DL by d11 times the local
derivative of d11 which in this case is just b11 because that's what's multiplying a11 here
so uh and likewise here the local derivative of D12 with respect to a11 is just B12 and so B12 well in the chain
rule therefore multiply the L by d 1 2. and then because a11 is used both to
produce d11 and D12 we need to add up the contributions of both of those sort
of chains that are running in parallel and that's why we get a plus just adding up those two
um those two contributions and that gives us DL by d a one one we can do the exact same analysis for the other one
for all the other elements of a and when you simply write it out it's just super simple
um taking of gradients on you know expressions like this you find that
this Matrix DL by D A that we're after right if we just arrange all the all of
them in the same shape as a takes so a is just too much Matrix so d l by D A
here will be also just the same shape tester with the derivatives now so deal
by D a11 Etc and we see that actually we can express what we've written out here as a matrix
multiplied and so it just so happens that D all by that all of these formulas that we've
derived here by taking gradients can actually be expressed as a matrix multiplication and in particular we see
that it is the matrix multiplication of these two array matrices so it is the um DL by D and then Matrix
multiplying B but B transpose actually so you see that B21 and b12 have changed
place whereas before we had of course b11 B12 B2 on B22 so you see that this other
Matrix B is transposed and so basically what we have long story short just by doing very simple
reasoning here by breaking up the expression in the case of a very simple example is that DL by d a is which is
this is simply equal to DL by DD Matrix multiplied with B transpose
so that is what we have so far now we also want the derivative with respect to um B and C now
for B I'm not actually doing the full derivation because honestly it's um it's not deep it's just uh annoying it's
exhausting you can actually do this analysis yourself you'll also find that if you take this these expressions and
you differentiate with respect to b instead of a you will find that DL by DB is also a matrix multiplication in this
case you have to take the Matrix a and transpose it and Matrix multiply that with bl by DD
and that's what gives you a deal by DB and then here for the offsets C1 and C2
if you again just differentiate with respect to C1 you will find an expression like this
and C2 an expression like this and basically you'll find the DL by DC
is simply because they're just offsetting these Expressions you just have to take the deal by DD Matrix
of the derivatives of D and you just have to sum across the columns and that
gives you the derivatives for C so long story short the backward Paths of a matrix multiply
is a matrix multiply and instead of just like we had D equals a times B plus C in the scalar case uh
we sort of like arrive at something very very similar but now uh with a matrix multiplication instead of a scalar
multiplication so the derivative of D with respect to a is
DL by DD Matrix multiplied B trespose and here it's a transpose multiply deal
by DD but in both cases it's a matrix multiplication with the derivative and
the other term in the multiplication and for C it is a sum
now I'll tell you a secret I can never remember the formulas that we just arrived for back proper gain information
multiplication and I can back propagate through these Expressions just fine and the reason this works is because the
dimensions have to work out uh so let me give you an example say I want to create DH
then what should the H be number one I have to know that the shape of DH must
be the same as the shape of H and the shape of H is 32 by 64. and then
the other piece of information I know is that DH must be some kind of matrix multiplication of the logits with W2
and delojits is 32 by 27 and W2 is a 64
by 27. there is only a single way to make the shape work out in this case and
it is indeed the correct result in particular here H needs to be 32 by 64. the only way to achieve that is to take
a deluges and Matrix multiply it with you see how
I have to take W2 but I have to transpose it to make the dimensions work out so w to transpose and it's the only way
to make these to Matrix multiply those two pieces to make the shapes work out and that turns out to be the correct
formula so if we come here we want DH which is d a and we see that d a is DL
by DD Matrix multiply B transpose so that's Delo just multiply and B is W2
so W2 transpose which is exactly what we have here so there's no need to remember these formulas similarly now if I want
dw2 well I know that it must be a matrix multiplication of D logits and H
and maybe there's a few transpose like there's one transpose in there as well and I don't know which way it is so I have to come to W2 and I see that its
shape is 64 by 27 and that has to come from some interest multiplication of these two
and so to get a 64 by 27 I need to take um
H I need to transpose it and then I need to Matrix multiply it um so that will become 64 by 32 and then
I need to make sure to multiply with the 32 by 27 and that's going to give me a 64 by 27. so I need to make sure it's
multiplied this with the logist that shape just like that that's the only way to make the dimensions work out and just
use matrix multiplication and if we come here we see that that's exactly what's here so a transpose a for us is H
multiplied with deloaches so that's W2 and then db2
is just the um vertical sum and actually in the same
way there's only one way to make the shapes work out I don't have to remember that it's a vertical Sum along the zero
axis because that's the only way that this makes sense because B2 shape is 27 so in order to get a um delugits
here is 30 by 27 so knowing that it's just sum over deloaches in some
Direction that direction must be zero because I
need to eliminate this Dimension so it's this so this is so let's kind of like the
hacky way let me copy paste and delete that and let me swing over here and this
is our backward pass for the linear layer uh hopefully so now let's uncomment
these three and we're checking that we got all the three derivatives correct and run
and we see that h wh and B2 are all exactly correct so we back propagated
through a linear layer now next up we have derivative for the h
already and we need to back propagate through 10h into h preact so we want to derive DH preact
and here we have to back propagate through a 10 H and we've already done this in micrograd and we remember that
10h has a very simple backward formula now unfortunately if I just put in D by DX of 10 h of X into both from alpha it
lets us down it tells us that it's a hyperbolic secant function squared of X it's not exactly helpful but luckily
Google image search does not let us down and it gives us the simpler formula and in particular if you have that a is
equal to 10 h of Z then d a by DZ by propagating through 10 H is just one
minus a square and take note that 1 minus a square a here is the output of
the 10h not the input to the 10h Z so the D A by DZ is here formulated in
terms of the output of that 10h and here also in Google image search we have the full derivation if you want to
actually take the actual definition of 10h and work through the math to figure out 1 minus standard square of Z
so 1 minus a square is the local derivative in our case that is 1 minus
uh the output of 10 H squared which here is H so it's h squared and that is the local
derivative and then times the chain rule DH so that is going to be our candidate
implementation so if we come here and then uncomment this let's hope for
the best and we have the right answer okay next up we have DH preact and we
want to back propagate into the gain the B and raw and the B and bias so here this is the bathroom parameters
being gained in bias inside the bash term that take the B and raw that is exact unit caution and then scale it and
shift it and these are the parameters of The Bachelor now here we have a
multiplication but it's worth noting that this multiply is very very different from this Matrix multiply here Matrix multiply are DOT products between
rows and Columns of these matrices involved this is an element twice multiply so things are quite a bit
simpler now we do have to be careful with some of the broadcasting happening in this line of code though so you see how BN
gain and B and bias are 1 by 64. but H preact and B and raw are 32 by 64.
so we have to be careful with that and make sure that all the shapes work out fine and that the broadcasting is correctly back propagated
so in particular let's start with the B and Gain so DB and gain should be
and here this is again elementorized multiply and whenever we have a times b equals c we saw that the local
derivative here is just if this is a the local derivative is just the B the other one so the local derivative is just B
and raw and then times chain rule so DH preact
so this is the candidate gradient now again we have to be careful because B
and Gain Is of size 1 by 64. but this here would be 32 by 64.
and so um the correct thing to do in this case of course is that b and gain here is a
rule Vector of 64 numbers it gets replicated vertically in this operation and so therefore the correct thing to do
is to sum because it's being replicated and therefore all the gradients in each
of the rows that are now flowing backwards need to sum up to that same tensor DB and Gain so we have to sum
across all the zero all the examples basically which is the direction in which this
gets replicated and now we have to be also careful because we um being gain is of shape 1 by 64. so in
fact I need to keep them as true otherwise I would just get 64.
now I don't actually really remember why the being gain and the BN bias I made them be 1 by 64.
um but the biases B1 and B2 I just made them be one-dimensional vectors they're
not two-dimensional tensors so I can't recall exactly why I left the gain and
the bias as two-dimensional but it doesn't really matter as long as you are consistent and you're keeping it the same
so in this case we want to keep the dimension so that the tensor shapes work next up we have B and raw so DB and raw
will be BN gain multiplying
dhreact that's our chain rule now what about the
um dimensions of this we have to be careful right so DH preact is 32 by 64. B and
gain is 1 by 64. so it will just get replicated and to create this
multiplication which is the correct thing because in a forward pass it also gets replicated in just the same way
so in fact we don't need the brackets here we're done and the shapes are already correct
and finally for the bias very similar this bias here is very very
similar to the bias we saw when you layer in the linear layer and we see that the gradients from each preact will
simply flow into the biases and add up because these are just these are just offsets
and so basically we want this to be DH preact but it needs to Sum along the right Dimension and in this case similar
to the gain we need to sum across the zeroth dimension the examples because of the way that the bias gets replicated
vertically and we also want to have keep them as true and so this will basically take this and
sum it up and give us a 1 by 64. so this is the candidate implementation
it makes all the shapes work let me bring it up down here and then
let me uncomment these three lines to check that we are getting the correct result for all the three tensors and
indeed we see that all of that got back propagated correctly so now we get to the batch Norm layer we see how here
being gay and being bias are the parameters so the back propagation ends but B and raw now is the output of the
standardization so here what I'm doing of course is I'm breaking up the batch form into manageable pieces so we can back
propagate through each line individually but basically what's happening is BN mean I is the sum
so this is the B and mean I I apologize for the variable naming B and diff is x
minus mu B and div 2 is x minus mu squared here inside the variance
B and VAR is the variance so uh Sigma Square this is B and bar and it's
basically the sum of squares so this is the x minus mu squared and
then the sum now you'll notice one departure here here it is normalized as 1 over m
uh which is number of examples here I'm normalizing as one over n minus 1 instead of N and this is deliberate and
I'll come back to that in a bit when we are at this line it is something called the bezels correction
but this is how I want it in our case bienvar inv then becomes basically
bienvar plus Epsilon Epsilon is one negative five and then it's one over
square root is the same as raising to the power of negative 0.5 right because 0.5 is square
root and then negative makes it one over square root so BM Bar M is a one over this uh
denominator here and then we can see that b and raw which is the X hat here is equal to the BN diff the numerator
multiplied by the um BN bar in
and this line here that creates pre-h pre-act was the last piece we've already back propagated through it
so now what we want to do is we are here and we have B and raw and we have to first back propagate into B and diff and
B and Bar M so now we're here and we have DB and raw and we need to back propagate through
this line now I've written out the shapes here and indeed bien VAR m is a shape 1 by 64. so
there is a broadcasting happening here that we have to be careful with but it is just an element-wise simple
multiplication by now we should be pretty comfortable with that to get DB and diff we know that this is just B and
varm multiplied with DP and raw
and conversely to get dbmring we need to take the end if
and multiply that by DB and raw so this is the candidate but of course
we need to make sure that broadcasting is obeyed so in particular B and VAR M
multiplying with DB and raw will be okay and give us 32 by 64 as we
expect but dbm VAR inv would be taking a 32 by
64. multiplying it by 32 by 64. so this is a 32 by 64. but of course DB this uh B and
VAR in is only 1 by 64. so the second line here needs a sum across the
examples and because there's this Dimension here we need to make sure that keep them is true
so this is the candidate let's erase this and let's swing down
here and implement it and then let's comment out dbm barif and DB and diff
now we'll actually notice that DB and diff by the way is going to be incorrect
so when I run this BMR m is correct B and diff is not
correct and this is actually expected because we're not done with b and diff
so in particular when we slide here we see here that b and raw as a function of B and diff but actually B and far of is
a function of B of R which is a function of B and df2 which is a function of B and diff so it comes here so bdn diff
um these variable names are crazy I'm sorry it branches out into two branches and we've only done one branch of it we
have to continue our back propagation and eventually come back to B and diff and then we'll be able to do a plus equals and get the actual card gradient
for now it is good to verify that CMP also works it doesn't just lie to us and tell us that everything is always
correct it can in fact detect when your gradient is not correct so it's that's
good to see as well okay so now we have the derivative here and we're trying to back propagate through this line
and because we're raising to a power of negative 0.5 I brought up the power rule and we see that basically we have that
the BM bar will now be we bring down the exponent so negative 0.5 times
uh X which is this and now raised to the power of negative
0.5 minus 1 which is negative 1.5 now we would have to also apply a small
chain rule here in our head because we need to take further the derivative of B
and VAR with respect to this expression here inside the bracket but because this is an elementalized operation and
everything is fairly simple that's just one and so there's nothing to do there so this is the local derivative and then
times the global derivative to create the chain rule this is just times the BM bar have
so this is our candidate let me bring this down and uncommon to the check
and we see that we have the correct result now before we propagate through the next line I want to briefly talk about the

è¿™æ®µä»£ç è®²è§£äº† **åå‘ä¼ æ’­ï¼ˆbackpropagationï¼‰** åœ¨ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æ¨¡å‹ä¸­çš„è¯¦ç»†å®ç°ã€‚ä¸‹é¢æˆ‘å°†é€æ­¥è§£é‡Šæ¯ä¸ªéƒ¨åˆ†çš„æ“ä½œï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åå‘ä¼ æ’­è¿‡ç¨‹çš„ç»†èŠ‚ã€‚

---

### 1ï¸âƒ£ **ç›®æ ‡ï¼š**

æˆ‘ä»¬è¦é€šè¿‡åå‘ä¼ æ’­è®¡ç®—å„ä¸ªå±‚çš„æ¢¯åº¦ï¼Œæœ€ç»ˆæ›´æ–°æ¨¡å‹çš„å‚æ•°ã€‚åœ¨è¿™é‡Œï¼Œé‡ç‚¹æ˜¯ **log-probs**ï¼ˆlog æ¦‚ç‡ï¼‰ç›¸å…³çš„æ¢¯åº¦è®¡ç®—å’Œä¸å…¶ä»–å¼ é‡ï¼ˆå¦‚ `probs`, `counts` ç­‰ï¼‰çš„åå‘ä¼ æ’­ã€‚

---

### 2ï¸âƒ£ **log-probs åˆ° loss çš„åå‘ä¼ æ’­**

é¦–å…ˆï¼Œè®¡ç®— **log-probs**ï¼ˆlog æ¦‚ç‡ï¼‰å¯¹ **loss**ï¼ˆæŸå¤±ï¼‰çš„å½±å“ï¼š

```python
# loss æ˜¯é€šè¿‡ log-probs è®¡ç®—çš„
logprobs[range(n), Yb] = -logprobs  # å–å‡ºçœŸå®æ ‡ç­¾å¯¹åº”çš„ log æ¦‚ç‡
```

* æŸå¤±æ˜¯ `logprobs` å¯¹çœŸå®æ ‡ç­¾çš„ log æ¦‚ç‡çš„å¹³å‡å€¼ã€‚
* å¯¹äºæŸå¤±å‡½æ•°çš„åå‘ä¼ æ’­ï¼Œé¦–å…ˆéœ€è¦è®¡ç®— `d_logprobs`ï¼Œè¿™å®é™…ä¸Šæ˜¯ **æŸå¤±å¯¹ log-probs çš„æ¢¯åº¦**ï¼Œå®ƒæ˜¯å‡åŒ€åˆ†å¸ƒçš„ï¼Œå› æ­¤æ¯ä¸ªä½ç½®çš„æ¢¯åº¦æ˜¯ `-1 / n`ã€‚

```python
d_logprobs = torch.zeros_like(logprobs)
d_logprobs[range(n), Yb] = -1.0 / n
```

* **æ¢¯åº¦çš„å¤§å°** æ˜¯ `-1/n`ï¼Œåªæœ‰é‚£äº›å®é™…å‚ä¸è®¡ç®—çš„ `logprobs` è¢«èµ‹å€¼ä¸ºéé›¶å€¼ï¼Œå…¶å®ƒæœªå‚ä¸çš„å…ƒç´ çš„æ¢¯åº¦ä¸º 0ã€‚

---

### 3ï¸âƒ£ **ä» log-probs åˆ° probs çš„åå‘ä¼ æ’­**

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¦è®¡ç®—ä» **log-probs** åˆ° **probs** çš„æ¢¯åº¦ï¼Œ`probs` æ˜¯é€šè¿‡å¯¹ `log-probs` è¿›è¡ŒæŒ‡æ•°è¿ç®—å¾—åˆ°çš„ã€‚

```python
probs = torch.exp(logits - logit_maxes) / counts_sum_inv
```

* è®¡ç®—åå‘ä¼ æ’­æ—¶ï¼Œlogit çš„æ¢¯åº¦æ˜¯é€šè¿‡å¯¹ **log** çš„å±€éƒ¨å¯¼æ•°ï¼ˆ`1 / probs`ï¼‰æ¥å¾—åˆ°çš„ï¼Œå› æ­¤éœ€è¦æŒ‰é“¾å¼æ³•åˆ™ä¼ é€’æ¢¯åº¦ï¼š

```python
d_probs = d_logprobs / probs
```

---

### 4ï¸âƒ£ **ä» probs åˆ° counts å’Œ counts\_sum çš„åå‘ä¼ æ’­**

æ¥ç€ï¼Œæˆ‘ä»¬è®¡ç®—ä» **probs** åˆ° **counts** å’Œ **counts\_sum** çš„æ¢¯åº¦ã€‚

#### 4.1 ä» **counts** åˆ° **counts\_sum\_inv**ï¼š

```python
counts_sum_inv = 1.0 / counts_sum
```

* `counts_sum` æ˜¯æ‰€æœ‰ `counts` çš„å’Œï¼Œåå‘ä¼ æ’­æ—¶ä½¿ç”¨ **é“¾å¼æ³•åˆ™** è®¡ç®—æ¢¯åº¦ã€‚

#### 4.2 è®¡ç®— `counts` å’Œ `counts_sum_inv` çš„æ¢¯åº¦ï¼š

```python
d_counts_sum_inv = -counts_sum_inv ** 2 * d_counts_sum
```

* é€šè¿‡å¯¹ `counts_sum_inv` è¿›è¡Œåå‘ä¼ æ’­ï¼Œå¾—åˆ° `counts_sum` å’Œ `counts_sum_inv` çš„æ¢¯åº¦ã€‚

---

### 5ï¸âƒ£ **ä» counts\_sum åˆ° logits çš„åå‘ä¼ æ’­**

`counts_sum` æ˜¯ä» `logits`ï¼ˆå³è¾“å‡ºå±‚çš„ç»“æœï¼‰é€šè¿‡ `softmax` è®¡ç®—å¾—æ¥çš„ï¼Œå› æ­¤éœ€è¦å¯¹ `logits` è¿›è¡Œåå‘ä¼ æ’­ã€‚

#### 5.1 ä» **logits** åˆ° **norm\_logits**ï¼ˆå½’ä¸€åŒ–åçš„ logitsï¼‰ï¼š

```python
norm_logits = logits - logit_maxes
```

* é€šè¿‡å‡å»æœ€å¤§å€¼ï¼ˆ`logit_maxes`ï¼‰æ¥é¿å…æ•°å€¼ç¨³å®šæ€§é—®é¢˜ã€‚åå‘ä¼ æ’­æ—¶çš„æ¢¯åº¦è®¡ç®—å¯ä»¥ç›´æ¥ä½¿ç”¨ **å¹¿æ’­**ã€‚

#### 5.2 ä» **logit\_maxes** åˆ° **logits**ï¼š

* åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œç”±äº `logit_maxes` çš„æ¢¯åº¦æ˜¯ `0`ï¼Œå®ƒä¸ä¼šå½±å“ **logits**ã€‚

---

### 6ï¸âƒ£ **ä» logits åˆ° W2 å’Œ b2 çš„åå‘ä¼ æ’­**

åå‘ä¼ æ’­æ—¶ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®¡ç®—ä» **logits** åˆ°æƒé‡ **W2** å’Œåç½® **b2** çš„æ¢¯åº¦ã€‚

* **W2** å’Œ **b2** çš„æ¢¯åº¦é€šè¿‡çŸ©é˜µä¹˜æ³•å’Œ **é“¾å¼æ³•åˆ™** è®¡ç®—ï¼š

```python
dW2 = H.T @ d_logits
db2 = d_logits.sum(0)
```

* `dW2` æ˜¯é€šè¿‡ `H` çš„è½¬ç½®ä¸ `d_logits` çš„ä¹˜ç§¯å¾—åˆ°çš„ã€‚
* `db2` æ˜¯ `d_logits` åœ¨ç¬¬ä¸€ä¸ªç»´åº¦ä¸Šæ±‚å’Œï¼Œå¾—åˆ°åç½®é¡¹çš„æ¢¯åº¦ã€‚

---

### 7ï¸âƒ£ **ä» H åˆ° dH çš„åå‘ä¼ æ’­**

* è®¡ç®—éšè—å±‚çš„æ¢¯åº¦æ—¶ï¼Œä½¿ç”¨ **tanh** å‡½æ•°çš„åå‘ä¼ æ’­å…¬å¼ `1 - H^2`ï¼š

```python
dH = d_logits @ W2.T * (1 - H**2)
```

---

### 8ï¸âƒ£ **æ‰¹å½’ä¸€åŒ–å±‚çš„åå‘ä¼ æ’­ï¼ˆBatchNormï¼‰**

æ‰¹å½’ä¸€åŒ–å±‚æ¶‰åŠåˆ° **å‡å€¼ï¼ˆmeanï¼‰** å’Œ **æ–¹å·®ï¼ˆvarianceï¼‰**ï¼Œä»¥åŠä¸¤ä¸ªå‚æ•° **gamma** å’Œ **beta**ï¼š

```python
d_gamma = dH * bnraw
d_beta = dH.sum(0)
```

* `d_gamma` å’Œ `d_beta` æ˜¯æ‰¹å½’ä¸€åŒ–å±‚çš„ **gamma** å’Œ **beta** çš„æ¢¯åº¦ã€‚

---

### 9ï¸âƒ£ **Bessel æ ¡æ­£ï¼ˆBesselâ€™s correctionï¼‰**

* åœ¨æ‰¹å½’ä¸€åŒ–ä¸­ï¼Œä½¿ç”¨ **`n-1`** è€Œä¸æ˜¯ **`n`** æ¥è®¡ç®—æ–¹å·®ï¼Œè¿™æ˜¯ **Bessel æ ¡æ­£**ï¼Œæœ‰åŠ©äºåœ¨å°æ ·æœ¬æƒ…å†µä¸‹å¾—åˆ°æ›´å‡†ç¡®çš„æ–¹å·®ä¼°è®¡ã€‚
* è¿™å¯¹äºå°æ‰¹é‡è®­ç»ƒç‰¹åˆ«é‡è¦ï¼Œå› ä¸ºä½¿ç”¨ `n-1` å¯ä»¥é˜²æ­¢æ–¹å·®çš„åå·®ä¼°è®¡ã€‚

---

### 10ï¸âƒ£ **æ€»ç»“**

* é€šè¿‡ä¸€ç³»åˆ—çš„æ¢¯åº¦è®¡ç®—ï¼Œæˆ‘ä»¬æ‰‹åŠ¨å®ç°äº†åå‘ä¼ æ’­çš„æ¯ä¸€æ­¥ï¼Œè¦†ç›–äº† **log-probs**ã€**probs**ã€**counts**ã€**logits** ç­‰å¤šä¸ªè®¡ç®—å›¾èŠ‚ç‚¹çš„æ¢¯åº¦ã€‚
* è¿™ä¸ªè¿‡ç¨‹ä¸ä»…å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£æ¯ä¸ªå±‚æ¬¡çš„æ¢¯åº¦å¦‚ä½•ä¼ é€’ï¼Œè¿˜ç¡®ä¿äº†æ‰‹å†™å®ç°ä¸ PyTorch autograd çš„ä¸€è‡´æ€§ã€‚

å¦‚æœä½ æœ‰å…·ä½“çš„ç–‘é—®æˆ–è€…éœ€è¦æ›´è¿›ä¸€æ­¥çš„å¸®åŠ©ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼

```python
emb = C[Xb] # embed the characters into vectors
embcat = emb.view(emb.shape[0], -1) # concatenate the vectors
```

è§£é‡Šä¸€ä¸‹

å½“ç„¶å¯ä»¥ï¼è¿™ä¸¤è¡Œä»£ç æ˜¯ **å‰å‘ä¼ æ’­ï¼ˆforward passï¼‰** çš„ç¬¬ä¸€æ­¥ï¼Œä½œç”¨æ˜¯æŠŠè¾“å…¥çš„å­—ç¬¦ç¼–ç æˆå‘é‡ï¼Œæ‹¼æ¥èµ·æ¥ï¼Œä½œä¸ºè¾“å…¥é€å…¥ç¥ç»ç½‘ç»œã€‚

æˆ‘ä»¬ä¸€è¡Œä¸€è¡Œè§£é‡Šï¼š

---

### 1ï¸âƒ£ `emb = C[Xb]`

* `C` æ˜¯ **embedding è¡¨**ï¼Œå½¢çŠ¶å¤§æ¦‚æ˜¯ `(vocab_size, embedding_dim)`ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªâ€œæŸ¥è¡¨â€çŸ©é˜µã€‚

  * å‡è®¾ï¼šæ€»å…±æœ‰ 27 ä¸ªå­—ç¬¦ï¼ˆvocab\_size = 27ï¼‰ï¼Œembedding\_dim = 10ã€‚
  * æ‰€ä»¥ `C.shape == (27, 10)`ï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªå­—ç¬¦çš„å‘é‡è¡¨ç¤ºã€‚

* `Xb` æ˜¯è¿™ä¸ª batch çš„è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶æ˜¯ `(batch_size, context_length)`ã€‚

  * å‡è®¾ batch\_size = 32ï¼Œcontext\_length = 3ï¼ˆä¸Šä¸‹æ–‡æœ‰ 3 ä¸ªå­—ç¬¦ï¼‰ã€‚
  * æ‰€ä»¥ `Xb.shape == (32, 3)`ï¼Œé‡Œé¢å­˜çš„éƒ½æ˜¯å­—ç¬¦çš„ç¼–å·ï¼ˆ0 åˆ° 26ï¼‰ã€‚

#### é‚£ `C[Xb]` æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

ğŸ‘‰ ç”¨ `Xb` é‡Œçš„ç¼–å·ï¼Œ**æŸ¥è¡¨**å–å‡ºå¯¹åº”çš„ embedding å‘é‡ã€‚

ç»“æœ `emb.shape == (32, 3, 10)`ï¼š

* 32 ä¸ªæ ·æœ¬
* æ¯ä¸ªæ ·æœ¬ 3 ä¸ªå­—ç¬¦
* æ¯ä¸ªå­—ç¬¦ 10 ç»´å‘é‡

---

### 2ï¸âƒ£ `embcat = emb.view(emb.shape[0], -1)`

* `.view()` æ˜¯ reshape çš„æ“ä½œï¼ŒæŠŠ Tensor æ”¹æˆæ–°çš„å½¢çŠ¶ã€‚

```python
embcat = emb.view(32, 3 * 10) = emb.view(32, 30)
```

ä¹Ÿå°±æ˜¯è¯´ï¼ŒæŠŠ `(32, 3, 10)` æ”¹æˆ `(32, 30)`ï¼š

ğŸ‘‰ **æŠŠ 3 ä¸ªå­—ç¬¦çš„å‘é‡ï¼Œæ‹¼æ¥æˆä¸€ä¸ª 30 ç»´çš„å‘é‡**ï¼Œä½œä¸ºåé¢ MLP è¾“å…¥å±‚çš„ç‰¹å¾ã€‚

---

### 3ï¸âƒ£ å°ç»“

ä¸€å¥è¯æ€»ç»“è¿™ä¸¤è¡Œä»£ç ä½œç”¨ï¼š

ğŸ‘‰ **æŠŠ 32 ä¸ªæ ·æœ¬çš„å­—ç¬¦ç´¢å¼•ï¼ŒæŸ¥ embedding è¡¨è½¬æˆå‘é‡ï¼Œæ‹¼æ¥åï¼Œä½œä¸ºè¾“å…¥é€å…¥ç¥ç»ç½‘ç»œ**ã€‚

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘å¯ä»¥ç”»ä¸€å¼ å°å›¾ï¼Œå¸®ä½ å½¢è±¡ç†è§£â€œXb â†’ C\[Xb] â†’ emb â†’ embcatâ€çš„æ•´ä¸ªè¿‡ç¨‹ï¼Œè¦ä¸è¦ï¼Ÿ ğŸŒŸ


C[Xb]æ˜¯å¦‚ä½•æŸ¥è¡¨çš„ï¼Œç»™ä¸ªä¾‹å­

å½“ç„¶å¯ä»¥ï¼æˆ‘æ¥ä¸¾ä¸ªç®€å•çš„ä¾‹å­ï¼Œå¸®ä½ çœ‹æ‡‚ `C[Xb]` æ˜¯æ€ä¹ˆâ€œæŸ¥è¡¨â€çš„ã€‚

---

### å‡è®¾ï¼š

* `C` æ˜¯ä¸€ä¸ª embedding è¡¨ï¼ˆçŸ©é˜µï¼‰ï¼Œshape æ˜¯ `(vocab_size, embedding_dim)`ã€‚
  å‡è®¾ï¼š

```text
vocab_size = 5  # ä¸€å…± 5 ä¸ªå­—ç¬¦ï¼Œç¼–å· 0-4
embedding_dim = 3  # æ¯ä¸ªå­—ç¬¦ embedding æ˜¯ 3 ç»´å‘é‡
```

é‚£ C é•¿è¿™æ ·ï¼š

```python
C = torch.tensor([
  [0.1, 0.2, 0.3],  # ç¬¬ 0 ä¸ªå­—ç¬¦çš„ embedding
  [0.4, 0.5, 0.6],  # ç¬¬ 1 ä¸ªå­—ç¬¦çš„ embedding
  [0.7, 0.8, 0.9],  # ç¬¬ 2 ä¸ªå­—ç¬¦çš„ embedding
  [1.0, 1.1, 1.2],  # ç¬¬ 3 ä¸ªå­—ç¬¦çš„ embedding
  [1.3, 1.4, 1.5],  # ç¬¬ 4 ä¸ªå­—ç¬¦çš„ embedding
])
```

---

### è¾“å…¥ batch: Xb

å‡è®¾è¿™ä¸ª batch é‡Œï¼Œæœ‰ 2 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰ 3 ä¸ªå­—ç¬¦ï¼ˆcontext\_length = 3ï¼‰ï¼š

```python
Xb = torch.tensor([
  [2, 3, 0],  # ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼šå­—ç¬¦ 2ï¼Œ3ï¼Œ0
  [1, 4, 2],  # ç¬¬äºŒä¸ªæ ·æœ¬ï¼šå­—ç¬¦ 1ï¼Œ4ï¼Œ2
])
```

---

### `C[Xb]` æ€ä¹ˆæŸ¥è¡¨ï¼Ÿ

ä½ å¯ä»¥æŠŠå®ƒç†è§£æˆï¼š
**Xb é‡Œæ¯ä¸ªæ•°å­—ï¼Œå°±æ˜¯è¦æŸ¥ C è¿™ä¸ªè¡¨çš„â€œè¡Œå·â€**ï¼Œå–å‡ºå¯¹åº”çš„ embeddingã€‚

æ‰€ä»¥ï¼š

```text
C[Xb] = [
  [C[2], C[3], C[0]],
  [C[1], C[4], C[2]],
]
```

å±•å¼€åï¼ŒC\[Xb] ç»“æœçš„ shape æ˜¯ `(2, 3, 3)`ï¼š

```python
tensor([
  [ [0.7, 0.8, 0.9],  # C[2]
    [1.0, 1.1, 1.2],  # C[3]
    [0.1, 0.2, 0.3] ],# C[0]

  [ [0.4, 0.5, 0.6],  # C[1]
    [1.3, 1.4, 1.5],  # C[4]
    [0.7, 0.8, 0.9] ] # C[2]
])
```

---

### å°ç»“ï¼š

```python
C[Xb] å…¶å®å°±æ˜¯ç”¨ Xb é‡Œçš„æ•°å­—å»â€œæŸ¥è¡¨â€ï¼Œ  
æ¯ä¸ªæ•°å­—æŸ¥ C é‡Œçš„å¯¹åº”è¡Œï¼Œå¾—åˆ°å¯¹åº”çš„ embedding å‘é‡ã€‚
```

---

å¦‚æœä½ è¿˜æƒ³ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ ç”»ä¸€ä¸ªè¡¨æ ¼ï¼Œæˆ–è€…å†™ä¸€æ®µå®Œæ•´ä¾‹å­ä»£ç è·‘ç»™ä½ çœ‹ï¼ˆä½ å¯ä»¥å¤åˆ¶åˆ° notebook é‡Œè¯•ï¼‰ï¼Œè¦ä¸è¦ï¼Ÿ ğŸŒŸ


å½“ç„¶å¯ä»¥ï¼æˆ‘ä»¬æ¥çœ‹è¿™è¡Œä»£ç ï¼š

```python
hprebn = embcat @ W1 + b1  # hidden layer pre-activation
```

---

### 1ï¸âƒ£ æ¯ä¸ªå˜é‡çš„å«ä¹‰ï¼š

* `embcat` ï¼šæ˜¯å‰é¢é‚£ä¸€æ­¥æ‹¼æ¥å¥½çš„ embedding å‘é‡ï¼Œshape æ˜¯ `(batch_size, input_dim)`ï¼Œæ¯”å¦‚ `(32, 30)`ã€‚
  ï¼ˆæ¯”å¦‚ 3 ä¸ªå­—ç¬¦ï¼Œæ¯ä¸ªå­—ç¬¦ 10 ç»´ï¼Œæ‹¼èµ·æ¥æ˜¯ 30 ç»´ï¼‰

* `W1` ï¼šæ˜¯ç¬¬ 1 å±‚çº¿æ€§å±‚ï¼ˆå…¨è¿æ¥å±‚ã€Linear Layer 1ï¼‰çš„ **æƒé‡çŸ©é˜µ**ï¼Œshape æ˜¯ `(input_dim, hidden_dim)`ï¼Œæ¯”å¦‚ `(30, 64)`ã€‚
  ï¼ˆæŠŠ 30 ç»´è¾“å…¥æ˜ å°„åˆ° 64 ç»´éšè—å±‚ï¼‰

* `b1` ï¼šæ˜¯ç¬¬ 1 å±‚çº¿æ€§å±‚çš„ **åç½®å‘é‡**ï¼Œshape æ˜¯ `(hidden_dim,)`ï¼Œæ¯”å¦‚ `(64,)`ã€‚

---

### 2ï¸âƒ£ è¿™è¡Œä»£ç åœ¨å¹²ä»€ä¹ˆï¼Ÿ

```python
hprebn = embcat @ W1 + b1
```

ğŸ‘‰ å°±æ˜¯åœ¨åšä¸€ä¸ª **çº¿æ€§å˜æ¢**ï¼š

```text
hprebn = embcat Â· W1 + b1
```

* `@` ç¬¦å·æ˜¯ PyTorch é‡Œçš„çŸ©é˜µä¹˜æ³•ï¼Œç›¸å½“äº `torch.matmul()`ã€‚
* æ¯ä¸€ä¸ª batch é‡Œçš„æ ·æœ¬ï¼ˆ1 è¡Œï¼‰é€šè¿‡ `W1` å˜æ¢æˆä¸€ä¸ª 64 ç»´çš„å‘é‡ï¼Œå†åŠ ä¸Šåç½® `b1`ã€‚

---

### 3ï¸âƒ£ ä¸¾ä¸ªä¾‹å­ï¼š

å‡è®¾ï¼š

```python
embcat.shape = (32, 30)  # batch é‡Œ 32 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬ 30 ç»´å‘é‡
W1.shape = (30, 64)      # æƒé‡çŸ©é˜µï¼ŒæŠŠ 30 ç»´è¾“å…¥å˜æˆ 64 ç»´
b1.shape = (64,)         # åç½®ï¼Œä½œç”¨åˆ°æ¯ä¸ªæ ·æœ¬çš„ 64 ç»´è¾“å‡º
```

è¿ç®—è¿‡ç¨‹ï¼š

```python
(32, 30) @ (30, 64) = (32, 64)
```

å†åŠ ä¸Šåç½® `b1`ï¼Œå¹¿æ’­åˆ° (32, 64)ï¼Œæœ€ç»ˆï¼š

```python
hprebn.shape == (32, 64)
```

---

### 4ï¸âƒ£ ä¸ºä»€ä¹ˆå« `hprebn`ï¼Ÿ

* è¿™æ˜¯â€œhidden layer pre-activationâ€ çš„æ„æ€ï¼š
  â†’ **éšè—å±‚çš„â€œæ¿€æ´»å‰â€ç»“æœ**ï¼ˆpre-activationï¼‰
  å› ä¸ºä¸‹ä¸€æ­¥è¿˜è¦æ¥ BatchNorm å’Œæ¿€æ´»å‡½æ•°ï¼ˆæ¯”å¦‚ tanhï¼‰ã€‚

---

### 5ï¸âƒ£ æ€»ç»“ä¸€å¥è¯ï¼š

```python
hprebn = embcat @ W1 + b1
```

ğŸ‘‰ æŠŠæ‹¼æ¥å¥½çš„ embedding å‘é‡ **é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆçŸ©é˜µä¹˜æ³• + åç½®ï¼‰**ï¼Œå¾—åˆ°éšè—å±‚çš„è¾“å…¥ï¼ˆè¿˜æ²¡æ¿€æ´»ï¼‰ã€‚

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ ç”»ä¸€å¼ å›¾ï¼Œç¤ºæ„ â€œembcat â†’ W1 â†’ + b1 â†’ hprebnâ€ æ•´ä¸ªè¿‡ç¨‹ï¼Œè¦ä¸è¦ï¼Ÿ ğŸŒŸ

```python
# BatchNorm layer
bnmeani = 1/n*hprebn.sum(0, keepdim=True)
bndiff = hprebn - bnmeani
bndiff2 = bndiff**2
bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)
bnvar_inv = (bnvar + 1e-5)**-0.5
bnraw = bndiff * bnvar_inv
hpreact = bngain * bnraw + bnbias
```

è§£é‡Šä¸€ä¸‹ï¼Œæ¯ä¸€æ­¥éœ€è¦ä¸¾ä¸ªä¾‹å­æ˜¾ç¤ºè®¡ç®—è¿‡ç¨‹

å¤ªå¥½äº†ï¼Œè¿™æ¬¡ä½ å¸Œæœ›â€œæ¯ä¸€æ­¥éƒ½ä¸¾ä¸ªä¾‹å­â€æ¥çœ‹è®¡ç®—è¿‡ç¨‹ï¼Œæˆ‘æ¥å¸®ä½ è¯¦ç»†æ‹†è§£ï¼
æˆ‘ä¸¾ä¸€ä¸ª**ç®€å•çš„ä¾‹å­**ï¼Œå‡è®¾ï¼š

* batch\_size = 4
* hidden\_dim = 2 ï¼ˆéšè—å±‚æœ‰ 2 ç»´ï¼‰

å‡è®¾ `hprebn` æ˜¯ 4 ä¸ªæ ·æœ¬ï¼Œ2 ä¸ªéšè—ç¥ç»å…ƒçš„å€¼ï¼Œå½¢çŠ¶æ˜¯ `(4,2)`ï¼Œä¸¾ä¸ªå°çŸ©é˜µä¾‹å­ï¼š

```text
hprebn = [
  [1.0, 2.0],
  [2.0, 3.0],
  [3.0, 4.0],
  [4.0, 5.0],
]
```

---

### 1ï¸âƒ£ è®¡ç®—å‡å€¼ `bnmeani`

```python
bnmeani = (1/n) * hprebn.sum(0, keepdim=True)
```

n = 4ï¼ˆbatch\_sizeï¼‰

è®¡ç®—å¯¹æ¯ä¸€åˆ—ï¼ˆéšè—å•å…ƒï¼‰æ±‚ **å‡å€¼**ï¼š

```text
ç¬¬ä¸€åˆ—å‡å€¼ = (1 + 2 + 3 + 4) / 4 = 10 / 4 = 2.5
ç¬¬äºŒåˆ—å‡å€¼ = (2 + 3 + 4 + 5) / 4 = 14 / 4 = 3.5
```

æ‰€ä»¥ï¼š

```text
bnmeani = [[2.5, 3.5]]
```

---

### 2ï¸âƒ£ è®¡ç®—å·®å€¼ `bndiff`

```python
bndiff = hprebn - bnmeani
```

æ¯ä¸ªå…ƒç´ å‡å»å¯¹åº”åˆ—çš„å‡å€¼ï¼š

```text
[[1.0 - 2.5, 2.0 - 3.5],   => [-1.5, -1.5]
 [2.0 - 2.5, 3.0 - 3.5],   => [-0.5, -0.5]
 [3.0 - 2.5, 4.0 - 3.5],   => [ 0.5,  0.5]
 [4.0 - 2.5, 5.0 - 3.5]]   => [ 1.5,  1.5]
```

ç»“æœï¼š

```text
bndiff = [
  [-1.5, -1.5],
  [-0.5, -0.5],
  [ 0.5,  0.5],
  [ 1.5,  1.5]
]
```

---

### 3ï¸âƒ£ å·®å€¼å¹³æ–¹ `bndiff2`

```python
bndiff2 = bndiff ** 2
```

æ¯ä¸ªå…ƒç´ å¹³æ–¹ï¼š

```text
[
 [2.25, 2.25],
 [0.25, 0.25],
 [0.25, 0.25],
 [2.25, 2.25]
]
```

---

### 4ï¸âƒ£ è®¡ç®—æ–¹å·® `bnvar`

```python
bnvar = (1 / (n-1)) * bndiff2.sum(0, keepdim=True)
```

n-1 = 3

åˆ†åˆ«å¯¹ä¸¤åˆ—æ±‚å’Œï¼š

```text
ç¬¬ä¸€åˆ—æ€»å’Œ = 2.25 + 0.25 + 0.25 + 2.25 = 5.0
ç¬¬äºŒåˆ—æ€»å’Œ = åŒä¸Š = 5.0
```

é™¤ä»¥ n-1ï¼š

```text
ç¬¬ä¸€åˆ—æ–¹å·® = 5.0 / 3 â‰ˆ 1.66667
ç¬¬äºŒåˆ—æ–¹å·® = åŒä¸Š
```

ç»“æœï¼š

```text
bnvar = [[1.66667, 1.66667]]
```

---

### 5ï¸âƒ£ è®¡ç®—æ ‡å‡†å·®å€’æ•° `bnvar_inv`

```python
bnvar_inv = (bnvar + 1e-5) ** -0.5
```

å…ˆåŠ ä¸€ä¸ªå°æ•° 1e-5 ä¿è¯æ•°å€¼ç¨³å®šï¼š

```text
1.66667 + 1e-5 â‰ˆ 1.66668
```

ç„¶åå¼€æ–¹å¹¶å–å€’æ•°ï¼š

```text
sqrt(1.66668) â‰ˆ 1.29099
å€’æ•° = 1 / 1.29099 â‰ˆ 0.7746
```

æ‰€ä»¥ï¼š

```text
bnvar_inv = [[0.7746, 0.7746]]
```

---

### 6ï¸âƒ£ æ ‡å‡†åŒ–è¾“å‡º `bnraw`

```python
bnraw = bndiff * bnvar_inv
```

ä¸¾ä¾‹ï¼š

```text
ç¬¬ä¸€è¡Œç¬¬ä¸€åˆ— = -1.5 * 0.7746 â‰ˆ -1.1619
ç¬¬ä¸€è¡Œç¬¬äºŒåˆ— = -1.5 * 0.7746 â‰ˆ -1.1619
...

ä¾æ¬¡è®¡ç®—å…¨éƒ¨å…ƒç´ ï¼š
```

ç»“æœï¼š

```text
bnraw = [
  [-1.1619, -1.1619],
  [-0.3873, -0.3873],
  [ 0.3873,  0.3873],
  [ 1.1619,  1.1619]
]
```

---

### 7ï¸âƒ£ æœ€ç»ˆ BatchNorm è¾“å‡º `hpreact`

```python
hpreact = bngain * bnraw + bnbias
```

å‡è®¾ï¼š

```text
bngain = [[1.0, 1.0]]  # Î³
bnbias = [[0.0, 0.0]]  # Î²
```

é‚£ä¹ˆç»“æœå°±æ˜¯ï¼š

```text
hpreact = bnraw  ï¼ˆå› ä¸º Î³=1, Î²=0ï¼‰
```

å¦‚æœä½ è®¾ï¼š

```text
bngain = [[2.0, 2.0]]
bnbias = [[0.5, 0.5]]
```

é‚£ï¼š

```text
hpreact = 2.0 * bnraw + 0.5
```

ä¾‹å¦‚ç¬¬ä¸€è¡Œï¼š

```text
hpreact[0,0] = 2*(-1.1619) + 0.5 = -2.3238 + 0.5 = -1.8238
```

---

### æ€»ç»“æµç¨‹

```text
hprebn --> å‡å€¼ --> ä¸­å¿ƒåŒ– --> æ–¹å·® --> æ ‡å‡†å·® --> å½’ä¸€åŒ– --> Î³ã€Î² è°ƒæ•´ --> hpreact
```

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘è¿˜å¯ä»¥å†™ä¸€ä»½å®Œæ•´çš„å°ä»£ç ï¼Œå¸®ä½ ç›´æ¥è¿è¡Œè¿™ä¸ªä¾‹å­ï¼ŒæŠŠæ‰€æœ‰çš„ä¸­é—´ç»“æœéƒ½ print å‡ºæ¥çœ‹ï¼Œä½ è¦ä¸è¦ï¼Ÿ ğŸŒŸ

```python
# Non-linearity
h = torch.tanh(hpreact) # hidden layer
```

è§£é‡Šä¸€ä¸‹

å½“ç„¶å¯ä»¥ï¼Œè¿™è¡Œéå¸¸é‡è¦ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹ï¼š

```python
# Non-linearity
h = torch.tanh(hpreact)  # hidden layer
```

---

### 1ï¸âƒ£ ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

* è¿™æ˜¯ **æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionï¼‰**ï¼Œ
* ç”¨çš„æ˜¯ **tanhï¼ˆåŒæ›²æ­£åˆ‡ï¼‰** å‡½æ•°ï¼š

```text
tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
```

* ç»“æœçš„å–å€¼èŒƒå›´æ˜¯ **\[-1, 1]**ã€‚

---

### 2ï¸âƒ£ ä¸ºä»€ä¹ˆè¦åŠ æ¿€æ´»å‡½æ•°ï¼Ÿ

ğŸ‘‰ æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œæ•´ä¸ªç¥ç»ç½‘ç»œå°±åªæ˜¯çŸ©é˜µä¹˜æ³• + åç½®ï¼Œæ•´ä½“æ˜¯ä¸ªçº¿æ€§å‡½æ•°ã€‚
**å¤šä¸ªçº¿æ€§å±‚å åŠ ï¼Œè¿˜æ˜¯çº¿æ€§ï¼Œæ— æ³•æ‹Ÿåˆå¤æ‚çš„éçº¿æ€§é—®é¢˜ã€‚**

**åŠ äº†æ¿€æ´»å‡½æ•°ï¼Œæ‰èƒ½å¼•å…¥éçº¿æ€§ï¼Œç½‘ç»œæ‰èƒ½æ‹Ÿåˆå¤æ‚çš„å‡½æ•°å…³ç³»ã€‚**

---

### 3ï¸âƒ£ ä¸ºä»€ä¹ˆé€‰ tanhï¼Ÿ

* tanh çš„ä¼˜ç‚¹ï¼š

  * è¾“å‡ºæ˜¯æœ‰ç•Œçš„ `[-1, 1]`ï¼Œä¸ä¼šåƒ ReLU é‚£æ ·æœ‰ unbounded çš„ä¸€ä¾§ã€‚
  * è¾“å‡ºå‡å€¼æ˜¯ 0ï¼Œ**æ›´åˆ©äºåç»­å±‚çš„å­¦ä¹ **ï¼ˆæ¯” sigmoid å¥½ï¼Œsigmoid è¾“å‡ºåæ­£æ•°ï¼Œå‡å€¼ > 0ï¼‰ã€‚
  * æ¢¯åº¦è®¡ç®—ä¹Ÿç®€å•ï¼Œ`d/dx tanh(x) = 1 - tanh(x)^2`ã€‚

---

### 4ï¸âƒ£ ä¸¾ä¸ªä¾‹å­

å‡è®¾ï¼š

```text
hpreact = [
  [-2.0, -0.5],
  [0.0,  1.0],
  [2.0,  3.0]
]
```

é‚£ä¹ˆï¼š

```text
h = torch.tanh(hpreact)
```

å¤§æ¦‚ç»“æœï¼ˆæŸ¥ tanh è¡¨ï¼‰ï¼š

```text
tanh(-2.0) â‰ˆ -0.964
tanh(-0.5) â‰ˆ -0.462
tanh(0.0)  = 0.0
tanh(1.0)  â‰ˆ 0.761
tanh(2.0)  â‰ˆ 0.964
tanh(3.0)  â‰ˆ 0.995
```

æ‰€ä»¥ï¼š

```text
h = [
  [-0.964, -0.462],
  [ 0.0,    0.761],
  [ 0.964,  0.995]
]
```

---

### 5ï¸âƒ£ æ€»ç»“ä¸€å¥è¯ï¼š

```text
è¿™ä¸€è¡Œç”¨ tanh æ¿€æ´»å‡½æ•°ï¼ŒæŠŠéšè—å±‚è¾“å‡º hpreact è½¬æ¢æˆ [-1, 1] åŒºé—´çš„ hï¼Œ  
å¼•å…¥éçº¿æ€§èƒ½åŠ›ï¼Œå¸®åŠ©ç½‘ç»œæ‹Ÿåˆæ›´å¤æ‚çš„å‡½æ•°ã€‚
```

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ å¯¹æ¯”ä¸€ä¸‹ï¼š

* tanh vs sigmoid vs ReLU çš„åŒºåˆ«åœºæ™¯ï¼Œ
* ä»¥åŠ tanh åœ¨ä»€ä¹ˆæƒ…å†µä¸‹æ›´é€‚åˆç”¨ï¼Œéœ€ä¸éœ€è¦ï¼Ÿ ğŸŒŸ

```python
# cross entropy loss (same as F.cross_entropy(logits, Yb))
logit_maxes = logits.max(1, keepdim=True).values
norm_logits = logits - logit_maxes # subtract max for numerical stability
counts = norm_logits.exp()
counts_sum = counts.sum(1, keepdims=True)
counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...
probs = counts * counts_sum_inv
logprobs = probs.log()
loss = -logprobs[range(n), Yb].mean()
```

è§£é‡Šä¸€ä¸‹ï¼Œæ¯ä¸€æ­¥éœ€è¦ä¸¾ä¸ªä¾‹å­æ˜¾ç¤ºè®¡ç®—è¿‡ç¨‹


è¿™æ®µä»£ç å®ç°äº† **äº¤å‰ç†µæŸå¤±**ï¼ˆCross-Entropy Lossï¼‰çš„è®¡ç®—ï¼Œå¹¶å¯¹ **æ•°å€¼ç¨³å®šæ€§** åšäº†å¤„ç†ã€‚
è®©æˆ‘ä»¬é€æ­¥è§£é‡Šæ¯ä¸ªæ­¥éª¤ï¼Œå¹¶ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­å¸®åŠ©ç†è§£è®¡ç®—è¿‡ç¨‹ã€‚

---

### 1ï¸âƒ£ `logit_maxes = logits.max(1, keepdim=True).values`

* **ä½œç”¨**ï¼šå– `logits` æ¯ä¸€è¡Œçš„æœ€å¤§å€¼ã€‚
* **ä¸ºä»€ä¹ˆè¦å–æœ€å¤§å€¼ï¼Ÿ**ï¼šä¸ºäº† **æ•°å€¼ç¨³å®šæ€§**ï¼Œé¿å…åç»­è®¡ç®—æ—¶ç”±äºæŒ‡æ•°å‡½æ•°å¯¼è‡´çš„æº¢å‡ºé—®é¢˜ã€‚

**ä¾‹å­ï¼š**
å‡è®¾ `logits` æ˜¯ 2 ä¸ªæ ·æœ¬ï¼Œ3 ä¸ªç±»åˆ«çš„è¾“å‡ºï¼š

```text
logits = [
  [2.0, 1.0, 0.1],  # ç¬¬ä¸€ä¸ªæ ·æœ¬
  [0.5, 2.2, 1.5]   # ç¬¬äºŒä¸ªæ ·æœ¬
]
```

å¯¹æ¯è¡Œå–æœ€å¤§å€¼ï¼š

```text
logit_maxes = [
  [2.0],  # ç¬¬ä¸€ä¸ªæ ·æœ¬æœ€å¤§å€¼
  [2.2]   # ç¬¬äºŒä¸ªæ ·æœ¬æœ€å¤§å€¼
]
```

---

### 2ï¸âƒ£ `norm_logits = logits - logit_maxes`

* **ä½œç”¨**ï¼šä»æ¯ä¸ª `logits` çš„å€¼ä¸­å‡å»è¯¥è¡Œçš„æœ€å¤§å€¼ï¼Œå¾—åˆ° **å½’ä¸€åŒ–åçš„ logits**ã€‚
* **ä¸ºä»€ä¹ˆè¦åšè¿™ä¸€æ­¥ï¼Ÿ**ï¼šé€šè¿‡å‡å»æœ€å¤§å€¼ï¼Œé¿å…è®¡ç®—æ—¶å‡ºç°æŒ‡æ•°æº¢å‡ºé—®é¢˜ï¼ˆåœ¨ softmax ä¸­è®¡ç®—æŒ‡æ•°æ—¶ï¼ŒæŒ‡æ•°å€¼è¿‡å¤§ä¼šå¯¼è‡´æº¢å‡ºï¼‰ã€‚

**ä¾‹å­ï¼š**

```text
norm_logits = [
  [2.0 - 2.0, 1.0 - 2.0, 0.1 - 2.0],  # ç¬¬ä¸€ä¸ªæ ·æœ¬
  [0.5 - 2.2, 2.2 - 2.2, 1.5 - 2.2]   # ç¬¬äºŒä¸ªæ ·æœ¬
]
= [
  [0.0, -1.0, -1.9],
  [-1.7, 0.0, -0.7]
]
```

---

### 3ï¸âƒ£ `counts = norm_logits.exp()`

* **ä½œç”¨**ï¼šå¯¹ `norm_logits` è¿›è¡ŒæŒ‡æ•°è¿ç®—ï¼Œå¾—åˆ° **æŒ‡æ•°åŒ–çš„ logits**ã€‚
* **ä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼Ÿ**ï¼šä¸ºäº†å¾—åˆ°æ¯ä¸ªç±»åˆ«çš„ **æœªå½’ä¸€åŒ–çš„æ¦‚ç‡**ï¼Œè¿™æ˜¯ softmax æ­¥éª¤çš„ä¸€éƒ¨åˆ†ã€‚

**ä¾‹å­ï¼š**

```text
counts = exp(norm_logits) = [
  [exp(0.0), exp(-1.0), exp(-1.9)],
  [exp(-1.7), exp(0.0), exp(-0.7)]
]
â‰ˆ [
  [1.0, 0.3679, 0.1496],
  [0.1827, 1.0, 0.4966]
]
```

---

### 4ï¸âƒ£ `counts_sum = counts.sum(1, keepdims=True)`

* **ä½œç”¨**ï¼šå¯¹æ¯è¡Œï¼ˆæ¯ä¸ªæ ·æœ¬ï¼‰è¿›è¡Œæ±‚å’Œï¼Œå¾—åˆ° **æ¯è¡Œçš„å’Œ**ï¼Œå³æ¯ä¸ªæ ·æœ¬æ‰€æœ‰ç±»åˆ«çš„æŒ‡æ•°å€¼ä¹‹å’Œã€‚
* **ä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼Ÿ**ï¼šå¾—åˆ°æ¯ä¸ªæ ·æœ¬çš„å½’ä¸€åŒ–å› å­ï¼ˆsoftmax ä¸­éœ€è¦å°†æŒ‡æ•°å€¼å½’ä¸€åŒ–ï¼‰ã€‚

**ä¾‹å­ï¼š**

```text
counts_sum = [
  [1.0 + 0.3679 + 0.1496],  # ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å’Œ
  [0.1827 + 1.0 + 0.4966]   # ç¬¬äºŒä¸ªæ ·æœ¬çš„å’Œ
]
â‰ˆ [
  [1.5175],
  [1.6793]
]
```

---

### 5ï¸âƒ£ `counts_sum_inv = counts_sum**-1`

* **ä½œç”¨**ï¼šå¯¹ `counts_sum` è¿›è¡Œå€’æ•°è¿ç®—ï¼Œå¾—åˆ° **å½’ä¸€åŒ–å› å­**ã€‚
* **ä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼Ÿ**ï¼šå€’æ•°æ˜¯ä¸ºäº†åœ¨è®¡ç®— softmax æ—¶å½’ä¸€åŒ–ï¼Œä½¿å¾—æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡åŠ èµ·æ¥ä¸º 1ã€‚

**ä¾‹å­ï¼š**

```text
counts_sum_inv = [
  [1 / 1.5175],
  [1 / 1.6793]
]
â‰ˆ [
  [0.6592],
  [0.5952]
]
```

---

### 6ï¸âƒ£ `probs = counts * counts_sum_inv`

* **ä½œç”¨**ï¼šå°† `counts` ä¹˜ä»¥ `counts_sum_inv`ï¼Œå¾—åˆ° **å½’ä¸€åŒ–åçš„æ¦‚ç‡åˆ†å¸ƒ**ï¼ˆsoftmax çš„ç»“æœï¼‰ã€‚
* **ä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼Ÿ**ï¼šå°†æ¯ä¸ªç±»åˆ«çš„æŒ‡æ•°å€¼é™¤ä»¥æ¯ä¸ªæ ·æœ¬çš„æ€»å’Œï¼Œä»è€Œå¾—åˆ°æ¯ä¸ªç±»åˆ«çš„æœ€ç»ˆæ¦‚ç‡ã€‚

**ä¾‹å­ï¼š**

```text
probs = counts * counts_sum_inv = [
  [1.0, 0.3679, 0.1496] * 0.6592,
  [0.1827, 1.0, 0.4966] * 0.5952
]
â‰ˆ [
  [0.6592, 0.2426, 0.0986],
  [0.1089, 0.5952, 0.2960]
]
```

---

### 7ï¸âƒ£ `logprobs = probs.log()`

* **ä½œç”¨**ï¼šå¯¹ `probs` è¿›è¡Œ **å¯¹æ•°å˜æ¢**ï¼Œå¾—åˆ° **log-probabilities**ï¼ˆå¯¹æ•°æ¦‚ç‡ï¼‰ã€‚
* **ä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼Ÿ**ï¼šä¸ºäº†è®¡ç®—äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆcross-entropy lossï¼‰ï¼Œéœ€è¦ä½¿ç”¨å¯¹æ•°æ¦‚ç‡ã€‚

**ä¾‹å­ï¼š**

```text
logprobs = log(probs) = [
  [log(0.6592), log(0.2426), log(0.0986)],
  [log(0.1089), log(0.5952), log(0.2960)]
]
â‰ˆ [
  [-0.4161, -1.4182, -2.3199],
  [-2.2076, -0.5187, -1.2170]
]
```

---

### 8ï¸âƒ£ `loss = -logprobs[range(n), Yb].mean()`

* **ä½œç”¨**ï¼šè®¡ç®— **äº¤å‰ç†µæŸå¤±**ï¼ˆcross-entropy lossï¼‰ï¼Œè¿™æ˜¯åˆ†ç±»ä»»åŠ¡ä¸­å¸¸ç”¨çš„æŸå¤±å‡½æ•°ã€‚

  * `Yb` æ˜¯çœŸå®æ ‡ç­¾ï¼Œ`range(n)` æ˜¯ batch ä¸­æ¯ä¸ªæ ·æœ¬çš„ç´¢å¼•ã€‚
  * `logprobs[range(n), Yb]`ï¼šå–å‡ºæ¯ä¸ªæ ·æœ¬å¯¹åº”ç±»åˆ«çš„å¯¹æ•°æ¦‚ç‡ã€‚
  * `-`ï¼šå› ä¸ºäº¤å‰ç†µæŸå¤±æ˜¯ **è´Ÿå¯¹æ•°ä¼¼ç„¶**ã€‚
  * `.mean()`ï¼šè®¡ç®— batch ä¸­æ‰€æœ‰æ ·æœ¬çš„å¹³å‡æŸå¤±ã€‚

**ä¾‹å­ï¼š**

å‡è®¾çœŸå®æ ‡ç­¾ `Yb` æ˜¯ï¼š

```text
Yb = [0, 2]  # ç¬¬ä¸€ä¸ªæ ·æœ¬æ ‡ç­¾ä¸º 0ï¼Œç¬¬äºŒä¸ªæ ·æœ¬æ ‡ç­¾ä¸º 2
```

ä» `logprobs` å–å‡ºçœŸå®æ ‡ç­¾å¯¹åº”çš„å€¼ï¼š

```text
logprobs[range(2), Yb] = [-0.4161, -1.2170]
```

ç„¶åè®¡ç®—å¹³å‡æŸå¤±ï¼š

```text
loss = (-(-0.4161) - (-1.2170)) / 2 â‰ˆ 0.8165
```

---

### æ€»ç»“ï¼š

è¿™æ®µä»£ç å®ç°äº† **softmax + äº¤å‰ç†µæŸå¤±**ï¼š

1. å¯¹ logits è¿›è¡Œ **æ ‡å‡†åŒ–**ï¼ˆå‡å»æœ€å¤§å€¼ï¼Œè®¡ç®—æŒ‡æ•°ï¼Œå½’ä¸€åŒ–ï¼‰ã€‚
2. è®¡ç®— **æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡**ï¼ˆsoftmaxï¼‰ã€‚
3. ä½¿ç”¨ **å¯¹æ•°æ¦‚ç‡** è®¡ç®—æŸå¤±ã€‚
4. æœ€ç»ˆå¾—åˆ°çš„ `loss` æ˜¯ç”¨æ¥è®­ç»ƒæ¨¡å‹çš„ **äº¤å‰ç†µæŸå¤±**ã€‚

é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹åœ¨è®­ç»ƒæ—¶ä¼šé€šè¿‡æ¢¯åº¦ä¸‹é™æœ€å°åŒ– **äº¤å‰ç†µæŸå¤±**ï¼Œä¸æ–­è°ƒæ•´æƒé‡ï¼Œä»è€Œæé«˜åˆ†ç±»å‡†ç¡®æ€§ã€‚

å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œæˆ–è€…æƒ³è¦æ›´è¯¦ç»†çš„è§£é‡Šï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼ğŸŒŸ


8ï¸âƒ£ loss = -logprobs[range(n), Yb].mean()

è¿™ä¸ªè¿‡ç¨‹è®²å¾—å†è¯¦ç»†ä¸€ç‚¹

å¥½çš„ï¼Œä¸‹é¢æˆ‘ä¼šéå¸¸è¯¦ç»†åœ°è®²è§£è¿™ä¸€è¡Œä»£ç ï¼š

```python
loss = -logprobs[range(n), Yb].mean()
```

æˆ‘ä»¬è¦ä¸€æ­¥ä¸€æ­¥æ¥ï¼Œç†è§£å®ƒèƒŒåå‘ç”Ÿçš„è®¡ç®—è¿‡ç¨‹ã€‚

---

### 1ï¸âƒ£ **èƒŒæ™¯**

è¿™è¡Œä»£ç è®¡ç®—çš„æ˜¯ **äº¤å‰ç†µæŸå¤±**ï¼ˆcross-entropy lossï¼‰ï¼Œç”¨æ¥è¡¡é‡æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®æ ‡ç­¾åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚
äº¤å‰ç†µæŸå¤±æ˜¯åˆ†ç±»é—®é¢˜ä¸­æœ€å¸¸è§çš„æŸå¤±å‡½æ•°ï¼Œå°¤å…¶æ˜¯åœ¨å¤šç±»åˆ†ç±»ä»»åŠ¡ä¸­ã€‚

---

### 2ï¸âƒ£ **äº¤å‰ç†µæŸå¤±å‡½æ•°çš„å…¬å¼**

å¯¹äºæ¯ä¸€ä¸ªæ ·æœ¬ï¼Œäº¤å‰ç†µæŸå¤±å‡½æ•°çš„å…¬å¼æ˜¯ï¼š

```text
L = - âˆ‘(y_true * log(y_pred))
```

* `y_true` æ˜¯ **çœŸå®æ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒ**ã€‚åœ¨å®é™…ä¸­ï¼Œ`y_true` åªæœ‰ä¸€ä¸ªå…ƒç´ ä¸º 1ï¼Œå…¶å®ƒä¸º 0ï¼Œå³çœŸå®ç±»åˆ«çš„æ¦‚ç‡æ˜¯ 1ã€‚
* `y_pred` æ˜¯æ¨¡å‹çš„ **é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ**ã€‚
* `log(y_pred)` æ˜¯å¯¹é¢„æµ‹æ¦‚ç‡è¿›è¡Œå¯¹æ•°å˜æ¢ã€‚
* å¯¹äºå¤šç±»åˆ«åˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªæ ·æœ¬çš„æ‰€æœ‰ç±»åˆ«è®¡ç®—æŸå¤±ï¼Œç„¶åå¯¹æ‰€æœ‰æ ·æœ¬çš„æŸå¤±å– **å¹³å‡**ã€‚

---

### 3ï¸âƒ£ **è¯¦ç»†è§£é‡Š**

#### `logprobs[range(n), Yb]`

* **`logprobs`** æ˜¯æ¨¡å‹è¾“å‡ºçš„å¯¹æ•°æ¦‚ç‡çŸ©é˜µï¼Œå½¢çŠ¶ä¸º `(n, num_classes)`ï¼Œå³ `n` æ˜¯æ ·æœ¬æ•°ï¼Œ`num_classes` æ˜¯ç±»åˆ«æ•°ã€‚

  æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬å¯¹åº”çš„å„ä¸ªç±»åˆ«çš„å¯¹æ•°æ¦‚ç‡ã€‚
  æ¯”å¦‚å¯¹äºæŸä¸ªæ ·æœ¬ `i`ï¼Œ`logprobs[i]` æ˜¯ä¸€ä¸ªå¤§å°ä¸º `num_classes` çš„å‘é‡ï¼Œè¡¨ç¤ºè¯¥æ ·æœ¬å¯¹æ¯ä¸ªç±»åˆ«çš„å¯¹æ•°æ¦‚ç‡ã€‚

* **`Yb`** æ˜¯ä¸€ä¸ªåŒ…å«çœŸå®æ ‡ç­¾çš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º `(n,)`ï¼Œå³æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºè¯¥æ ·æœ¬çš„çœŸå®ç±»åˆ«ï¼ˆç±»åˆ«ç¼–å·ï¼‰ã€‚

* **`logprobs[range(n), Yb]`**ï¼š

  * `range(n)` æ˜¯ `[0, 1, 2, ..., n-1]`ï¼Œè¡¨ç¤ºæ¯ä¸ªæ ·æœ¬çš„ç´¢å¼•ã€‚
  * `Yb` æ˜¯æ¯ä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾ç´¢å¼•ã€‚

  é€šè¿‡ `logprobs[range(n), Yb]`ï¼Œæˆ‘ä»¬ä» `logprobs` ä¸­æå–å‡ºæ¯ä¸ªæ ·æœ¬çš„ **çœŸå®ç±»åˆ«å¯¹åº”çš„å¯¹æ•°æ¦‚ç‡**ã€‚

**ä¸¾ä¾‹ï¼š**

å‡è®¾ï¼š

```text
logprobs = [
  [-0.5, -1.0, -2.0],  # ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ log æ¦‚ç‡
  [-0.7, -0.3, -1.5],  # ç¬¬äºŒä¸ªæ ·æœ¬çš„ log æ¦‚ç‡
]

Yb = [0, 2]  # ç¬¬ä¸€ä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾æ˜¯ 0ï¼Œç¬¬äºŒä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾æ˜¯ 2
```

é‚£ä¹ˆï¼š

```text
logprobs[range(2), Yb] = logprobs[0, 0], logprobs[1, 2]
```

ç»“æœï¼š

```text
logprobs[range(2), Yb] = [-0.5, -1.5]
```

è¿™å°±æ˜¯æˆ‘ä»¬æå–å‡ºæ¥çš„ **æ¯ä¸ªæ ·æœ¬çœŸå®æ ‡ç­¾å¯¹åº”çš„å¯¹æ•°æ¦‚ç‡**ã€‚

---

#### `-logprobs[range(n), Yb]`

* **`-logprobs[range(n), Yb]`** æ˜¯å¯¹æ¯ä¸ªæ ·æœ¬çš„çœŸå®ç±»åˆ«çš„ **å¯¹æ•°æ¦‚ç‡** å–è´Ÿå·ï¼Œè¿™æ˜¯å› ä¸ºäº¤å‰ç†µçš„å…¬å¼é‡Œæ˜¯è´Ÿå·çš„ï¼š

  ```text
  L = - âˆ‘(y_true * log(y_pred))
  ```

**ä¸¾ä¾‹ï¼š**

å¯¹äºä¸Šé¢ä¾‹å­ä¸­çš„ `logprobs[range(2), Yb] = [-0.5, -1.5]`ï¼š

```text
-logprobs[range(2), Yb] = [0.5, 1.5]
```

---

#### `.mean()`

* **`mean()`** æ˜¯è®¡ç®—æ‰€æœ‰æ ·æœ¬æŸå¤±çš„ **å¹³å‡å€¼**ã€‚

  æˆ‘ä»¬å¯¹æ¯ä¸ªæ ·æœ¬çš„æŸå¤±è®¡ç®—å®Œæ¯•åï¼Œå– **å¹³å‡**ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ‰¹æ¬¡æŸå¤±ã€‚

**ä¸¾ä¾‹ï¼š**

å¯¹äºä¸Šé¢çš„ä¾‹å­ï¼Œæˆ‘ä»¬çš„æŸå¤±æ˜¯ `[0.5, 1.5]`ï¼Œå¹³å‡å€¼æ˜¯ï¼š

```text
loss = (0.5 + 1.5) / 2 = 1.0
```

---

### 4ï¸âƒ£ **æ€»ç»“**

1. **`logprobs[range(n), Yb]`**ï¼šä» `logprobs` ä¸­å–å‡ºæ¯ä¸ªæ ·æœ¬çš„çœŸå®ç±»åˆ«å¯¹åº”çš„å¯¹æ•°æ¦‚ç‡ã€‚
2. **`-logprobs[range(n), Yb]`**ï¼šå–è´Ÿå·ï¼Œå› ä¸ºäº¤å‰ç†µæŸå¤±æ˜¯è´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚
3. **`.mean()`**ï¼šå¯¹æ‰€æœ‰æ ·æœ¬çš„æŸå¤±å–å¹³å‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ‰¹æ¬¡æŸå¤±ã€‚

æœ€ç»ˆå¾—åˆ°çš„ **`loss`** æ˜¯è¯¥æ‰¹æ¬¡æ ·æœ¬çš„ **å¹³å‡äº¤å‰ç†µæŸå¤±**ã€‚

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥å†ç»™ä½ ä¸€ä¸ªå¸¦æœ‰æ›´å¤æ‚ä¾‹å­çš„ä»£ç ï¼Œæˆ–è€…ç”»ä¸ªæµç¨‹å›¾å¸®åŠ©ä½ æ›´æ¸…æ™°åœ°ç†è§£è¿™ä¸€è¿‡ç¨‹ï¼ğŸŒŸ


```python
# PyTorch backward pass
for p in parameters:
  p.grad = None
for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way
          norm_logits, logit_maxes, logits, h, hpreact, bnraw,
         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,
         embcat, emb]:
  t.retain_grad()
loss.backward()
```

è§£é‡Šä¸€ä¸‹

è¿™æ®µä»£ç æ˜¯ PyTorch çš„ **åå‘ä¼ æ’­ï¼ˆbackward passï¼‰** çš„å®ç°éƒ¨åˆ†ï¼Œä¸»è¦åŠŸèƒ½æ˜¯ï¼š

1. **æ¸…ç©ºç°æœ‰çš„æ¢¯åº¦**ã€‚
2. **ä¿ç•™ä¸­é—´å˜é‡çš„æ¢¯åº¦**ï¼Œä»¥ä¾¿åœ¨åå‘ä¼ æ’­æ—¶è®¡ç®—æ¢¯åº¦ã€‚
3. **æ‰§è¡Œåå‘ä¼ æ’­**ï¼Œè®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ã€‚

æˆ‘ä¼šé€è¡Œè¯¦ç»†è§£é‡Šï¼š

---

### 1ï¸âƒ£ **`for p in parameters: p.grad = None`**

* **ä½œç”¨**ï¼šéå†æ‰€æœ‰çš„æ¨¡å‹å‚æ•°ï¼ˆ`parameters`ï¼‰ï¼Œå°†å®ƒä»¬çš„æ¢¯åº¦ **æ¸…ç©º**ã€‚
  è¿™æ˜¯å› ä¸º PyTorch é»˜è®¤ä¼šç´¯åŠ æ¢¯åº¦ï¼Œå³æ¯æ¬¡è°ƒç”¨ `backward()` æ—¶ï¼Œè®¡ç®—çš„æ¢¯åº¦ä¼šä¸ä¹‹å‰çš„æ¢¯åº¦ç´¯åŠ ã€‚
  å› æ­¤ï¼Œåœ¨æ¯æ¬¡è¿›è¡Œæ–°çš„ä¸€æ¬¡åå‘ä¼ æ’­å‰ï¼Œå¿…é¡» **å…ˆæ¸…ç©ºæ¢¯åº¦**ã€‚

**ç¤ºä¾‹**ï¼š
å‡è®¾æœ‰ä¸€ä¸ªå‚æ•° `W`ï¼Œå®ƒçš„æ¢¯åº¦æ˜¯ `W.grad`ã€‚å¦‚æœä¸æ¸…ç©ºï¼Œæ–°çš„åå‘ä¼ æ’­ç»“æœä¼šä¸ä¹‹å‰çš„ç»“æœç´¯åŠ ï¼Œä»è€Œå½±å“å½“å‰è®¡ç®—ã€‚

```python
W.grad = None  # æ¸…ç©º W çš„æ¢¯åº¦
```

---

### 2ï¸âƒ£ **`for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, norm_logits, logit_maxes, logits, h, hpreact, bnraw, bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani, embcat, emb]:`**

* **ä½œç”¨**ï¼šéå†è¿™äº› **ä¸­é—´å˜é‡**ï¼ˆå¦‚ `logprobs`, `probs`, `logits` ç­‰ï¼‰ï¼Œå¹¶å¯¹æ¯ä¸ªå˜é‡è°ƒç”¨ `.retain_grad()` æ¥ **ä¿ç•™å®ƒä»¬çš„æ¢¯åº¦**ã€‚

ä¸ºä»€ä¹ˆè¦ä¿ç•™æ¢¯åº¦ï¼Ÿ

* PyTorch é»˜è®¤åªä¼šä¿ç•™ **æ¨¡å‹å‚æ•°çš„æ¢¯åº¦**ï¼ˆ`model.parameters()`ï¼‰ï¼Œå¦‚æœæˆ‘ä»¬éœ€è¦å¯¹ **éæ¨¡å‹å‚æ•°çš„ä¸­é—´å˜é‡**ï¼ˆå¦‚ `logits`ã€`hprebn` ç­‰ï¼‰è¿›è¡Œåå‘ä¼ æ’­å¹¶è®¡ç®—æ¢¯åº¦ï¼Œæˆ‘ä»¬éœ€è¦æ˜¾å¼åœ°ä½¿ç”¨ `.retain_grad()` ä¿ç•™å®ƒä»¬çš„æ¢¯åº¦ã€‚

**ä¾‹å­**ï¼š

```python
logprobs.retain_grad()  # ä¿ç•™ logprobs çš„æ¢¯åº¦
probs.retain_grad()     # ä¿ç•™ probs çš„æ¢¯åº¦
```

è¿™äº›ä¸­é—´å˜é‡çš„æ¢¯åº¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¼šè¢«è®¡ç®—å¹¶å­˜å‚¨ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥åœ¨åç»­æ­¥éª¤ä¸­æŸ¥çœ‹å®ƒä»¬çš„æ¢¯åº¦ã€‚

---

### 3ï¸âƒ£ **`loss.backward()`**

* **ä½œç”¨**ï¼šè°ƒç”¨ **`backward()`**ï¼Œè®¡ç®— **æŸå¤±å‡½æ•°ï¼ˆlossï¼‰** å¯¹æ‰€æœ‰æ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚

å…·ä½“æ¥è¯´ï¼Œ`loss.backward()` ä¼šæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

* è‡ªåŠ¨è®¡ç®—æŸå¤±å‡½æ•°å¯¹ **æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦**ã€‚
* ç”±äºæˆ‘ä»¬åœ¨å‰é¢ä½¿ç”¨äº† `.retain_grad()` æ¥ä¿ç•™ä¸­é—´å˜é‡çš„æ¢¯åº¦ï¼ŒPyTorch ä¼šæ ¹æ®è®¡ç®—å›¾ï¼Œä» **æŸå¤±å‡½æ•°** å¼€å§‹ï¼Œé€šè¿‡é“¾å¼æ³•åˆ™ï¼ˆé“¾å¼æ±‚å¯¼ï¼‰é€å±‚åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ã€‚

PyTorch ä¼šæ²¿ç€è®¡ç®—å›¾åå‘ä¼ æ’­ï¼Œä¾æ¬¡è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚

**ç®€å•æ­¥éª¤**ï¼š

1. ä» **æŸå¤±å‡½æ•°ï¼ˆlossï¼‰** å¼€å§‹ï¼Œè®¡ç®— **loss å¯¹æ¨¡å‹è¾“å‡ºçš„æ¢¯åº¦**ã€‚
2. ä½¿ç”¨ **é“¾å¼æ³•åˆ™**ï¼Œé€’æ¨è®¡ç®—æ¯ä¸€å±‚çš„ **æ¢¯åº¦**ï¼Œç›´åˆ°æœ€åˆçš„è¾“å…¥å±‚ã€‚
3. åå‘ä¼ æ’­çš„ç»“æœä¼šä¿å­˜åœ¨å„ä¸ªå¼ é‡ï¼ˆtensorï¼‰çš„ `.grad` å±æ€§ä¸­ã€‚

---

### 4ï¸âƒ£ **æ€»ç»“**

è¿™æ®µä»£ç å®ç°äº†åå‘ä¼ æ’­çš„æ ¸å¿ƒæµç¨‹ï¼š

* **æ¸…ç©ºå‚æ•°çš„æ¢¯åº¦**ï¼Œé¿å…ç´¯åŠ ã€‚
* **ä¿ç•™ä¸­é—´å˜é‡çš„æ¢¯åº¦**ï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶æŸ¥çœ‹å®ƒä»¬çš„æ¢¯åº¦ã€‚
* æœ€å **æ‰§è¡Œåå‘ä¼ æ’­**ï¼Œè®¡ç®—å¹¶å­˜å‚¨æ‰€æœ‰æ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚

æ¯æ¬¡è°ƒç”¨ `loss.backward()` åï¼Œæ¨¡å‹çš„ **æ‰€æœ‰å¯è®­ç»ƒå‚æ•°** éƒ½ä¼šæ›´æ–°å®ƒä»¬çš„ `.grad` å±æ€§ï¼ŒåŒ…å«ç›¸åº”çš„æ¢¯åº¦ï¼Œä¾›ä¼˜åŒ–å™¨ï¼ˆå¦‚ SGDã€Adam ç­‰ï¼‰ä½¿ç”¨ã€‚

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥è¿›ä¸€æ­¥è§£é‡Š **åå‘ä¼ æ’­çš„æ•°å­¦åŸç†**ï¼Œæˆ–è€…å¸®ä½ æ›´è¯¦ç»†åœ°äº†è§£ PyTorch **å¦‚ä½•è®¡ç®—æ¢¯åº¦**ï¼ğŸŒŸ


ä»€ä¹ˆæ˜¯ æ¨¡å‹å‚æ•°å’Œéæ¨¡å‹å‚æ•°ï¼Ÿ

åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œ**æ¨¡å‹å‚æ•°** å’Œ **éæ¨¡å‹å‚æ•°** æ˜¯ä¸¤ä¸ªä¸åŒçš„æ¦‚å¿µï¼Œå®ƒä»¬çš„ä½œç”¨å’Œè®¡ç®—æ–¹å¼æœ‰æ‰€ä¸åŒã€‚

### 1ï¸âƒ£ **æ¨¡å‹å‚æ•°ï¼ˆModel Parametersï¼‰**

**æ¨¡å‹å‚æ•°** æ˜¯æŒ‡åœ¨ç¥ç»ç½‘ç»œä¸­éœ€è¦å­¦ä¹ çš„å‚æ•°ï¼Œå®ƒä»¬å†³å®šäº†æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚è¿™äº›å‚æ•°æ˜¯ **é€šè¿‡è®­ç»ƒæ•°æ®å­¦ä¹ å¾—åˆ°çš„**ï¼Œå¹¶ä¸”æ˜¯ **åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ›´æ–°çš„**ã€‚

é€šå¸¸ï¼Œæ¨¡å‹å‚æ•°åŒ…æ‹¬ï¼š

* **æƒé‡ï¼ˆWeightsï¼‰**ï¼šæ¯ä¸€å±‚çš„æƒé‡çŸ©é˜µï¼Œè¡¨ç¤ºå„ä¸ªç¥ç»å…ƒä¹‹é—´çš„è¿æ¥å¼ºåº¦ã€‚
* **åç½®ï¼ˆBiasesï¼‰**ï¼šæ¯ä¸€å±‚çš„åç½®é¡¹ï¼Œè¡¨ç¤ºè¯¥å±‚çš„ç¥ç»å…ƒçš„åç§»ã€‚

è¿™äº›å‚æ•°åœ¨ **æ¯æ¬¡è®­ç»ƒæ—¶** ä¼šé€šè¿‡ **æ¢¯åº¦ä¸‹é™**ï¼ˆæˆ–è€…å…¶ä»–ä¼˜åŒ–ç®—æ³•ï¼‰æ¥è°ƒæ•´ï¼Œä»¥ä¾¿æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚

#### ä¸¾ä¸ªä¾‹å­ï¼š

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹ï¼š

```python
y = W * x + b
```

* `W` æ˜¯ **æƒé‡**ï¼Œæ˜¯æ¨¡å‹çš„å‚æ•°ã€‚
* `b` æ˜¯ **åç½®**ï¼Œæ˜¯æ¨¡å‹çš„å‚æ•°ã€‚

è¿™äº›æ˜¯ç¥ç»ç½‘ç»œä¸­çš„æ¨¡å‹å‚æ•°ï¼Œå®ƒä»¬æ˜¯éœ€è¦ **å­¦ä¹ å’Œä¼˜åŒ–** çš„ã€‚

---

### 2ï¸âƒ£ **éæ¨¡å‹å‚æ•°ï¼ˆNon-Model Parametersï¼‰**

**éæ¨¡å‹å‚æ•°** æ˜¯æŒ‡ **åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸è¢«ä¼˜åŒ–**ï¼Œä½†æ˜¯åœ¨æ¨¡å‹çš„è®¡ç®—å›¾ä¸­èµ·åˆ°è¾…åŠ©ä½œç”¨çš„å‚æ•°ã€‚å®ƒä»¬é€šå¸¸åœ¨ **è®¡ç®—è¿‡ç¨‹ä¸­å‚ä¸æ“ä½œï¼Œä½†ä¸ä¼šé€šè¿‡æ¢¯åº¦ä¸‹é™ç›´æ¥æ›´æ–°**ã€‚

ä¸€äº›å¸¸è§çš„éæ¨¡å‹å‚æ•°åŒ…æ‹¬ï¼š

* **BatchNorm ä¸­çš„å‚æ•°**ï¼šä¾‹å¦‚ `bngain` å’Œ `bnbias`ï¼Œè¿™äº›æ˜¯ **BatchNorm å±‚** çš„ç¼©æ”¾å› å­å’Œåç½®é¡¹ã€‚å®ƒä»¬ä¹Ÿå¯ä»¥è¿›è¡Œå­¦ä¹ ï¼Œä½†å¹¶ä¸åƒ `W` å’Œ `b` é‚£æ ·ç›´æ¥å½±å“æ¯ä¸€å±‚çš„æƒé‡ã€‚

* **ä¸­é—´å˜é‡ï¼ˆå¦‚ `logits`ã€`embeddings`ï¼‰**ï¼šè¿™äº›æ˜¯ **åœ¨å‰å‘ä¼ æ’­ä¸­è®¡ç®—å‡ºæ¥çš„ä¸­é—´ç»“æœ**ï¼Œé€šå¸¸åªæ˜¯ç”¨äºè®¡ç®—æŸå¤±ï¼Œåå‘ä¼ æ’­æ—¶æˆ‘ä»¬å¯èƒ½ä¼šè®¡ç®—å®ƒä»¬çš„æ¢¯åº¦ï¼Œä½†å®ƒä»¬å¹¶ä¸ä¼šå‚ä¸ä¼˜åŒ–è¿‡ç¨‹ã€‚

#### ä¸¾ä¸ªä¾‹å­ï¼š

åœ¨ BatchNorm å±‚ä¸­ï¼Œ**`bngain`** å’Œ **`bnbias`** æ˜¯éæ¨¡å‹å‚æ•°ï¼Œå®ƒä»¬ç”¨æ¥è°ƒæ•´æ¯ä¸€å±‚çš„æ ‡å‡†åŒ–è¾“å‡ºã€‚å®ƒä»¬åœ¨ **è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè¢«ä¼˜åŒ–**ï¼Œä½†å®ƒä»¬å’Œæƒé‡çŸ©é˜µ `W` ä¸åŒï¼Œå› ä¸ºå®ƒä»¬ä¸ç›´æ¥å‚ä¸æ¯å±‚çš„å­¦ä¹ ï¼ˆå®ƒä»¬åªæ˜¯è°ƒæ•´æ ‡å‡†åŒ–çš„è¾“å‡ºï¼‰ã€‚

---

### 3ï¸âƒ£ **æ€»ç»“ï¼š**

* **æ¨¡å‹å‚æ•°**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡ **åå‘ä¼ æ’­** æ¥è°ƒæ•´ï¼Œç›´æ¥å½±å“ç½‘ç»œçš„å­¦ä¹ è¿‡ç¨‹ã€‚é€šå¸¸åŒ…æ‹¬ **æƒé‡** å’Œ **åç½®**ã€‚

* **éæ¨¡å‹å‚æ•°**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸é€šè¿‡ **æ¢¯åº¦ä¸‹é™** æ¥ä¼˜åŒ–ï¼Œè™½ç„¶å®ƒä»¬åœ¨è®¡ç®—è¿‡ç¨‹ä¸­å‚ä¸äº†å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œä½†å®ƒä»¬ä¸ç›´æ¥å½±å“å­¦ä¹ è¿‡ç¨‹ã€‚å¸¸è§çš„åŒ…æ‹¬ **BatchNorm** ä¸­çš„ç¼©æ”¾å› å­ã€åç½®é¡¹ä»¥åŠä¸­é—´è®¡ç®—ç»“æœã€‚

---

### ä¸ºä»€ä¹ˆè¦åŒºåˆ†è¿™ä¸¤è€…ï¼Ÿ

* **æ¨¡å‹å‚æ•°**ï¼šç›´æ¥å½±å“æ¨¡å‹çš„è¡¨ç°ï¼Œæ˜¯é€šè¿‡è®­ç»ƒæ¥ **ä¼˜åŒ–çš„å¯¹è±¡**ã€‚ä¼˜åŒ–è¿™äº›å‚æ•°ä¼šä½¿æ¨¡å‹æ›´é€‚åº”è®­ç»ƒæ•°æ®ã€‚

* **éæ¨¡å‹å‚æ•°**ï¼šè¾…åŠ©æ¨¡å‹çš„è®¡ç®—æˆ–æ˜¯å±‚çº§çš„çŠ¶æ€å˜é‡ï¼Œå®ƒä»¬é€šå¸¸å‚ä¸æ¨¡å‹çš„è®­ç»ƒï¼Œä½†ä¸æ˜¯ä¼˜åŒ–çš„æ ¸å¿ƒç›®æ ‡ã€‚

å¦‚æœä½ éœ€è¦æ›´æ·±å…¥çš„ä¾‹å­æˆ–è€…ä»£ç å®ç°ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘ï¼ğŸŒŸ



# brief digression: besselâ€™s correction in batchnorm

note here where I'm using the bezels correction dividing by n minus 1 instead of dividing by n when I normalize here
the sum of squares now you'll notice that this is departure from the paper which uses one over n
instead not one over n minus one their m is RN and
um so it turns out that there are two ways of estimating variance of an array one is the biased estimate which is one
over n and the other one is the unbiased estimate which is one over n minus one now confusingly in the paper this is uh
not very clearly described and also it's a detail that kind of matters I think um they are using the biased version
training time but later when they are talking about the inference they are mentioning that when they do the
inference they are using the unbiased estimate which is the n minus one version in
um basically for inference and to calibrate the running mean and
the running variance basically and so they they actually introduce a trained test mismatch where in training they use
the biased version and in the in test time they use the unbiased version I find this extremely confusing you can
read more about the bezels correction and why uh dividing by n minus one gives you a better estimate of the variance in
a case where you have population size or samples for the population that are very small and that is indeed
the case for us because we are dealing with many patches and these mini matches are a small sample of a larger
population which is the entire training set and so it just turns out that if you just estimate it using one over n that
actually almost always underestimates the variance and it is a biased estimator and it is advised that you use
the unbiased version and divide by n minus one and you can go through this article here that I liked that actually
describes the full reasoning and I'll link it in the video description now when you calculate the torture
variance you'll notice that they take the unbiased flag whether or not you want to divide by n or n minus one confusingly
they do not mention what the default is for unbiased but I believe unbiased by
default is true I'm not sure why the docs here don't cite that now in The Bachelor
1D the documentation again is kind of wrong and confusing it says that the standard deviation is calculated via the
biased estimator but this is actually not exactly right and people have pointed out that it is not right in a number of issues since
then because actually the rabbit hole is deeper and they follow the paper exactly
and they use the biased version for training but when they're estimating the running standard deviation we are using
the unbiased version so again there's the train test mismatch so long story short I'm not a fan of trained test
discrepancies I basically kind of consider the fact that we use the bias version
the training time and the unbiased test time I basically consider this to be a bug and I don't think that there's a
good reason for that it's not really they don't really go into the detail of the reasoning behind it in this paper so
that's why I basically prefer to use the bestless correction in my own work unfortunately Bastion does not take a
keyword argument that tells you whether or not you want to use the unbiased version of the bias version in both
train and test and so therefore anyone using batch normalization basically in my view has a bit of a bug in the code
um and this turns out to be much less of a problem if your batch mini batch sizes
are a bit larger but still I just might kind of uh unpardable so maybe someone can explain why this is okay but for now
I prefer to use the unbiased version consistently both during training and at this time and that's why I'm using one
over n minus one here okay so let's now actually back propagate through this line
so the first thing that I always like to do is I like to scrutinize the shapes first so in particular here looking at the
shapes of what's involved I see that b and VAR shape is 1 by 64. so it's a row
vector and BND if two dot shape is 32 by 64. so clearly here we're doing a sum over
the zeroth axis to squash the first dimension of of the shapes here using a
sum so that right away actually hints to me that there will be some kind of a replication or broadcasting in the
backward pass and maybe you're noticing the pattern here but basically anytime you have a sum in the forward pass that
turns into a replication or broadcasting in the backward pass along the same Dimension and conversely when we have a
replication or a broadcasting in the forward pass that indicates a variable reuse and so in the backward pass that
turns into a sum over the exact same dimension and so hopefully you're noticing that Duality that those two are kind of like
the opposite of each other in the forward and backward pass now once we understand the shapes the
next thing I like to do always is I like to look at a toy example in my head to sort of just like understand roughly how
uh the variable the variable dependencies go in the mathematical formula so here we have a two-dimensional array
of the end of two which we are scaling by a constant and then we are summing uh
vertically over the columns so if we have a two by two Matrix a and then we sum over the columns and scale we would
get a row Vector B1 B2 and B1 depends on a in this way whereas just sum they're
scaled of a and B2 in this way where it's the second column sump and scale
and so looking at this basically what we want to do now is we have the derivatives on B1 and B2 and we want to
back propagate them into Ace and so it's clear that just differentiating in your head the local derivative here is one
over n minus 1 times uh one uh for each one of these A's and um
basically the derivative of B1 has to flow through The Columns of a scaled by one over n minus one
and that's roughly What's Happening Here so intuitively the derivative flow tells us that DB and diff2
will be the local derivative of this operation and there are many ways to do this by the way but I like to do
something like this torch dot once like of bndf2 so I'll create a large array
two-dimensional of ones and then I will scale it so 1.0 divided by n minus 1.
so this is a array of um one over n minus one and that's sort of like the local derivative
and now for the chain rule I will simply just multiply it by dbm bar
and notice here what's going to happen this is 32 by 64 and this is just 1 by 64. so I'm letting the broadcasting do
the replication because internally in pytorch basically dbnbar which is 1 by
64 row vector well in this multiplication get um copied vertically until the two are
of the same shape and then there will be an element wise multiply and so that uh so that the broadcasting is basically
doing the replication and I will end up with the derivatives of DB and diff2 here
so this is the candidate solution let's bring it down here let's uncomment this line where we check
it and let's hope for the best and indeed we see that this is the correct formula next up let's
differentiate here and to be in this so here we have that b and diff is element y squared to create B and F2
so this is a relatively simple derivative because it's a simple element wise operation so it's kind of like the
scalar case and we have that DB and div should be if this is x squared then the
derivative of this is 2x right so it's simply 2 times B and if that's the local
derivative and then times chain Rule and the shape of these is the same they are of the
same shape so times this so that's the backward pass for this variable let me bring that down here
and now we have to be careful because we already calculated dbm depth right so this is just the end of the other uh you
know other Branch coming back to B and diff because B and diff was already back propagated to way over here
from being raw so we now completed the second branch and so that's why I have to do plus equals and if you recall we
had an incorrect derivative for being diff before and I'm hoping that once we append this last missing piece we have
the exact correctness so let's run ambient to be in div now actually shows
the exact correct derivative um so that's comforting okay so let's now back propagate through this line
here um the first thing we do of course is we check the shapes and I wrote them out
here and basically the shape of this is 32 by 64. hpbn is the same shape
but B and mean I is a row Vector 1 by 64. so this minus here will actually do
broadcasting and so we have to be careful with that and as a hint to us again because of The Duality a
broadcasting and the forward pass means a variable reuse and therefore there will be a sum in the backward pass
so let's write out the backward pass here now um back propagate into the hpbn
because this is these are the same shape then the local derivative for each one of the elements here is just one for the
corresponding element in here so basically what this means is that the gradient just simply copies it's just a
variable assignment it's quality so I'm just going to clone this tensor just for safety to create an exact copy of DB and
div and then here to back propagate into this one what I'm inclined to do here is
will basically be uh what is the local derivative well
it's negative torch.1's like of the shape of uh B and diff
right and then times
the um the derivative here dbf
and this here is the back propagation for the replicated B and mean I so I still have to back propagate
through the uh replication in the broadcasting and I do that by doing a sum so I'm going to take this whole
thing and I'm going to do a sum over the zeroth dimension which was the replication
so if you scrutinize this by the way you'll notice that this is the same shape as that and so what I'm doing uh
what I'm doing here doesn't actually make that much sense because it's just a array of ones multiplying DP and diff so
in fact I can just do this um and that is equivalent
so this is the candidate backward pass let me copy it here and then let me
comment out this one and this one enter
and it's wrong damn
actually sorry this is supposed to be wrong and it's supposed to be wrong because
we are back propagating from a b and diff into hpbn and but we're not done
because B and mean I depends on hpbn and there will be a second portion of that
derivative coming from this second Branch so we're not done yet and we expect it to be incorrect so there you
go uh so let's now back propagate from uh B and mean I into hpbn
um and so here again we have to be careful because there's a broadcasting along
um or there's a Sum along the zeroth dimension so this will turn into broadcasting in the backward pass now
and I'm going to go a little bit faster on this line because it is very similar to the line that we had before and
multiplies in the past in fact so the hpbn
will be the gradient will be scaled by 1 over n and then basically this gradient here on
dbn mean I is going to be scaled by 1 over n and then it's going to flow across all the
columns and deposit itself into the hpvn so what we want is this thing scaled by
1 over n only put the constant up front here
um so scale down the gradient and now we need to replicate it across all the um
across all the rows here so we I like to do that by torch.lunslike of basically
um hpbn and I will let the broadcasting do the work of replication
so
like that so this is uh the hppn and hopefully
we can plus equals that
so this here is broadcasting um and then this is the scaling so this
should be current okay so that completes the back propagation of the bathroom layer and we are now
here let's back propagate through the linear layer one here now because everything is getting a little
vertically crazy I copy pasted the line here and let's just back properly through this one line
so first of course we inspect the shapes and we see that this is 32 by 64. MCAT
is 32 by 30. W1 is 30 30 by 64 and B1 is just 64. so
as I mentioned back propagating through linear layers is fairly easy just by matching the shapes so let's do that we
have that dmcat should be um some matrix multiplication of dhbn
with uh W1 and one transpose thrown in there so to make uh MCAT be 32 by 30
I need to take dhpn 32 by 64 and multiply it by w1.
transpose to get the only one I need to end up
with 30 by 64. so to get that I need to take uh MCAT
transpose and multiply that by uh dhpion
and finally to get DB1 this is a addition and we saw that
basically I need to just sum the elements in dhpbn along some Dimension and to make the dimensions work out I
need to Sum along the zeroth axis here to eliminate this Dimension and we do
not keep dims uh so that we want to just get a single one-dimensional lecture of 64.
so these are the claimed derivatives let me put that here and let me
uncomment three lines and cross our fingers everything is great okay so we now
continue almost there we have the derivative of MCAT and we want to derivative we want to back propagate
into m so I again copied this line over here so this is the forward pass and then
this is the shapes so remember that the shape here was 32 by 30 and the original shape of M plus 32 by 3 by 10. so this
layer in the forward pass as you recall did the concatenation of these three 10-dimensional character vectors
and so now we just want to undo that so this is actually relatively straightforward operation because uh the
backward pass of the what is the view view is just a representation of the array it's just a logical form of how
you interpret the array so let's just reinterpret it to be what it was before so in other words the end is not uh 32
by 30. it is basically dmcat but if you view it as the original shape
so just m dot shape uh you can you can pass in tuples into
view and so this should just be okay
we just re-represent that view and then we uncomment this line here and
hopefully yeah so the derivative of M is correct
so in this case we just have to re-represent the shape of those derivatives into the original View so now we are at the final line and the
only thing that's left to back propagate through is this indexing operation here MSC at xB so as I did before I copy
pasted this line here and let's look at the shapes of everything that's involved and remind ourselves how this worked
so m.shape was 32 by 3 by 10. it says 32 examples and then we have
three characters each one of them has a 10 dimensional embedding and this was achieved by taking the
lookup table C which have 27 possible characters each of them 10 dimensional and we
looked up at the rows that were specified inside this tensor xB
so XB is 32 by 3 and it's basically giving us for each example the Identity or the index of which character is part
of that example and so here I'm showing the first five rows of three of this tensor xB
and so we can see that for example here it was the first example in this batch is that the first character and the
first character and the fourth character comes into the neural net and then we want to predict the next
character in a sequence after the character is one one four so basically What's Happening Here is
there are integers inside XB and each one of these integers is specifying which row of C we want to pluck out
right and then we arrange those rows that we've plucked out into 32 by 3 by
10 tensor and we just package them in we just package them into the sensor and now what's happening is that we have
D amp so for every one of these uh basically plucked out rows we have their gradients
now but they're arranged inside this 32 by 3 by 10 tensor so all we have to do now is
we just need to Route this gradient backwards through this assignment so we need to find which row of C that every
one of these um 10 dimensional embeddings come from and then we need to deposit them into DC
so we just need to undo the indexing and of course if any of these rows of C was
used multiple times which almost certainly is the case like the row one and one was used multiple times then we
have to remember that the gradients that arrive there have to add so for each occurrence we have to have
an addition so let's now write this out and I don't actually know if like a much better way
to do this than a for Loop unfortunately in Python um so maybe someone can come up with a vectorized efficient operation but for
now let's just use for loops so let me create a torch.zeros like C to initialize uh just uh 27 by 10
tensor of all zeros and then honestly 4K in range XB dot
shape at zero maybe someone has a better way to do this but for J and range
be that shape at one this is going to iterate over all the
um all the elements of XB all these integers and then let's get the index at this
position so the index is basically x b at KJ
so that an example of that like is 11 or 14 and so on and now in the forward pass we took
and we basically took um the row of C at index and we deposited
it into M at K of J that's what happened that's where they are packaged so now we need to go
backwards and we just need to route DM at the position KJ
we now have these derivatives for each position and it's 10 dimensional
and you just need to go into the correct row of C so DC rather at IX is this but plus
equals because there could be multiple occurrences uh like the same row could have been used many many times and so
all of those derivatives will just go backwards through the indexing and they
will add so this is my candidate solution
let's copy it here let's uncomment this and cross our
fingers hey so that's it we've back propagated through
this entire Beast so there we go totally makes sense


å½“ç„¶å¯ä»¥ï¼æˆ‘å¸®ä½ æŠŠè¿™ä¸€å¤§æ®µå†…å®¹ **è¯¦ç»†æ€»ç»“ + ç¿»è¯‘æˆä¸­æ–‡**ï¼Œé…åˆä½ å‰é¢çš„ç†è§£ä¼šæ›´é¡ºï¼š

---

### ğŸ“Œ è¿™æ˜¯åœ¨è®²ä»€ä¹ˆï¼Ÿ

è¿™ä¸€æ®µæ˜¯ä½œè€…åœ¨å®ç° **BatchNorm å±‚çš„åå‘ä¼ æ’­**ï¼Œé‡Œé¢æ’å…¥äº†ä¸€æ®µ "Bessel æ ¡æ­£ï¼ˆBessel's correctionï¼‰" ç›¸å…³çš„è®²è§£ã€‚ä¸»è¦è®² 2 ä»¶äº‹ï¼š

1ï¸âƒ£ ä¸ºä»€ä¹ˆåœ¨è®¡ç®—æ–¹å·®æ—¶é™¤ä»¥ $n-1$ è€Œä¸æ˜¯ $n$ â€”â€” è¿™ä¸ªå« Bessel æ ¡æ­£ï¼›
2ï¸âƒ£ å¦‚ä½•ä¸€æ­¥æ­¥ç”¨ **æ‰‹åŠ¨åå‘ä¼ æ’­**ï¼Œä»å‰å‘ä¼ æ’­å…¬å¼ã€Œä¸€å±‚ä¸€å±‚åæ¨å›å»ã€ã€‚

---

### ğŸ“• Bessel's Correctionï¼ˆè´å¡å°”æ ¡æ­£ï¼‰

#### â“ ä¸ºä»€ä¹ˆç”¨ $\frac{1}{n-1}$ è€Œä¸æ˜¯ $\frac{1}{n}$ï¼Ÿ

* åœ¨è®¡ç®—æ–¹å·®æ—¶ï¼š

  * ç”¨ $\frac{1}{n}$ å« **æœ‰åä¼°è®¡ï¼ˆbiased estimateï¼‰**ï¼Œå®ƒé€šå¸¸ä½ä¼°æ–¹å·®ã€‚
  * ç”¨ $\frac{1}{n-1}$ å« **æ— åä¼°è®¡ï¼ˆunbiased estimateï¼‰**ï¼Œç»“æœæ›´å‡†ç¡®ã€‚
* ä¸ºä»€ä¹ˆå« Bessel æ ¡æ­£ï¼Ÿ

  * å› ä¸ºé™¤ä»¥ $n-1$ å®é™…ä¸Šæ˜¯å¯¹æ ·æœ¬æ–¹å·®çš„æœŸæœ›åšäº†ã€Œä¿®æ­£ã€ï¼Œå½“æ ·æœ¬é‡è¾ƒå°æ—¶æ›´å‡†ç¡®ã€‚
* ä½œè€…è¯´ï¼š

  * è®ºæ–‡é‡Œè®­ç»ƒæ—¶ç”¨çš„æ˜¯ $1/n$ï¼ˆæœ‰åï¼‰ï¼›
  * æ¨ç†æ—¶ï¼ˆinferenceï¼‰ç”¨çš„æ˜¯ $1/(n-1)$ï¼ˆæ— åï¼‰ï¼›
  * è®­ç»ƒ/æ¨ç†ä¹‹é—´äº§ç”Ÿäº†ã€Œä¸ä¸€è‡´ã€ï¼
    ä»–è®¤ä¸ºè¿™æ ·è®¾è®¡ä¸å¤ªåˆç†ï¼Œåº”è¯¥è®­ç»ƒ + æ¨ç†éƒ½ç»Ÿä¸€ç”¨ $1/(n-1)$ æ›´å¥½ã€‚

---

### ğŸ›  åå‘ä¼ æ’­çš„å…·ä½“å®ç°

#### ğŸ” 1. çœ‹ shapeï¼ˆå½¢çŠ¶ï¼‰

* **bndiff2.shape** æ˜¯ `(32, 64)`
* **bnvar.shape** æ˜¯ `(1, 64)`
* è¯´æ˜åœ¨å‰å‘ä¼ æ’­ä¸­åšäº†ã€Œåˆ—æ±‚å’Œã€ï¼Œæ‰€ä»¥åœ¨åå‘ä¼ æ’­æ—¶è¦åšã€Œè¡Œå¹¿æ’­ã€ã€‚

#### ğŸ” 2. å¾®åˆ†ï¼ˆæ±‚å¯¼ï¼‰

##### ä¾‹å­ï¼ˆå½¢è±¡è®²è§£ï¼‰ï¼š

```text
å‰å‘ä¼ æ’­æ—¶ï¼š
  bnvar = (1 / (n-1)) * sum(bndiff2, dim=0)

åå‘ä¼ æ’­æ—¶ï¼š
  dbndiff2 = (1 / (n-1)) * dbnvar  # ç›´æ¥ä¹˜å¸¸æ•°å³å¯

  å½¢çŠ¶å¹¿æ’­ â†’ æŠŠ (1, 64) å¤åˆ¶ 32 æ¬¡ï¼Œå¾—åˆ° (32, 64)
```

ä»£ç å®ç°ï¼š

```python
dbndiff2 = (1.0 / (n-1)) * torch.ones_like(bndiff2) * dbnvar
```

#### ğŸ” 3. bndiff â†’ bndiff2 çš„å¯¼æ•°

```text
bndiff2 = bndiff ** 2

åå‘ä¼ æ’­æ—¶ï¼š
  dbndiff += 2 * bndiff * dbndiff2
```

å°±æ˜¯ï¼š

```python
dbndiff += (2 * bndiff) * dbndiff2
```

#### ğŸ” 4. bndiff â†’ hprebn

```text
hprebn = bndiff + bnmeani

åå‘ä¼ æ’­æ—¶ï¼š
  dhprebn = dbndiff
```

ç®€å•æ¥è¯´ï¼Œå› ä¸º bndiff å’Œ hprebn å½¢çŠ¶ä¸€æ ·ï¼Œå±€éƒ¨å¯¼æ•°æ˜¯ 1ï¼Œç›´æ¥ copyã€‚

---

### ğŸ“ æ€»ç»“ï¼š

åå‘ä¼ æ’­çš„æ ¸å¿ƒæ€æƒ³ï¼š

| å‰å‘æ“ä½œ    | åå‘æ“ä½œ          |
| ------- | ------------- |
| **æ±‚å’Œ**  | å¹¿æ’­ï¼ˆbroadcastï¼‰ |
| **å¹¿æ’­**  | æ±‚å’Œï¼ˆsumï¼‰       |
| **ä¹˜å¸¸æ•°** | ä¹˜å¸¸æ•°           |
| **å¹³æ–¹**  | 2 \* åŸå€¼       |

---

### âš ï¸ ä¸€ä¸ªå°å‘ï¼š

å› ä¸º **bndiff** åœ¨ç½‘ç»œä¸­è¢«ã€Œç”¨è¿‡ä¸¤æ¬¡ã€ï¼š

1ï¸âƒ£ ç”¨åœ¨è®¡ç®—æ–¹å·® $bndiff2$ï¼›
2ï¸âƒ£ ç”¨åœ¨æ ‡å‡†åŒ– $bnraw$ã€‚

æ‰€ä»¥åå‘ä¼ æ’­æ—¶ï¼Œéœ€è¦ç”¨ `+=`ï¼ˆåŠ æ³•ï¼‰ï¼ŒæŠŠæ¥è‡ªä¸¤ä¸ªè·¯å¾„çš„æ¢¯åº¦ç´¯åŠ èµ·æ¥ï¼Œæœ€ç»ˆæ‰æ˜¯å®Œæ•´æ­£ç¡®çš„æ¢¯åº¦ï¼

---

### æœ€åè®²çš„ä¸€æ®µï¼š

* æœ€åè®²äº†æ€ä¹ˆåä¼ åˆ° **embedding C** è¿™ä¸ª lookup è¡¨ï¼ˆ`C[Xb]`ï¼‰ï¼š

  * å› ä¸º lookup è¿‡ç¨‹æ˜¯ã€ŒæŸ¥è¡¨ + æŠ½å– rowã€ï¼Œåå‘ä¼ æ’­å°±éœ€è¦ã€Œæ‰¾åˆ°è¿™äº›è¡Œ + ç´¯åŠ å›å»ã€ï¼›
  * å› ä¸ºä¸€ä¸ªå­—ç¬¦ embedding å¯èƒ½åœ¨å¤šä¸ªä½ç½®è¢«ç”¨åˆ°ï¼Œæ¢¯åº¦éœ€è¦ç›¸åŠ ï¼›

```python
for k in range(Xb.shape[0]):
    for j in range(Xb.shape[1]):
        ix = Xb[k, j]
        dC[ix] += demb[k, j]
```

å°±æ˜¯è¿™ä¹ˆå®ç°çš„ï¼

---

### ğŸ’¡ æ€»ç»“ä¸€å¥è¯ï¼š

è¿™æ®µè®²çš„æ˜¯ï¼š

* Bessel æ ¡æ­£ä¸ºä»€ä¹ˆç”¨ $n-1$ï¼›
* å¦‚ä½•æ‰‹åŠ¨å®ç° **BatchNorm åå‘ä¼ æ’­**ï¼ŒåŒ…æ‹¬å¹¿æ’­æ±‚å’Œçš„ç»†èŠ‚ï¼›
* å¤šè·¯å¾„çš„æ¢¯åº¦å¦‚ä½•åˆå¹¶ï¼ˆ`+=`ï¼‰ï¼›
* æœ€å embedding æŸ¥è¡¨å±‚ï¼ˆ`C[Xb]`ï¼‰å¦‚ä½•åšåå‘ä¼ æ’­ã€‚

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ æ•´ç†å‡ºã€Œä¸€å¼ å›¾ã€æŠŠæ•´ä¸ª batchnorm çš„å‰å‘ + åå‘æ­¥éª¤ç”»å‡ºæ¥ï¼Œå¸®ä½ æ›´å¥½è®°å¿†ï¼Œæƒ³è¦å—ï¼ŸğŸŒŸ


# exercise 2: cross entropy loss backward pass

so now we come to exercise two it basically turns out that in this first exercise we were doing way too much work
uh we were back propagating way too much and it was all good practice and so on but it's not what you would do in
practice and the reason for that is for example here I separated out this loss calculation over multiple lines and I
broke it up all all to like its smallest atomic pieces and we back propagated through all of those individually
but it turns out that if you just look at the mathematical expression for the loss um then actually you can do the
differentiation on pen and paper and a lot of terms cancel and simplify and the mathematical expression you end up with
can be significantly shorter and easier to implement than back propagating through all the little pieces of
everything you've done so before we had this complicated forward paths going from logits to the
loss but in pytorch everything can just be glued together into a single call at that cross entropy you just pass in
logits and the labels and you get the exact same loss as I verify here so our previous loss and the fast loss coming
from the chunk of operations as a single mathematical expression is the same but
it's much much faster in a forward pass it's also much much faster in backward pass and the reason for that is if you
just look at the mathematical form of this and differentiate again you will end up with a very small and short expression so that's what we want to do
here we want to in a single operation or in a single go or like very quickly go
directly to delojits and we need to implement the logits as a function of logits and yb's
but it will be significantly shorter than whatever we did here where to get to deluggets we had to go all the way
here so all of this work can be skipped in a much much simpler mathematical
expression that you can Implement here so you can give it a shot yourself basically look at what exactly is the
mathematical expression of loss and differentiate with respect to the logits so let me show you a hint you can of
course try it fully yourself but if not I can give you some hint of how to get started mathematically
so basically What's Happening Here is we have logits then there's a softmax that takes the logits and gives you
probabilities then we are using the identity of the correct next character to pluck out a row of probabilities take
the negative log of it to get our negative block probability and then we average up all the log probabilities or
negative block probabilities to get our loss so basically what we have is for a single individual example rather we have
that loss is equal to negative log probability uh where P here is kind of
like thought of as a vector of all the probabilities so at the Y position where Y is the label
and we have that P here of course is the softmax so the ith component of P of
this probability Vector is just the softmax function so raising all the logits uh basically to the power of E
and normalizing so everything comes to 1. now if you write out P of Y here you can
just write out the soft Max and then basically what we're interested in is we're interested in the derivative of the loss with respect to the I logit
and so basically it's a d by DLI of this expression here where we have L indexed with the
specific label Y and on the bottom we have a sum over J of e to the L J and the negative block of all that so
potentially give it a shot pen and paper and see if you can actually derive the expression for the loss by DLI and then
we're going to implement it here okay so I'm going to give away the result here so this is some of the math I did to
derive the gradients analytically and so we see here that I'm just applying the
rules of calculus from your first or second year of bachelor's degree if you took it and we see that the expression
is actually simplify quite a bit you have to separate out the analysis in the case where the ith index that you're
interested in inside logits is either equal to the label or it's not equal to the label and then the expression
simplify and cancel in a slightly different way and what we end up with is something very very simple
and we either end up with basically pirai where p is again this Vector of
probabilities after a soft Max or P at I minus 1 where we just simply subtract a
one but in any case we just need to calculate the soft Max p e and then in the correct Dimension we need to
subtract one and that's the gradient the form that it takes analytically so let's implement this basically and we have to
keep in mind that this is only done for a single example but here we are working with batches of examples
so we have to be careful of that and then the loss for a batch is the average
loss over all the examples so in other words is the example for all the individual examples is the loss for each
individual example summed up and then divided by n and we have to back propagate through that as well and be
careful with it so deluggets is going to be of that soft Max
uh pytorch has a softmax function that you can call and we want to apply the softmax on the logits and we want to go
in the dimension that is one so basically we want to do the softmax along the rows of these logits
then at the correct positions we need to subtract a 1. so delugits at iterating
over all the rows and indexing into the columns provided by the correct labels inside YB
we need to subtract one and then finally it's the average loss
that is the loss and in the average there's a one over n of all the losses added up and so we need to also
propagate through that division so the gradient has to be scaled down by by n as well because of the mean
but this otherwise should be the result so now if we verify this we see that we don't get an exact match
but at the same time the maximum difference from logits from pytorch and
RD logits here is uh on the order of 5e negative 9. so it's a tiny tiny number
so because of floating point wantiness we don't get the exact bitwise result
but we basically get the correct answer approximately now I'd like to pause here briefly
before we move on to the next exercise because I'd like us to get an intuitive sense of what the logits is because it
has a beautiful and very simple explanation honestly um so here I'm taking the logits and I'm
visualizing it and we can see that we have a batch of 32 examples of 27 characters
and what is the logits intuitively right the logits is the probabilities that the
properties Matrix in the forward pass but then here these black squares are the positions of the correct indices
where we subtracted a one and so uh what is this doing right these
are the derivatives on the logits and so let's look at just the first row here
so that's what I'm doing here I'm clocking the probabilities of these logits and then I'm taking just the first row and this is the probability
row and then the logits of the first row and multiplying by n just for us so that
we don't have the scaling by n in here and everything is more interpretable we see that it's exactly equal to the
probability of course but then the position of the correct index has a minus equals one so minus one on that
position and so notice that um if you take Delo Jets at zero and you
sum it it actually sums to zero and so you should think of these uh gradients here
at each cell as like a force um we are going to be basically pulling
down on the probabilities of the incorrect characters and we're going to be pulling up on the probability at the
correct index and that's what's basically happening in each row and thus
the amount of push and pull is exactly equalized because the sum is zero so the
amount to which we pull down in the probabilities and the demand that we push up on the probability of the correct character is equal
so sort of the the repulsion and the attraction are equal and think of the neural app now as a like a massive uh
pulley system or something like that we're up here on top of the logits and we're pulling up we're pulling down the
properties of Incorrect and pulling up the property of the correct and in this complicated pulley system because everything is mathematically uh just
determined just think of it as sort of like this tension translating to this complicating pulling mechanism and then
eventually we get a tug on the weights and the biases and basically in each update we just kind of like tug in the
direction that we like for each of these elements and the parameters are slowly given in to the tug and that's what
training in neural net kind of like looks like on a high level and so I think the the forces of push
and pull in these gradients are actually uh very intuitive here we're pushing and pulling on the correct answer and the
incorrect answers and the amount of force that we're applying is actually proportional to uh the probabilities
that came out in the forward pass and so for example if our probabilities came out exactly correct so they would
have had zero everywhere except for one at the correct uh position then the the
logits would be all a row of zeros for that example there would be no push and pull so the amount to which your
prediction is incorrect is exactly the amount by which you're going to get a pull or a push in that dimension
so if you have for example a very confidently mispredicted element here then
um what's going to happen is that element is going to be pulled down very heavily and the correct answer is going
to be pulled up to the same amount and the other characters are not going to be influenced too much
so the amounts to which you mispredict is then proportional to the strength of the pole and that's happening
independently in all the dimensions of this of this tensor and it's sort of very intuitive and varies to think
through and that's basically the magic of the cross-entropy loss and what it's doing dynamically in the backward pass
of the neural net so now we get to exercise number three which is a very fun exercise


å½“ç„¶å¯ä»¥ï¼Œè¿™æ®µæ˜¯ã€Œexercise 2: cross entropy loss backward passã€çš„è®²è§£ï¼Œæˆ‘æ¥å¸®ä½  **æ€»ç»“æˆä¸­æ–‡**ï¼Œå¹¶ä¸”æ‹†è§£æ¯ä¸ªé‡ç‚¹ï¼š

---

### ğŸŒŸ å¤§æ„æ€»ç»“

* ç¬¬ä¸€èŠ‚ç»ƒä¹ ï¼ˆexercise 1ï¼‰è™½ç„¶æˆ‘ä»¬æ‰‹åŠ¨ä¸€æ­¥æ­¥åå‘ä¼ æ’­ï¼ˆbackpropï¼‰äº†æ•´ä¸ªè®¡ç®—å›¾ï¼Œå¾ˆæœ‰ç»ƒä¹ ä»·å€¼ï¼›
* ä½†æ˜¯å¯¹äº **cross entropy loss** è¿™ä¸€å—ï¼Œå…¶å®æ²¡å¿…è¦æ‹†æˆé‚£ä¹ˆå¤šå°æ­¥éª¤ï¼Œæ‰‹ç®—ä¸€éæ•°å­¦è¡¨è¾¾å¼åï¼Œå‘ç°å¯ä»¥ç›´æ¥å†™æˆä¸€ä¸ªç®€æ´çš„æ¢¯åº¦è¡¨è¾¾å¼ â€”â€” é€Ÿåº¦å¿«ï¼Œä»£ç ç®€å•ï¼Œå®é™…è®­ç»ƒä¹Ÿæ›´é«˜æ•ˆï¼

---

### ğŸ“– ä»€ä¹ˆæ˜¯ cross entropy lossï¼Ÿ

å¤§è‡´æµç¨‹ï¼š

```text
logits â†’ softmax â†’ probs â†’ logprobs â†’ å–æ­£ç¡®ç±»åˆ« logprob â†’ å¹³å‡åæ±‚ loss
```

å³ï¼š

```text
loss = -log(p_y) = -log(softmax(logits)[y])
```

---

### âœï¸ ç›®æ ‡

æˆ‘ä»¬æƒ³æ±‚ï¼š

```text
d(loss) / d(logits)
```

---

### ğŸ“• æ•°å­¦æ¨å¯¼è¿‡ç¨‹

ä½œè€…ç»™å‡ºæç¤ºï¼š

* logits å…ˆç»è¿‡ softmax å¾—åˆ° probsï¼›
* å–å‡ºæ­£ç¡®ç±»åˆ« y ä½ç½®çš„æ¦‚ç‡ï¼›
* å–è´Ÿå¯¹æ•°ä½œä¸º lossï¼›
* è¿™å°±æ˜¯ classic çš„ã€Œsoftmax + cross entropyã€ç»„åˆã€‚

é€šè¿‡ **é“¾å¼æ³•åˆ™** è®¡ç®—ï¼š

```text
âˆ‚loss / âˆ‚logits[i]
```

åˆ†ä¸¤ç§æƒ…å†µï¼š

1ï¸âƒ£ å½“ i == yï¼ˆæ­£ç¡®ç±»åˆ«ä½ç½®ï¼‰ï¼š

```text
âˆ‚loss / âˆ‚logits[i] = p_i - 1
```

2ï¸âƒ£ å½“ i â‰  yï¼ˆå…¶ä»–ç±»åˆ«ï¼‰ï¼š

```text
âˆ‚loss / âˆ‚logits[i] = p_i
```

éå¸¸ç®€å•ï¼æ ¸å¿ƒå°±æ˜¯ã€Œsoftmax - one hotã€ã€‚

---

### ğŸš€ ä»£ç å®ç°

ç›´æ¥å†™å‡º logits çš„æ¢¯åº¦ï¼š

```python
dlogits = F.softmax(logits, dim=1)
dlogits[range(n), Yb] -= 1
dlogits /= n
```

---

### ğŸ§® ä¸¾ä¾‹

å‡è®¾ logits æ˜¯ 1 è¡Œï¼š

```text
logits = [3, 1, -2] â†’ softmax â†’ probs â‰ˆ [0.90, 0.09, 0.01]
```

å‡è®¾ Yb = 0ï¼ˆæ­£ç¡®ç±»åˆ«æ˜¯ç¬¬ 0 ä¸ªï¼‰ï¼š

```text
dlogits = [0.90 - 1, 0.09, 0.01] = [-0.10, 0.09, 0.01]
```

---

### ğŸ’¡ ä¸ºä»€ä¹ˆè¿™ä¹ˆå†™æ›´é«˜æ•ˆï¼Ÿ

* å› ä¸ºä¹‹å‰æˆ‘ä»¬ä¸€æ­¥æ­¥æ‹†å¼€ï¼š

```text
logits â†’ norm_logits â†’ counts â†’ counts_sum â†’ probs â†’ logprobs â†’ loss
```

* ç°åœ¨ç›´æ¥ä¸€æ­¥åˆ°ä½ï¼Œé¿å…ä¸å¿…è¦çš„ä¸­é—´å˜é‡ã€‚

---

### ğŸˆ ç›´è§‰ç†è§£ï¼ˆéå¸¸æ£’çš„è®²è§£ï¼ï¼‰

ä½œè€…ç”¨äº†ã€Œ**åŠ›ï¼ˆforceï¼‰æ‹‰æ‰¯**ã€çš„ç±»æ¯”ï¼š

* æ¯ä¸€è¡Œçš„ logitsï¼Œå¯¹åº”ä¸€ä¸ªæ ·æœ¬çš„ 27 ä¸ªç±»åˆ«æ¦‚ç‡ï¼›
* backward æ—¶ï¼š

```text
æ­£ç¡®ç±»åˆ«é‚£ä¸€é¡¹ã€Œå¾€ä¸Šæ‹‰ã€(push up, +1)
é”™è¯¯ç±»åˆ«é‚£ä¸€é¡¹ã€Œå¾€ä¸‹æ‹‰ã€(pull down, -p_i)
```

è€Œä¸”æ•´è¡Œçš„ã€Œæ€»æ‹‰åŠ›ã€æ˜¯ 0ï¼Œå› ä¸ºï¼š

```text
sum(dlogits[i]) = 0
```

---

### ğŸ“ ä¸¾ä¾‹ç†è§£

å‡è®¾ï¼š

```text
softmax â†’ probs = [0.6, 0.3, 0.1]
Yb = 1
```

é‚£ä¹ˆï¼š

```text
dlogits = [0.6, 0.3 - 1, 0.1] = [0.6, -0.7, 0.1]
```

---

### ğŸ æ€»ç»“ä¸€å¥è¯

* è¿™ä¸€èŠ‚çš„ç›®çš„æ˜¯ï¼š

```text
é€šè¿‡æ•°å­¦æ¨å¯¼ï¼Œç›´æ¥å†™å‡º cross entropy loss å¯¹ logits çš„æ¢¯åº¦å…¬å¼ï¼Œæå‡è®¡ç®—æ•ˆç‡ï¼Œä»£ç æ›´ç®€å•ã€‚
```

---

å¦‚æœä½ éœ€è¦çš„è¯ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ æŠŠã€Œæ•´ä¸ª cross entropy backwardã€ç”»ä¸ª **å°è®¡ç®—å›¾**ï¼Œæ›´å®¹æ˜“è®°ä½ï½è¦ä¸è¦ï¼Ÿ ğŸŒŸ



# exercise 3: batch norm layer backward pass

um depending on your definition of fun and we are going to do for batch normalization exactly what we did for cross entropy loss in exercise number
two that is we are going to consider it as a glued single mathematical expression and back propagate through it
in a very efficient manner because we are going to derive a much simpler formula for the backward path of batch
normalization and we're going to do that using pen and paper so previously we've broken up
bastionalization into all of the little intermediate pieces and all the atomic operations inside it and then we back
propagate it through it one by one now we just have a single sort of
forward pass of a batch form and it's all glued together and we see that we get the exact same
result as before now for the backward pass we'd like to also Implement a single formula
basically for back propagating through this entire operation that is the bachelorization so in the forward pass previously we
took hpvn the hidden states of the pre-batch realization and created H
preact which is the hidden States just before the activation in the bachelorization paper each pbn is
X and each preact is y so in the backward pass what we'd like to do now is we have DH preact and we'd
like to produce d h previous and we'd like to do that in a very efficient manner so that's the name of
the game calculate the H previan given DH preact and for the purposes of this
exercise we're going to ignore gamma and beta and their derivatives because they take on a very simple form in a very
similar way to what we did up above so let's calculate this given that right
here so to help you a little bit like I did before I started off the implementation
here on pen and paper and I took two sheets of paper to derive the mathematical formulas for the backward
pass and basically to set up the problem uh just write out the MU Sigma Square
variance x i hat and Y I exactly as in the paper except for the bezel
correction and then in a backward pass we have the derivative of the loss with respect to
all the elements of Y and remember that Y is a vector there's there's multiple numbers here
so we have all the derivatives with respect to all the Y's and then there's a demo and a beta and
this is kind of like the compute graph the gamma and the beta there's the X hat and then the MU and the sigma squared
and the X so we have DL by DYI and we won't DL by d x i for all the I's in
these vectors so this is the compute graph and you have to be careful because I'm trying to
note here that these are vectors so there's many nodes here inside x x hat
and Y but mu and sigma sorry Sigma Square are just individual scalars
single numbers so you have to be careful with that you have to imagine there's multiple nodes here or you're going to
get your math wrong um so as an example I would suggest that you go in the following order one two
three four in terms of the back propagation so back propagating to X hat then into Sigma Square then into mu and
then into X um just like in a topological sort in micrograd we would go from right to left
you're doing the exact same thing except you're doing it with symbols and on a piece of paper
so for number one uh I'm not giving away too much if you want DL of d x i hat
then we just take DL by DYI and multiply it by gamma because of this expression
here where any individual Yi is just gamma times x i hat plus beta so it
doesn't help you too much there but this gives you basically the derivatives for all the X hats and so now try to go
through this computational graph and derive what is DL by D Sigma Square
and then what is DL by B mu and then one is D L by DX eventually so give it a go and I'm going
to be revealing the answer one piece at a time okay so to get DL by D Sigma Square we have to remember again like I
mentioned that there are many excess X hats here and remember that Sigma square is just a
single individual number here so when we look at the expression
for the L by D Sigma Square we have that we have to actually consider all the possible paths that um
we basically have that there's many X hats and they all feed off from they all depend on Sigma Square so Sigma square
has a large fan out there's lots of arrows coming out from Sigma square into all the X hats
and then there's a back propagating signal from each X hat into Sigma square and that's why we actually need to sum
over all those I's from I equal to 1 to m of the DL by d x i hat which is the
global gradient times the x i Hat by D Sigma Square which is the local gradient
of this operation here and then mathematically I'm just working it out here and I'm simplifying and you
get a certain expression for DL by D Sigma square and we're going to be using this expression when we back propagate
into mu and then eventually into X so now let's continue our back propagation into mu so what is D L by D mu now again
be careful that mu influences X hat and X hat is actually lots of values so for
example if our mini batch size is 32 as it is in our example that we were working on then this is 32 numbers and
32 arrows going back to mu and then mu going to Sigma square is just a single Arrow because Sigma square is a scalar
so in total there are 33 arrows emanating from you and then all of them
have gradients coming into mu and they all need to be summed up and so that's why when we look at the
expression for DL by D mu I am summing up over all the gradients of DL by d x i
hat times the x i Hat by being mu uh so that's the that's this arrow and
that's 32 arrows here and then plus the one Arrow from here which is the L by the sigma Square Times the sigma squared
by D mu so now we have to work out that expression and let me just reveal the rest of it
uh simplifying here is not complicated the first term and you just get an expression here
for the second term though there's something really interesting that happens when we look at the sigma squared by D
mu and we simplify at one point if we assume that in a
special case where mu is actually the average of X I's as it is in this case
then if we plug that in then actually the gradient vanishes and becomes exactly zero and that makes the entire
second term cancel and so these uh if you just have a mathematical expression like this and
you look at D Sigma Square by D mu you would get some mathematical formula for
how mu impacts Sigma Square but if it is the special case that Nu is actually equal to the average as it is
in the case of pastoralization that gradient will actually vanish and become zero so the whole term cancels and we
just get a fairly straightforward expression here for DL by D mu okay and now we get to the craziest part which is
uh deriving DL by dxi which is ultimately what we're after now let's count
first of all how many numbers are there inside X as I mentioned there are 32 numbers there are 32 Little X I's and
let's count the number of arrows emanating from each x i there's an arrow going to Mu an arrow
going to Sigma Square and then there's an arrow going to X hat but this Arrow here let's scrutinize
that a little bit each x i hat is just a function of x i and all the other scalars so x i hat
only depends on x i and none of the other X's and so therefore there are actually in
this single Arrow there are 32 arrows but those 32 arrows are going exactly parallel they don't interfere and
they're just going parallel between x and x hat you can look at it that way and so how many arrows are emanating
from each x i there are three arrows mu Sigma squared and the associated X hat
and so in back propagation we now need to apply the chain rule and we need to add up those three contributions
so here's what that looks like if I just write that out we have uh we're going through we're
chaining through mu Sigma square and through X hat and those three terms are just here
now we already have three of these we have d l by d x i hat
we have DL by D mu which we derived here and we have DL by D Sigma Square which we derived here but we need three other
terms here the this one this one and this one so I invite you to try to derive them it's
not that complicated you're just looking at these Expressions here and differentiating with respect to x i
so give it a shot but here's the result or at least what I got
um yeah I'm just I'm just differentiating with respect to x i for all these expressions and honestly I don't think
there's anything too tricky here it's basic calculus now it gets a little bit more tricky is
we are now going to plug everything together so all of these terms multiplied with all of these terms and
add it up according to this formula and that gets a little bit hairy so what ends up happening is
uh you get a large expression and the thing to be very careful with here of course
is we are working with a DL by dxi for specific I here but when we are plugging
in some of these terms like say um this term here deal by D signal squared
you see how the L by D Sigma squared I end up with an expression and I'm iterating over little I's here but I
can't use I as the variable when I plug in here because this is a different I from this eye
this I here is just a place or like a local variable for for a for Loop in here so here when I plug that in you
notice that I rename the I to a j because I need to make sure that this J is not that this J is not this I this J
is like like a little local iterator over 32 terms and so you have to be careful with that when you're plugging
in the expressions from here to here you may have to rename eyes into J's and you have to be very careful what is actually
an I with respect to the L by t x i so some of these are J's some of these
are I's and then we simplify this expression and I guess like the big thing to notice
here is a bunch of terms just kind of come out to the front and you can refactor them there's a sigma squared plus Epsilon raised to the power of
negative three over two uh this Sigma squared plus Epsilon can be actually separated out into three terms each of
them are Sigma squared plus Epsilon to the negative one over two so the three of them multiplied is equal to this and
then those three terms can go different places because of the multiplication so one of them actually comes out to the
front and will end up here outside one of them joins up with this term and one
of them joins up with this other term and then when you simplify the expression you'll notice that some of
these terms that are coming out are just the x i hats so you can simplify just by rewriting
that and what we end up with at the end is a fairly simple mathematical expression over here that I cannot simplify further
but basically you'll notice that it only uses the stuff we have and it derives the thing we need so we have the L by d
y for all the I's and those are used plenty of times here and also in
addition what we're using is these x i hats and XJ hats and they just come from the forward pass
and otherwise this is a simple expression and it gives us DL by d x i for all the I's and that's ultimately
what we're interested in so that's the end of Bachelor backward pass analytically let's now implement
this final result okay so I implemented the expression into a single line of code here and you
can see that the max diff is Tiny so this is the correct implementation of this formula now I'll just uh
basically tell you that getting this formula here from this mathematical expression was not trivial and there's a
lot going on packed into this one formula and this is a whole exercise by itself because you have to consider the
fact that this formula here is just for a single neuron and a batch of 32 examples but what I'm doing here is I'm
actually we actually have 64 neurons and so this expression has to in parallel evaluate the bathroom backward pass for
all of those 64 neurons in parallel independently so this has to happen basically in every single
um column of the inputs here and in addition to that you see how
there are a bunch of sums here and we need to make sure that when I do those sums that they broadcast correctly onto
everything else that's here and so getting this expression is just like highly non-trivial and I invite you
to basically look through it and step through it and it's a whole exercise to make sure that this this checks out but
once all the shapes are green and once you convince yourself that it's correct you can also verify that Patrick's gets
the exact same answer as well and so that gives you a lot of peace of mind that this mathematical formula is
correctly implemented here and broadcasted correctly and replicated in parallel for all of the 64 neurons
inside this bastrum layer okay and finally exercise number four asks you to

å½“ç„¶å¯ä»¥ï¼
è¿™ä¸€æ®µæ˜¯ **Exercise 3: batch norm layer backward pass** çš„è®²è§£ï¼Œå†…å®¹æŒºå¯†ï¼Œä¸‹é¢æˆ‘å¸®ä½ è¯¦ç»†æ€»ç»“æˆä¸­æ–‡ï¼Œå¹¶ä¸”æ‹†é‡ç‚¹æ–¹ä¾¿ä½ ç†è§£ ğŸ‘‡ï¼š

---

## ğŸŒŸ å¤§æ„æ€»ç»“

> ç›®æ ‡ï¼šåƒä¸Šä¸€ä¸ª Cross Entropy ä¸€æ ·ï¼Œæˆ‘ä»¬å¸Œæœ› **BatchNorm å±‚çš„åå‘ä¼ æ’­ï¼ˆbackwardï¼‰ä¹Ÿç”¨æ›´é«˜æ•ˆçš„å…¬å¼** ä¸€æ­¥ç®—å‡ºæ¥ï¼Œè€Œä¸ç”¨åƒ Exercise 1 é‚£æ ·ä¸€æ­¥æ­¥æ‹†è§£ã€‚

---

## âœï¸ ä¸ºä»€ä¹ˆè¦ç®€åŒ–ï¼Ÿ

* ä¸Šæ¬¡æˆ‘ä»¬åšäº†å®Œæ•´æ‹†è§£ç‰ˆï¼ˆatomic operationsï¼‰ï¼Œæ¯”å¦‚ï¼š

```text
å‡å€¼ -> æ–¹å·® -> æ ‡å‡†åŒ– -> Î³, Î² -> æ¿€æ´»
```

* æ¯ä¸€å°æ­¥éƒ½åšåå‘ä¼ æ’­ï¼Œç´¯åˆæ…¢ï¼Œå®é™…è®­ç»ƒæ—¶ä¸ä¼šè¿™ä¹ˆåšã€‚

* æˆ‘ä»¬å¸Œæœ›é€šè¿‡ **æ‰‹ç®—ï¼ˆpen & paperï¼‰æ¨å¯¼å‡ºä¸€ä¸ªæ€»çš„æ•°å­¦è¡¨è¾¾å¼**ï¼Œä¸€è¡Œä»£ç å°±èƒ½æŠŠ d(hprebn) ç®—å‡ºæ¥ï¼Œé€Ÿåº¦æ›´å¿«ï¼

---

## ğŸ“ æ€ä¹ˆæ¨å¯¼ï¼Ÿ

* **å‰å‘æµç¨‹**ï¼š

```text
è¾“å…¥ x â†’ BatchNorm â†’ è¾“å‡º y
```

è®ºæ–‡é‡Œå¯¹åº”çš„å˜é‡ï¼š

```text
x_i â†’ Î¼ â†’ ÏƒÂ² â†’ xÌ‚_i â†’ Î³xÌ‚_i + Î² â†’ y_i
```

* åå‘ä¼ æ’­ç›®æ ‡ï¼š

```text
å·²çŸ¥ dL/dy_iï¼Œæ±‚ dL/dx_i
```

---

## ğŸ“ æ¨å¯¼é¡ºåºå»ºè®®

è®²è§£é‡Œå»ºè®®ä½ æŒ‰è¿™ä¸ªé¡ºåºæ¨å¯¼ï¼š

1ï¸âƒ£ dL / dxÌ‚\_i ï¼ˆæ ‡å‡†åŒ–åçš„ xÌ‚\_iï¼‰
2ï¸âƒ£ dL / dÏƒÂ²
3ï¸âƒ£ dL / dÎ¼
4ï¸âƒ£ dL / dx\_i

**ä¸ºä»€ä¹ˆï¼Ÿ**

* è¿™æ˜¯ä»ã€Œè¾“å‡ºå¾€è¾“å…¥ã€å›ä¼ çš„æ­£ç¡®æ‹“æ‰‘é¡ºåºï¼ˆmicrograd ä¹Ÿæè¿‡ï¼‰ã€‚

---

## ğŸ“ å…³é”®æ¨å¯¼è¦ç‚¹

### ğŸ§® dL / dxÌ‚\_i

è¿™ä¸ªå¾ˆç®€å•ï¼Œå› ä¸ºï¼š

```text
y_i = Î³ * xÌ‚_i + Î²
```

æ‰€ä»¥ï¼š

```text
dL/dxÌ‚_i = dL/dy_i * Î³
```

---

### ğŸ§® dL / dÏƒÂ²

å› ä¸º ÏƒÂ² æ˜¯æ ‡é‡ï¼Œè€Œæ¯ä¸ª xÌ‚\_i éƒ½ä¾èµ– ÏƒÂ²ï¼Œæœ‰ã€Œæ‰‡å‡ºã€å…³ç³»ï¼š

```text
ÏƒÂ² â†’ xÌ‚_1
ÏƒÂ² â†’ xÌ‚_2
...
ÏƒÂ² â†’ xÌ‚_m
```

æ‰€ä»¥éœ€è¦ **å¯¹æ‰€æœ‰ i ç´¯åŠ ï¼ˆsum over iï¼‰**ã€‚

---

### ğŸ§® dL / dÎ¼

Î¼ ä¹Ÿæ‰‡å‡ºåˆ°å¤šä¸ª xÌ‚\_iï¼Œå…¬å¼é‡Œå‘ç°ä¸€ä¸ªã€Œæœ‰è¶£ç°è±¡ã€ï¼š

* å› ä¸º Î¼ æ˜¯ x\_i çš„å‡å€¼ï¼Œéƒ¨åˆ†æ¢¯åº¦é¡¹å®é™…ä¸Šä¼š **å˜æˆ 0**ï¼Œå¯ä»¥å¤§å¤§ç®€åŒ–è¡¨è¾¾å¼ã€‚

---

### ğŸ§® dL / dx\_i

è¿™ä¸ªæ˜¯æœ€ç»ˆç›®æ ‡ï¼

ä¸€ä¸ª x\_i ä¼šæœ‰ 3 æ¡å›ä¼ è·¯å¾„ï¼š

```text
x_i â†’ Î¼
x_i â†’ ÏƒÂ²
x_i â†’ xÌ‚_i
```

æ‰€ä»¥æœ€ç»ˆçš„ dL/dx\_i æ˜¯ 3 éƒ¨åˆ†ä¹‹å’Œï¼Œæ¯éƒ¨åˆ†éƒ½è¦é“¾å¼æ³•åˆ™ (chain rule)ã€‚

---

## ğŸ–ï¸ æœ€ç»ˆå…¬å¼

æ¨å¯¼å®Œä¹‹åï¼Œå¤æ‚é¡¹ï¼ˆä¾‹å¦‚ sum, broadcastï¼‰å¯ä»¥æ•´ç†ï¼Œä½œè€…æŠŠå®ƒæ•´ç†æˆï¼š

```text
dX = ä¸€è¡Œä»£ç ï¼ŒåŒ…å«ï¼š

- dL/dy_i
- xÌ‚_i
- Î¼, ÏƒÂ², eps
- mini batch size m
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

* è¿™å…¬å¼åŸæœ¬æ˜¯ã€Œ1 ä¸ª neuronã€ï¼Œä½†å®é™…è®­ç»ƒé‡Œ **BatchNorm æœ‰ 64 ä¸ª neuron**ï¼ˆå³ 64 ä¸ª channelï¼‰ï¼Œæ‰€ä»¥ **è¦å¯¹æ¯ä¸ª neuron å¹³è¡Œè®¡ç®—**ã€‚
* è¿˜è¦æ³¨æ„ PyTorch é‡Œçš„ **å¹¿æ’­è§„åˆ™**ï¼Œæ¯”å¦‚ sum è¦æ­£ç¡® broadcast åˆ° (batch\_size, num\_neurons) è¿™ä¸ª shapeã€‚
* å†™æˆä¸€è¡Œåï¼Œè¦æ‰‹åŠ¨æ£€æŸ¥ã€Œshapeã€å¯¹ä¸å¯¹ã€å¹¿æ’­å¯¹ä¸å¯¹ã€‚

---

## ğŸš€ æ€»ç»“ä¸€å¥è¯

> è¿™èŠ‚è¯¾æ˜¯æ‰‹æ¨ **BatchNorm backward çš„æ€»å…¬å¼**ï¼Œæœ€åå†™æˆä¸€è¡Œä»£ç ï¼Œæ—¢å¿«åˆé«˜æ•ˆï¼Œå®æˆ˜è®­ç»ƒä¹Ÿæ›´å¸¸ç”¨ï¼

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘è¿˜å¯ä»¥å¸®ä½ ç”»ä¸€å¼ ï¼š

```text
BatchNorm backward æµç¨‹å›¾
```

æˆ–è€…å¸®ä½ æ€»ç»“å‡ºã€Œæ‰‹æ¨å¸¸ç”¨å…¬å¼ã€æ¸…å•ï¼Œè®­ç»ƒæ—¶å¯ä»¥ç›´æ¥å‚è€ƒã€‚è¦ä¸è¦æˆ‘ç»§ç»­æ•´ç†ä¸€ä¸‹ï¼ŸğŸŒŸ


# exercise 4: putting it all together

put it all together and uh here we have a redefinition of the entire problem so you see that we reinitialize the neural
nut from scratch and everything and then here instead of calling loss that backward we want to have the manual back
propagation here as we derived It Up Above so go up copy paste all the chunks of code that we've already derived put
them here and drive your own gradients and then optimize this neural nut basically using your own gradients all
the way to the calibration of The Bachelor and the evaluation of the loss and I was able to achieve quite a good
loss basically the same loss you would achieve before and that shouldn't be surprising because all we've done is
we've really gotten to Lost That backward and we've pulled out all the code and inserted it here but those gradients
are identical and everything is identical and the results are identical it's just that we have full visibility
on exactly what goes on under the hood I'll plot that backward in this specific case and this is all of our code this is
the full backward pass using basically the simplified backward pass for the cross entropy loss and the mass
generalization so back propagating through cross entropy the second layer the 10 H nonlinearity the batch
normalization uh through the first layer and through the embedding and so you see that this
is only maybe what is this 20 lines of code or something like that and that's what gives us gradients and now we can
potentially erase losses backward so the way I have the code set up is you should be able to run this entire cell once you
fill this in and this will run for only 100 iterations and then break and it breaks because it gives you an
opportunity to check your gradients against pytorch so here our gradients we see are not
exactly equal they are approximately equal and the differences are tiny wanting negative 9 or so and I don't
exactly know where they're coming from to be honest um so once we have some confidence that the gradients are basically correct we
can take out the gradient tracking we can disable this breaking statement
and then we can basically disable lost of backward we don't need it anymore it feels amazing
to say that and then here when we are doing the update we're not going to use P dot grad
this is the old way of pytorch we don't have that anymore because we're not doing backward we are going to use this
update where we you see that I'm iterating over I've arranged the grads to be in the
same order as the parameters and I'm zipping them up the gradients and the parameters into p and grad and then here
I'm going to step with just the grad that we derived manually so the last piece
um is that none of this now requires gradients from pytorch and so one thing
you can do here um is you can do with no grad and offset
this whole code block and really what you're saying is you're telling Pat George that hey I'm not going to call backward on any of this
and this allows pytorch to be a bit more efficient with all of it and then we should be able to just uh
run this and it's running
and you see that losses backward is commented out and we're optimizing
so we're going to leave this run and uh hopefully we get a good result
okay so I allowed the neural net to finish optimization then here I calibrate the bachelor
parameters because I did not keep track of the running mean and very variants in their training Loop
then here I ran the loss and you see that we actually obtained a pretty good loss very similar to what we've achieved
before and then here I'm sampling from the model and we see some of the name like gibberish that we're sort of used to so
basically the model worked and samples uh pretty decent results compared to
what we were used to so everything is the same but of course the big deal is that we did not use lots of backward we
did not use package Auto grad and we estimated our gradients ourselves by hand and so hopefully you're looking at this
the backward pass of this neural net and you're thinking to yourself actually that's not too complicated
um each one of these layers is like three lines of code or something like that and most of it is fairly straightforward
potentially with the notable exception of the batch normalization backward pass otherwise it's pretty good okay and

å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯ **Exercise 4: putting it all together** çš„ä¸­æ–‡è§£é‡Šï¼š

---

### ğŸ¯ è¿™ä¸€èŠ‚çš„ç›®æ ‡

> ç»ƒä¹ æŠŠå‰é¢æ‰€æœ‰çš„æ‰‹å·¥æ¨å¯¼çš„ **åå‘ä¼ æ’­ä»£ç ï¼ˆbackward passï¼‰**ï¼Œæ•´åˆæˆä¸€ä¸ªå®Œæ•´çš„ç¥ç»ç½‘ç»œè®­ç»ƒå¾ªç¯ï¼Œå½»åº•ä¸ç”¨ `loss.backward()` äº†ï¼

---

### ğŸ“ åšæ³•æ­¥éª¤

1ï¸âƒ£ **é‡æ–°åˆå§‹åŒ–æ¨¡å‹**

* é‡æ–°åˆ›å»ºæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼ˆEmbeddingã€Linearå±‚ã€BatchNormå‚æ•°ï¼‰

2ï¸âƒ£ **å®ç°æ‰‹å†™åå‘ä¼ æ’­**

* æŠŠå‰é¢æ¨å¯¼å¥½çš„ä»£ç ï¼ˆexercise 2: cross entropy backwardï¼Œexercise 3: batch norm backwardï¼Œlinear layer backwardï¼‰éƒ½ copy åˆ°è®­ç»ƒå¾ªç¯é‡Œ

3ï¸âƒ£ **å®Œæ•´è®­ç»ƒå¾ªç¯**

* ç”¨æˆ‘ä»¬è‡ªå·±æ‰‹å†™çš„ gradientsï¼ˆæ¢¯åº¦ï¼‰æ¥æ›´æ–°å‚æ•°ï¼ˆå‚æ•° -= å­¦ä¹ ç‡ \* æ¢¯åº¦ï¼‰
* æ•´ä¸ªè¿‡ç¨‹ä¸­ **ä¸å†è°ƒç”¨ loss.backward()**

4ï¸âƒ£ **BatchNorm ç‰¹åˆ«å¤„ç†**

* å› ä¸º BatchNorm æœ‰ running mean / varianceï¼Œéœ€è¦å•ç‹¬åš â€œcalibrateâ€ ä¸€æ­¥ï¼ˆæ ¡å‡†ï¼‰

---

### ğŸ” ç»†èŠ‚è§£é‡Š

#### ä¸ºä»€ä¹ˆè¿™æ ·åšï¼Ÿ

* ä¹‹å‰ Exercise 1 æ˜¯æŠŠæ¯ä¸€å°æ­¥éƒ½æ‹†å¼€ï¼ˆæ…¢ï¼Œä½†ç»ƒä¹ ç”¨ï¼‰
* çœŸå®è®­ç»ƒæ—¶ï¼Œè‚¯å®šè¦åƒ Exercise 4 è¿™æ ·ï¼Œ**æŠŠæ‰€æœ‰ backward å…¬å¼å†™æˆ 20 è¡Œä»¥å†…çš„ä»£ç **ï¼Œæ•ˆç‡å¿«ï¼Œä»£ç ç®€æ´
* ç»ƒä¹ çš„ç›®çš„å°±æ˜¯è®©ä½ å½»åº•ç†è§£å„å±‚çš„ backward æ˜¯æ€ä¹ˆå›äº‹ï¼Œè‡ªå·±å†™å‡º backward

---

#### è®­ç»ƒå¾ªç¯æ€ä¹ˆæ”¹ï¼Ÿ

```python
for iter in range(max_iters):
    # forward pass
    
    # æ‰‹å†™ backward pass
    dlogits = ... 
    dW2 = ...
    ...
    dC = ...
    
    # æ›´æ–°å‚æ•°ï¼ˆä¸ç”¨ .grad äº†ï¼‰
    for p, dp in zip(parameters, grads):
        p += -lr * dp
```

å…³é”®æ˜¯è¿™é‡Œä¸å†æœ‰ï¼š

```python
loss.backward()
p.grad
```

è€Œæ˜¯ **å…¨éƒ¨ç”¨æˆ‘ä»¬è‡ªå·±æ¨å¯¼å‡ºæ¥çš„æ¢¯åº¦æ›´æ–°å‚æ•°**ã€‚

---

#### BatchNorm å°å‘

* å› ä¸º PyTorch ç‰ˆ BatchNorm ä¼šç»´æŠ¤ running mean å’Œ running var
* æˆ‘ä»¬æ‰‹å†™çš„åå‘ä¼ æ’­æ—¶æ²¡æœ‰åŠ¨æ€æ›´æ–°å®ƒ
* è®­ç»ƒå®Œåï¼Œéœ€è¦ **æ‰‹åŠ¨ calibrate** ä¸€ä¸‹ï¼ˆè·‘ä¸€éå…¨é‡æ•°æ®ï¼Œé‡æ–°è®¡ç®—å‡å€¼æ–¹å·®ï¼‰

---

### ğŸ è®­ç»ƒæ•ˆæœå¦‚ä½•ï¼Ÿ

* æ‰‹å†™ backward + æ‰‹å†™æ›´æ–°ï¼Œæœ€åå¾—åˆ°çš„ loss å’Œä¹‹å‰ç”¨ `loss.backward()` æ˜¯ **å·®ä¸å¤šçš„**
* è¯´æ˜æˆ‘ä»¬æ¨å¯¼çš„ backward æ˜¯å¯¹çš„ï¼Œæ•ˆæœæ²¡é—®é¢˜
* æœ€å sampling å‡ºæ¥çš„ "åå­—" ä¹Ÿèƒ½ç”Ÿæˆå¾—ä¸é”™

---

### ğŸš€ æ€»ç»“ä¸€å¥è¯

> Exercise 4 æ˜¯æŠŠæ‰‹å†™ backward pass æ•´åˆæˆä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼Œè·‘å‡ºå’Œè‡ªåŠ¨å¾®åˆ†ä¸€æ ·çš„æ•ˆæœï¼Œè¯æ˜æˆ‘ä»¬å¯¹ backward çœŸæ­£æŒæ¡äº†ï¼Œè¿‡ç¨‹å…¨é€æ˜ï¼Œç†è§£æ›´æ·±äº†ï¼

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥å¸®ä½ ï¼š

1ï¸âƒ£ ç”»ä¸€å¼ å›¾ï¼Œè¯´æ˜ **forward + backward å…¨æµç¨‹**
2ï¸âƒ£ å¸®ä½ æ•´ç†ä¸€ä»½ã€Œæ‰‹å†™ backward æ¨¡æ¿ä»£ç ã€ï¼Œä»¥åå¯ä»¥ç›´æ¥ç”¨

ä½ è¦ä¸è¦æˆ‘ç»§ç»­å¸®ä½ æ•´ç†ï¼ŸğŸŒŸ


# outro

that's everything I wanted to cover for this lecture so hopefully you found this interesting and what I liked about it
honestly is that it gave us a very nice diversity of layers to back propagate through and
um I think it gives a pretty nice and comprehensive sense of how these backward passes are implemented and how they work and you'd be able to derive
them yourself but of course in practice you probably don't want to and you want to use the pythonograd but hopefully you
have some intuition about how gradients flow backwards through the neural net starting at the loss and how they flow
through all the variables and all the intermediate results and if you understood a good chunk of it
and if you have a sense of that then you can count yourself as one of these buff doji's on the left instead of the uh
those on the right here now in the next lecture we're actually going to go to recurrent neural nuts lstms and all the
other variants of RNs and we're going to start to complexify the architecture and start to achieve better uh log
likelihoods and so I'm really looking forward to that and I'll see you then