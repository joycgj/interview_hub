We take the 2-layer MLP (with BatchNorm) from the previous video and backpropagate through it manually without using PyTorch autograd's loss.backward(): through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table. Along the way, we get a strong intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in micrograd. This helps build competence and intuition around how neural nets are optimized and sets you up to more confidently innovate on and debug modern neural networks.

!!!!!!!!!!!!
I recommend you work through the exercise yourself but work with it in tandem and whenever you are stuck unpause the video and see me give away the answer. This video is not super intended to be simply watched. The exercise is here:
https://colab.research.google.com/dri...
!!!!!!!!!!!!

Links:
- makemore on github: https://github.com/karpathy/makemore
- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-t...
- collab notebook: https://colab.research.google.com/dri...
- my website: https://karpathy.ai
- my twitter:   / karpathy  
- our Discord channel:   / discord  

Supplementary links:
- Yes you should understand backprop:   / yes-you-should-understand-backprop  
- BatchNorm paper: https://arxiv.org/abs/1502.03167
- Besselâ€™s Correction: http://math.oxford.emory.edu/site/mat...
- Bengio et al. 2003 MLP LM https://www.jmlr.org/papers/volume3/b... 

Chapters:
```
00:00:00 intro: why you should care & fun history
00:07:26 starter code
00:13:01 exercise 1: backproping the atomic compute graph
01:05:17 brief digression: besselâ€™s correction in batchnorm
01:26:31 exercise 2: cross entropy loss backward pass
01:36:37 exercise 3: batch norm layer backward pass
01:50:02 exercise 4: putting it all together
01:54:24 outro
```


å½“ç„¶å¯ä»¥ï¼Œä¸‹é¢æ˜¯è¿™æ®µè§†é¢‘è¯´æ˜çš„ **ä¸­æ–‡ç¿»è¯‘ç‰ˆ**ï¼š

---

æˆ‘ä»¬ä¼šç”¨ä¸Šä¸€ä¸ªè§†é¢‘é‡Œè®­ç»ƒå¥½çš„ **2 å±‚ MLPï¼ˆå¸¦ BatchNormï¼‰**ï¼Œè¿™æ¬¡ä¸ç”¨ `PyTorch` çš„è‡ªåŠ¨åå‘ä¼ æ’­ï¼ˆ`loss.backward()`ï¼‰ï¼Œè€Œæ˜¯**æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­**ï¼Œä¸€æ­¥æ­¥é€šè¿‡ï¼š

âœ… äº¤å‰ç†µæŸå¤±ï¼ˆcross entropy lossï¼‰
âœ… ç¬¬äºŒä¸ªçº¿æ€§å±‚ï¼ˆlinear layerï¼‰
âœ… Tanh æ¿€æ´»å‡½æ•°
âœ… BatchNorm
âœ… ç¬¬ä¸€ä¸ªçº¿æ€§å±‚
âœ… å­—ç¬¦ embedding table

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬èƒ½å»ºç«‹ **éå¸¸æ‰å®çš„ç›´è§‰**ï¼Œç†è§£æ¢¯åº¦åˆ°åº•æ˜¯æ€ä¹ˆæ²¿ç€è®¡ç®—å›¾æµåŠ¨çš„ï¼Œé‡ç‚¹æ˜¯ **ä» tensor çº§åˆ«** ç†è§£ï¼ˆä¸æ˜¯å•ç‹¬æ ‡é‡çš„å¾®å°ç½‘ç»œäº† micrograd è¿™ç§ï¼‰ï¼Œè¿™æ ·æœ‰åŠ©äºï¼š

âœ… çœŸæ­£ç†è§£ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•è¢«ä¼˜åŒ–çš„
âœ… æ›´æœ‰ä¿¡å¿ƒ debug æˆ–æ”¹è¿›ç°ä»£ç¥ç»ç½‘ç»œæ¶æ„

---

ğŸš¨ğŸš¨ğŸš¨
**å¼ºçƒˆå»ºè®®ä½ åŠ¨æ‰‹åšè¿™ä¸ªç»ƒä¹ ï¼**
å¯ä»¥å’Œè§†é¢‘ä¸€èµ·å¯¹ç€åšï¼Œå¡ä½çš„æ—¶å€™æš‚åœè§†é¢‘çœ‹è®²è§£ã€‚
è¿™ä¸ªè§†é¢‘ **ä¸æ˜¯ç»™ä½ çº¯â€œçœ‹ç€â€å°±è¡Œçš„å†…å®¹**ã€‚
ç»ƒä¹ åœ°å€ ğŸ‘‰
\[[https://colab.research.google.com/drive/](https://colab.research.google.com/drive/)...]

---

ğŸ”— é“¾æ¥ï¼š

* makemore ä»£ç ä»“åº“ï¼š [https://github.com/karpathy/makemore](https://github.com/karpathy/makemore)
* æœ¬è§†é¢‘ç”¨åˆ°çš„ jupyter notebook: [https://github.com/karpathy/nn-zero-to...](https://github.com/karpathy/nn-zero-to...)
* Colab notebook: [https://colab.research.google.com/dri...](https://colab.research.google.com/dri...)
* æˆ‘çš„åšå®¢ç½‘ç«™: [https://karpathy.ai](https://karpathy.ai)
* æˆ‘çš„æ¨ç‰¹: /karpathy
* æˆ‘ä»¬çš„ Discord é¢‘é“: /discord

---

ğŸ“š è¡¥å……èµ„æ–™ï¼š

* ä¸ºä»€ä¹ˆä½ åº”è¯¥ç†è§£åå‘ä¼ æ’­: /yes-you-should-understand-backprop
* BatchNorm åŸè®ºæ–‡: [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)
* Bessel æ ¡æ­£ï¼ˆBatchNorm é‡Œä¼šæåˆ°ï¼‰: [http://math.oxford.emory.edu/site/mat...](http://math.oxford.emory.edu/site/mat...)
* Bengio ç­‰äºº 2003 å¹´ MLP è¯­è¨€æ¨¡å‹è®ºæ–‡: [https://www.jmlr.org/papers/volume3/b...](https://www.jmlr.org/papers/volume3/b...)

---

ğŸ“º è§†é¢‘ç« èŠ‚ï¼š

```
00:00:00 å¼€åœºï¼šä¸ºä»€ä¹ˆè¦å…³å¿ƒåå‘ä¼ æ’­ + æœ‰è¶£çš„å†å²
00:07:26 èµ·å§‹ä»£ç 
00:13:01 ç»ƒä¹  1ï¼šåå‘ä¼ æ’­ â€œæœ€å°è®¡ç®—å›¾â€ (atomic compute graph)
01:05:17 å°æ’æ›²ï¼šBatchNorm é‡Œçš„ Bessel æ ¡æ­£
01:26:31 ç»ƒä¹  2ï¼šäº¤å‰ç†µæŸå¤±çš„ backward pass
01:36:37 ç»ƒä¹  3ï¼šBatchNorm å±‚çš„ backward pass
01:50:02 ç»ƒä¹  4ï¼šæ•´ä½“ä¸²èµ·æ¥ end-to-end backward
01:54:24 æ€»ç»“
```

---

å¦‚æœä½ è¿˜æƒ³è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ æ•´ç†ä¸€ä¸ªï¼š

âœ… æœ¬èŠ‚è¯¾é‡ç‚¹çŸ¥è¯†åˆ—è¡¨
âœ… å­¦åå‘ä¼ æ’­ï¼ˆbackpropï¼‰æ¨èæ€è€ƒé¡ºåº
âœ… å¸¸è§æ–°æ‰‹è¸©å‘æ€»ç»“

è¦ä¸è¦ï¼Ÿ ğŸš€âœ¨
