We dive into some of the internals of MLPs with multiple layers and scrutinize the statistics of the forward pass activations, backward pass gradients, and some of the pitfalls when they are improperly scaled. We also look at the typical diagnostic tools and visualizations you'd want to use to understand the health of your deep network. We learn why training deep neural nets can be fragile and introduce the first modern innovation that made doing so much easier: Batch Normalization. Residual connections and the Adam optimizer remain notable todos for later video.

Links:
- makemore on github: https://github.com/karpathy/makemore
- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-t...
- collab notebook: https://colab.research.google.com/dri...
- my website: https://karpathy.ai
- my twitter:   / karpathy  
- Discord channel:   / discord  

Useful links:
- "Kaiming init" paper: https://arxiv.org/abs/1502.01852
- BatchNorm paper: https://arxiv.org/abs/1502.03167
- Bengio et al. 2003 MLP language model paper (pdf): https://www.jmlr.org/papers/volume3/b...
- Good paper illustrating some of the problems with batchnorm in practice: https://arxiv.org/abs/2105.07576

Exercises:
- E01: I did not get around to seeing what happens when you initialize all weights and biases to zero. Try this and train the neural net. You might think either that 1) the network trains just fine or 2) the network doesn't train at all, but actually it is 3) the network trains but only partially, and achieves a pretty bad final performance. Inspect the gradients and activations to figure out what is happening and why the network is only partially training, and what part is being trained exactly.
- E02: BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be "folded into" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then "fold" the batchnorm gamma/beta into the preceeding Linear layer's W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool.

Chapters:
- 00:00:00 intro
- 00:01:22 starter code
- 00:04:19 fixing the initial loss 
- 00:12:59 fixing the saturated tanh
- 00:27:53 calculating the init scale: â€œKaiming initâ€
- 00:40:40 batch normalization
- 01:03:07 batch normalization: summary
- 01:04:50 real example: resnet50 walkthrough
- 01:14:10 summary of the lecture
- 01:18:35 just kidding: part2: PyTorch-ifying the code
- 01:26:51 viz #1: forward pass activations statistics
- 01:30:54 viz #2: backward pass gradient statistics
- 01:32:07 the fully linear case of no non-linearities
- 01:36:15 viz #3: parameter activation and gradient statistics
- 01:39:55 viz #4: update:data ratio over time
- 01:46:04 bringing back batchnorm, looking at the visualizations
- 01:51:34 summary of the lecture for real this time

å½“ç„¶å¯ä»¥ï¼Œä¸‹é¢æ˜¯è¿™æ®µä»‹ç»çš„ä¸­æ–‡ç¿»è¯‘ï¼š

---

**Building makemore ç¬¬ 3 éƒ¨åˆ†ï¼šæ¿€æ´» & æ¢¯åº¦ï¼ŒBatchNorm**

æˆ‘ä»¬æ·±å…¥æ¢è®¨äº†å¤šå±‚ MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰çš„å†…éƒ¨æœºåˆ¶ï¼Œä»”ç»†åˆ†æäº†å‰å‘ä¼ æ’­ä¸­çš„æ¿€æ´»ç»Ÿè®¡ã€åå‘ä¼ æ’­ä¸­çš„æ¢¯åº¦ç»Ÿè®¡ï¼Œä»¥åŠå½“è¿™äº›å€¼ç¼©æ”¾ä¸å½“æ—¶ä¼šå‡ºç°çš„ä¸€äº›é—®é¢˜ã€‚è§†é¢‘ä¸­è¿˜ä»‹ç»äº†ä½ é€šå¸¸ä¼šç”¨åˆ°çš„è¯Šæ–­å·¥å…·å’Œå¯è§†åŒ–æ–¹æ³•ï¼Œå¸®åŠ©ç†è§£ç¥ç»ç½‘ç»œè®­ç»ƒçš„å¥åº·çŠ¶æ€ã€‚æˆ‘ä»¬äº†è§£åˆ°ä¸ºä»€ä¹ˆè®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œä¼šæ¯”è¾ƒè„†å¼±ï¼Œå¹¶ä»‹ç»äº†ç¬¬ä¸€ä¸ªæå¤§æ”¹å–„è®­ç»ƒè¿‡ç¨‹çš„ç°ä»£æŠ€æœ¯ï¼š**æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰**ã€‚
ï¼ˆæ®‹å·®è¿æ¥ï¼ˆResidual connectionsï¼‰å’Œ Adam ä¼˜åŒ–å™¨å°†åœ¨åç»­è§†é¢‘ä¸­ä»‹ç»ã€‚ï¼‰

**ç›¸å…³é“¾æ¥ï¼š**

* makemore ä»£ç ä»“åº“ï¼š[https://github.com/karpathy/makemore](https://github.com/karpathy/makemore)
* æœ¬è§†é¢‘ç”¨åˆ°çš„ Jupyter notebook: [https://github.com/karpathy/nn-zero-t](https://github.com/karpathy/nn-zero-t)...
* Colab notebook: [https://colab.research.google.com/dri](https://colab.research.google.com/dri)...
* æˆ‘çš„ä¸ªäººç½‘ç«™ï¼š[https://karpathy.ai](https://karpathy.ai)
* æˆ‘çš„ Twitterï¼š / karpathy
* Discord é¢‘é“ï¼š / discord

**å‚è€ƒè®ºæ–‡ï¼š**

* â€œKaiming åˆå§‹åŒ–â€ è®ºæ–‡ï¼š[https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852)
* BatchNorm è®ºæ–‡ï¼š[https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)
* Bengio ç­‰äºº 2003 å¹´ MLP è¯­è¨€æ¨¡å‹è®ºæ–‡ï¼ˆpdfï¼‰ï¼š[https://www.jmlr.org/papers/volume3/b](https://www.jmlr.org/papers/volume3/b)...
* å®é™…åº”ç”¨ä¸­ BatchNorm å­˜åœ¨çš„ä¸€äº›é—®é¢˜ï¼š[https://arxiv.org/abs/2105.07576](https://arxiv.org/abs/2105.07576)

**ç»ƒä¹ é¢˜ï¼š**

* **E01:** å¦‚æœå°†ç¥ç»ç½‘ç»œçš„æ‰€æœ‰æƒé‡å’Œåç½®éƒ½åˆå§‹åŒ–ä¸º 0ï¼Œè®­ç»ƒæ•ˆæœä¼šæ€æ ·ï¼Ÿè¯·å°è¯•è®­ç»ƒç½‘ç»œï¼Œä½ å¯èƒ½ä¼šçŒœæµ‹ 1ï¼‰ç½‘ç»œæ­£å¸¸è®­ç»ƒï¼Œæˆ– 2ï¼‰ç½‘ç»œå®Œå…¨æ— æ³•è®­ç»ƒï¼Œä½†å®é™…ä¸Šæ˜¯ 3ï¼‰ç½‘ç»œéƒ¨åˆ†è®­ç»ƒï¼Œæœ€ç»ˆæ€§èƒ½è¾ƒå·®ã€‚åˆ†ææ¢¯åº¦å’Œæ¿€æ´»ï¼Œç†è§£ä¸ºä½•ä¼šè¿™æ ·ï¼Œå“ªäº›éƒ¨åˆ†è¢«è®­ç»ƒäº†ï¼Œå“ªäº›æ²¡æœ‰ã€‚
* **E02:** BatchNorm ä¸åŒäº LayerNormã€GroupNorm çš„ä¸€ç‚¹æ˜¯ï¼Œåœ¨è®­ç»ƒå®Œæˆåï¼ŒBatchNorm ä¸­çš„ gamma å’Œ beta å‚æ•°å¯ä»¥â€œæŠ˜å â€è¿›å‰ä¸€å±‚ Linear å±‚çš„æƒé‡å’Œåç½®ä¸­ï¼Œè¿™æ ·åœ¨æ¨ç†æ—¶å°±å¯ä»¥ä¸ç”¨å†å•ç‹¬è®¡ç®— BatchNormã€‚è¯•ç€æ„å»ºä¸€ä¸ª 3 å±‚ MLPï¼Œä½¿ç”¨ BatchNormï¼Œè®­ç»ƒå¥½ä¹‹åæŠŠ BatchNorm â€œæŠ˜å â€è¿› Linear å±‚ï¼ŒéªŒè¯å‰å‘ä¼ æ’­æ˜¯å¦ä¸€è‡´ï¼Œä»è€Œè¯æ˜ BatchNorm ä¸»è¦ä½œç”¨åœ¨äºè®­ç»ƒæ—¶çš„ç¨³å®šæ€§ï¼Œè®­ç»ƒåå¯ä»¥ä¸¢å¼ƒï¼ŒæŒºé…·å§ã€‚

**è§†é¢‘ç« èŠ‚ï¼š**
- 00:00:00 ä»‹ç»
- 00:01:22 åˆå§‹ä»£ç 
- 00:04:19 ä¿®æ­£åˆå§‹æŸå¤±
- 00:12:59 ä¿®æ­£ tanh é¥±å’Œé—®é¢˜
- 00:27:53 è®¡ç®—åˆå§‹åŒ–ç¼©æ”¾æ¯”ä¾‹ï¼ˆKaiming initï¼‰
- 00:40:40 æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatchNormï¼‰
- 01:03:07 BatchNorm æ€»ç»“
- 01:04:50 å®ä¾‹ï¼šResNet50 ç»“æ„è§£æ
- 01:14:10 è®²è§£æ€»ç»“
- 01:18:35 å½©è›‹ï¼šå°†ä»£ç æ”¹æˆ PyTorch é£æ ¼
- 01:26:51 å¯è§†åŒ– #1ï¼šå‰å‘ä¼ æ’­æ¿€æ´»çš„ç»Ÿè®¡ä¿¡æ¯
- 01:30:54 å¯è§†åŒ– #2ï¼šåå‘ä¼ æ’­æ¢¯åº¦çš„ç»Ÿè®¡ä¿¡æ¯
- 01:32:07 çº¯çº¿æ€§ï¼ˆæ— æ¿€æ´»å‡½æ•°ï¼‰æƒ…å†µåˆ†æ
- 01:36:15 å¯è§†åŒ– #3ï¼šå‚æ•°æ¿€æ´»å€¼å’Œæ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯
- 01:39:55 å¯è§†åŒ– #4ï¼šæ›´æ–°ä¸æ•°æ®æ¯”ä¾‹éšæ—¶é—´å˜åŒ–
- 01:46:04 é‡æ–°å¼•å…¥ BatchNormï¼Œè§‚å¯Ÿå¯è§†åŒ–æ•ˆæœ
- 01:51:34 è¿™æ¬¡æ˜¯çœŸæ­£çš„è®²è§£æ€»ç»“

---

å¦‚æœä½ è¿˜éœ€è¦ï¼Œæˆ‘å¯ä»¥å¸®ä½ æŠŠé‡ç‚¹å†…å®¹å†åšä¸ª**ç®€æ˜ç‰ˆç¬”è®°**ï¼Œæ–¹ä¾¿å¤ä¹  ğŸ‘Œã€‚è¦ä¸è¦ï¼Ÿ
