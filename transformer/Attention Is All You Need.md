# Attention Is All You Need

## æ‘˜è¦

ä¸»æµçš„åºåˆ—è½¬æ¢æ¨¡å‹ä¸»è¦åŸºäºå¤æ‚çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æˆ–å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸åŒ…æ‹¬ä¸€ä¸ªç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨ã€‚æ€§èƒ½æœ€å¥½çš„æ¨¡å‹è¿˜é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å°†ç¼–ç å™¨å’Œè§£ç å™¨è¿æ¥èµ·æ¥ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°çš„ã€åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç®€å•ç½‘ç»œæ¶æ„â€”â€”Transformerï¼Œå®ƒå®Œå…¨æ‘’å¼ƒäº†å¾ªç¯å’Œå·ç§¯ç»“æ„ã€‚

åœ¨ä¸¤ä¸ªæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç¿»è¯‘è´¨é‡ä¸Šæ›´ä¼˜ï¼ŒåŒæ—¶å…·æœ‰æ›´å¼ºçš„å¹¶è¡Œèƒ½åŠ›ï¼Œè®­ç»ƒæ‰€éœ€æ—¶é—´ä¹Ÿå¤§å¤§å‡å°‘ã€‚åœ¨WMT 2014è‹±å¾·ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å–å¾—äº†28.4çš„BLEUåˆ†æ•°ï¼Œæ¯”ç°æœ‰æœ€å¥½çš„ç»“æœï¼ˆåŒ…æ‹¬æ¨¡å‹é›†æˆï¼‰æé«˜äº†è¶…è¿‡2ä¸ªBLEUåˆ†ã€‚åœ¨WMT 2014è‹±æ³•ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨8å—GPUä¸Šè®­ç»ƒ3.5å¤©åï¼Œè¾¾åˆ°äº†41.8çš„å•æ¨¡å‹BLEUåˆ†æ•°ï¼Œåˆ›ä¸‹äº†æ–°çš„å•æ¨¡å‹æœ€ä½³æˆç»©ï¼Œè€Œè®­ç»ƒæˆæœ¬åªæ˜¯æ–‡çŒ®ä¸­æœ€ä¼˜æ¨¡å‹çš„ä¸€å°éƒ¨åˆ†ã€‚

æˆ‘ä»¬è¿˜å±•ç¤ºäº†Transformeråœ¨å…¶ä»–ä»»åŠ¡ä¸Šçš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œå®ƒåœ¨å¤„ç†è‹±è¯­æˆåˆ†å¥æ³•åˆ†æä»»åŠ¡æ—¶ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œæ— è®ºæ˜¯å¤§è§„æ¨¡è®­ç»ƒæ•°æ®è¿˜æ˜¯æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹éƒ½å–å¾—äº†æˆåŠŸã€‚

> <sup>*</sup>å…±åŒè´¡çŒ®ï¼Œä½œè€…æ’åºéšæœºã€‚Jakob æå‡ºäº†ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ›¿ä»£RNNçš„æƒ³æ³•ï¼Œå¹¶ç‡å…ˆå¼€å±•äº†å¯¹è¿™ä¸€æƒ³æ³•çš„éªŒè¯å·¥ä½œã€‚Ashish ä¸ Illia ä¸€èµ·è®¾è®¡å¹¶å®ç°äº†æœ€åˆçš„Transformeræ¨¡å‹ï¼Œå¹¶åœ¨æœ¬é¡¹å·¥ä½œçš„å„ä¸ªæ–¹é¢éƒ½å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚Noam æå‡ºäº†ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶ã€å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä»¥åŠæ— éœ€å‚æ•°çš„ä½ç½®è¡¨ç¤ºæ–¹æ³•ï¼Œæ˜¯å¦ä¸€ä½å‡ ä¹å‚ä¸æ‰€æœ‰ç»†èŠ‚å·¥ä½œçš„ä½œè€…ã€‚Niki åœ¨æˆ‘ä»¬çš„åˆå§‹ä»£ç åº“å’Œ tensor2tensor ä¸­è®¾è®¡ã€å®ç°ã€è°ƒè¯•å¹¶è¯„ä¼°äº†æ— æ•°æ¨¡å‹å˜ä½“ã€‚Llion ä¹Ÿå°è¯•äº†æ–°é¢–çš„æ¨¡å‹å˜ä½“ï¼Œè´Ÿè´£æœ€åˆçš„ä»£ç åº“å¼€å‘ã€é«˜æ•ˆçš„æ¨ç†å®ç°ä»¥åŠå¯è§†åŒ–å·¥ä½œã€‚Lukasz å’Œ Aidan åˆ™èŠ±è´¹äº†æ— æ•°ä¸ªæ—¥å¤œè®¾è®¡å¹¶å®ç°äº† tensor2tensorï¼Œæ›¿ä»£äº†æˆ‘ä»¬æ—©æœŸçš„ä»£ç åº“ï¼Œæå¤§åœ°æå‡äº†å®éªŒç»“æœå¹¶æ˜¾è‘—åŠ é€Ÿäº†æˆ‘ä»¬çš„ç ”ç©¶è¿›å±•ã€‚
> 
> â€  å·¥ä½œå®Œæˆæ—¶ä¾›èŒäº Google Brainã€‚
> 
> â€¡ å·¥ä½œå®Œæˆæ—¶ä¾›èŒäº Google Researchã€‚
> 
> å‘è¡¨äºç¬¬31å±Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿå¤§ä¼šï¼ˆNIPS 2017ï¼‰ï¼Œåœ°ç‚¹ï¼šç¾å›½åŠ åˆ©ç¦å°¼äºšå·é•¿æ»©ã€‚

## 1 å¼•è¨€

è¿™æ˜¯ã€ŠAttention Is All You Needã€‹è®ºæ–‡çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œå®ƒä¸»è¦è®²äº†å‡ ä¸ªæ ¸å¿ƒç‚¹ï¼š

---

### âœ… **èƒŒæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ**

ä¼ ç»Ÿä¸Šï¼Œå¤„ç†è¯­è¨€ï¼ˆæ¯”å¦‚ç¿»è¯‘ã€è¯­è¨€å»ºæ¨¡ï¼‰çš„æ–¹æ³•ä¸»è¦æ˜¯ç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰å’Œé—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰ã€‚è¿™äº›æ–¹æ³•å·²ç»è¢«å¹¿æ³›è®¤ä¸ºæ˜¯æ•ˆæœæœ€å¥½çš„è§£å†³æ–¹æ¡ˆã€‚å¾ˆå¤šç ”ç©¶ä¹Ÿåœ¨ä¸æ–­æ”¹è¿›è¿™äº›æ¨¡å‹ã€‚

---

### ğŸ¤” **è¿™äº›æ—§æ–¹æ³•æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ**

RNNç±»çš„æ¨¡å‹æœ‰ä¸€ä¸ªæœ¬è´¨çš„é—®é¢˜ï¼š**å®ƒä»¬æ˜¯ä¸€æ­¥ä¸€æ­¥å¤„ç†çš„**ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒåªèƒ½å…ˆå¤„ç†ç¬¬ä¸€ä¸ªè¯ï¼Œå†å¤„ç†ç¬¬äºŒä¸ªè¯ï¼Œç„¶åç¬¬ä¸‰ä¸ªâ€¦â€¦æ‰€ä»¥æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸èƒ½å¹¶è¡Œï¼Œé€Ÿåº¦æ…¢ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥å­å¾ˆé•¿çš„æ—¶å€™ï¼Œæ›´éš¾æï¼Œå› ä¸ºå†…å­˜ä¸å¤Ÿç”¨ï¼Œä¸èƒ½æ‰¹é‡å¤„ç†å¤ªå¤šå¥å­ã€‚

è™½ç„¶æœ‰ä¸€äº›æ–°æŠ€æœ¯ï¼ˆæ¯”å¦‚æŠŠæ¨¡å‹åˆ†æ®µè®¡ç®—æˆ–åªåœ¨æŸäº›æƒ…å†µä¸‹æ¿€æ´»éƒ¨åˆ†æ¨¡å‹ï¼‰å¯ä»¥æé«˜æ•ˆç‡å’Œæ•ˆæœï¼Œä½†**å¿…é¡»æŒ‰é¡ºåºå¤„ç†**è¿™ä¸ªé™åˆ¶è¿˜æ˜¯æ²¡æœ‰è§£å†³ã€‚

---

### ğŸ§  **æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰æ˜¯ä¸ªå¥½ä¸œè¥¿**

æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è®©æ¨¡å‹ç›´æ¥å…³æ³¨è¾“å…¥ä¸­çš„å…³é”®éƒ¨åˆ†ï¼Œä¸ç®¡å®ƒä»¬ç¦»å¾—è¿œä¸è¿œã€‚æ¯”å¦‚ç¿»è¯‘æ—¶ï¼Œå¥å­å¼€å¤´çš„è¯å¯èƒ½è·Ÿå¥å­ç»“å°¾çš„è¯æœ‰å…³ï¼Œæ³¨æ„åŠ›æœºåˆ¶èƒ½æ•æ‰åˆ°è¿™ç§è¿œè·ç¦»çš„å…³ç³»ã€‚

ä½†ä¹‹å‰å¤§éƒ¨åˆ†æ¨¡å‹éƒ½æ˜¯æŠŠæ³¨æ„åŠ›å’ŒRNNä¸€èµ·ç”¨çš„ï¼Œè¿˜æ˜¯å—é™äºRNNçš„â€œé¡ºåºå¤„ç†â€ã€‚

---

### ğŸ’¡ **æˆ‘ä»¬åšäº†å•¥åˆ›æ–°ï¼Ÿ**

è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°æ¨¡å‹å« **Transformer**ã€‚å®ƒ**å®Œå…¨ä¸ç”¨RNNï¼Œä¹Ÿä¸ç”¨å·ç§¯ç¥ç»ç½‘ç»œ**ï¼Œ**åªé æ³¨æ„åŠ›æœºåˆ¶**ï¼

å¥½å¤„æ˜¯ï¼š

* å®ƒå¯ä»¥å¹¶è¡Œå¤„ç†æ‰€æœ‰è¯ï¼Œæ›´å¿«ï¼›
* æ•ˆæœè¿˜æ›´å¥½ï¼›
* æˆ‘ä»¬ç”¨8å—P100æ˜¾å¡è®­ç»ƒ12å°æ—¶ï¼Œå°±åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚

---

### æ€»ç»“ä¸€å¥è¯ï¼š

> ä»¥å‰ç¿»è¯‘æ¨¡å‹éƒ½é ä¸€æ­¥æ­¥æ¥çš„RNNï¼Œæˆ‘ä»¬è¿™æ¬¡å¹²è„†ä¸ç”¨RNNäº†ï¼Œç›´æ¥ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥åšï¼Œè¿˜åšå¾—æ›´å¿«æ›´å¥½ï¼

---




## 7 Conclusion

In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.
For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.
We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.
The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.

**Acknowledgements** We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.

## 7 ç»“è®º

åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Transformerï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„åºåˆ—è½¬æ¢æ¨¡å‹ï¼Œå®ƒç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å–ä»£äº†ç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸­å¸¸è§çš„å¾ªç¯å±‚ã€‚

åœ¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œä¸åŸºäºå¾ªç¯æˆ–å·ç§¯å±‚çš„æ¶æ„ç›¸æ¯”ï¼ŒTransformerçš„è®­ç»ƒé€Ÿåº¦æ˜¾è‘—æ›´å¿«ã€‚åœ¨WMT 2014è‹±å¾·ç¿»è¯‘ä»»åŠ¡å’ŒWMT 2014è‹±æ³•ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬éƒ½å–å¾—äº†æ–°çš„æœ€å…ˆè¿›æˆç»©ã€‚åœ¨è‹±å¾·ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹ç”šè‡³è¶…è¿‡äº†æ­¤å‰æŠ¥é“çš„æ‰€æœ‰æ¨¡å‹é›†æˆç»“æœã€‚

æˆ‘ä»¬å¯¹åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹æœªæ¥å‘å±•æ„Ÿåˆ°éå¸¸å…´å¥‹ï¼Œå¹¶è®¡åˆ’å°†å…¶åº”ç”¨äºæ›´å¤šä»»åŠ¡ã€‚æˆ‘ä»¬è®¡åˆ’å°†Transformeræ‰©å±•åˆ°å¤„ç†è¾“å…¥å’Œè¾“å‡ºæ¨¡æ€ä¸é™äºæ–‡æœ¬çš„é—®é¢˜ï¼Œå¹¶ç ”ç©¶å±€éƒ¨çš„ã€å—é™çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥é«˜æ•ˆå¤„ç†å¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤§è§„æ¨¡è¾“å…¥è¾“å‡ºæ•°æ®ã€‚è®©ç”Ÿæˆè¿‡ç¨‹æ›´å°‘ä¾èµ–é¡ºåºæ€§ä¹Ÿæ˜¯æˆ‘ä»¬çš„ç ”ç©¶ç›®æ ‡ä¹‹ä¸€ã€‚

æˆ‘ä»¬ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹çš„ä»£ç å·²å…¬å¼€ï¼Œåœ°å€ä¸ºï¼šhttps://github.com/tensorflow/tensor2tensorã€‚

**è‡´è°¢** æˆ‘ä»¬æ„Ÿè°¢Nal Kalchbrennerå’ŒStephan Gouwsæå‡ºçš„å®è´µæ„è§ã€ä¿®æ­£å»ºè®®ä»¥åŠç»™äºˆæˆ‘ä»¬çš„å¯å‘ã€‚