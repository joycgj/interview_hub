# Attention Is All You Need

## æ‘˜è¦

ä¸»æµçš„åºåˆ—è½¬æ¢æ¨¡å‹ä¸»è¦åŸºäºå¤æ‚çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æˆ–å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸åŒ…æ‹¬ä¸€ä¸ªç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨ã€‚æ€§èƒ½æœ€å¥½çš„æ¨¡å‹è¿˜é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å°†ç¼–ç å™¨å’Œè§£ç å™¨è¿æ¥èµ·æ¥ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°çš„ã€åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç®€å•ç½‘ç»œæ¶æ„â€”â€”Transformerï¼Œå®ƒå®Œå…¨æ‘’å¼ƒäº†å¾ªç¯å’Œå·ç§¯ç»“æ„ã€‚

åœ¨ä¸¤ä¸ªæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç¿»è¯‘è´¨é‡ä¸Šæ›´ä¼˜ï¼ŒåŒæ—¶å…·æœ‰æ›´å¼ºçš„å¹¶è¡Œèƒ½åŠ›ï¼Œè®­ç»ƒæ‰€éœ€æ—¶é—´ä¹Ÿå¤§å¤§å‡å°‘ã€‚åœ¨WMT 2014è‹±å¾·ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å–å¾—äº†28.4çš„BLEUåˆ†æ•°ï¼Œæ¯”ç°æœ‰æœ€å¥½çš„ç»“æœï¼ˆåŒ…æ‹¬æ¨¡å‹é›†æˆï¼‰æé«˜äº†è¶…è¿‡2ä¸ªBLEUåˆ†ã€‚åœ¨WMT 2014è‹±æ³•ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨8å—GPUä¸Šè®­ç»ƒ3.5å¤©åï¼Œè¾¾åˆ°äº†41.8çš„å•æ¨¡å‹BLEUåˆ†æ•°ï¼Œåˆ›ä¸‹äº†æ–°çš„å•æ¨¡å‹æœ€ä½³æˆç»©ï¼Œè€Œè®­ç»ƒæˆæœ¬åªæ˜¯æ–‡çŒ®ä¸­æœ€ä¼˜æ¨¡å‹çš„ä¸€å°éƒ¨åˆ†ã€‚

æˆ‘ä»¬è¿˜å±•ç¤ºäº†Transformeråœ¨å…¶ä»–ä»»åŠ¡ä¸Šçš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œå®ƒåœ¨å¤„ç†è‹±è¯­æˆåˆ†å¥æ³•åˆ†æä»»åŠ¡æ—¶ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œæ— è®ºæ˜¯å¤§è§„æ¨¡è®­ç»ƒæ•°æ®è¿˜æ˜¯æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹éƒ½å–å¾—äº†æˆåŠŸã€‚

---

<sup>*</sup>å…±åŒè´¡çŒ®ï¼Œä½œè€…æ’åºéšæœºã€‚Jakob æå‡ºäº†ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ›¿ä»£RNNçš„æƒ³æ³•ï¼Œå¹¶ç‡å…ˆå¼€å±•äº†å¯¹è¿™ä¸€æƒ³æ³•çš„éªŒè¯å·¥ä½œã€‚Ashish ä¸ Illia ä¸€èµ·è®¾è®¡å¹¶å®ç°äº†æœ€åˆçš„Transformeræ¨¡å‹ï¼Œå¹¶åœ¨æœ¬é¡¹å·¥ä½œçš„å„ä¸ªæ–¹é¢éƒ½å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚Noam æå‡ºäº†ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶ã€å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä»¥åŠæ— éœ€å‚æ•°çš„ä½ç½®è¡¨ç¤ºæ–¹æ³•ï¼Œæ˜¯å¦ä¸€ä½å‡ ä¹å‚ä¸æ‰€æœ‰ç»†èŠ‚å·¥ä½œçš„ä½œè€…ã€‚Niki åœ¨æˆ‘ä»¬çš„åˆå§‹ä»£ç åº“å’Œ tensor2tensor ä¸­è®¾è®¡ã€å®ç°ã€è°ƒè¯•å¹¶è¯„ä¼°äº†æ— æ•°æ¨¡å‹å˜ä½“ã€‚Llion ä¹Ÿå°è¯•äº†æ–°é¢–çš„æ¨¡å‹å˜ä½“ï¼Œè´Ÿè´£æœ€åˆçš„ä»£ç åº“å¼€å‘ã€é«˜æ•ˆçš„æ¨ç†å®ç°ä»¥åŠå¯è§†åŒ–å·¥ä½œã€‚Lukasz å’Œ Aidan åˆ™èŠ±è´¹äº†æ— æ•°ä¸ªæ—¥å¤œè®¾è®¡å¹¶å®ç°äº† tensor2tensorï¼Œæ›¿ä»£äº†æˆ‘ä»¬æ—©æœŸçš„ä»£ç åº“ï¼Œæå¤§åœ°æå‡äº†å®éªŒç»“æœå¹¶æ˜¾è‘—åŠ é€Ÿäº†æˆ‘ä»¬çš„ç ”ç©¶è¿›å±•ã€‚

â€  å·¥ä½œå®Œæˆæ—¶ä¾›èŒäº Google Brainã€‚

â€¡ å·¥ä½œå®Œæˆæ—¶ä¾›èŒäº Google Researchã€‚

å‘è¡¨äºç¬¬31å±Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿå¤§ä¼šï¼ˆNIPS 2017ï¼‰ï¼Œåœ°ç‚¹ï¼šç¾å›½åŠ åˆ©ç¦å°¼äºšå·é•¿æ»©ã€‚

---

## 1 å¼•è¨€

è¿™æ˜¯ã€ŠAttention Is All You Needã€‹è®ºæ–‡çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œå®ƒä¸»è¦è®²äº†å‡ ä¸ªæ ¸å¿ƒç‚¹ï¼š

---

### âœ… **èƒŒæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ**

ä¼ ç»Ÿä¸Šï¼Œå¤„ç†è¯­è¨€ï¼ˆæ¯”å¦‚ç¿»è¯‘ã€è¯­è¨€å»ºæ¨¡ï¼‰çš„æ–¹æ³•ä¸»è¦æ˜¯ç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰å’Œé—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰ã€‚è¿™äº›æ–¹æ³•å·²ç»è¢«å¹¿æ³›è®¤ä¸ºæ˜¯æ•ˆæœæœ€å¥½çš„è§£å†³æ–¹æ¡ˆã€‚å¾ˆå¤šç ”ç©¶ä¹Ÿåœ¨ä¸æ–­æ”¹è¿›è¿™äº›æ¨¡å‹ã€‚

---

### ğŸ¤” **è¿™äº›æ—§æ–¹æ³•æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ**

RNNç±»çš„æ¨¡å‹æœ‰ä¸€ä¸ªæœ¬è´¨çš„é—®é¢˜ï¼š**å®ƒä»¬æ˜¯ä¸€æ­¥ä¸€æ­¥å¤„ç†çš„**ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒåªèƒ½å…ˆå¤„ç†ç¬¬ä¸€ä¸ªè¯ï¼Œå†å¤„ç†ç¬¬äºŒä¸ªè¯ï¼Œç„¶åç¬¬ä¸‰ä¸ªâ€¦â€¦æ‰€ä»¥æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸èƒ½å¹¶è¡Œï¼Œé€Ÿåº¦æ…¢ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥å­å¾ˆé•¿çš„æ—¶å€™ï¼Œæ›´éš¾æï¼Œå› ä¸ºå†…å­˜ä¸å¤Ÿç”¨ï¼Œä¸èƒ½æ‰¹é‡å¤„ç†å¤ªå¤šå¥å­ã€‚

è™½ç„¶æœ‰ä¸€äº›æ–°æŠ€æœ¯ï¼ˆæ¯”å¦‚æŠŠæ¨¡å‹åˆ†æ®µè®¡ç®—æˆ–åªåœ¨æŸäº›æƒ…å†µä¸‹æ¿€æ´»éƒ¨åˆ†æ¨¡å‹ï¼‰å¯ä»¥æé«˜æ•ˆç‡å’Œæ•ˆæœï¼Œä½†**å¿…é¡»æŒ‰é¡ºåºå¤„ç†**è¿™ä¸ªé™åˆ¶è¿˜æ˜¯æ²¡æœ‰è§£å†³ã€‚

---

### ğŸ§  **æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰æ˜¯ä¸ªå¥½ä¸œè¥¿**

æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è®©æ¨¡å‹ç›´æ¥å…³æ³¨è¾“å…¥ä¸­çš„å…³é”®éƒ¨åˆ†ï¼Œä¸ç®¡å®ƒä»¬ç¦»å¾—è¿œä¸è¿œã€‚æ¯”å¦‚ç¿»è¯‘æ—¶ï¼Œå¥å­å¼€å¤´çš„è¯å¯èƒ½è·Ÿå¥å­ç»“å°¾çš„è¯æœ‰å…³ï¼Œæ³¨æ„åŠ›æœºåˆ¶èƒ½æ•æ‰åˆ°è¿™ç§è¿œè·ç¦»çš„å…³ç³»ã€‚

ä½†ä¹‹å‰å¤§éƒ¨åˆ†æ¨¡å‹éƒ½æ˜¯æŠŠæ³¨æ„åŠ›å’ŒRNNä¸€èµ·ç”¨çš„ï¼Œè¿˜æ˜¯å—é™äºRNNçš„â€œé¡ºåºå¤„ç†â€ã€‚

---

### ğŸ’¡ **æˆ‘ä»¬åšäº†å•¥åˆ›æ–°ï¼Ÿ**

è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°æ¨¡å‹å« **Transformer**ã€‚å®ƒ**å®Œå…¨ä¸ç”¨RNNï¼Œä¹Ÿä¸ç”¨å·ç§¯ç¥ç»ç½‘ç»œ**ï¼Œ**åªé æ³¨æ„åŠ›æœºåˆ¶**ï¼

å¥½å¤„æ˜¯ï¼š

* å®ƒå¯ä»¥å¹¶è¡Œå¤„ç†æ‰€æœ‰è¯ï¼Œæ›´å¿«ï¼›
* æ•ˆæœè¿˜æ›´å¥½ï¼›
* æˆ‘ä»¬ç”¨8å—P100æ˜¾å¡è®­ç»ƒ12å°æ—¶ï¼Œå°±åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚

---

### æ€»ç»“ä¸€å¥è¯ï¼š

> ä»¥å‰ç¿»è¯‘æ¨¡å‹éƒ½é ä¸€æ­¥æ­¥æ¥çš„RNNï¼Œæˆ‘ä»¬è¿™æ¬¡å¹²è„†ä¸ç”¨RNNäº†ï¼Œç›´æ¥ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥åšï¼Œè¿˜åšå¾—æ›´å¿«æ›´å¥½ï¼

---

å½“ç„¶å¯ä»¥ï¼ä¸‹é¢æˆ‘æ¥**é€å¥**ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€å¸®ä½ è§£é‡Šè¿™æ®µè¯ï¼š

---

### åŸæ–‡ï¼š

> **Recurrent models typically factor computation along the symbol positions of the input and output sequences.**

âœ… **è§£é‡Šï¼š**
å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰è¿™ç§æ¨¡å‹åœ¨å¤„ç†è¾“å…¥å’Œè¾“å‡ºçš„æ—¶å€™ï¼Œé€šå¸¸æ˜¯**ä¸€ä¸ªå­—ï¼ˆæˆ–è¯ï¼‰ä¸€ä¸ªå­—åœ°å¤„ç†**ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒæ˜¯**æŒ‰é¡ºåºæ¥å¤„ç†å¥å­ä¸­æ¯ä¸€ä¸ªä½ç½®çš„å†…å®¹**ã€‚

---

â€œ**factor computation**â€ æ˜¯ä¸€ä¸ªåœ¨æœºå™¨å­¦ä¹ å’Œæ•°å­¦ä¸­å¸¸è§çš„æœ¯è¯­ï¼Œå­—é¢æ„æ€æ˜¯â€œå°†è®¡ç®—è¿›è¡Œå› å¼åˆ†è§£â€æˆ–â€œåˆ†æ­¥åˆ†è§£å¤„ç†â€ã€‚åœ¨è¿™æ®µè®ºæ–‡ä¸­çš„è¯­å¢ƒä¸‹ï¼Œå®ƒæŒ‡çš„æ˜¯ï¼š**å°†ä¸€ä¸ªå®Œæ•´çš„è®¡ç®—ä»»åŠ¡æ‹†è§£æˆè‹¥å¹²ä¸ªæ›´å°çš„ã€ä¸è¾“å…¥ä½ç½®ç›¸å…³çš„æ­¥éª¤æ¥å¤„ç†ã€‚**

æˆ‘ä»¬æ¥**è¯¦ç»†æ‹†è§£ä¸€ä¸‹**è¿™ä¸ªæœ¯è¯­åœ¨è¿™é‡Œçš„å«ä¹‰ï¼š

---

### ğŸ“˜ åŸå¥ï¼š

> "Recurrent models typically factor computation along the symbol positions of the input and output sequences."

### ğŸ§  ç¿»è¯‘ç›´ç™½ç‚¹æ˜¯ï¼š

> å¾ªç¯æ¨¡å‹é€šå¸¸ä¼š**æŒ‰ç…§è¾“å…¥è¾“å‡ºåºåˆ—ä¸­æ¯ä¸ªç¬¦å·ï¼ˆæ¯”å¦‚æ¯ä¸ªè¯æˆ–å­—ç¬¦ï¼‰çš„ä½ç½®ï¼ŒæŠŠæ•´ä¸ªè®¡ç®—ä»»åŠ¡æ‹†å¼€æ¥åš**ã€‚

---

### âœ… é€šä¿—è§£é‡Šï¼š

æˆ‘ä»¬ä»¥ä¸€å¥è¯ç¿»è¯‘ä¸ºä¾‹ï¼Œæ¯”å¦‚è‹±è¯­å¥å­ â€œI love cats.â€ï¼š

å¾ªç¯ç¥ç»ç½‘ç»œä¼šåƒè¿™æ ·åšï¼š

1. ç¬¬ä¸€æ­¥å¤„ç† â€œIâ€ â†’ å¾—åˆ°éšè—çŠ¶æ€ $h_1$
2. ç¬¬äºŒæ­¥å¤„ç† â€œloveâ€ï¼Œç”¨ â€œloveâ€ + ä¸Šä¸€æ­¥ç»“æœ $h_1$ â†’ å¾—åˆ° $h_2$
3. ç¬¬ä¸‰æ­¥å¤„ç† â€œcatsâ€ï¼Œç”¨ â€œcatsâ€ + $h_2$ â†’ å¾—åˆ° $h_3$

æ•´ä¸ªæ¨¡å‹ä¸æ˜¯ä¸€ä¸‹å­å¤„ç†å®Œæ•´å¥è¯ï¼Œè€Œæ˜¯**æŠŠæ•´ä¸ªè®¡ç®—æµç¨‹â€œæ‹†å¼€â€ï¼ˆfactorï¼‰æˆäº†æ¯ä¸ªè¯ä¸€æ­¥çš„è®¡ç®—**ã€‚è¿™å°±æ˜¯â€œfactor computation along the symbol positionsâ€ï¼š

ğŸ‘‰ **æ¯ä¸ªè¯ä¸€ä¸ªæ­¥éª¤ï¼Œæ•´ä¸ªè¿‡ç¨‹å°±æ˜¯å¤šä¸ªæ­¥éª¤çš„ç»„åˆã€‚**

---

### ğŸ“Œ æ›´æŠ€æœ¯ä¸€ç‚¹çš„è§£é‡Šï¼š

> "factor computation" åœ¨æ·±åº¦å­¦ä¹ ä¸­æœ‰æ—¶ä¹Ÿæ„å‘³ç€ï¼š**å°†ä¸€ä¸ªå¤æ‚çš„å‡½æ•°æ‹†åˆ†ä¸ºå¤šä¸ªå­å‡½æ•°**ï¼Œæ¯”å¦‚å…ˆåšåµŒå…¥ï¼Œå†åšç¼–ç ï¼Œå†åšè§£ç ï¼Œæ¯ä¸€å±‚å¤„ç†ä¸€ä¸ªæ–¹é¢ã€‚
> åœ¨RNNä¸­ï¼Œè¿™ç§â€œå› å¼åˆ†è§£â€æ˜¯**æ²¿ç€æ—¶é—´ç»´åº¦ï¼ˆåºåˆ—ä½ç½®ï¼‰æ¥åˆ†è§£**çš„â€”â€”æ¯ä¸ªæ—¶é—´ç‚¹/åºåˆ—ä½ç½®ç‹¬ç«‹å®Œæˆä¸€å°æ­¥ã€‚

---

### ğŸ” ç±»æ¯”ä¸¾ä¸ªä¾‹å­ï¼š

å°±å¥½æ¯”ä½ åœ¨åšä¸€é“æ•°å­¦é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•æ˜¯ä¸€æ­¥åˆ°å°¾ä¸€å£æ°”åšå®Œã€‚è€ŒRNNçš„æ€è·¯æ˜¯ï¼š

> æŠŠè¿™é“é¢˜æ‹†æˆ 3 ä¸ªæ­¥éª¤ï¼Œæ¯æ­¥éƒ½åªç®—å½“å‰è¿™ä¸€æ­¥ï¼Œå¿…é¡»æŒ‰é¡ºåºå®Œæˆã€‚
> è¿™å°±æ˜¯ â€œfactor computation along time stepsâ€ã€‚

---

### âœï¸ æ€»ç»“ä¸€ä¸‹ï¼š

* **factor computation** = æŠŠæ•´ä½“è®¡ç®—ä»»åŠ¡æ‹†æˆå¤šä¸ªå°æ­¥éª¤æ¥åšï¼›
* åœ¨ RNN é‡Œï¼Œæ˜¯ **æŒ‰ç…§è¯çš„ä½ç½®/é¡ºåº** æ¥åˆ†æ­¥éª¤å¤„ç†ï¼›
* å¥½å¤„æ˜¯ç»“æ„æ¸…æ™°ï¼Œä½†åå¤„æ˜¯**ä¸èƒ½å¹¶è¡Œå¤„ç†**ï¼Œé€Ÿåº¦æ…¢ã€‚

---

è¯¦ç»†è§£é‡Šä¸€ä¸‹ Aligning the positions to steps in computation time

éå¸¸å¥½ï¼Œè¿™å¥çœ‹èµ·æ¥ç®€å•ï¼Œå…¶å®æŒºå…³é”®çš„ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£ RNN çš„å·¥ä½œæœºåˆ¶æ—¶ã€‚

---

### ğŸ“˜ åŸå¥ï¼š

> **Aligning the positions to steps in computation time**

---

## âœ… ä¸€å¥è¯è§£é‡Šï¼š

è¿™å¥è¯çš„æ„æ€æ˜¯ï¼š

> **â€œæŠŠæ¯ä¸ªè¯ï¼ˆæˆ–å­—ç¬¦ï¼‰åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œå¯¹åº”åˆ°ä¸€ä¸ªå…·ä½“çš„è®¡ç®—æ­¥éª¤ã€‚â€**

ä¹Ÿå°±æ˜¯è¯´ï¼Œ**æ¨¡å‹å¤„ç†ç¬¬1ä¸ªè¯å°±æ˜¯ç¬¬1æ­¥è®¡ç®—ï¼Œå¤„ç†ç¬¬2ä¸ªè¯å°±æ˜¯ç¬¬2æ­¥è®¡ç®—â€¦â€¦æ¯ä¸ªä½ç½®éƒ½å’Œè®¡ç®—æ—¶é—´ä¸€ä¸€å¯¹åº”**ã€‚

---

## ğŸ” é€å±‚æ‹†è§£ï¼š

### 1. **positionï¼ˆä½ç½®ï¼‰æŒ‡çš„æ˜¯å•¥ï¼Ÿ**

å°±æ˜¯è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªè¯çš„é¡ºåºï¼Œæ¯”å¦‚è¿™ä¸ªå¥å­ï¼š

```
I     love     cats
1      2        3
```

è¿™äº›æ•°å­—å°±æ˜¯â€œä½ç½®â€ã€‚

---

### 2. **computation timeï¼ˆè®¡ç®—æ—¶é—´ï¼‰æŒ‡çš„æ˜¯å•¥ï¼Ÿ**

RNN æ˜¯**ä¸€æ­¥ä¸€æ­¥å¤„ç†**çš„ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ï¼ˆtimestepï¼‰æ¨¡å‹éƒ½ä¼šæ‰§è¡Œä¸€æ¬¡è®¡ç®—ã€‚æ¯”å¦‚ï¼š

* ç¬¬1æ­¥ â†’ å¤„ç†ç¬¬1ä¸ªè¯ â€œIâ€
* ç¬¬2æ­¥ â†’ å¤„ç† â€œloveâ€
* ç¬¬3æ­¥ â†’ å¤„ç† â€œcatsâ€

è¿™ä¸ªå¤„ç†è¿‡ç¨‹æ˜¯æœ‰**é¡ºåºçš„æ—¶é—´æ­¥ï¼ˆtime stepï¼‰**çš„ã€‚

---

### 3. **Aligningâ€¦ æ˜¯ä»€ä¹ˆåŠ¨ä½œï¼Ÿ**

â€œAligningâ€ è¡¨ç¤º**å¯¹é½ã€å¯¹åº”ã€é…å¯¹**ã€‚

è¿™é‡Œçš„æ„æ€æ˜¯ï¼š

> æŠŠè¾“å…¥ä¸­çš„â€œç¬¬å‡ ä¸ªè¯â€å¯¹é½ï¼ˆå¯¹åº”ï¼‰åˆ°â€œç¬¬å‡ æ­¥è®¡ç®—â€ã€‚

æ‰€ä»¥ï¼š

| è¾“å…¥ä½ç½®ï¼ˆPositionï¼‰ | è®¡ç®—æ—¶é—´ï¼ˆTime stepï¼‰ |
| -------------- | --------------- |
| 1ï¼ˆâ€œIâ€ï¼‰         | t = 1           |
| 2ï¼ˆâ€œloveâ€ï¼‰      | t = 2           |
| 3ï¼ˆâ€œcatsâ€ï¼‰      | t = 3           |

è¿™å°±æ˜¯â€œAligning the positions to steps in computation timeâ€ã€‚

---

## ğŸ§  æ›´ç›´ç™½çš„è¯´æ³•ï¼š

RNN çš„è®¡ç®—é€»è¾‘æ˜¯ï¼š**ä¸€ä¸ªè¯å¯¹åº”ä¸€æ­¥è®¡ç®—ï¼Œä¸èƒ½è·³ç€æ¥ï¼Œä¹Ÿä¸èƒ½ä¸€èµ·ç®—ã€‚**

å°±åƒä½ åœ¨æ•°æ•°ï¼Œå¿…é¡»å…ˆæ•°1ï¼Œå†æ•°2ï¼Œæœ€åæ•°3â€”â€”è¿™ä¸ªè¿‡ç¨‹ä¸èƒ½å¹¶è¡Œã€‚

---

## ğŸ“Œ æ€»ç»“ä¸€ä¸‹ï¼š

| åŸè¯                           | é€šä¿—è§£é‡Š         |
| ---------------------------- | ------------ |
| Aligning the positions       | æŠŠæ¯ä¸ªè¯çš„ä½ç½®      |
| to steps in computation time | å’Œæ¨¡å‹çš„æ¯ä¸€æ­¥è®¡ç®—å¯¹ä¸Šå· |

â¡ï¸ ä¹Ÿå°±æ˜¯ï¼š
**â€œæ¨¡å‹ç¬¬å‡ æ­¥å°±å¤„ç†ç¬¬å‡ ä¸ªè¯ï¼Œæ¯ä¸ªè¯å’Œä¸€ä¸ªæ—¶é—´æ­¥ç»‘å®šåœ¨ä¸€èµ·ï¼ŒæŒ‰é¡ºåºæ¥ã€‚â€**

---

![](../pictures/attention_1.png "")

è¿™å¼ å›¾å°±æ˜¯å¯¹â€œ**Aligning the positions to steps in computation time**â€çš„ç›´è§‚å±•ç¤ºï¼š

* ä¸Šé¢è“è‰²çš„æ˜¯è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªè¯çš„ä½ç½®ï¼ˆPosition 1, 2, 3ï¼‰ã€‚
* ä¸‹é¢ç»¿è‰²çš„æ˜¯è®¡ç®—æ—¶é—´çš„æ­¥éª¤ï¼ˆt=1, t=2, t=3ï¼‰ã€‚
* æ¯ä¸ªç®­å¤´è¡¨ç¤ºï¼šç¬¬å‡ ä¸ªè¯å¯¹åº”ç¬¬å‡ æ­¥è®¡ç®—ã€‚

ğŸ” å°±æ˜¯è¯´ï¼š**æ¨¡å‹ç¬¬1æ­¥å°±å¤„ç†ç¬¬1ä¸ªè¯ï¼Œç¬¬2æ­¥å¤„ç†ç¬¬2ä¸ªè¯â€¦â€¦å¿…é¡»æŒ‰é¡ºåºæ¥ï¼Œä¸èƒ½è·³è¿‡ï¼Œä¹Ÿä¸èƒ½ä¸€èµ·ç®—ã€‚**

è¿™å°±æ˜¯RNNçš„â€œé¡ºåºæ€§â€â€”â€”å®ƒä¸èƒ½å¹¶è¡Œï¼Œåªèƒ½ä¸€æ­¥ä¸€æ­¥æ¥ã€‚è¿™ä¹Ÿæ­£æ˜¯Transformeræƒ³è¦çªç ´çš„é™åˆ¶ã€‚éœ€è¦æˆ‘å¯¹æ¯”ä¸€ä¸‹Transformeræ˜¯æ€ä¹ˆåšçš„å—ï¼Ÿ

éœ€è¦

![](../pictures/attention_1.png "")

è¿™å¼ å›¾å±•ç¤ºäº† **Transformer å’Œ RNN çš„æ ¸å¿ƒåŒºåˆ«**ï¼š

* ğŸ”µ ä¸Šæ–¹ï¼šæ¯ä¸ªè¯çš„ä½ç½®ï¼ˆPosition 1, 2, 3ï¼‰
* ğŸŸ¢ ä¸‹æ–¹ç»¿è‰²åŒºåŸŸï¼š**Transformer æ˜¯â€œåŒæ—¶å¤„ç†æ‰€æœ‰è¯â€**ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„â€œå¹¶è¡Œè®¡ç®—ï¼ˆParallel Computationï¼‰â€
* æ¯ä¸ªç®­å¤´éƒ½æŒ‡å‘åŒä¸€ä¸ªè®¡ç®—åŒºåŸŸï¼Œè¡¨ç¤ºè¿™äº›è¯**å¯ä»¥ä¸€èµ·è¢«å¤„ç†**

---

### ğŸ“Œ æ€»ç»“å¯¹æ¯”ï¼š

| æ¨¡å‹          | å¦‚ä½•å¤„ç†è¯åºåˆ—        |
| ----------- | -------------- |
| RNN         | ä¸€æ­¥ä¸€æ­¥æ¥ï¼ˆä¸èƒ½å¹¶è¡Œï¼‰    |
| Transformer | ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰è¯ï¼ˆå¯ä»¥å¹¶è¡Œï¼‰ |

è¿™æ­£æ˜¯ Transformer æ›´å¿«ã€æ›´é«˜æ•ˆçš„åŸå› ä¹‹ä¸€ã€‚
å¦‚æœä½ è¿˜æƒ³äº†è§£â€œTransformer æ˜¯æ€ä¹ˆåšåˆ°å¹¶è¡Œâ€çš„ï¼Œæ¯”å¦‚å¤šå¤´æ³¨æ„åŠ›æˆ–ä½ç½®ç¼–ç ï¼Œæˆ‘ä¹Ÿå¯ä»¥ç»§ç»­è®²è§£ã€‚

---

### åŸæ–‡ï¼š

> **Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state htâˆ’1 and the input for position t.**

âœ… **è§£é‡Šï¼š**
è¿™ä¸ªæ¨¡å‹åœ¨æ¯ä¸€æ­¥éƒ½ä¼šç”Ÿæˆä¸€ä¸ªâ€œéšè—çŠ¶æ€â€ï¼ˆå¯ä»¥ç†è§£ä¸ºå½“å‰å¤„ç†ç»“æœçš„ä¸­é—´è®°å¿†ï¼‰ï¼Œå«åš $h_t$ã€‚è¿™ä¸ªçŠ¶æ€æ˜¯æ ¹æ®ä¸¤ä¸ªä¸œè¥¿ç®—å‡ºæ¥çš„ï¼š

1. ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ $h_{t-1}$
2. å½“å‰è¿™ä¸€æ­¥è¾“å…¥çš„è¯ï¼ˆæ¯”å¦‚ç¬¬tä¸ªè¯ï¼‰

ç®€å•è¯´å°±æ˜¯ï¼š**å½“å‰çš„å¤„ç†ç»“æœï¼Œå–å†³äºä½ ä¸Šä¸€æ­¥çš„ç»“æœåŠ ä¸Šè¿™ä¸€æ­¥çš„è¾“å…¥**ã€‚

---

### åŸæ–‡ï¼š

> **This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.**

âœ… **è§£é‡Šï¼š**
å› ä¸ºè¿™ä¸ªæ¨¡å‹å¿…é¡»**ä¸€æ­¥ä¸€æ­¥å¤„ç†**ï¼ˆæ¯”å¦‚å¿…é¡»å…ˆå¤„ç†ç¬¬1ä¸ªè¯ï¼Œæ‰èƒ½å¤„ç†ç¬¬2ä¸ªè¯ï¼‰ï¼Œæ‰€ä»¥å®ƒ**ä¸èƒ½å¹¶è¡Œå¤„ç†åŒä¸€ä¸ªå¥å­é‡Œçš„æ‰€æœ‰è¯**ã€‚
è¿™åœ¨å¥å­å¾ˆé•¿çš„æ—¶å€™å°±æˆäº†å¤§é—®é¢˜ï¼Œå› ä¸ºï¼š

* å¹¶è¡Œä¸äº† â†’ æ…¢ï¼›
* æ¯æ¬¡åªèƒ½å¤„ç†æœ‰é™æ•°é‡çš„å¥å­ â†’ **å†…å­˜ä¸å¤Ÿï¼Œæ‰¹å¤„ç†æ•°é‡å—é™**ã€‚

---

### åŸæ–‡ï¼š

> **Recent work has achieved significant improvements in computational efficiency through factorization tricks \[21] and conditional computation \[32], while also improving model performance in case of the latter.**

âœ… **è§£é‡Šï¼š**
æœ€è¿‘æœ‰äº›ç ”ç©¶å°è¯•ç”¨ä¸€äº›â€œæŠ€å·§â€æ¥è®©è®¡ç®—å˜å¿«ï¼Œæ¯”å¦‚ï¼š

* **åˆ†è§£è®¡ç®—**ï¼šæŠŠå¤§çš„é—®é¢˜æ‹†æˆå¤šä¸ªå°å—æ¥å¤„ç†ï¼›
* **æ¡ä»¶è®¡ç®—**ï¼šæ¨¡å‹åªæ¿€æ´»ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯å…¨éƒ¨éƒ½è·‘ä¸€éã€‚

è¿™äº›æ–¹æ³•åœ¨æé«˜è¿è¡Œæ•ˆç‡çš„åŒæ—¶ï¼Œæœ‰æ—¶è¿˜èƒ½è®©æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚

---

### åŸæ–‡ï¼š

> **The fundamental constraint of sequential computation, however, remains.**

âœ… **è§£é‡Šï¼š**
ä½†**æœ€å¤§çš„é—®é¢˜è¿˜åœ¨**ï¼šè¿™ç§â€œå¿…é¡»ä¸€æ­¥æ­¥æ¥â€çš„æ–¹å¼ï¼Œè¿˜æ˜¯æ²¡æ³•è§£å†³ã€‚

---

### æ€»ç»“ä¸€å¥è¯ï¼š

å¾ªç¯æ¨¡å‹æ˜¯â€œæŒ‰é¡ºåºâ€ä¸€ä¸ªä¸€ä¸ªè¯åœ°å¤„ç†çš„ï¼Œè¿™ç§å¤„ç†æ–¹å¼é™åˆ¶äº†å¹¶è¡Œè®¡ç®—ï¼Œå°¤å…¶åœ¨é•¿å¥å­é‡Œæ•ˆç‡ä½ã€‚è™½ç„¶æœ‰äº›æŠ€å·§èƒ½ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†**æœ¬è´¨ä¸Šçš„é¡ºåºé™åˆ¶è¿˜æ˜¯å­˜åœ¨**ã€‚

---






## 7 Conclusion

In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.
For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.
We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.
The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.

**Acknowledgements** We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.

## 7 ç»“è®º

åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Transformerï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„åºåˆ—è½¬æ¢æ¨¡å‹ï¼Œå®ƒç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å–ä»£äº†ç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸­å¸¸è§çš„å¾ªç¯å±‚ã€‚

åœ¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œä¸åŸºäºå¾ªç¯æˆ–å·ç§¯å±‚çš„æ¶æ„ç›¸æ¯”ï¼ŒTransformerçš„è®­ç»ƒé€Ÿåº¦æ˜¾è‘—æ›´å¿«ã€‚åœ¨WMT 2014è‹±å¾·ç¿»è¯‘ä»»åŠ¡å’ŒWMT 2014è‹±æ³•ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬éƒ½å–å¾—äº†æ–°çš„æœ€å…ˆè¿›æˆç»©ã€‚åœ¨è‹±å¾·ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹ç”šè‡³è¶…è¿‡äº†æ­¤å‰æŠ¥é“çš„æ‰€æœ‰æ¨¡å‹é›†æˆç»“æœã€‚

æˆ‘ä»¬å¯¹åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹æœªæ¥å‘å±•æ„Ÿåˆ°éå¸¸å…´å¥‹ï¼Œå¹¶è®¡åˆ’å°†å…¶åº”ç”¨äºæ›´å¤šä»»åŠ¡ã€‚æˆ‘ä»¬è®¡åˆ’å°†Transformeræ‰©å±•åˆ°å¤„ç†è¾“å…¥å’Œè¾“å‡ºæ¨¡æ€ä¸é™äºæ–‡æœ¬çš„é—®é¢˜ï¼Œå¹¶ç ”ç©¶å±€éƒ¨çš„ã€å—é™çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥é«˜æ•ˆå¤„ç†å¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤§è§„æ¨¡è¾“å…¥è¾“å‡ºæ•°æ®ã€‚è®©ç”Ÿæˆè¿‡ç¨‹æ›´å°‘ä¾èµ–é¡ºåºæ€§ä¹Ÿæ˜¯æˆ‘ä»¬çš„ç ”ç©¶ç›®æ ‡ä¹‹ä¸€ã€‚

æˆ‘ä»¬ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹çš„ä»£ç å·²å…¬å¼€ï¼Œåœ°å€ä¸ºï¼šhttps://github.com/tensorflow/tensor2tensorã€‚

**è‡´è°¢** æˆ‘ä»¬æ„Ÿè°¢Nal Kalchbrennerå’ŒStephan Gouwsæå‡ºçš„å®è´µæ„è§ã€ä¿®æ­£å»ºè®®ä»¥åŠç»™äºˆæˆ‘ä»¬çš„å¯å‘ã€‚