# Attention Is All You Need

## æ‘˜è¦

ä¸»æµçš„åºåˆ—è½¬æ¢æ¨¡å‹ä¸»è¦åŸºäºå¤æ‚çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æˆ–å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸åŒ…æ‹¬ä¸€ä¸ªç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨ã€‚æ€§èƒ½æœ€å¥½çš„æ¨¡å‹è¿˜é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å°†ç¼–ç å™¨å’Œè§£ç å™¨è¿æ¥èµ·æ¥ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°çš„ã€åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç®€å•ç½‘ç»œæ¶æ„â€”â€”Transformerï¼Œå®ƒå®Œå…¨æ‘’å¼ƒäº†å¾ªç¯å’Œå·ç§¯ç»“æ„ã€‚

åœ¨ä¸¤ä¸ªæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç¿»è¯‘è´¨é‡ä¸Šæ›´ä¼˜ï¼ŒåŒæ—¶å…·æœ‰æ›´å¼ºçš„å¹¶è¡Œèƒ½åŠ›ï¼Œè®­ç»ƒæ‰€éœ€æ—¶é—´ä¹Ÿå¤§å¤§å‡å°‘ã€‚åœ¨WMT 2014è‹±å¾·ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å–å¾—äº†28.4çš„BLEUåˆ†æ•°ï¼Œæ¯”ç°æœ‰æœ€å¥½çš„ç»“æœï¼ˆåŒ…æ‹¬æ¨¡å‹é›†æˆï¼‰æé«˜äº†è¶…è¿‡2ä¸ªBLEUåˆ†ã€‚åœ¨WMT 2014è‹±æ³•ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨8å—GPUä¸Šè®­ç»ƒ3.5å¤©åï¼Œè¾¾åˆ°äº†41.8çš„å•æ¨¡å‹BLEUåˆ†æ•°ï¼Œåˆ›ä¸‹äº†æ–°çš„å•æ¨¡å‹æœ€ä½³æˆç»©ï¼Œè€Œè®­ç»ƒæˆæœ¬åªæ˜¯æ–‡çŒ®ä¸­æœ€ä¼˜æ¨¡å‹çš„ä¸€å°éƒ¨åˆ†ã€‚

æˆ‘ä»¬è¿˜å±•ç¤ºäº†Transformeråœ¨å…¶ä»–ä»»åŠ¡ä¸Šçš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œå®ƒåœ¨å¤„ç†è‹±è¯­æˆåˆ†å¥æ³•åˆ†æä»»åŠ¡æ—¶ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œæ— è®ºæ˜¯å¤§è§„æ¨¡è®­ç»ƒæ•°æ®è¿˜æ˜¯æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹éƒ½å–å¾—äº†æˆåŠŸã€‚

---

<sup>*</sup>å…±åŒè´¡çŒ®ï¼Œä½œè€…æ’åºéšæœºã€‚Jakob æå‡ºäº†ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ›¿ä»£RNNçš„æƒ³æ³•ï¼Œå¹¶ç‡å…ˆå¼€å±•äº†å¯¹è¿™ä¸€æƒ³æ³•çš„éªŒè¯å·¥ä½œã€‚Ashish ä¸ Illia ä¸€èµ·è®¾è®¡å¹¶å®ç°äº†æœ€åˆçš„Transformeræ¨¡å‹ï¼Œå¹¶åœ¨æœ¬é¡¹å·¥ä½œçš„å„ä¸ªæ–¹é¢éƒ½å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚Noam æå‡ºäº†ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶ã€å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä»¥åŠæ— éœ€å‚æ•°çš„ä½ç½®è¡¨ç¤ºæ–¹æ³•ï¼Œæ˜¯å¦ä¸€ä½å‡ ä¹å‚ä¸æ‰€æœ‰ç»†èŠ‚å·¥ä½œçš„ä½œè€…ã€‚Niki åœ¨æˆ‘ä»¬çš„åˆå§‹ä»£ç åº“å’Œ tensor2tensor ä¸­è®¾è®¡ã€å®ç°ã€è°ƒè¯•å¹¶è¯„ä¼°äº†æ— æ•°æ¨¡å‹å˜ä½“ã€‚Llion ä¹Ÿå°è¯•äº†æ–°é¢–çš„æ¨¡å‹å˜ä½“ï¼Œè´Ÿè´£æœ€åˆçš„ä»£ç åº“å¼€å‘ã€é«˜æ•ˆçš„æ¨ç†å®ç°ä»¥åŠå¯è§†åŒ–å·¥ä½œã€‚Lukasz å’Œ Aidan åˆ™èŠ±è´¹äº†æ— æ•°ä¸ªæ—¥å¤œè®¾è®¡å¹¶å®ç°äº† tensor2tensorï¼Œæ›¿ä»£äº†æˆ‘ä»¬æ—©æœŸçš„ä»£ç åº“ï¼Œæå¤§åœ°æå‡äº†å®éªŒç»“æœå¹¶æ˜¾è‘—åŠ é€Ÿäº†æˆ‘ä»¬çš„ç ”ç©¶è¿›å±•ã€‚

â€  å·¥ä½œå®Œæˆæ—¶ä¾›èŒäº Google Brainã€‚

â€¡ å·¥ä½œå®Œæˆæ—¶ä¾›èŒäº Google Researchã€‚

å‘è¡¨äºç¬¬31å±Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿå¤§ä¼šï¼ˆNIPS 2017ï¼‰ï¼Œåœ°ç‚¹ï¼šç¾å›½åŠ åˆ©ç¦å°¼äºšå·é•¿æ»©ã€‚

---

## 1 å¼•è¨€

è¿™æ˜¯ã€ŠAttention Is All You Needã€‹è®ºæ–‡çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œå®ƒä¸»è¦è®²äº†å‡ ä¸ªæ ¸å¿ƒç‚¹ï¼š

---

### âœ… **èƒŒæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ**

ä¼ ç»Ÿä¸Šï¼Œå¤„ç†è¯­è¨€ï¼ˆæ¯”å¦‚ç¿»è¯‘ã€è¯­è¨€å»ºæ¨¡ï¼‰çš„æ–¹æ³•ä¸»è¦æ˜¯ç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰å’Œé—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰ã€‚è¿™äº›æ–¹æ³•å·²ç»è¢«å¹¿æ³›è®¤ä¸ºæ˜¯æ•ˆæœæœ€å¥½çš„è§£å†³æ–¹æ¡ˆã€‚å¾ˆå¤šç ”ç©¶ä¹Ÿåœ¨ä¸æ–­æ”¹è¿›è¿™äº›æ¨¡å‹ã€‚

---

### ğŸ¤” **è¿™äº›æ—§æ–¹æ³•æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ**

RNNç±»çš„æ¨¡å‹æœ‰ä¸€ä¸ªæœ¬è´¨çš„é—®é¢˜ï¼š**å®ƒä»¬æ˜¯ä¸€æ­¥ä¸€æ­¥å¤„ç†çš„**ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒåªèƒ½å…ˆå¤„ç†ç¬¬ä¸€ä¸ªè¯ï¼Œå†å¤„ç†ç¬¬äºŒä¸ªè¯ï¼Œç„¶åç¬¬ä¸‰ä¸ªâ€¦â€¦æ‰€ä»¥æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸èƒ½å¹¶è¡Œï¼Œé€Ÿåº¦æ…¢ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥å­å¾ˆé•¿çš„æ—¶å€™ï¼Œæ›´éš¾æï¼Œå› ä¸ºå†…å­˜ä¸å¤Ÿç”¨ï¼Œä¸èƒ½æ‰¹é‡å¤„ç†å¤ªå¤šå¥å­ã€‚

è™½ç„¶æœ‰ä¸€äº›æ–°æŠ€æœ¯ï¼ˆæ¯”å¦‚æŠŠæ¨¡å‹åˆ†æ®µè®¡ç®—æˆ–åªåœ¨æŸäº›æƒ…å†µä¸‹æ¿€æ´»éƒ¨åˆ†æ¨¡å‹ï¼‰å¯ä»¥æé«˜æ•ˆç‡å’Œæ•ˆæœï¼Œä½†**å¿…é¡»æŒ‰é¡ºåºå¤„ç†**è¿™ä¸ªé™åˆ¶è¿˜æ˜¯æ²¡æœ‰è§£å†³ã€‚

---

### ğŸ§  **æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰æ˜¯ä¸ªå¥½ä¸œè¥¿**

æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è®©æ¨¡å‹ç›´æ¥å…³æ³¨è¾“å…¥ä¸­çš„å…³é”®éƒ¨åˆ†ï¼Œä¸ç®¡å®ƒä»¬ç¦»å¾—è¿œä¸è¿œã€‚æ¯”å¦‚ç¿»è¯‘æ—¶ï¼Œå¥å­å¼€å¤´çš„è¯å¯èƒ½è·Ÿå¥å­ç»“å°¾çš„è¯æœ‰å…³ï¼Œæ³¨æ„åŠ›æœºåˆ¶èƒ½æ•æ‰åˆ°è¿™ç§è¿œè·ç¦»çš„å…³ç³»ã€‚

ä½†ä¹‹å‰å¤§éƒ¨åˆ†æ¨¡å‹éƒ½æ˜¯æŠŠæ³¨æ„åŠ›å’ŒRNNä¸€èµ·ç”¨çš„ï¼Œè¿˜æ˜¯å—é™äºRNNçš„â€œé¡ºåºå¤„ç†â€ã€‚

---

### ğŸ’¡ **æˆ‘ä»¬åšäº†å•¥åˆ›æ–°ï¼Ÿ**

è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°æ¨¡å‹å« **Transformer**ã€‚å®ƒ**å®Œå…¨ä¸ç”¨RNNï¼Œä¹Ÿä¸ç”¨å·ç§¯ç¥ç»ç½‘ç»œ**ï¼Œ**åªé æ³¨æ„åŠ›æœºåˆ¶**ï¼

å¥½å¤„æ˜¯ï¼š

* å®ƒå¯ä»¥å¹¶è¡Œå¤„ç†æ‰€æœ‰è¯ï¼Œæ›´å¿«ï¼›
* æ•ˆæœè¿˜æ›´å¥½ï¼›
* æˆ‘ä»¬ç”¨8å—P100æ˜¾å¡è®­ç»ƒ12å°æ—¶ï¼Œå°±åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚

---

### æ€»ç»“ä¸€å¥è¯ï¼š

> ä»¥å‰ç¿»è¯‘æ¨¡å‹éƒ½é ä¸€æ­¥æ­¥æ¥çš„RNNï¼Œæˆ‘ä»¬è¿™æ¬¡å¹²è„†ä¸ç”¨RNNäº†ï¼Œç›´æ¥ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥åšï¼Œè¿˜åšå¾—æ›´å¿«æ›´å¥½ï¼

---

å½“ç„¶å¯ä»¥ï¼ä¸‹é¢æˆ‘æ¥**é€å¥**ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€å¸®ä½ è§£é‡Šè¿™æ®µè¯ï¼š

---

### åŸæ–‡ï¼š

> **Recurrent models typically factor computation along the symbol positions of the input and output sequences.**

âœ… **è§£é‡Šï¼š**
å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰è¿™ç§æ¨¡å‹åœ¨å¤„ç†è¾“å…¥å’Œè¾“å‡ºçš„æ—¶å€™ï¼Œé€šå¸¸æ˜¯**ä¸€ä¸ªå­—ï¼ˆæˆ–è¯ï¼‰ä¸€ä¸ªå­—åœ°å¤„ç†**ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒæ˜¯**æŒ‰é¡ºåºæ¥å¤„ç†å¥å­ä¸­æ¯ä¸€ä¸ªä½ç½®çš„å†…å®¹**ã€‚

---

â€œ**factor computation**â€ æ˜¯ä¸€ä¸ªåœ¨æœºå™¨å­¦ä¹ å’Œæ•°å­¦ä¸­å¸¸è§çš„æœ¯è¯­ï¼Œå­—é¢æ„æ€æ˜¯â€œå°†è®¡ç®—è¿›è¡Œå› å¼åˆ†è§£â€æˆ–â€œåˆ†æ­¥åˆ†è§£å¤„ç†â€ã€‚åœ¨è¿™æ®µè®ºæ–‡ä¸­çš„è¯­å¢ƒä¸‹ï¼Œå®ƒæŒ‡çš„æ˜¯ï¼š**å°†ä¸€ä¸ªå®Œæ•´çš„è®¡ç®—ä»»åŠ¡æ‹†è§£æˆè‹¥å¹²ä¸ªæ›´å°çš„ã€ä¸è¾“å…¥ä½ç½®ç›¸å…³çš„æ­¥éª¤æ¥å¤„ç†ã€‚**

æˆ‘ä»¬æ¥**è¯¦ç»†æ‹†è§£ä¸€ä¸‹**è¿™ä¸ªæœ¯è¯­åœ¨è¿™é‡Œçš„å«ä¹‰ï¼š

---

### ğŸ“˜ åŸå¥ï¼š

> "Recurrent models typically factor computation along the symbol positions of the input and output sequences."

### ğŸ§  ç¿»è¯‘ç›´ç™½ç‚¹æ˜¯ï¼š

> å¾ªç¯æ¨¡å‹é€šå¸¸ä¼š**æŒ‰ç…§è¾“å…¥è¾“å‡ºåºåˆ—ä¸­æ¯ä¸ªç¬¦å·ï¼ˆæ¯”å¦‚æ¯ä¸ªè¯æˆ–å­—ç¬¦ï¼‰çš„ä½ç½®ï¼ŒæŠŠæ•´ä¸ªè®¡ç®—ä»»åŠ¡æ‹†å¼€æ¥åš**ã€‚

---

### âœ… é€šä¿—è§£é‡Šï¼š

æˆ‘ä»¬ä»¥ä¸€å¥è¯ç¿»è¯‘ä¸ºä¾‹ï¼Œæ¯”å¦‚è‹±è¯­å¥å­ â€œI love cats.â€ï¼š

å¾ªç¯ç¥ç»ç½‘ç»œä¼šåƒè¿™æ ·åšï¼š

1. ç¬¬ä¸€æ­¥å¤„ç† â€œIâ€ â†’ å¾—åˆ°éšè—çŠ¶æ€ $h_1$
2. ç¬¬äºŒæ­¥å¤„ç† â€œloveâ€ï¼Œç”¨ â€œloveâ€ + ä¸Šä¸€æ­¥ç»“æœ $h_1$ â†’ å¾—åˆ° $h_2$
3. ç¬¬ä¸‰æ­¥å¤„ç† â€œcatsâ€ï¼Œç”¨ â€œcatsâ€ + $h_2$ â†’ å¾—åˆ° $h_3$

æ•´ä¸ªæ¨¡å‹ä¸æ˜¯ä¸€ä¸‹å­å¤„ç†å®Œæ•´å¥è¯ï¼Œè€Œæ˜¯**æŠŠæ•´ä¸ªè®¡ç®—æµç¨‹â€œæ‹†å¼€â€ï¼ˆfactorï¼‰æˆäº†æ¯ä¸ªè¯ä¸€æ­¥çš„è®¡ç®—**ã€‚è¿™å°±æ˜¯â€œfactor computation along the symbol positionsâ€ï¼š

ğŸ‘‰ **æ¯ä¸ªè¯ä¸€ä¸ªæ­¥éª¤ï¼Œæ•´ä¸ªè¿‡ç¨‹å°±æ˜¯å¤šä¸ªæ­¥éª¤çš„ç»„åˆã€‚**

---

### ğŸ“Œ æ›´æŠ€æœ¯ä¸€ç‚¹çš„è§£é‡Šï¼š

> "factor computation" åœ¨æ·±åº¦å­¦ä¹ ä¸­æœ‰æ—¶ä¹Ÿæ„å‘³ç€ï¼š**å°†ä¸€ä¸ªå¤æ‚çš„å‡½æ•°æ‹†åˆ†ä¸ºå¤šä¸ªå­å‡½æ•°**ï¼Œæ¯”å¦‚å…ˆåšåµŒå…¥ï¼Œå†åšç¼–ç ï¼Œå†åšè§£ç ï¼Œæ¯ä¸€å±‚å¤„ç†ä¸€ä¸ªæ–¹é¢ã€‚
> åœ¨RNNä¸­ï¼Œè¿™ç§â€œå› å¼åˆ†è§£â€æ˜¯**æ²¿ç€æ—¶é—´ç»´åº¦ï¼ˆåºåˆ—ä½ç½®ï¼‰æ¥åˆ†è§£**çš„â€”â€”æ¯ä¸ªæ—¶é—´ç‚¹/åºåˆ—ä½ç½®ç‹¬ç«‹å®Œæˆä¸€å°æ­¥ã€‚

---

### ğŸ” ç±»æ¯”ä¸¾ä¸ªä¾‹å­ï¼š

å°±å¥½æ¯”ä½ åœ¨åšä¸€é“æ•°å­¦é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•æ˜¯ä¸€æ­¥åˆ°å°¾ä¸€å£æ°”åšå®Œã€‚è€ŒRNNçš„æ€è·¯æ˜¯ï¼š

> æŠŠè¿™é“é¢˜æ‹†æˆ 3 ä¸ªæ­¥éª¤ï¼Œæ¯æ­¥éƒ½åªç®—å½“å‰è¿™ä¸€æ­¥ï¼Œå¿…é¡»æŒ‰é¡ºåºå®Œæˆã€‚
> è¿™å°±æ˜¯ â€œfactor computation along time stepsâ€ã€‚

---

### âœï¸ æ€»ç»“ä¸€ä¸‹ï¼š

* **factor computation** = æŠŠæ•´ä½“è®¡ç®—ä»»åŠ¡æ‹†æˆå¤šä¸ªå°æ­¥éª¤æ¥åšï¼›
* åœ¨ RNN é‡Œï¼Œæ˜¯ **æŒ‰ç…§è¯çš„ä½ç½®/é¡ºåº** æ¥åˆ†æ­¥éª¤å¤„ç†ï¼›
* å¥½å¤„æ˜¯ç»“æ„æ¸…æ™°ï¼Œä½†åå¤„æ˜¯**ä¸èƒ½å¹¶è¡Œå¤„ç†**ï¼Œé€Ÿåº¦æ…¢ã€‚

---

### åŸæ–‡ï¼š

> **Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_t$, as a function of the previous hidden state $h_{t-1}$ and the input for position $t$.**

âœ… **è§£é‡Šï¼š**
è¿™ä¸ªæ¨¡å‹åœ¨æ¯ä¸€æ­¥éƒ½ä¼šç”Ÿæˆä¸€ä¸ªâ€œéšè—çŠ¶æ€â€ï¼ˆå¯ä»¥ç†è§£ä¸ºå½“å‰å¤„ç†ç»“æœçš„ä¸­é—´è®°å¿†ï¼‰ï¼Œå«åš $h_t$ã€‚è¿™ä¸ªçŠ¶æ€æ˜¯æ ¹æ®ä¸¤ä¸ªä¸œè¥¿ç®—å‡ºæ¥çš„ï¼š

1. ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ $h_{t-1}$
2. å½“å‰è¿™ä¸€æ­¥è¾“å…¥çš„è¯ï¼ˆæ¯”å¦‚ç¬¬tä¸ªè¯ï¼‰

ç®€å•è¯´å°±æ˜¯ï¼š**å½“å‰çš„å¤„ç†ç»“æœï¼Œå–å†³äºä½ ä¸Šä¸€æ­¥çš„ç»“æœåŠ ä¸Šè¿™ä¸€æ­¥çš„è¾“å…¥**ã€‚

---

è¯¦ç»†è§£é‡Šä¸€ä¸‹ Aligning the positions to steps in computation time

éå¸¸å¥½ï¼Œè¿™å¥çœ‹èµ·æ¥ç®€å•ï¼Œå…¶å®æŒºå…³é”®çš„ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£ RNN çš„å·¥ä½œæœºåˆ¶æ—¶ã€‚

---

### ğŸ“˜ åŸå¥ï¼š

> **Aligning the positions to steps in computation time**

---

## âœ… ä¸€å¥è¯è§£é‡Šï¼š

è¿™å¥è¯çš„æ„æ€æ˜¯ï¼š

> **â€œæŠŠæ¯ä¸ªè¯ï¼ˆæˆ–å­—ç¬¦ï¼‰åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œå¯¹åº”åˆ°ä¸€ä¸ªå…·ä½“çš„è®¡ç®—æ­¥éª¤ã€‚â€**

ä¹Ÿå°±æ˜¯è¯´ï¼Œ**æ¨¡å‹å¤„ç†ç¬¬1ä¸ªè¯å°±æ˜¯ç¬¬1æ­¥è®¡ç®—ï¼Œå¤„ç†ç¬¬2ä¸ªè¯å°±æ˜¯ç¬¬2æ­¥è®¡ç®—â€¦â€¦æ¯ä¸ªä½ç½®éƒ½å’Œè®¡ç®—æ—¶é—´ä¸€ä¸€å¯¹åº”**ã€‚

---

## ğŸ” é€å±‚æ‹†è§£ï¼š

### 1. **positionï¼ˆä½ç½®ï¼‰æŒ‡çš„æ˜¯å•¥ï¼Ÿ**

å°±æ˜¯è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªè¯çš„é¡ºåºï¼Œæ¯”å¦‚è¿™ä¸ªå¥å­ï¼š

```
I     love     cats
1      2        3
```

è¿™äº›æ•°å­—å°±æ˜¯â€œä½ç½®â€ã€‚

---

### 2. **computation timeï¼ˆè®¡ç®—æ—¶é—´ï¼‰æŒ‡çš„æ˜¯å•¥ï¼Ÿ**

RNN æ˜¯**ä¸€æ­¥ä¸€æ­¥å¤„ç†**çš„ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ï¼ˆtimestepï¼‰æ¨¡å‹éƒ½ä¼šæ‰§è¡Œä¸€æ¬¡è®¡ç®—ã€‚æ¯”å¦‚ï¼š

* ç¬¬1æ­¥ â†’ å¤„ç†ç¬¬1ä¸ªè¯ â€œIâ€
* ç¬¬2æ­¥ â†’ å¤„ç† â€œloveâ€
* ç¬¬3æ­¥ â†’ å¤„ç† â€œcatsâ€

è¿™ä¸ªå¤„ç†è¿‡ç¨‹æ˜¯æœ‰**é¡ºåºçš„æ—¶é—´æ­¥ï¼ˆtime stepï¼‰**çš„ã€‚

---

### 3. **Aligningâ€¦ æ˜¯ä»€ä¹ˆåŠ¨ä½œï¼Ÿ**

â€œAligningâ€ è¡¨ç¤º**å¯¹é½ã€å¯¹åº”ã€é…å¯¹**ã€‚

è¿™é‡Œçš„æ„æ€æ˜¯ï¼š

> æŠŠè¾“å…¥ä¸­çš„â€œç¬¬å‡ ä¸ªè¯â€å¯¹é½ï¼ˆå¯¹åº”ï¼‰åˆ°â€œç¬¬å‡ æ­¥è®¡ç®—â€ã€‚

æ‰€ä»¥ï¼š

| è¾“å…¥ä½ç½®ï¼ˆPositionï¼‰ | è®¡ç®—æ—¶é—´ï¼ˆTime stepï¼‰ |
| -------------- | --------------- |
| 1ï¼ˆâ€œIâ€ï¼‰         | t = 1           |
| 2ï¼ˆâ€œloveâ€ï¼‰      | t = 2           |
| 3ï¼ˆâ€œcatsâ€ï¼‰      | t = 3           |

è¿™å°±æ˜¯â€œAligning the positions to steps in computation timeâ€ã€‚

---

## ğŸ§  æ›´ç›´ç™½çš„è¯´æ³•ï¼š

RNN çš„è®¡ç®—é€»è¾‘æ˜¯ï¼š**ä¸€ä¸ªè¯å¯¹åº”ä¸€æ­¥è®¡ç®—ï¼Œä¸èƒ½è·³ç€æ¥ï¼Œä¹Ÿä¸èƒ½ä¸€èµ·ç®—ã€‚**

å°±åƒä½ åœ¨æ•°æ•°ï¼Œå¿…é¡»å…ˆæ•°1ï¼Œå†æ•°2ï¼Œæœ€åæ•°3â€”â€”è¿™ä¸ªè¿‡ç¨‹ä¸èƒ½å¹¶è¡Œã€‚

---

## ğŸ“Œ æ€»ç»“ä¸€ä¸‹ï¼š

| åŸè¯                           | é€šä¿—è§£é‡Š         |
| ---------------------------- | ------------ |
| Aligning the positions       | æŠŠæ¯ä¸ªè¯çš„ä½ç½®      |
| to steps in computation time | å’Œæ¨¡å‹çš„æ¯ä¸€æ­¥è®¡ç®—å¯¹ä¸Šå· |

â¡ï¸ ä¹Ÿå°±æ˜¯ï¼š
**â€œæ¨¡å‹ç¬¬å‡ æ­¥å°±å¤„ç†ç¬¬å‡ ä¸ªè¯ï¼Œæ¯ä¸ªè¯å’Œä¸€ä¸ªæ—¶é—´æ­¥ç»‘å®šåœ¨ä¸€èµ·ï¼ŒæŒ‰é¡ºåºæ¥ã€‚â€**

---

![](../pictures/attention_1.png "")

è¿™å¼ å›¾å°±æ˜¯å¯¹â€œ**Aligning the positions to steps in computation time**â€çš„ç›´è§‚å±•ç¤ºï¼š

* ä¸Šé¢è“è‰²çš„æ˜¯è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªè¯çš„ä½ç½®ï¼ˆPosition 1, 2, 3ï¼‰ã€‚
* ä¸‹é¢ç»¿è‰²çš„æ˜¯è®¡ç®—æ—¶é—´çš„æ­¥éª¤ï¼ˆt=1, t=2, t=3ï¼‰ã€‚
* æ¯ä¸ªç®­å¤´è¡¨ç¤ºï¼šç¬¬å‡ ä¸ªè¯å¯¹åº”ç¬¬å‡ æ­¥è®¡ç®—ã€‚

ğŸ” å°±æ˜¯è¯´ï¼š**æ¨¡å‹ç¬¬1æ­¥å°±å¤„ç†ç¬¬1ä¸ªè¯ï¼Œç¬¬2æ­¥å¤„ç†ç¬¬2ä¸ªè¯â€¦â€¦å¿…é¡»æŒ‰é¡ºåºæ¥ï¼Œä¸èƒ½è·³è¿‡ï¼Œä¹Ÿä¸èƒ½ä¸€èµ·ç®—ã€‚**

è¿™å°±æ˜¯RNNçš„â€œé¡ºåºæ€§â€â€”â€”å®ƒä¸èƒ½å¹¶è¡Œï¼Œåªèƒ½ä¸€æ­¥ä¸€æ­¥æ¥ã€‚è¿™ä¹Ÿæ­£æ˜¯Transformeræƒ³è¦çªç ´çš„é™åˆ¶ã€‚éœ€è¦æˆ‘å¯¹æ¯”ä¸€ä¸‹Transformeræ˜¯æ€ä¹ˆåšçš„å—ï¼Ÿ

éœ€è¦

![](../pictures/attention_1.png "")

è¿™å¼ å›¾å±•ç¤ºäº† **Transformer å’Œ RNN çš„æ ¸å¿ƒåŒºåˆ«**ï¼š

* ğŸ”µ ä¸Šæ–¹ï¼šæ¯ä¸ªè¯çš„ä½ç½®ï¼ˆPosition 1, 2, 3ï¼‰
* ğŸŸ¢ ä¸‹æ–¹ç»¿è‰²åŒºåŸŸï¼š**Transformer æ˜¯â€œåŒæ—¶å¤„ç†æ‰€æœ‰è¯â€**ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„â€œå¹¶è¡Œè®¡ç®—ï¼ˆParallel Computationï¼‰â€
* æ¯ä¸ªç®­å¤´éƒ½æŒ‡å‘åŒä¸€ä¸ªè®¡ç®—åŒºåŸŸï¼Œè¡¨ç¤ºè¿™äº›è¯**å¯ä»¥ä¸€èµ·è¢«å¤„ç†**

---

### ğŸ“Œ æ€»ç»“å¯¹æ¯”ï¼š

| æ¨¡å‹          | å¦‚ä½•å¤„ç†è¯åºåˆ—        |
| ----------- | -------------- |
| RNN         | ä¸€æ­¥ä¸€æ­¥æ¥ï¼ˆä¸èƒ½å¹¶è¡Œï¼‰    |
| Transformer | ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰è¯ï¼ˆå¯ä»¥å¹¶è¡Œï¼‰ |

è¿™æ­£æ˜¯ Transformer æ›´å¿«ã€æ›´é«˜æ•ˆçš„åŸå› ä¹‹ä¸€ã€‚
å¦‚æœä½ è¿˜æƒ³äº†è§£â€œTransformer æ˜¯æ€ä¹ˆåšåˆ°å¹¶è¡Œâ€çš„ï¼Œæ¯”å¦‚å¤šå¤´æ³¨æ„åŠ›æˆ–ä½ç½®ç¼–ç ï¼Œæˆ‘ä¹Ÿå¯ä»¥ç»§ç»­è®²è§£ã€‚

---

è¿™é‡Œé¢çš„functionæ˜¯ä»€ä¹ˆæ„æ€ï¼Œè¯¦ç»†è§£é‡Šä¸€ä¸‹

---

è¿™ä¸ªå¥å­é‡Œæåˆ°çš„ **â€œas a function of...â€** æ˜¯ä¸€ä¸ªéå¸¸å¸¸è§çš„æ•°å­¦è¡¨è¾¾æ–¹å¼ï¼Œæˆ‘ä»¬æ¥ä¸€æ­¥ä¸€æ­¥è§£æã€‚

---

## ğŸ“˜ åŸå¥å†è¯»ä¸€éï¼š

> **â€œ...they generate a sequence of hidden states $h_t$, as a function of the previous hidden state $h_{t-1}$ and the input for position $t$.â€**

---

## âœ… è¿™å¥è¯å¤§æ„æ˜¯ï¼š

æ¨¡å‹ä¼šç”Ÿæˆä¸€ç³»åˆ—éšè—çŠ¶æ€ $h_t$ï¼Œ
æ¯ä¸€ä¸ª $h_t$ æ˜¯ä¸€ä¸ªâ€œå‡½æ•°â€â€”â€”å®ƒ**ä¾èµ–äº**ï¼š

1. ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ $h_{t-1}$
2. å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥ $x_t$

---

## ğŸ” ä»€ä¹ˆæ˜¯ â€œfunction ofâ€ï¼Ÿ

### ğŸ‘‰ é€šä¿—è§£é‡Šï¼š

â€œFunction ofâ€ å¯ä»¥ç†è§£ä¸ºï¼š

> **æŸä¸ªä¸œè¥¿æ˜¯ç”±å…¶ä»–å› ç´ é€šè¿‡æŸç§â€œè®¡ç®—å…³ç³»â€äº§ç”Ÿçš„ã€‚**

ä¸¾ä¸ªç”Ÿæ´»ä¸­ç±»æ¯”ï¼š

* å’–å•¡çš„å‘³é“æ˜¯ç‰›å¥¶ã€å’–å•¡ç²‰ã€æ°´ã€ç³–çš„ functionï¼ˆå‡½æ•°ï¼‰ â†’ å‘³é“æ˜¯å®ƒä»¬æ··åˆåçš„ç»“æœã€‚
* å¤©æ°”æ¸©åº¦æ˜¯é˜³å…‰å¼ºåº¦ã€é£é€Ÿã€æ¹¿åº¦çš„ function â†’ æ¸©åº¦å–å†³äºè¿™äº›å› ç´ ã€‚

---

### ğŸ‘‰ åœ¨ç¥ç»ç½‘ç»œé‡Œï¼š

è¿™ä¸ªâ€œå‡½æ•°â€å…¶å®æ˜¯ä¸€ä¸ªè®¡ç®—è¿‡ç¨‹ï¼Œæ¯”å¦‚ï¼š

$$
h_t = \tanh(W_h \cdot h_{t-1} + W_x \cdot x_t + b)
$$

è¿™å°±æ˜¯ï¼š

* å½“å‰éšè—çŠ¶æ€ $h_t$
* æ˜¯ç”±ä¸Šä¸€æ­¥çŠ¶æ€ $h_{t-1}$ å’Œ å½“å‰è¾“å…¥ $x_t$
* **é€šè¿‡çŸ©é˜µè®¡ç®—å’Œæ¿€æ´»å‡½æ•°ç»„åˆå‡ºæ¥çš„ç»“æœ**

---

### âœ… æ›´å£è¯­åŒ–ç¿»è¯‘åŸå¥ï¼š

â€œæ¨¡å‹ä¼šä¾æ¬¡ç”Ÿæˆéšè—çŠ¶æ€ $h_t$ï¼Œæ¯ä¸€ä¸ªéƒ½æ˜¯æ ¹æ®ä¸Šä¸€ä¸ªçŠ¶æ€å’Œå½“å‰è¾“å…¥è®¡ç®—å‡ºæ¥çš„ç»“æœã€‚â€

---

### âœ… æ›´æŠ€æœ¯ä¸€ç‚¹çš„è¯´æ³•ï¼š

â€œæ¯ä¸€ä¸ªéšè—çŠ¶æ€ $h_t$ï¼Œæ˜¯ä¸€ä¸ªå‡½æ•°ï¼š

$$
h_t = f(h_{t-1}, x_t)
$$

å…¶ä¸­ $f$ æ˜¯ç¥ç»ç½‘ç»œå†…éƒ¨å®šä¹‰å¥½çš„è®¡ç®—å…¬å¼ï¼ˆæ¯”å¦‚å‰é¦ˆå±‚+æ¿€æ´»å‡½æ•°ï¼‰ã€‚â€

---

## ğŸ“Œ æ€»ç»“ï¼š

| æœ¯è¯­                  | æ„æ€                                              |
| ------------------- | ----------------------------------------------- |
| Function of A and B | ç”± A å’Œ B ç»è¿‡æŸç§è§„åˆ™/å…¬å¼/è¿ç®— å¾—å‡ºçš„ç»“æœ                      |
| In this context     | å½“å‰éšè—çŠ¶æ€ $h_t$ æ˜¯å‰ä¸€ä¸ªçŠ¶æ€ $h_{t-1}$ å’Œå½“å‰è¾“å…¥ $x_t$ çš„è®¡ç®—ç»“æœ |
| å®è´¨ä¸Šå°±æ˜¯               | ç¥ç»ç½‘ç»œçš„ä¸€ä¸ªâ€œè®¡ç®—æ­¥éª¤â€                                   |

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥ç”»å‡ºè¿™ä¸ªè®¡ç®—æµç¨‹å›¾ï¼ˆRNNçš„åŸºæœ¬ç»“æ„å›¾ï¼‰æ¥å¸®åŠ©ä½ æ›´å½¢è±¡åœ°ç†è§£è¿™ä¸ªâ€œfunctionâ€çš„å«ä¹‰ï¼Œè¦ä¸è¦ï¼Ÿ


---

### åŸæ–‡ï¼š

> **This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.**

âœ… **è§£é‡Šï¼š**
å› ä¸ºè¿™ä¸ªæ¨¡å‹å¿…é¡»**ä¸€æ­¥ä¸€æ­¥å¤„ç†**ï¼ˆæ¯”å¦‚å¿…é¡»å…ˆå¤„ç†ç¬¬1ä¸ªè¯ï¼Œæ‰èƒ½å¤„ç†ç¬¬2ä¸ªè¯ï¼‰ï¼Œæ‰€ä»¥å®ƒ**ä¸èƒ½å¹¶è¡Œå¤„ç†åŒä¸€ä¸ªå¥å­é‡Œçš„æ‰€æœ‰è¯**ã€‚
è¿™åœ¨å¥å­å¾ˆé•¿çš„æ—¶å€™å°±æˆäº†å¤§é—®é¢˜ï¼Œå› ä¸ºï¼š

* å¹¶è¡Œä¸äº† â†’ æ…¢ï¼›
* æ¯æ¬¡åªèƒ½å¤„ç†æœ‰é™æ•°é‡çš„å¥å­ â†’ **å†…å­˜ä¸å¤Ÿï¼Œæ‰¹å¤„ç†æ•°é‡å—é™**ã€‚

---

è¯¦ç»†è§£é‡Šä¸€ä¸‹ sequential nature

---

å½“ç„¶å¯ä»¥ï¼æˆ‘ä»¬æ¥è¯¦ç»†å‰–æè¿™å¥è¯é‡Œçš„ **â€œsequential natureâ€** æ˜¯ä»€ä¹ˆï¼Œä»¥åŠå®ƒä¸ºä»€ä¹ˆä¼šæˆä¸ºé—®é¢˜ã€‚

---

## ğŸ“˜ åŸå¥ï¼š

> **This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.**

---

## âœ… ä»€ä¹ˆæ˜¯ "sequential nature"?

### âœ… åŸè¯è§£é‡Šï¼š

* **Sequential** = æŒ‰é¡ºåºï¼Œä¸€ä¸ªæ¥ä¸€ä¸ª
* **Nature** = æœ¬è´¨ã€ç‰¹æ€§

â¡ï¸ æ‰€ä»¥ **sequential nature** æŒ‡çš„æ˜¯ï¼š

> **æ¨¡å‹å¤©ç”Ÿå°±å¿…é¡»â€œæŒ‰é¡ºåºâ€ä¸€æ­¥æ­¥æ¥å¤„ç†æ•°æ®çš„ç‰¹æ€§ã€‚**

---

## ğŸ” åœ¨RNNä¸­ï¼Œä¸ºä»€ä¹ˆæ˜¯ sequentialï¼Ÿ

RNN çš„åŸºæœ¬å·¥ä½œæ–¹å¼æ˜¯è¿™æ ·çš„ï¼š

$$
h_t = f(h_{t-1}, x_t)
$$

ä¹Ÿå°±æ˜¯è¯´ï¼š

* å½“å‰è¿™ä¸€æ­¥çš„éšè—çŠ¶æ€ $h_t$ï¼Œ**å¿…é¡»ç­‰ä¸Šä¸€æ­¥ $h_{t-1}$ å¤„ç†å®Œä¹‹åæ‰èƒ½è®¡ç®—å‡ºæ¥**
* æ‰€ä»¥ä½ æ²¡æ³•åŒæ—¶ç®—ç¬¬1æ­¥ã€ç¬¬2æ­¥ã€ç¬¬3æ­¥ â†’ **åªèƒ½ä¸€æ­¥ä¸€æ­¥æ¥**

ğŸ‘‰ è¿™å°±æ˜¯ RNN çš„â€œé¡ºåºæ€§æœ¬è´¨â€ï¼ˆ**inherently sequential nature**ï¼‰

---

## ğŸ’¡ ä¸¾ä¸ªä¾‹å­ï¼š

æƒ³è±¡ä½ åœ¨è¯»ä¸€å¥è¯ï¼šâ€œæˆ‘ å–œæ¬¢ çŒ«ã€‚â€

RNN çš„æ€è·¯æ˜¯è¿™æ ·ï¼š

1. å…ˆå¤„ç†â€œæˆ‘â€ â†’ å¾—åˆ°çŠ¶æ€ h1
2. ç”¨ h1 å’Œâ€œå–œæ¬¢â€ â†’ å¾—åˆ° h2
3. ç”¨ h2 å’Œâ€œçŒ«â€ â†’ å¾—åˆ° h3

ä½ ä¸èƒ½ç›´æ¥ç®— h3ï¼Œå¿…é¡»ç­‰å‰é¢çš„éƒ½ç®—å®Œã€‚è¿™å°±æ˜¯ **ä¸èƒ½å¹¶è¡Œï¼Œåªèƒ½æŒ‰é¡ºåºæ¥**ã€‚

---

## ğŸš« ä¸ºä»€ä¹ˆè¿™ä¸ªâ€œé¡ºåºæ€§â€å¾ˆç³Ÿç³•ï¼Ÿ

### åŸæ–‡è¯´çš„æ˜¯ï¼š

> **It precludes parallelization within training examples**
> æ„æ€æ˜¯ï¼š

* **åœ¨ä¸€ä¸ªè®­ç»ƒæ ·æœ¬å†…éƒ¨ï¼Œæ²¡æ³•å¹¶è¡Œå¤„ç†**ï¼ˆå› ä¸ºä½ å¿…é¡»ç­‰å‰ä¸€æ­¥æ‰èƒ½ç®—ä¸‹ä¸€æ­¥ï¼‰

---

## ğŸ˜“ é‚£ä¸ºä»€ä¹ˆâ€œcritical at longer sequence lengthsâ€ï¼Ÿ

éšç€å¥å­è¶Šæ¥è¶Šé•¿ï¼Œæ¯”å¦‚ï¼š

> â€œåœ¨é˜³å…‰æ˜åªšçš„åˆåï¼Œæˆ‘ä¸€ä¸ªäººååœ¨å›¾ä¹¦é¦†çš„è§’è½é‡Œé˜…è¯»â€¦â€¦â€

* æ­¥æ•°å˜å¤šï¼ˆæ¯ä¸ªè¯éƒ½è¦å•ç‹¬å¤„ç†ä¸€æ­¥ï¼‰
* æ€»ä½“è®¡ç®—æ—¶é—´å˜é•¿
* **ä¸èƒ½åŠ å¿«é€Ÿåº¦ï¼Œå› ä¸ºä¸èƒ½å¹¶è¡Œ**

åŒæ—¶ï¼Œå†…å­˜ä¹Ÿåƒä¸æ¶ˆï¼Œä¸èƒ½åŒæ—¶å¤„ç†å¤ªå¤šå¥å­ã€‚

---

## ğŸ“Œ æ€»ç»“ä¸€å¥è¯ï¼š

| æœ¯è¯­                    | é€šä¿—è§£é‡Š                       |
| --------------------- | -------------------------- |
| **Sequential nature** | æ¨¡å‹å¿…é¡»ä¸€æ­¥ä¸€æ­¥ã€æŒ‰é¡ºåºå¤„ç†æ¯ä¸ªè¯          |
| **ä¸ºä»€ä¹ˆæ˜¯é—®é¢˜ï¼Ÿ**           | æ²¡æ³•åŒæ—¶å¤„ç†å¤šä¸ªè¯ï¼Œè®­ç»ƒæ…¢ï¼Œå°¤å…¶æ˜¯å¥å­å¾ˆé•¿æ—¶     |
| **Transformerçš„çªç ´ï¼Ÿ**   | å½»åº•æ‰“ç ´è¿™ä¸ªé¡ºåºæ€§ â†’ ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªå¥å­ï¼ˆå¹¶è¡Œï¼ï¼‰ |

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘ä¹Ÿå¯ä»¥ç”»ä¸€å¼ å›¾æ¥å¯¹æ¯” RNN çš„â€œé¡ºåºå¤„ç†â€ å’Œ Transformer çš„â€œå¹¶è¡Œå¤„ç†â€æµç¨‹å›¾ï¼Œè¦çœ‹çœ‹å—ï¼Ÿ

---

across examplesä¸­çš„acrossæ˜¯ä»€ä¹ˆæ„æ€

---

å¾ˆå¥½ï¼Œè¿™æ˜¯ä¸ªéå¸¸å¸¸è§ä½†å®¹æ˜“ææ··çš„è¯ã€‚

---

## ğŸ“˜ å¥å­åŸæ–‡ï¼š

> **"...as memory constraints limit batching across examples."**

æˆ‘ä»¬æ¥é‡ç‚¹è§£é‡Šè¿™é‡Œçš„ **â€œacross examplesâ€**ã€‚

---

## âœ… across çš„åŸºæœ¬æ„æ€ï¼š

â€œ**across**â€ æ˜¯ä¸€ä¸ªä»‹è¯ï¼ŒåŸºæœ¬æ„æ€æ˜¯ï¼š

> â€œæ¨ªè·¨â€ã€â€œè·¨è¶Šâ€ã€â€œè¦†ç›–å¤šä¸ªâ€

æ‰€ä»¥ â€œacross examplesâ€ å°±æ˜¯ï¼š

> **â€œåœ¨å¤šä¸ªè®­ç»ƒæ ·æœ¬ä¹‹é—´â€**ï¼Œæˆ–è€…è¯´ **â€œè·¨æ ·æœ¬â€**

---

### ğŸ“ ä¸¾ä¸ªä¾‹å­ï¼š

æ¯”å¦‚ä½ è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ—¶ï¼Œä¼šä¸€æ¬¡æ€§æŠŠå¾ˆå¤šå¥å­æ‰“åŒ…æˆä¸€ä¸ªâ€œbatchâ€ï¼ˆæ‰¹æ¬¡ï¼‰ï¼Œè¿™æ ·è®¡ç®—æ›´é«˜æ•ˆã€‚è¿™ä¸ªæ‰“åŒ…è¿‡ç¨‹å°±å« batchingã€‚

* å¦‚æœä½ æ¯æ¬¡è®­ç»ƒå¤„ç†10å¥è¯ï¼Œå°±æ˜¯åœ¨ **batching across 10 examples**ã€‚
* å¦‚æœä½ å†…å­˜ä¸å¤Ÿï¼Œåªèƒ½è£…ä¸‹3å¥è¯ï¼Œé‚£ä½ åªèƒ½ batching across 3 examplesã€‚

---

## âœ… å›åˆ°åŸå¥æ„æ€ï¼š

> **"as memory constraints limit batching across examples."**

ç¿»è¯‘æˆå¤§ç™½è¯å°±æ˜¯ï¼š

> â€œç”±äºå†…å­˜æœ‰é™ï¼Œä¸èƒ½ä¸€æ¬¡æ€§å¤„ç†å¤ªå¤šå¥å­ã€‚â€

ä¹Ÿå°±æ˜¯ï¼š

* ä¸€è¾¹æ˜¯ï¼š**æ¨¡å‹å†…éƒ¨ï¼ˆå¥å­é‡Œï¼‰ä¸èƒ½å¹¶è¡Œï¼ˆå› ä¸ºé¡ºåºæ€§ï¼‰**
* å¦ä¸€è¾¹æ˜¯ï¼š**å†…å­˜åˆé™åˆ¶äº†æˆ‘ä»¬åœ¨å¤šä¸ªå¥å­ä¹‹é—´çš„å¹¶è¡Œå¤„ç†ï¼ˆacross examplesï¼‰**

â¡ï¸ **åŒé‡å—é™ï¼Œè®­ç»ƒæ•ˆç‡ä½ã€‚**

---

## ğŸ“Œ æ€»ç»“ï¼š

| è¡¨è¾¾                       | æ„æ€                  |
| ------------------------ | ------------------- |
| across examples          | åœ¨å¤šä¸ªè®­ç»ƒæ ·æœ¬ä¹‹é—´           |
| batching across examples | æŠŠå¤šä¸ªè®­ç»ƒæ ·æœ¬ï¼ˆå¥å­ï¼‰ä¸€èµ·æ‰“åŒ…å¤„ç†   |
| é™åˆ¶                       | å†…å­˜ä¸è¶³ï¼Œå°±ä¸èƒ½åœ¨å¤šä¸ªæ ·æœ¬ä¹‹é—´å¹¶è¡Œè®­ç»ƒ |

---

å¦‚æœä½ è¿˜æƒ³çŸ¥é“â€œbatchingâ€æ˜¯æ€ä¹ˆæ“ä½œçš„ï¼Œä¹Ÿå¯ä»¥ç»§ç»­é—®ï¼Œæˆ‘å¯ä»¥ç”»ä¸ªæµç¨‹å›¾å¸®ä½ ç›´è§‚ç†è§£ã€‚


---

### åŸæ–‡ï¼š

> **Recent work has achieved significant improvements in computational efficiency through factorization tricks \[21] and conditional computation \[32], while also improving model performance in case of the latter.**

âœ… **è§£é‡Šï¼š**
æœ€è¿‘æœ‰äº›ç ”ç©¶å°è¯•ç”¨ä¸€äº›â€œæŠ€å·§â€æ¥è®©è®¡ç®—å˜å¿«ï¼Œæ¯”å¦‚ï¼š

* **åˆ†è§£è®¡ç®—**ï¼šæŠŠå¤§çš„é—®é¢˜æ‹†æˆå¤šä¸ªå°å—æ¥å¤„ç†ï¼›
* **æ¡ä»¶è®¡ç®—**ï¼šæ¨¡å‹åªæ¿€æ´»ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯å…¨éƒ¨éƒ½è·‘ä¸€éã€‚

è¿™äº›æ–¹æ³•åœ¨æé«˜è¿è¡Œæ•ˆç‡çš„åŒæ—¶ï¼Œæœ‰æ—¶è¿˜èƒ½è®©æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚

---

å½“ç„¶å¯ä»¥ï¼æˆ‘ä»¬æ¥è¯¦ç»†è§£é‡Šä¸€ä¸‹ **â€œin case of the latterâ€** è¿™ä¸ªçŸ­è¯­çš„æ„æ€å’Œç”¨æ³•ã€‚

---

## ğŸ“˜ åŸå¥ï¼ˆä¸Šä¸‹æ–‡ï¼‰æ˜¯è¿™æ ·çš„ï¼š

> "Recent work has achieved significant improvements in computational efficiency through factorization tricks \[21] and conditional computation \[32], **while also improving model performance in case of the latter.**"

---

## âœ… åˆ†æç»“æ„

è¿™ä¸ªå¥å­é‡Œæœ‰ä¸¤ä¸ªå…³é”®ç‚¹ï¼ˆå¹¶åˆ—çš„ï¼‰ï¼š

1. **factorization tricks**ï¼ˆåˆ†è§£æŠ€å·§ï¼‰
2. **conditional computation**ï¼ˆæ¡ä»¶è®¡ç®—ï¼‰

è¿™ä¸¤ä¸ªæ˜¯â€œæœ€è¿‘æé«˜æ•ˆç‡çš„åšæ³•â€ã€‚

---

è¯¦ç»†è§£é‡Šä¸€ä¸‹ in case of the latter

---

## âœ… ä»€ä¹ˆæ˜¯ "the latter"ï¼Ÿ

â€œ**the latter**â€ æ˜¯ä¸ªä»£è¯ï¼Œæ„æ€æ˜¯ï¼š

> **â€œåè€…â€**ï¼Œå³ä¸Šé¢åˆ—å‡ºçš„ä¸¤ä¸ªä¸œè¥¿ä¸­çš„ç¬¬äºŒä¸ªã€‚

ğŸ‘† æ‰€ä»¥è¿™é‡Œçš„ "the latter" å°±æ˜¯ï¼š

> **conditional computation**

---

## âœ… ä»€ä¹ˆæ˜¯ "in case of..."ï¼Ÿ

è¿™ä¸ªçŸ­è¯­å¯ä»¥ç†è§£ä¸ºï¼š

* **â€œåœ¨â€¦â€¦çš„æƒ…å†µä¸‹â€**
* **â€œå°±â€¦â€¦è€Œè¨€â€**

æ‰€ä»¥ **"in case of the latter"** å…¶å®ç­‰äºï¼š

> **â€œåœ¨æ¡ä»¶è®¡ç®—ï¼ˆconditional computationï¼‰çš„æƒ…å†µä¸‹â€**
> **â€œå°±åè€…è€Œè¨€â€**

---

## âœ… æ•´å¥è¯å¤§ç™½è¯ç¿»è¯‘ï¼š

> â€œæœ€è¿‘çš„ä¸€äº›ç ”ç©¶é€šè¿‡åˆ†è§£æŠ€å·§å’Œæ¡ä»¶è®¡ç®—æ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ï¼Œ
> è€Œ**æ¡ä»¶è®¡ç®—**åœ¨æå‡æ•ˆç‡çš„åŒæ—¶ï¼Œè¿˜èƒ½æå‡æ¨¡å‹æ€§èƒ½ã€‚â€

---

## ğŸ“Œ æ€»ç»“ï¼š

| çŸ­è¯­                    | å«ä¹‰                                         |
| --------------------- | ------------------------------------------ |
| the latter            | åè€…ï¼Œè¿™é‡ŒæŒ‡ conditional computation             |
| in case of the latter | åœ¨åè€…ï¼ˆconditional computationï¼‰çš„æƒ…å†µä¸‹ï¼Œä¹Ÿå°±æ˜¯â€œè¯´åˆ°åè€…â€ |

---

å¦‚æœä½ è¿˜ä¸ç¡®å®šâ€œformerâ€å’Œâ€œlatterâ€è¿™ç§ç»“æ„çš„ç”¨æ³•ï¼Œæˆ‘ä¹Ÿå¯ä»¥ç»™ä½ åˆ—å‡ ä¸ªå°ä¾‹å­æ¥å¯¹æ¯”ç†è§£ã€‚éœ€è¦å—ï¼Ÿ


---

### åŸæ–‡ï¼š

> **The fundamental constraint of sequential computation, however, remains.**

âœ… **è§£é‡Šï¼š**
ä½†**æœ€å¤§çš„é—®é¢˜è¿˜åœ¨**ï¼šè¿™ç§â€œå¿…é¡»ä¸€æ­¥æ­¥æ¥â€çš„æ–¹å¼ï¼Œè¿˜æ˜¯æ²¡æ³•è§£å†³ã€‚

---

### æ€»ç»“ä¸€å¥è¯ï¼š

å¾ªç¯æ¨¡å‹æ˜¯â€œæŒ‰é¡ºåºâ€ä¸€ä¸ªä¸€ä¸ªè¯åœ°å¤„ç†çš„ï¼Œè¿™ç§å¤„ç†æ–¹å¼é™åˆ¶äº†å¹¶è¡Œè®¡ç®—ï¼Œå°¤å…¶åœ¨é•¿å¥å­é‡Œæ•ˆç‡ä½ã€‚è™½ç„¶æœ‰äº›æŠ€å·§èƒ½ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†**æœ¬è´¨ä¸Šçš„é¡ºåºé™åˆ¶è¿˜æ˜¯å­˜åœ¨**ã€‚

---

Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.

é€å¥å¸®æˆ‘è§£é‡Šæ¸…æ¥šï¼Œéœ€è¦ç”¨æµ…æ˜¾æ˜“æ‡‚çš„è¯æ¥è§£é‡Šã€‚

---

å½“ç„¶å¯ä»¥ï¼æˆ‘ä»¬æ¥ä¸€å¥ä¸€å¥åœ°ç”¨**å¤§ç™½è¯**è§£é‡Šè¿™æ®µå†…å®¹ï¼š

---

## ğŸ“˜ åŸå¥ 1ï¼š

> **"Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks..."**

âœ… **é€šä¿—è§£é‡Šï¼š**

æ³¨æ„åŠ›æœºåˆ¶å·²ç»å˜æˆäº†å„ç§â€œå‰å®³çš„â€åºåˆ—å»ºæ¨¡å’Œè½¬æ¢æ¨¡å‹ä¸­çš„**é‡è¦ç»„æˆéƒ¨åˆ†**ï¼Œå‡ ä¹æ˜¯â€œæ ‡é…â€äº†ã€‚

* â€œAttention mechanismâ€ï¼šæ³¨æ„åŠ›æœºåˆ¶ï¼Œè®©æ¨¡å‹è‡ªåŠ¨å…³æ³¨é‡è¦çš„ä¿¡æ¯ï¼›
* â€œIntegral partâ€ï¼šä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ï¼›
* â€œSequence modeling and transductionâ€ï¼šå¤„ç†åºåˆ—ï¼ˆåƒè¯­è¨€ã€éŸ³é¢‘ã€ç¿»è¯‘ï¼‰ï¼›
* â€œCompellingâ€ï¼šæ•ˆæœå¾ˆå¥½çš„ã€æœ‰è¯´æœåŠ›çš„ã€‚

ğŸ¯ æ¢å¥è¯è¯´ï¼š

> ç°åœ¨åªè¦æ˜¯å¤„ç†è¯­è¨€ç­‰åºåˆ—é—®é¢˜çš„å¥½æ¨¡å‹ï¼ŒåŸºæœ¬ä¸Šéƒ½ä¼šç”¨ä¸Šâ€œæ³¨æ„åŠ›æœºåˆ¶â€ã€‚

---

è¯¦ç»†è§£é‡Šä¸€ä¸‹ sequence modeling and transduction models

å¥½é—®é¢˜ï¼æˆ‘ä»¬æ¥è¯¦ç»†è§£é‡Šè¿™ä¸¤ä¸ªæœ¯è¯­ï¼š

---

## âœ… 1. **Sequence Modelingï¼ˆåºåˆ—å»ºæ¨¡ï¼‰**

### ğŸ“˜ å®šä¹‰ï¼š

**Sequence modeling** å°±æ˜¯ï¼š

> **ç†è§£ã€é¢„æµ‹æˆ–ç”Ÿæˆâ€œåºåˆ—â€æ•°æ®çš„æ¨¡å‹ã€‚**

### ğŸ”¢ ä»€ä¹ˆæ˜¯â€œåºåˆ—â€ï¼Ÿ

åºåˆ—å°±æ˜¯æœ‰é¡ºåºçš„ä¸€ä¸²ä¸œè¥¿ï¼Œæ¯”å¦‚ï¼š

* ä¸€å¥è¯ï¼ˆè¯æ˜¯æœ‰é¡ºåºçš„ï¼‰ğŸ‘‰ è‡ªç„¶è¯­è¨€å¤„ç†
* ä¸€æ®µè¯­éŸ³ï¼ˆå£°éŸ³å¸§æ˜¯æŒ‰æ—¶é—´æ’çš„ï¼‰ğŸ‘‰ è¯­éŸ³è¯†åˆ«
* è‚¡ç¥¨ä»·æ ¼ï¼ˆä¸€å¤©ä¸€å¤©çš„æ•°æ®ï¼‰ğŸ‘‰ æ—¶é—´åºåˆ—åˆ†æ

### ğŸ¯ ç›®çš„ï¼š

* é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼ˆè¯­è¨€æ¨¡å‹ï¼‰
* ç†è§£æ•´æ®µè¯çš„æ„æ€ï¼ˆæƒ…æ„Ÿåˆ†æï¼‰
* æ‰¾å‡ºå…³é”®è¯ï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰

### ğŸ§  ä¸¾ä¾‹ï¼š

* RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰
* LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼‰
* Transformer

---

## âœ… 2. **Transduction Modelsï¼ˆè½¬æ¢æ¨¡å‹ / è½¬å¯¼æ¨¡å‹ï¼‰**

### ğŸ“˜ å®šä¹‰ï¼š

**Transduction** æ˜¯ä¸€ä¸ªä¸“ä¸šæœ¯è¯­ï¼Œæ„æ€æ˜¯ï¼š

> **æŠŠä¸€ä¸ªåºåˆ—â€œè½¬æ¢â€æˆå¦ä¸€ä¸ªåºåˆ—ã€‚**

è¿™ç§æ¨¡å‹ä¸ä»…è¦ç†è§£è¾“å…¥åºåˆ—ï¼Œè¿˜è¦**ç”Ÿæˆè¾“å‡ºåºåˆ—**ã€‚

### ğŸ” ä¸¾ä¾‹ä»»åŠ¡ï¼š

| è¾“å…¥åºåˆ— | è¾“å‡ºåºåˆ— | ä»»åŠ¡åç§°   |
| ---- | ---- | ------ |
| è‹±æ–‡å¥å­ | ä¸­æ–‡å¥å­ | æœºå™¨ç¿»è¯‘   |
| éŸ³é¢‘ä¿¡å· | è‹±æ–‡æ–‡å­— | è¯­éŸ³è¯†åˆ«   |
| é—®é¢˜   | å›ç­”   | é—®ç­”ç³»ç»Ÿ   |
| å›¾åƒæè¿° | æ–‡å­—   | å›¾åƒå­—å¹•ç”Ÿæˆ |

### ğŸ§  ä¸¾ä¾‹æ¨¡å‹ï¼š

* ç¼–ç å™¨-è§£ç å™¨ï¼ˆEncoder-Decoderï¼‰æ¶æ„
* Seq2Seq æ¨¡å‹ï¼ˆsequence-to-sequenceï¼‰
* Transformerï¼ˆä¹Ÿæ˜¯ä¸€ç§è½¬å¯¼æ¨¡å‹ï¼‰

---

## ğŸ” æ‰€ä»¥è¿™ä¸¤ä¸ªçš„åŒºåˆ«æ˜¯ï¼š

| ç±»å‹                 | æ ¸å¿ƒä»»åŠ¡         | ä¸¾ä¸ªä¾‹å­      |
| ------------------ | ------------ | --------- |
| Sequence Modeling  | ç†è§£æˆ–é¢„æµ‹ä¸€ä¸ªåºåˆ—    | é¢„æµ‹ä¸‹ä¸€ä¸ªè¯    |
| Transduction Model | æŠŠä¸€ä¸ªåºåˆ—å˜æˆå¦ä¸€ä¸ªåºåˆ— | è‹±è¯‘ä¸­ã€è¯­éŸ³è½¬æ–‡å­— |

---

## ğŸ“Œ æ€»ç»“ä¸€å¥è¯ï¼š

> * **Sequence modeling** æ˜¯ç†è§£/å»ºæ¨¡ä¸€ä¸ªåºåˆ—ï¼›
> * **Transduction models** æ˜¯æŠŠä¸€ä¸ªåºåˆ—å˜æˆå¦ä¸€ä¸ªåºåˆ—ï¼ˆæ¯”å¦‚ç¿»è¯‘ï¼‰ã€‚

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘ä¹Ÿå¯ä»¥ç”»å¼ å›¾ï¼ŒæŠŠâ€œsequence modelingâ€å’Œâ€œtransductionâ€åœ¨æµç¨‹å›¾ä¸Šå±•ç¤ºå‡ºæ¥ï¼Œç›´è§‚å¯¹æ¯”ã€‚éœ€è¦å—ï¼Ÿ


---

## ğŸ“˜ åŸå¥ 2ï¼š

> **"...allowing modeling of dependencies without regard to their distance in the input or output sequences \[2, 19]."**

âœ… **é€šä¿—è§£é‡Šï¼š**

æ³¨æ„åŠ›æœºåˆ¶çš„å‰å®³ä¹‹å¤„åœ¨äºï¼š

> å®ƒå¯ä»¥è®©æ¨¡å‹å­¦ä¹ **è¯å’Œè¯ä¹‹é—´çš„å…³ç³»ï¼ˆä¾èµ–ï¼‰**ï¼Œ**ä¸ç®¡å®ƒä»¬åœ¨å¥å­é‡Œç¦»å¾—æœ‰å¤šè¿œ**ï¼

æ¯”å¦‚ï¼š

* â€œä»Šå¤©çš„å¤©æ°”å¾ˆå†·ï¼Œæˆ‘**ç©¿äº†åšå¤–å¥—**ã€‚â€ â†’ â€œå†·â€å’Œâ€œå¤–å¥—â€åœ¨å¥å­ä¸­å¯èƒ½éš”å¾ˆè¿œï¼Œä½†å®ƒä»¬æœ‰å…³ç³»ï¼Œæ³¨æ„åŠ›æœºåˆ¶èƒ½å¤„ç†åˆ°è¿™ä¸€ç‚¹ã€‚

---

## ğŸ“˜ åŸå¥ 3ï¼š

> **"In all but a few cases \[27], however, such attention mechanisms are used in conjunction with a recurrent network."**

âœ… **é€šä¿—è§£é‡Šï¼š**

ä¸è¿‡ï¼Œè¦æ³¨æ„ä¸€ç‚¹ï¼š

> **å¤§å¤šæ•°æƒ…å†µä¸‹**ï¼Œæ³¨æ„åŠ›æœºåˆ¶æ˜¯å’Œ **å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ­é…ä½¿ç”¨**çš„ï¼Œ
> åªæœ‰æå°‘æ•°ä¾‹å¤–ï¼ˆæ¯”å¦‚ç¬¬\[27]ç¯‡è®ºæ–‡ï¼‰ã€‚

ğŸ“ æ¢å¥è¯è¯´ï¼š

* ä¹‹å‰çš„æ¨¡å‹ï¼š**RNN æ˜¯ä¸»åŠ›ï¼Œæ³¨æ„åŠ›åªæ˜¯è¾…åŠ©**ï¼›
* Transformer æƒ³åšçš„å°±æ˜¯ï¼š**æŠŠ RNN å®Œå…¨æ¢æ‰ï¼Œåªç”¨æ³¨æ„åŠ›ï¼**

---

## ğŸ“Œ æ€»ç»“æ•´æ®µï¼š

1. æ³¨æ„åŠ›æœºåˆ¶ç°åœ¨å·²ç»æˆäº†åºåˆ—æ¨¡å‹ä¸­éå¸¸é‡è¦çš„ä¸€éƒ¨åˆ†ï¼›
2. å®ƒå¯ä»¥å¸®åŠ©æ¨¡å‹ç†è§£**ä»»æ„è·ç¦»çš„è¯ä¹‹é—´çš„å…³ç³»**ï¼›
3. ä½†å‡ ä¹æ‰€æœ‰æ¨¡å‹è¿˜æ˜¯æŠŠå®ƒè·Ÿ**RNN ç»“åˆç€ç”¨**ï¼Œå¾ˆå°‘å•ç‹¬ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ã€‚

ğŸ¯ è¿™å°±å¼•å‡ºäº† Transformer çš„åŠ¨æœºï¼šæˆ‘ä»¬èƒ½ä¸èƒ½**åªç”¨æ³¨æ„åŠ›æœºåˆ¶ä¹Ÿèƒ½åšå¥½è¿™äº›äº‹**ï¼Ÿç­”æ¡ˆå°±æ˜¯â€”â€”**Yesï¼Attention is all you need!**

éœ€è¦æˆ‘ç»§ç»­è®²ä¸‹ä¸€æ®µå—ï¼Ÿæˆ–è€…è¦ä¸è¦æˆ‘ç”»ä¸ªâ€œè¿œè·ç¦»ä¾èµ– + æ³¨æ„åŠ›â€çš„å°å›¾ï¼Ÿ





## 7 Conclusion

In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.
For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.
We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.
The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.

**Acknowledgements** We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.

## 7 ç»“è®º

åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Transformerï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„åºåˆ—è½¬æ¢æ¨¡å‹ï¼Œå®ƒç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å–ä»£äº†ç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸­å¸¸è§çš„å¾ªç¯å±‚ã€‚

åœ¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œä¸åŸºäºå¾ªç¯æˆ–å·ç§¯å±‚çš„æ¶æ„ç›¸æ¯”ï¼ŒTransformerçš„è®­ç»ƒé€Ÿåº¦æ˜¾è‘—æ›´å¿«ã€‚åœ¨WMT 2014è‹±å¾·ç¿»è¯‘ä»»åŠ¡å’ŒWMT 2014è‹±æ³•ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬éƒ½å–å¾—äº†æ–°çš„æœ€å…ˆè¿›æˆç»©ã€‚åœ¨è‹±å¾·ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹ç”šè‡³è¶…è¿‡äº†æ­¤å‰æŠ¥é“çš„æ‰€æœ‰æ¨¡å‹é›†æˆç»“æœã€‚

æˆ‘ä»¬å¯¹åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹æœªæ¥å‘å±•æ„Ÿåˆ°éå¸¸å…´å¥‹ï¼Œå¹¶è®¡åˆ’å°†å…¶åº”ç”¨äºæ›´å¤šä»»åŠ¡ã€‚æˆ‘ä»¬è®¡åˆ’å°†Transformeræ‰©å±•åˆ°å¤„ç†è¾“å…¥å’Œè¾“å‡ºæ¨¡æ€ä¸é™äºæ–‡æœ¬çš„é—®é¢˜ï¼Œå¹¶ç ”ç©¶å±€éƒ¨çš„ã€å—é™çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥é«˜æ•ˆå¤„ç†å¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤§è§„æ¨¡è¾“å…¥è¾“å‡ºæ•°æ®ã€‚è®©ç”Ÿæˆè¿‡ç¨‹æ›´å°‘ä¾èµ–é¡ºåºæ€§ä¹Ÿæ˜¯æˆ‘ä»¬çš„ç ”ç©¶ç›®æ ‡ä¹‹ä¸€ã€‚

æˆ‘ä»¬ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹çš„ä»£ç å·²å…¬å¼€ï¼Œåœ°å€ä¸ºï¼šhttps://github.com/tensorflow/tensor2tensorã€‚

**è‡´è°¢** æˆ‘ä»¬æ„Ÿè°¢Nal Kalchbrennerå’ŒStephan Gouwsæå‡ºçš„å®è´µæ„è§ã€ä¿®æ­£å»ºè®®ä»¥åŠç»™äºˆæˆ‘ä»¬çš„å¯å‘ã€‚